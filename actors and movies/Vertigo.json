{"page":{"0":"Vertigo","1":"Acoustic reflex","2":"Aminoglycoside","3":"Alternobaric vertigo","4":"Acrophobia","5":"Alfred Hitchcock","6":"Anticholinergics","7":"Antiemetic","8":"Antihistamines","9":"Anticonvulsants","10":"Audiometry","11":"Aspirin","12":"Auditory brainstem response","13":"Auditory processing disorder","14":"Auditory system","15":"BPPV","16":"Balance disorder","17":"Benign paroxysmal positional vertigo","18":"Balance (ability)","19":"Beta blockers","20":"Betahistine","21":"Bezold's abscess","22":"Blurred vision","23":"Brain tumor","24":"Broken escalator phenomenon","25":"Brain tumors","26":"Brainstem","27":"CT scan","28":"Calcium carbonate","29":"Caloric reflex test","30":"Carbon monoxide poisoning","31":"Central nervous system","32":"Cerebellopontine angle","33":"Cervical spine","34":"Cervical spondylosis","35":"Cerebellum","36":"Chemical synapse","37":"Chiari malformation","38":"Cholesteatoma","39":"Chronic subjective dizziness","40":"Computed tomography angiography","41":"Common cold","42":"Conductive hearing loss","43":"Computerized tomography","44":"Cortical deafness","45":"Corticosteroids","46":"Consciousness","47":"Deafblindness","48":"Deafness","49":"Developed world","50":"Decompression sickness","51":"Dimenhydrinate","52":"Dexamethasone","53":"Differential diagnosis","54":"Diplopia","55":"Diseases Database","56":"Disequilibrium (medicine)","57":"Dix-Hallpike test","58":"Dive computer","59":"Dix\u2013Hallpike test","60":"Dizziness","61":"Doi (identifier)","62":"EMedicine","63":"Ear disease","64":"Ear pain","65":"Dopamine","66":"Edward D. Thalmann","67":"Electronystagmography","68":"Electrocochleography","69":"Endolymph","70":"Electrophysiology study","71":"Epilepsy","72":"Epley maneuver","73":"Equilibrioception","74":"Eustachian tube dysfunction","75":"Ethanol","76":"Gamma-Aminobutyric acid","77":"Fear of falling","78":"Gentamicin","79":"Gradenigo's syndrome","80":"Head for heights","81":"Hearing","82":"Hearing loss","83":"Hearing test","84":"Histamine","85":"Hemorrhagic stroke","86":"Histopathology","87":"Hyoscine hydrobromide","88":"Hyperacusis","89":"Hyperventilation syndrome","90":"Hyperbaric oxygen therapy","91":"ICD-10","92":"ISSN (identifier)","93":"Ideomotor phenomenon","94":"ISBN (identifier)","95":"Illusions of self-motion","96":"Infarction","97":"Inner ear","98":"Ischemia","99":"International Statistical Classification of Diseases and Related Health Problems","100":"Isobaric counterdiffusion","101":"Influenza","102":"Joanna Wardlaw","103":"Labyrinthine fistula","104":"Labyrinth (inner ear)","105":"Labyrinthitis","106":"Lateral medullary syndrome","107":"Lateral vestibular nucleus","108":"List of ICD-9 codes","109":"Lesion","110":"Magnetic resonance imaging","111":"Latin","112":"Mastoid","113":"Mastoiditis","114":"Meclizine","115":"Medial vestibular nucleus","116":"Medical Subject Headings","117":"Medical specialty","118":"Metoprolol","119":"Methylprednisolone","120":"Middle ear barotrauma","121":"Middle ear","122":"Migraine","123":"Motion sickness","124":"Multiple sclerosis","125":"Nausea","126":"Neurological deficit","127":"M\u00e9ni\u00e8re's disease","128":"Neurotransmitter","129":"Nonsyndromic deafness","130":"Norepinephrine","131":"Nystagmus","132":"OCLC (identifier)","133":"Otitis externa","134":"Osseous ampullae","135":"Otitis media","136":"Otoacoustic emissions","137":"Otolith","138":"Otomycosis","139":"Otorhinolaryngology","140":"Otoscope","141":"Outer ear","142":"Otosclerosis","143":"PMC (identifier)","144":"Parkinsonism","145":"PMID (identifier)","146":"Pathophysiology","147":"Pathologic nystagmus","148":"Perforated eardrum","149":"Patulous Eustachian tube","150":"Patient UK","151":"Phonophobia","152":"Perspiration","153":"Pneumatic otoscopy","154":"Positional alcohol nystagmus","155":"Posterior cranial fossa","156":"Posturography","157":"Presbycusis","158":"Presyncope","159":"Pure tone audiometry","160":"Proprioception","161":"Purkinje cells","162":"Quality of life","163":"Rinne test","164":"Romberg's test","165":"S2CID (identifier)","166":"SNOMED CT","167":"Saccule","168":"Semicircular canal","169":"Sense of balance","170":"Sensorineural hearing loss","171":"Signs and symptoms","172":"Slurred speech","173":"Serotonin","174":"Spatial disorientation","175":"Spatial hearing loss","176":"Superior canal dehiscence syndrome","177":"Therapeutic recompression","178":"Stroke","179":"Tone decay test","180":"Tinnitus","181":"Topiramate","182":"Transient ischemic attack","183":"Trigeminal nerve","184":"Tympanometry","185":"Tympanosclerosis","186":"Trimix (breathing gas)","187":"Unterberger test","188":"Utricle (ear)","189":"Underwater diving","190":"Usher syndrome","191":"Valproic acid","192":"Vasospasm","193":"Vertebrobasilar insufficiency","194":"Vertigo (disambiguation)","195":"Vestibular migraine","196":"Vertigo (film)","197":"Vestibular nerve","198":"Vestibular neuritis","199":"Vestibular schwannoma","200":"Vestibular system","201":"Vestibule of the ear","202":"Vestibulo\u2013ocular reflex","203":"Visual reinforcement audiometry","204":"Vestibulo-ocular reflex","205":"Vitiligo","206":"Vomiting","207":"Weber test","208":"Wolfram syndrome","221":"Category:Articles with unsourced statements from April 2022","222":"Category:CS1 maint: location missing publisher","224":"Category:Articles with unsourced statements from January 2021","225":"Category:CS1 maint: numeric names: authors list","226":"Category:CS1 maint: unfit URL","227":"Category:CS1 maint: multiple names: authors list","228":"Category:Wikipedia articles needing clarification from March 2019"},"text":{"0":"Vertigo is a condition in which a person has the sensation that they are moving, or that objects around them are moving, when they are not. Often it feels like a spinning or swaying movement. It may be associated with nausea, vomiting, perspiration, or difficulties walking. It is typically worse when the head is moved. Vertigo is the most common type of dizziness.\nThe most common disorders that result in vertigo are benign paroxysmal positional vertigo (BPPV), M\u00e9ni\u00e8re's disease, and vestibular neuritis. Less common causes include stroke, brain tumors, brain injury, multiple sclerosis, migraines, trauma, and uneven pressures between the middle ears. Physiologic vertigo may occur following being exposed to motion for a prolonged period such as when on a ship or simply following spinning with the eyes closed. Other causes may include toxin exposures such as to carbon monoxide, alcohol, or aspirin. Vertigo typically indicates a problem in a part of the vestibular system. Other causes of dizziness include presyncope, disequilibrium, and non-specific dizziness.\nBenign paroxysmal positional vertigo is more likely in someone who gets repeated episodes of vertigo with movement and is otherwise normal between these episodes. Benign vertigo episodes generally last less than one minute. The Dix-Hallpike test typically produces a period of rapid eye movements known as nystagmus in this condition. In M\u00e9ni\u00e8re's disease there is often ringing in the ears, hearing loss, and the attacks of vertigo last more than twenty minutes. In vestibular neuritis the onset of vertigo is sudden, and the nystagmus occurs even when the person has not been moving. In this condition vertigo can last for days. More severe causes should also be considered, especially if other problems such as weakness, headache, double vision, or numbness occur.\nDizziness affects approximately 20\u201340% of people at some point in time, while about 7.5\u201310% have vertigo. About 5% have vertigo in a given year. It becomes more common with age and affects women two to three times more often than men. Vertigo accounts for about 2\u20133% of emergency department visits in the developed world.\n\nClassification\nVertigo is classified into either peripheral or central depending on the location of the dysfunction of the vestibular pathway, although it can also be caused by psychological factors.\nVertigo can also be classified into objective, subjective, and pseudovertigo. Objective vertigo describes when the person has the sensation that stationary objects in the environment are moving. Subjective vertigo refers to when the person feels as if they are moving. The third type is known as pseudovertigo, an intensive sensation of rotation inside the person's head. While this classification appears in textbooks, it is unclear what relation it has to the pathophysiology or treatment of vertigo.\n\nPeripheral\nVertigo that is caused by problems with the inner ear or vestibular system, which is composed of the semicircular canals, the vestibule (utricle and saccule), and the vestibular nerve is called \"peripheral\", \"otologic\", or \"vestibular\" vertigo. The most common cause is benign paroxysmal positional vertigo (BPPV), which accounts for 32% of all peripheral vertigo. Other causes include M\u00e9ni\u00e8re's disease (12%), superior canal dehiscence syndrome, vestibular neuritis, and visual vertigo. Any cause of inflammation such as common cold, influenza, and bacterial infections may cause transient vertigo if it involves the inner ear, as may chemical insults (e.g., aminoglycosides) or physical trauma (e.g., skull fractures). Motion sickness is sometimes classified as a cause of peripheral vertigo.\nPeople with peripheral vertigo typically present with mild to moderate imbalance, nausea, vomiting, hearing loss, tinnitus, fullness, and pain in the ear. In addition, lesions of the internal auditory canal may be associated with facial weakness on the same side. Due to a rapid compensation process, acute vertigo as a result of a peripheral lesion tends to improve in a short period of time (days to weeks).\n\nCentral\nVertigo that arises from injury to the balance centers of the central nervous system (CNS), often from a lesion in the brainstem or cerebellum, is called \"central\" vertigo and is generally associated with less prominent movement illusion and nausea than vertigo of peripheral origin. Central vertigo may have accompanying neurologic deficits (such as slurred speech and double vision), and pathologic nystagmus (which is pure vertical\/torsional). Central pathology can cause disequilibrium, which is the sensation of being off balance. The balance disorder associated with central lesions causing vertigo is often so severe that many people are unable to stand or walk.\nA number of conditions that involve the central nervous system may lead to vertigo including:  lesions caused by infarctions or hemorrhage, tumors present in the cerebellopontine angle such as a vestibular schwannoma or cerebellar tumors, epilepsy, cervical spine disorders such as cervical spondylosis, degenerative ataxia disorders, migraine headaches, lateral medullary syndrome, Chiari malformation, multiple sclerosis, parkinsonism, as well as cerebral dysfunction. Central vertigo may not improve or may do so more slowly than vertigo caused by disturbance to peripheral structures. Alcohol can result in positional alcohol nystagmus (PAN).\n\nSigns and symptoms\nVertigo is a sensation of spinning while stationary. It is commonly associated with nausea or vomiting, unsteadiness (postural instability), falls, changes to a person's thoughts, and difficulties in walking. Recurrent episodes in those with vertigo are common and frequently impair the quality of life. Blurred vision, difficulty in speaking, a lowered level of consciousness, and hearing loss may also occur. The signs and symptoms of vertigo can present as a persistent (insidious) onset or an episodic (sudden) onset.\nPersistent onset vertigo is characterized by symptoms lasting for longer than one day and is caused by degenerative changes that affect balance as people age. Nerve conduction slows with aging, and a decreased vibratory sensation is common as a result. Additionally, there is a degeneration of the ampulla and otolith organs with an increase in age. Persistent onset is commonly paired with central vertigo signs and symptoms.\nThe characteristics of an episodic onset vertigo are indicated by symptoms lasting for a smaller, more memorable amount of time, typically lasting for only seconds to minutes.\n\nPathophysiology\nThe neurochemistry of vertigo includes six primary neurotransmitters that have been identified between the three-neuron arc that drives the vestibulo-ocular reflex (VOR). Glutamate maintains the resting discharge of the central vestibular neurons and may modulate synaptic transmission in all three neurons of the VOR arc. Acetylcholine appears to function as an excitatory neurotransmitter in both the peripheral and central synapses. Gamma-Aminobutyric acid (GABA) is thought to be inhibitory for the commissures of the medial vestibular nucleus, the connections among the cerebellar Purkinje cells, the lateral vestibular nucleus, and the vertical VOR.\nThree other neurotransmitters work centrally. Dopamine may accelerate vestibular compensation. Norepinephrine modulates the intensity of central reactions to vestibular stimulation and facilitates compensation. Histamine is present only centrally, but its role is unclear. Dopamine, histamine, serotonin, and acetylcholine are neurotransmitters thought to produce vomiting. It is known that centrally acting antihistamines modulate the symptoms of acute symptomatic vertigo.\n\nDiagnosis\nTests for vertigo often attempt to elicit nystagmus and to differentiate vertigo from other causes of dizziness such as presyncope, hyperventilation syndrome, disequilibrium, or psychiatric causes of lightheadedness. Tests of vestibular system (balance) function include electronystagmography (ENG), Dix-Hallpike maneuver, rotation tests, head-thrust test, caloric reflex test, and computerized dynamic posturography (CDP).\nThe HINTS test, which is a combination of three physical examination tests that may be performed by physicians at the bedside, has been deemed helpful in differentiating between central and peripheral causes of vertigo. The HINTS test involves the horizontal head impulse test, observation of nystagmus on primary gaze, and the test of skew. CT scans or MRIs are sometimes used by physicians when diagnosing vertigo.\nTests of auditory system (hearing) function include pure tone audiometry, speech audiometry, acoustic reflex, electrocochleography (ECoG), otoacoustic emissions (OAE), and the auditory brainstem response test.\nA number of specific conditions can cause vertigo. In the elderly, however, the condition is often multifactorial.\nA recent history of underwater diving can indicate a possibility of barotrauma or decompression sickness involvement, but does not exclude all other possibilities. The dive profile (which is frequently recorded by dive computer) can be useful to assess a probability for decompression sickness, which can be confirmed by therapeutic recompression.\n\nBenign paroxysmal positional vertigo\nBenign paroxysmal positional vertigo (BPPV) is the most common vestibular disorder and occurs when loose calcium carbonate debris has broken off of the otoconial membrane and enters a semicircular canal thereby creating the sensation of motion. People with BPPV may experience brief periods of vertigo, usually under a minute, which occur with change in the position.\nThis is the most common cause of vertigo. It occurs in 0.6% of the population yearly with 10% having an attack during their lifetime. It is believed to be due to a mechanical malfunction of the inner ear. BPPV may be diagnosed with the Dix-Hallpike test and can be effectively treated with repositioning movements such as the Epley maneuver.\n\nM\u00e9ni\u00e8re's disease\nM\u00e9ni\u00e8re's disease is an inner ear disorder of unknown origin, but is thought to be caused by an increase in the amount of endolymphatic fluid present in the inner ear (endolymphatic hydrops). However, this idea has not been directly confirmed with histopathologic studies, but electrophysiologic studies have been suggestive of this mechanism. M\u00e9ni\u00e8re's disease frequently presents with recurrent, spontaneous attacks of severe vertigo in combination with ringing in the ears (tinnitus), a feeling of pressure or fullness in the ear (aural fullness), severe nausea or vomiting, imbalance, and hearing loss. As the disease worsens, hearing loss will progress.\n\nVestibular neuritis\nVestibular neuritis presents with severe vertigo with associated nausea, vomiting, and generalized imbalance and is believed to be caused by a viral infection of the inner ear, although several theories have been put forward and the cause remains uncertain. Individuals with vestibular neuritis do not typically have auditory symptoms, but may experience a sensation of aural fullness or tinnitus. Persisting balance problems may remain in 30% of people affected.\n\nVestibular migraine\nVestibular migraine is the association of vertigo and migraines and is one of the most common causes of recurrent, spontaneous episodes of vertigo. The cause of vestibular migraines is currently unclear; however, one hypothesized cause is that the stimulation of the trigeminal nerve leads to nystagmus in individuals with migraines. Approximately 40% of all migraine patients will have an accompanying vestibular syndrome, such as vertigo, dizziness, or disruption of the balance system.\nOther suggested causes of vestibular migraines include the following: unilateral neuronal instability of the vestibular nerve, idiopathic asymmetric activation of the vestibular nuclei in the brainstem, and vasospasm of the blood vessels supplying the labyrinth or central vestibular pathways resulting in ischemia to these structures. Vestibular migraines are estimated to affect 1\u20133% of the general population and may affect 10% of people with migraine . Additionally, vestibular migraines tend to occur more often in women and rarely affect individuals after the sixth decade of life.\n\nMotion sickness\nMotion sickness is common and is related to vestibular migraine. It is nausea and vomiting in response to motion and is typically worse if the journey is on a winding road or involves many stops and starts, or if the person is reading in a moving car. It is caused by a mismatch between visual input and vestibular sensation. For example, the person is reading a book that is stationary in relation to the body, but the vestibular system senses that the car, and thus the body, is moving.\n\nAlternobaric vertigo\nAlternobaric vertigo is caused by a pressure difference between the middle ear cavities, usually due to blockage or partial blockage of one eustachian tube, usually when flying or diving underwater. It is most pronounced when the diver is in the vertical position; the spinning is toward the ear with the higher pressure and tends to develop when the pressures differ by 60 cm of water or more.\n\nDecompression sickness\nVertigo is recorded as a symptom of decompression sickness in 5.3% of cases by the U.S. Navy as reported by Powell, 2008 including isobaric decompression sickness.\nDecompression sickness can also be caused at a constant ambient pressure when switching between gas mixtures containing different proportions of different inert gases. This is known as isobaric counterdiffusion, and presents a problem for very deep dives. For example, after using a very helium-rich trimix at the deepest part of the dive, a diver will switch to mixtures containing progressively less helium and more oxygen and nitrogen during the ascent. Nitrogen diffuses into tissues 2.65 times slower than helium, but is about 4.5 times more soluble. Switching between gas mixtures that have very different fractions of nitrogen and helium can result in \"fast\" tissues (those tissues that have a good blood supply) increasing their total inert gas loading. This is often found to provoke inner ear decompression sickness, as the ear seems particularly sensitive to this effect.\n\nStroke\nA stroke (either ischemic or hemorrhagic) involving the posterior fossa is a cause of central vertigo. Risk factors for a stroke as a cause of vertigo include increasing age and known vascular risk factors. Presentation may more often involve headache or neck pain, additionally, those who have had multiple episodes of dizziness in the months leading up to presentation are suggestive of stroke with prodromal TIAs. The HINTS exam as well as imaging studies of the brain (CT, CT angiogram, MRI) are helpful in diagnosis of posterior fossa stroke.\n\nVertebrobasilar insufficiency\nVertebrobasilar insufficiency, notably Bow Hunter's syndrome, is a rare cause of positional vertigo, especially when vertigo is triggered by rotation of the head.\n\nManagement\nDefinitive treatment depends on the underlying cause of vertigo. People with M\u00e9ni\u00e8re's disease have a variety of treatment options to consider when receiving treatment for vertigo and tinnitus including: a low-salt diet and intratympanic injections of the antibiotic gentamicin or surgical measures such as a shunt or ablation of the labyrinth in refractory cases.\nCommon drug treatment options for vertigo may include the following:\n\nAnticholinergics such as hyoscine hydrobromide (scopolamine)\nAnticonvulsants such as topiramate or valproic acid for vestibular migraines\nAntihistamines such as betahistine, dimenhydrinate, or meclizine, which may have antiemetic properties\nBeta blockers such as metoprolol for vestibular migraine\nCorticosteroids such as methylprednisolone for inflammatory conditions such as vestibular neuritis or dexamethasone as a second-line agent for M\u00e9ni\u00e8re's disease\nAll cases of decompression sickness should be treated initially with 100% oxygen until hyperbaric oxygen therapy (100% oxygen delivered in a high-pressure chamber) can be provided. Several treatments may be necessary, and treatment will generally be repeated until either all symptoms resolve, or no further improvement is apparent.\n\nEtymology\nVertigo is from the Latin word, vert\u014d, which means \"a whirling or spinning movement\".\n\nSee also\nReferences\nExternal links\n\n\"Dizziness and Vertigo\". MedlinePlus. U.S. National Library of Medicine.","1":"The acoustic reflex (also known as the stapedius reflex, stapedial reflex, auditory reflex, middle-ear-muscle reflex (MEM reflex, MEMR), attenuation reflex, cochleostapedial reflex or intra-aural reflex) is an involuntary muscle contraction that occurs in the middle ear in response to loud sound stimuli or when the person starts to vocalize.\nWhen presented with an intense sound stimulus, the stapedius and tensor tympani muscles of the ossicles contract. The stapedius stiffens the ossicular chain by pulling the stapes (stirrup) of the middle ear away from the oval window of the cochlea and the tensor tympani muscle stiffens the ossicular chain by loading the tympanic membrane when it pulls the malleus (hammer) in toward the middle ear. The reflex decreases the transmission of vibrational energy to the cochlea, where it is converted into electrical impulses to be processed by the brain.\n\nAcoustic reflex threshold\nThe acoustic reflex threshold (ART) is the sound pressure level (SPL) from which a sound stimulus with a given frequency will trigger the acoustic reflex. The ART is a function of sound pressure level and frequency.\nPeople with normal hearing have an acoustic reflex threshold (ART) around 70\u2013100 dB SPL. People with conductive hearing loss (i.e., bad transmission in the middle ear) may have a greater or absent acoustic reflex threshold.\nThe acoustic reflex threshold is usually 10\u201320 dB below the discomfort threshold. However the discomfort threshold is not a relevant indicator of the harmfulness of a sound: industry workers tend to have a higher discomfort threshold, but the sound is just as harmful to their ears.\nThe acoustic reflex threshold can be decreased by the simultaneous presentation of a second tone (facilitator). The facilitator tone can be presented to either ear. This facilitation effect tends to be greater when the facilitator tone has a frequency lower than the frequency of the elicitor (i.e. the sound used to trigger the acoustic reflex).\n\nCharacteristics and effects\nFor most animals, the acoustic reflex is the contraction of both middle ear muscles: the stapedius and tensor tympani muscles. However, in humans, the acoustic reflex only involves the contraction of the stapedius muscle, not the tensor tympani.\nThe contraction of the stapedius muscle occurs bilaterally in normal ears, no matter which ear was exposed to the loud sound stimulation.\nThe prevalence of bilateral acoustic reflexes in persons 18\u201330 years old is 85.3% (82.9%, 87.4%) 95th percentile confidence interval N = 3280 and in all persons 74.6% (73.2%, 75.9%) N = 15,106.\nThe acoustic reflex mostly protects against low frequency sounds.\nWhen triggered by sounds 20 dB above the reflex threshold, the stapedius reflex decreases the intensity of the sound transmitted to the cochlea by around 15 dB.\nThe acoustic reflex is also invoked when a person vocalizes. In humans, the vocalization-induced stapedius reflex reduces sound intensities reaching the inner ear by approximately 20 decibels. The reflex is triggered in anticipation of the onset of vocalization. While the vocalization-induced stapedius reflex in humans results in an approximate 20 dB reduction in transduction to the inner ear, birds have a stronger stapedius reflex that is invoked just before the bird tweets.\n\nHypothesized function\nThe main hypothesized function of the acoustic reflex is the protection of the organ of Corti against excessive stimulation (especially that of the lower frequencies). This protection has been demonstrated both in humans and animals, but with limited effects.\nAccording to the article Significance of the stapedius reflex for the understanding of speech, the latency of contraction is only about 10ms, but maximum tension may not be reached for 100 ms or more.\nAccording to the article Le traumatisme acoustique, the latency of contraction is 150 ms with noise stimulus which SPL is at the threshold (ATR), and 25\u201335 ms at high sound pressure levels. Indeed, the amplitude of the contraction grows with the sound pressure level stimulus.\nBecause of this latency, the acoustic reflex cannot protect against sudden intense noises. However, when several sudden intense noises are presented at a pace higher than 2\u20133 seconds of interval, the acoustic reflex is able to play a role against auditory fatigue.\nMoreover, the full tension of the stapedius muscle cannot be maintained in response to continued stimulation. Indeed, the tension drops to about 50% of its maximum value after a few seconds.\nIn damage risk criteria for exposure to impulse noise, the acoustic reflex is integral to the Auditory Hazard Assessment Algorithm for Humans model and the Integrated Cochlear Energy models.  These two models estimate the response of the basilar membrane in response to an input stimulus and summate the vibration of the segments of the basilar membrane to predict the potential risk for hearing loss.  The acoustic reflex can be activated before an impulse reaches the ear through an assumed conditioned response or it can be activated after the stimulus exceeds a specific level (e.g. 134 dB).\nRecent measurements of the acoustic reflex with a group of 50 subjects found that only 2 of the subjects exhibited any pre-activation of the reflex in the warned (countdown) or volitional control of the eliciting stimulus.\nAn alternative hypothesis for the role of the acoustic reflex is the prevention of auditory masking of high-frequencies by low-frequencies, which are predominant in natural sounds.\n\nMeasurement\nMost of the time, the stapedius reflex is tested with tympanometry. The contraction of the stapedius muscle stiffens the middle-ear, thus decreasing middle-ear admittance; this can be measured thanks to tympanometry.\nThe acoustic stapedius reflex can also be recorded by means of extratympanic manometry (ETM).\nThe stapedial reflex can be measured with laser Doppler velocimetry.  Jones et al. focused a laser on the light reflex of the manubrium in awake human subjects.  The amplitude of a 500 Hz probe tone was used to monitor the vibrations of the tympanic membrane.  Various elicitors were presented to the subjects: 1000 Hz tone-burst for 0.5 s at 100 dB SPL, recorded .22 caliber gunshot noise with a peak level of 110 dB SPL.  The amplitude of the 500 Hz probe tone was reduced in response to the eliciting stimuli.  Time constants for the rate of onset and recovery were measured to be about 113 ms for the tone and 60-69 ms for the gunshot recordings. \n\nAs the stapedius muscle is innervated by the facial nerve, a measurement of the reflex can be used to locate the injury on the nerve. If the injury is distal to the stapedius muscle, the reflex is still functional.\nA measurement of the reflex can also be used to suggest a retrocochlear lesion (e.g., vestibular schwannoma).\nThe acoustic reflex normally occurs only at relatively high intensities; contraction of middle ear muscles for quieter sounds can indicate ear dysfunction (e.g. tonic tensor tympani syndrome -TTTS).\nThe pathway involved in the acoustic reflex is complex and can involve the ossicular chain (malleus, incus and stapes), the cochlea (organ of hearing), the auditory nerve, brain stem, facial nerve, superior olivary complex, and cochlear nucleus. Consequently, the absence of an acoustic reflex, by itself, may not be conclusive in identifying the source of the problem.\n\nSee also\nTensor tympani\nOtoacoustic emission\nEqual-loudness contours\nAudiometry\nHyperacusis\nStapedius muscle\nTympanometry\n\n\n== References ==","2":"Aminoglycoside is a medicinal and bacteriologic category of traditional Gram-negative antibacterial medications that inhibit protein synthesis and contain as a portion of the molecule an amino-modified glycoside (sugar). The term can also refer more generally to any organic molecule that contains amino sugar substructures. Aminoglycoside antibiotics display bactericidal activity against Gram-negative aerobes and some anaerobic bacilli where resistance has not yet arisen but generally not against Gram-positive and anaerobic Gram-negative bacteria.\nStreptomycin is the first-in-class aminoglycoside antibiotic. It is derived from Streptomyces griseus and is the earliest modern agent used against tuberculosis. Streptomycin lacks the common 2-deoxystreptamine moiety (image right, below) present in most other members of this class. Other examples of aminoglycosides include the deoxystreptamine-containing agents kanamycin, tobramycin, gentamicin, and neomycin (see below).\n\nNomenclature\nAminoglycosides that are derived from bacteria of the Streptomyces genus are named with the suffix -mycin, whereas those that are derived from Micromonospora are named with the suffix -micin. However, this nomenclature system is not specific for aminoglycosides, and so appearance of this set of suffixes does not imply common mechanism of action. (For instance, vancomycin, a glycopeptide antibiotic, and erythromycin, a macrolide antibiotic produced by Saccharopolyspora erythraea, along with its synthetic derivatives clarithromycin and azithromycin, all share the suffixes but have notably different mechanisms of action.)\nIn the following gallery, kanamycin A to netilmicin are examples of the 4,6-disubstituted deoxystreptamine sub-class of aminoglycosides, the neomycins are examples of the 4,5-disubstituted sub-class, and streptomycin is an example of a non-deoxystreptamine aminoglycoside.\n\nMechanisms of action\nAminoglycosides display concentration-dependent bactericidal activity against \"most gram-negative aerobic and facultative anaerobic bacilli\" but not against gram-negative anaerobes and most gram-positive bacteria. They require only short contact time, and are most effective against susceptible bacterial populations that are rapidly multiplying. These activities are attributed to a primary mode of action as protein synthesis inhibitors, though additional mechanisms are implicated for some specific agents, and\/or thorough mechanistic descriptions are as yet unavailable.\nThe inhibition of protein synthesis is mediated through aminoglycosides' energy-dependent, sometimes irreversible binding, to the cytosolic, membrane-associated bacterial ribosome (image at right). (Aminoglycosides first cross bacterial cell walls\u2014lipopolysaccharide in gram-negative bacteria\u2014and cell membranes, where they are actively transported.) While specific steps in protein synthesis affected may vary somewhat between specific aminoglycoside agents, as can their affinity and degree of binding, aminoglycoside presence in the cytosol generally disturbs peptide elongation at the 30S ribosomal subunit, giving rise to inaccurate mRNA translation and therefore biosynthesis of proteins that are truncated, or bear altered amino acid compositions at particular points. Specifically, binding impairs translational proofreading leading to misreading of the RNA message, premature termination, or both, and so to inaccuracy of the translated protein product. The subset of aberrant proteins that are incorporated into the bacterial cell membrane may then lead to changes in its permeability and then to \"further stimulation of aminoglycoside transport\". The amino sugar portion of this class of molecules (e.g., the 2-deoxystreptamine in kanamycins, gentamicins, and tobramycin, see above) are implicated in the association of the small molecule with ribosomal structures that lead to the infidelities in translation (ibid.). Inhibition of ribosomal translocation\u2014i.e., movement of the peptidyl-tRNA from the A- to the P-site\u2014has also been suggested. Recent single-molecule tracking experiments in live E. coli showed an ongoing but slower protein synthesis upon treatment with different aminoglycoside drugs. (Spectinomycin, a related but distinct chemical structure class often discussed with aminoglycosides, does not induce mRNA misreading and is generally not bactericidal.)\nIt has been proposed that aminoglycoside antibiotics cause oxidation of guanine nucleotides in the bacterial nucleotide pool, and that this contributes to the cytotoxicity of these antibiotics.  The incorporation of oxidized guanine nucleotides into DNA could be bactericidal since incomplete repair of closely spaced 8-oxo-2'-deoxyguanosine  in the DNA can result in lethal double-strand breaks.\nFinally, a further \"cell-membrane effect\" also occurs with aminoglycosides; \"functional integrity of the bacterial cell membrane\" can be lost, later in time courses of aminoglycoside exposure and transport.\n\nPharmacokinetics and pharmacodynamics\nThere is a significant variability in the relationship between the dose administered and the resultant plasma level in blood. Therapeutic drug monitoring (TDM) is necessary to obtain the correct dose. These agents exhibit a post-antibiotic effect in which there is no or very little drug level detectable in blood, but there still seems to be inhibition of bacterial re-growth. This is due to strong, irreversible binding to the ribosome, and remains intracellular long after plasma levels drop, and allows a prolonged dosage interval. Depending on their concentration, they act as bacteriostatic or bactericidal agents.\n\nIndications\nAminoglycosides are useful primarily in infections involving aerobic, Gram-negative bacteria, such as Pseudomonas, Acinetobacter, and Enterobacter. In addition, some Mycobacteria, including the bacteria that cause tuberculosis, are susceptible to aminoglycosides. Streptomycin was the first effective drug in the treatment of tuberculosis, though the role of aminoglycosides such as streptomycin and amikacin has been eclipsed (because of their toxicity and inconvenient route of administration) except for multiple-drug-resistant strains. The most frequent use of aminoglycosides is empiric therapy for serious infections such as sepsis, complicated intra-abdominal infections, complicated urinary tract infections, and nosocomial respiratory tract infections. Usually, once cultures of the causal organism are grown and their susceptibilities tested, aminoglycosides are discontinued in favor of less toxic antibiotics.\nAs noted, aminoglycosides are mostly ineffective against anaerobic bacteria, fungi, and viruses. Infections caused by Gram-positive bacteria can also be treated with aminoglycosides, but other types of antibiotics are more potent and less damaging to the host. In the past, the aminoglycosides have been used in conjunction with beta-lactam antibiotics in streptococcal infections for their synergistic effects, in particular in endocarditis. One of the most frequent combinations is ampicillin (a beta-lactam, or penicillin-related antibiotic) and gentamicin. Often, hospital staff refer to this combination as \"amp and gent\" or more recently called \"pen and gent\" for penicillin and gentamicin.\n\nNonsense suppression\nThe interference with mRNA proofreading has been exploited to treat genetic diseases that result from premature stop codons (leading to early termination of protein synthesis and truncated proteins). Aminoglycosides can cause the cell to overcome the stop codons, insert a random amino acid, and express a full-length protein. The aminoglycoside gentamicin has been used to treat cystic fibrosis (CF) cells in the laboratory to induce them to grow full-length proteins. CF is caused by a mutation in the gene coding for the cystic fibrosis transmembrane conductance regulator (CFTR) protein. In approximately 10% of CF cases, the mutation in this gene causes its early termination during translation, leading to the formation of a truncated and non-functional CFTR protein. It is believed that gentamicin distorts the structure of the ribosome-RNA complex, leading to a mis-reading of the termination codon, causing the ribosome to \"skip\" over the stop sequence and to continue with the normal elongation and production of the CFTR protein.\n\nRoutes of administration\nSince they are not absorbed from the gut, they are administered intravenously and intramuscularly. Some are used in topical preparations for wounds. Oral administration can be used for gut decontamination (e.g., in hepatic encephalopathy). Tobramycin may be administered in a nebulized form.\n\nClinical use\nThe recent emergence of infections due to Gram-negative bacterial strains with advanced patterns of antimicrobial resistance has prompted physicians to reevaluate the use of these antibacterial agents. This revived interest in the use of aminoglycosides has brought back to light the debate on the two major issues related to these compounds, namely the spectrum of antimicrobial susceptibility and toxicity. Current evidence shows that aminoglycosides do retain activity against the majority of Gram-negative clinical bacterial isolates in many parts of the world. Still, the relatively frequent occurrence of nephrotoxicity and ototoxicity during aminoglycoside treatment makes physicians reluctant to use these compounds in everyday practice. Recent advances in the understanding of the effect of various dosage schedules of aminoglycosides on toxicity have provided a partial solution to this problem, although more research still needs to be done in order to overcome this problem entirely.\nAminoglycosides are in pregnancy category D, that is, there is positive evidence of human fetal risk based on adverse reaction data from investigational or marketing experience or studies in humans, but potential benefits may warrant use of the drug in pregnant women despite potential risks.\n\nAdverse effects\nAminoglycosides can cause inner ear toxicity which can result in sensorineural hearing loss. The incidence of inner ear toxicity varies from 7 to 90%, depending on the types of antibiotics used, susceptibility of the patient to such antibiotics, and the duration of antibiotic administration.\nAnother serious and disabling side effect of aminoglycoside use is vestibular ototoxicity. This leads to oscillopsia (gaze instability) and balance impairments that impact all aspects of an individual's antigravity function. This loss is permanent and can happen at any dose.\nFrequent use of aminoglycosides could result in kidney damage (Acute kidney injury), that could lead to chronic kidney disease.\n\nContraindication for specific diseases\nAminoglycosides can exacerbate weakness in patients with myasthenia gravis, and use is therefore avoided in these patients.\nAminoglycosides are contraindicated in patients with mitochondrial diseases as they may result in impaired mtDNA translation, which can lead to irreversible hearing loss, tinnitus, cardiac toxicity, and renal toxicity. However, hearing loss and tinnitus have also been observed in some patients without mitochondrial diseases.\n\nReferences\nExternal links\nMedlinePlus drug information - Aminoglycosides (Systemic)\nScience Daily Bacterial 'Battle for Survival' - Rhodostreptomycin\nWikiversity page for the International Ototoxicity Management Group","3":"In aviation and underwater diving, alternobaric vertigo is dizziness resulting from unequal pressures being exerted between the ears due to one Eustachian tube being less patent than the other.\n\nSigns and symptoms\nCauses\nThis might have occurred due to barotrauma of descent, and\/or the effects of nasal decongestants. It is due to unequal increase in middle ear pressures on ascent, is usually mild, and most often cleared by further ascent. When the pressures in both ears reach ambient levels, the stimulus for the dizziness stops. Although most often mild, the vertigo can persist until the diver reaches the surface continuing the unequal pressures, which can damage the inner ear or ear drum.\nAlternobaric vertigo is most pronounced when the diver is in the vertical position; the spinning is towards the ear with the higher pressure and tends to develop when the pressures differ by 60 cm of water or more. Ear clearing may be a remedy. A similar vertigo can also occur as a result of unequal heating stimulation of one inner ear labyrinth over the other due to diving in a prone position in cold water - the undermost ear being stimulated.\n\nDiagnosis\nIn terms of diagnosis for alternobaric vertigo the medical history and physical examination, are important. Furthermore, Eustachian tube function testing is also performed\n\nManagement\nSee also\n\nVertigo \u2013 Type of dizziness where a person has the sensation of moving or surrounding objects moving\nInner ear barotrauma \u2013 Pressure injury to the inner ear\nInner ear decompression sickness \u2013 Medical condition caused by inert gas bubbles forming out of solution\n\n\n== References ==","4":"Acrophobia, also known as hypsophobia, is an extreme or irrational fear or phobia of heights, especially when one is not particularly high up. It belongs to a category of specific phobias, called space and motion discomfort, that share similar causes and options for treatment.\nMost people experience a degree of natural fear when exposed to heights, known as the fear of falling. On the other hand, those who have little fear of such exposure are said to have a head for heights. A head for heights is advantageous for hiking or climbing in mountainous terrain and also in certain jobs such as steeplejacks or wind turbine mechanics.\nPeople with acrophobia can experience a panic attack in high places and become too agitated to get themselves down safely. Approximately 2\u20135% of the general population has acrophobia, with twice as many women affected as men. The term is from the Greek: \u1f04\u03ba\u03c1\u03bf\u03bd, \u00e1kron, meaning \"peak, summit, edge\" and \u03c6\u03cc\u03b2\u03bf\u03c2, ph\u00f3bos, \"fear\". The term \"hypsophobia\" derives from the Greek word \u03cd\u03c8\u03bf\u03c2 (hypsos), meaning \"height\". In Greek, the actual term used for this condition is \"\u03c5\u03c8\u03bf\u03c6\u03bf\u03b2\u03af\u03b1\" (Hypsophobia).\n\nConfusion with vertigo\n\"Vertigo\" is often used to describe a fear of heights, but it is more accurately a spinning sensation that occurs when one is not actually spinning. It can be triggered by looking down from a high place, by looking straight up at a high place or tall object, or even by watching something (i.e. a car or a bird) go past at high speed, but this alone does not describe vertigo. True vertigo can be triggered by almost any type of movement (e.g. standing up, sitting down, walking) or change in visual perspective (e.g. squatting down, walking up or down stairs, looking out of the window of a moving car or train). Vertigo is called height vertigo when the sensation of vertigo is triggered by heights.\nHeight vertigo is caused by a conflict between vision, vestibular and somatosensory senses. This occurs when vestibular and somatosensory systems sense a body movement that is not detected by the eyes. More research indicates that this conflict leads to both motion sickness and anxiety. Confusion may arise in differentiating between height vertigo and acrophobia due to the conditions' overlapping symptom pools, including body swaying and dizziness. Further confusion can occur due to height vertigo being a direct symptom of acrophobia.\n\nCauses\nTraditionally, acrophobia has been attributed, like other phobias, to conditioning or a traumatic experience. Recent studies have cast doubt on this explanation. Individuals with acrophobia are found to be lacking in traumatic experiences. Nevertheless, this may be due to the failure to recall the experiences, as memory fades as time passes. To address the problems of self report and memory, a large cohort study with 1000 participants was conducted from birth; the results showed that participants with less fear of heights had more injuries because of falling. Psychologists Richie Poulton, Simon Davies, Ross G. Menzies, John D. Langley, and Phil A. Silva sampled subjects from the Dunedin Multidisciplinary Health and Development Study who had been injured in a fall between the ages of 5 and 9, compared them to children who had no similar injury, and found that at age 18, acrophobia was present in only 2 percent of the subjects who had an injurious fall but was present among 7 percent of subjects who had no injurious fall (with the same sample finding that typical basophobia was 7 times less common in subjects at age 18 who had injurious falls as children than subjects that did not).\nMore studies have suggested a possible explanation for acrophobia is that it emerges through accumulation of non-traumatic experiences of falling that are not memorable but can influence behaviours in the future. Also, fear of heights may be acquired when infants learn to crawl. If they fell, they would learn the concepts about surfaces, posture, balance, and movement. Cognitive factors may also contribute to the development of acrophobia. People tend to wrongly interpret visuo-vestibular discrepancies as dizziness and nausea and associate them with a forthcoming fall. Experiencing these cognitive factors while associating them with the idea of falling may be enough to cause the same fear that would be expected after a traumatic fall.\nA fear of falling, along with a fear of loud noises, is one of the most commonly suggested inborn or \"non-associative\" fears. The newer non-association theory is that a fear of heights is an evolved adaptation to a world where falls posed a significant danger. If this fear is inherited, it is possible that people can get rid of it by frequent exposure of heights in habituation. In other words, acrophobia could be associated with a lack of exposure to heights in early life. The degree of fear varies, and the term phobia is reserved for those at the extreme end of the spectrum. Researchers have argued that a fear of heights is an instinct found in many mammals, including domestic animals and humans. Experiments using visual cliffs have shown human infants and toddlers, as well as other animals of various ages, to be reluctant in venturing onto a glass floor with a view of a few meters of apparent fall-space below it. Although human infants initially experienced fear when crawling on the visual cliff, most of them overcame the fear through practice, exposure and mastery and retained a level of healthy cautiousness. While an innate cautiousness around heights is helpful for survival, extreme fear can interfere with the activities of everyday life, such as standing on a ladder or chair, or even walking up a flight of stairs. It is uncertain if acrophobia is related to the failure to reach a certain developmental stage. Besides associative accounts, a diathetic-stress model is also very appealing for considering both vicarious learning and hereditary factors such as personality traits (i.e., neuroticism).\nAnother possible contributing factor is a dysfunction in maintaining balance. In this case, the anxiety is both well-founded and secondary. The human balance system integrates proprioceptive, vestibular and nearby visual cues to reckon position and motion. As height increases, visual cues recede and balance becomes poorer in people without acrophobia. However, most people respond to such a situation by shifting to more reliance on the proprioceptive and vestibular branches of the equilibrium system.\nSome people are known to be more dependent on visual signals than others. People who rely more on visual cues to control body movements are less physically stable. An acrophobic, however, continues to over-rely on visual signals, whether because of inadequate vestibular function or incorrect strategy. Locomotion at a high elevation requires more than normal visual processing. The visual cortex becomes overloaded, resulting in confusion. Some proponents of the alternative view of acrophobia warn that it may be ill-advised to encourage acrophobics to expose themselves to height without first resolving the vestibular issues. Research is underway at several clinics. Recent studies found that participants experienced increased anxiety not only when the height increased, but also when they were required to move sideways at a fixed height.\nA recombinant model of the development of acrophobia is very possible, in which learning factors, cognitive factors (e.g. interpretations), perceptual factors (e.g. visual dependence), and biological factors (e.g. heredity) interact to provoke fear or habituation.\n\nAssessment\nICD-10 and DSM-5 are used to diagnose acrophobia. Acrophobia Questionnaire (AQ) is a self report that contains 40 items, assessing anxiety level on a 0\u20136 point scale and degree of avoidance on a 0\u20132 point scale. The Attitude Towards Heights Questionnaires (ATHQ) and Behavioural Avoidance Tests (BAT) are also used.\nHowever, acrophobic individuals tend to have biases in self-reporting. They often overestimate the danger and question their abilities of addressing height relevant issues. A Height Interpretation Questionnaire (HIQ) is a self-report to measure these height relevant judgements and interpretations. The Depression Scale of the Depression Anxiety Stress Scales short form (DASS21-DS) is a self report used to examine validity of the HIQ.\n\nTreatment\nTraditional treatment of phobias is still in use today. Its underlying theory states that phobic anxiety is conditioned and triggered by a conditional stimulus. By avoiding phobic situations, anxiety is reduced. However, avoidance behaviour is reinforced through negative reinforcement. Wolpe developed a technique called systematic desensitization to help participants avoid \"avoidance\". Research results have suggested that even with a decrease in therapeutic contact, desensitization is still very effective. However, other studies have shown that therapists play an essential role in acrophobia treatment. Treatments like reinforced practice and self-efficacy treatments also emerged.\nThere have been a number of studies into using virtual reality therapy for acrophobia. Botella and colleagues and Schneider were the first to use VR in treatment. Specifically, Schneider utilised inverted lenses in binoculars to \"alter\" the reality. Later in the mid-1990s, VR became computer-based and was widely available for therapists. A cheap VR equipment uses a normal PC with head-mounted display (HMD). In contrast, VRET uses an advanced computer automatic virtual environment (CAVE). VR has several advantages over in vivo treatment: (1) therapist can control the situation better by manipulating the stimuli, in terms of their quality, intensity, duration and frequency; (2) VR can help participants avoid public embarrassment and protect their confidentiality; (3) therapist's office can be well-maintained; (4) VR encourages more people to seek treatment; (5) VR saves time and money, as participants do not need to leave the consulting room.\nMany different types of medications are used in the treatment of phobias like fear of heights, including traditional anti-anxiety drugs such as benzodiazepines, and newer options such as antidepressants and beta-blockers.\n\nPrognosis\nSome desensitization treatments produce short-term improvements in symptoms. Long-term treatment success has been elusive.\n\nEpidemiology\nApproximately 2\u20135% of the general population has acrophobia, with twice as many women affected as men.\nA related, milder form of visually triggered fear or anxiety is called visual height intolerance (vHI). Up to one-third of people may have some level of visual height intolerance. Pure vHI usually has smaller impact on individuals compared to acrophobia, in terms of intensity of symptoms load, social life, and overall life quality. However, few people with visual height intolerance seek professional help.\n\nSee also\nAcclimatization\nList of phobias\n\nCitations\nGeneral and cited sources\nSartorius, N.; Henderson, A.S.; Strotzka, H.; Lipowski, Z.; Yu-cun, S.; You-xin, X.; Str\u00f6mgren, E.; Glatzel, J.; et al. \"The ICD-10 Classification of Mental and Behavioural Disorders Clinical descriptions and diagnostic guidelines\" (PDF). World Health Organization. p. 114. Retrieved 23 June 2021.\n\nExternal links\n\"The scariest path in the world?\", a direct test, video shot on El Camino del Rey, approaching Makinodromo\n\"Fear of Heights\"\u2014A comprehensive guide with useful resources on Acrophobia known as Fear of Heights.","5":"Sir Alfred Joseph Hitchcock  (13 August 1899 \u2013 29 April 1980) was an English film director. He is widely regarded as one of the most influential figures in the history of cinema. In a career spanning six decades, he directed over 50 feature films, many of which are still widely watched and studied today. Known as the \"Master of Suspense\", Hitchcock became as well known as any of his actors thanks to his many interviews, his cameo appearances in most of his films, and his hosting and producing the television anthology Alfred Hitchcock Presents (1955\u201365). His films garnered 46 Academy Award nominations, including six wins, although he never won the award for Best Director, despite five nominations.\nHitchcock initially trained as a technical clerk and copywriter before entering the film industry in 1919 as a title card designer. His directorial debut was the British\u2013German silent film The Pleasure Garden (1925). His first successful film, The Lodger: A Story of the London Fog (1927), helped to shape the thriller genre, and Blackmail (1929) was the first British \"talkie\". His thrillers The 39 Steps (1935) and The Lady Vanishes (1938) are ranked among the greatest British films of the 20th century. By 1939, he had international recognition and producer David O. Selznick persuaded him to move to Hollywood. A string of successful films followed, including Rebecca (1940), Foreign Correspondent (1940), Suspicion (1941), Shadow of a Doubt (1943) and Notorious (1946). Rebecca won the Academy Award for Best Picture, with Hitchcock nominated as Best Director. He also received Oscar nominations for Lifeboat (1944), Spellbound (1945), Rear Window (1954) and Psycho (1960).\nHitchcock's other notable films include Rope (1948), Strangers on a Train (1951), Dial M for Murder (1954), To Catch a Thief (1955), The Trouble with Harry (1955),  Vertigo (1958), North by Northwest (1959), The Birds (1963) and  Marnie (1964), all of which were also financially successful and are highly regarded by film historians. Hitchcock made a number of films with some of the biggest stars in Hollywood, including four with Cary Grant, four with James Stewart, three with Ingrid Bergman and three consecutively with Grace Kelly. Hitchcock became an American citizen in 1955.\nIn 2012, Hitchcock's psychological thriller Vertigo, starring Stewart, displaced Orson Welles' Citizen Kane (1941) as the British Film Institute's greatest film ever made based on its world-wide poll of hundreds of film critics. As of 2021, nine of his films had been selected for preservation in the United States National Film Registry, including his personal favourite, Shadow of a Doubt (1943). He received the BAFTA Fellowship in 1971, the AFI Life Achievement Award in 1979, and was knighted in December of that year, four months before his death on 29 April 1980.\n\nBiography\nEarly life: 1899\u20131919\nEarly childhood and education\nAlfred Joseph Hitchcock was born on 13 August 1899 in the flat above his parents' leased greengrocer's shop at 517 High Road in Leytonstone, which was then part of Essex (now on the outskirts of east London). He was the son of greengrocer and poulterer, William Edgar Hitchcock (1862\u20131914) and Emma Jane (n\u00e9e Whelan;1863\u20131942). The household was \"characterised by an atmosphere of discipline\". He had an older brother named William John (1888\u20131943) and an older sister named Ellen Kathleen (1892\u20131979) who used the nickname \"Nellie\". His parents were both Roman Catholics with partial Irish ancestry. His father was a greengrocer, as his grandfather had been. There was a large extended family, including uncle John Hitchcock with his five-bedroom Victorian house on Campion Road in Putney, complete with a maid, cook, chauffeur, and gardener. Every summer, his uncle rented a seaside house for the family in Cliftonville, Kent. Hitchcock said that he first became class-conscious there, noticing the differences between tourists and locals.\n\nDescribing himself as a well-behaved boy \u2014 his father called him his \"little lamb without a spot\" \u2014 Hitchcock said he could not remember ever having had a playmate. One of his favourite stories for interviewers was about his father sending him to the local police station with a note when he was five; the policeman looked at the note and locked him in a cell for a few minutes, saying, \"This is what we do to naughty boys.\" The experience left him with a lifelong phobia of law enforcement, and he told Tom Snyder in 1973 that he was \"scared stiff of anything ... to do with the law\" and that he would refuse to even drive a car in case he got a parking ticket. When he was six, the family moved to Limehouse and leased two stores at 130 and 175 Salmon Lane, which they ran as a fish-and-chip shop and fishmongers' respectively; they lived above the former. Hitchcock attended his first school, the Howrah House Convent in Poplar, which he entered in 1907, at age 7. According to biographer Patrick McGilligan, he stayed at Howrah House for at most two years. He also attended a convent school, the Wode Street School \"for the daughters of gentlemen and little boys\" run by the Faithful Companions of Jesus. He then attended a primary school near his home and was for a short time a boarder at Salesian College in Battersea.\nThe family moved again when Hitchcock was eleven, this time to Stepney, and on 5 October 1910 he was sent to St Ignatius College in Stamford Hill, a Jesuit grammar school with a reputation for discipline. As corporal punishment, the priests used a flat, hard, springy tool made of gutta-percha and known as a \"ferula\" which struck the whole palm; punishment was always at the end of the day, so the boys had to sit through classes anticipating the punishment if they had been written up for it. He later said that this is where he developed his sense of fear. The school register lists his year of birth as 1900 rather than 1899; biographer Donald Spoto says he was deliberately enrolled as a ten-year-old because he was a year behind with his schooling. While biographer Gene Adair reports that Hitchcock was \"an average, or slightly above-average, pupil\", Hitchcock said that he was \"usually among the four or five at the top of the class\"; at the end of his first year, his work in Latin, English, French and religious education was noted. He told Peter Bogdanovich: \"The Jesuits taught me organisation, control and, to some degree, analysis.\"\nHitchcock's favourite subject was geography and he became interested in maps and the timetables of trains, trams and buses; according to John Russell Taylor, he could recite all the stops on the Orient Express. He had a particular interest in London trams. An overwhelming majority of his films include rail or tram scenes, in particular The Lady Vanishes, Strangers on a Train and Number Seventeen. A clapperboard shows the number of the scene and the number of takes, and Hitchcock would often take the two numbers on the clapperboard and whisper the London tram route names. For example, if the clapperboard showed \"Scene 23; Take 3\", he would whisper \"Woodford, Hampstead\"\u2014Woodford being the terminus of the route 23 tram, and Hampstead the end of route 3.\n\nHenley's\nHitchcock told his parents that he wanted to be an engineer, and on 25 July 1913, he left St Ignatius and enrolled in night classes at the London County Council School of Engineering and Navigation in Poplar. In a book-length interview in 1962, he told Fran\u00e7ois Truffaut that he had studied \"mechanics, electricity, acoustics, and navigation\". Then, on 12 December 1914, his father, who had been suffering from emphysema and kidney disease, died at the age of 52. To support himself and his mother \u2014 his older siblings had left home by then \u2014 Hitchcock took a job, for 15 shillings a week (\u00a391 in 2023), as a technical clerk at the Henley Telegraph and Cable Company in Blomfield Street, near London Wall. He continued night classes, this time in art history, painting, economics and political science. His older brother ran the family shops, while he and his mother continued to live in Salmon Lane.\nHitchcock was too young to enlist when the First World War started in July 1914, and when he reached the required age of 18 in 1917, he received a C3 classification (\"free from serious organic disease, able to stand service conditions in garrisons at home ... only suitable for sedentary work\"). He joined a cadet regiment of the Royal Engineers and took part in theoretical briefings, weekend drills and exercises. John Russell Taylor wrote that, in one session of practical exercises in Hyde Park, Hitchcock was required to wear puttees. He could never master wrapping them around his legs, and they repeatedly fell down around his ankles.\nAfter the war, Hitchcock took an interest in creative writing. In June 1919, he became a founding editor and business manager of Henley's in-house publication, The Henley Telegraph (sixpence a copy), to which he submitted several short stories. Henley's promoted him to the advertising department, where he wrote copy and drew graphics for electric cable advertisements. He enjoyed the job and would stay late at the office to examine the proofs; he told Truffaut that this was his \"first step toward cinema\". He enjoyed watching films, especially American cinema, and from the age of 16 read the trade papers; he watched Charlie Chaplin, D. W. Griffith and Buster Keaton, and particularly liked Fritz Lang's Der m\u00fcde Tod (1921).\n\nInter-war career: 1919\u20131939\nFamous Players\u2013Lasky\nWhile still at Henley's, he read in a trade paper that Famous Players\u2013Lasky, the production arm of Paramount Pictures, was opening a studio in London. They were planning to film The Sorrows of Satan by Marie Corelli, so he produced some drawings for the title cards and sent his work to the studio. They hired him, and in 1919 he began working for Islington Studios in Poole Street, Hoxton, as a title-card designer.\nDonald Spoto wrote that most of the staff were Americans with strict job specifications, but the English workers were encouraged to try their hand at anything, which meant that Hitchcock gained experience as a co-writer, art director and production manager on at least 18 silent films. The Times wrote in February 1922 about the studio's \"special art title department under the supervision of Mr. A. J. Hitchcock\". His work included Number 13 (1922), also known as Mrs. Peabody; it was cancelled because of financial problems - the few finished scenes are lost \u2014 and Always Tell Your Wife (1923), which he and Seymour Hicks finished together when Hicks was about to give up on it. Hicks wrote later about being helped by \"a fat youth who was in charge of the property room ... [n]one other than Alfred Hitchcock\".\n\nGainsborough Pictures and work in Germany\nWhen Paramount pulled out of London in 1922, Hitchcock was hired as an assistant director by a new firm run in the same location by Michael Balcon, later known as Gainsborough Pictures. Hitchcock worked on Woman to Woman (1923) with the director Graham Cutts, designing the set, writing the script and producing. He said: \"It was the first film that I had really got my hands onto.\" The editor and \"script girl\" on Woman to Woman was Alma Reville, his future wife. He also worked as an assistant to Cutts on The White Shadow (1924), The Passionate Adventure (1924), The Blackguard (1925) and The Prude's Fall (1925). The Blackguard was produced at the Babelsberg Studios in Potsdam, where Hitchcock watched part of the making of F. W. Murnau's The Last Laugh (1924). He was impressed with Murnau's work, and later used many of his techniques for the set design in his own productions.\nIn the summer of 1925, Balcon asked Hitchcock to direct The Pleasure Garden (1925), starring Virginia Valli, a co-production of Gainsborough and the German firm Emelka at the Geiselgasteig studio near Munich. Reville, by then Hitchcock's fianc\u00e9e, was assistant director-editor. Although the film was a commercial flop, Balcon liked Hitchcock's work; a Daily Express headline called him the \"Young man with a master mind\". Production of The Pleasure Garden encountered obstacles which Hitchcock would later learn from: on arrival to Brenner Pass, he failed to declare his film stock to customs and it was confiscated; one actress could not enter the water for a scene because she was on her period; budget overruns meant that he had to borrow money from the actors. Hitchcock also needed a translator to give instructions to the cast and crew.\nIn Germany, Hitchcock observed the nuances of German cinema and filmmaking which had a big influence on him. When he was not working, he would visit Berlin's art galleries, concerts and museums. He would also meet with actors, writers and producers to build connections. Balcon asked him to direct a second film in Munich, The Mountain Eagle (1926), based on an original story titled Fear o' God. The film is lost, and Hitchcock called it \"a very bad movie\". A year later, Hitchcock wrote and directed The Ring; although the screenplay was credited solely to his name, Elliot Stannard assisted him with the writing. The Ring garnered positive reviews; the Bioscope critic called it \"the most magnificent British film ever made\".\nWhen he returned to England, Hitchcock was one of the early members of the London Film Society, newly formed in 1925. Through the Society, he became fascinated by the work by Soviet filmmakers: Dziga Vertov, Lev Kuleshov, Sergei Eisenstein and Vsevolod Pudovkin. He would also socialise with fellow English filmmakers Ivor Montagu, Adrian Brunel and Walter Mycroft.\n\nHitchcock established himself as a name director with his first thriller, The Lodger: A Story of the London Fog (1927). The film concerns the hunt for a Jack the Ripper-style serial killer who, wearing a black cloak and carrying a black bag, is murdering young blonde women in London, and only on Tuesdays. A landlady suspects that her lodger is the killer, but he turns out to be innocent. Hitchcock had wanted the leading man to be guilty, or for the film at least to end ambiguously, but the star was Ivor Novello, a matin\u00e9e idol, and the \"star system\" meant that Novello could not be the villain. Hitchcock told Truffaut: \"You have to clearly spell it out in big letters: 'He is innocent.'\" (He had the same problem years later with Cary Grant in Suspicion (1941).) Released in January 1927, The Lodger was a commercial and critical success in the UK. Upon its release, the trade journal Bioscope wrote: \"It is possible that this film is the finest British production ever made\". Hitchcock told Truffaut that the film was the first of his to be influenced by German Expressionism: \"In truth, you might almost say that The Lodger was my first picture.\" He made his first cameo appearance in the film, sitting in a newsroom.\n\nMarriage\nOn 2 December 1926, Hitchcock married the English screenwriter Alma Reville at the Brompton Oratory in South Kensington. The couple honeymooned in Paris, Lake Como and St. Moritz, before returning to London to live in a leased flat on the top two floors of 153 Cromwell Road, Kensington. Reville, who was born just hours after Hitchcock, converted from Protestantism to Catholicism, apparently at the insistence of Hitchcock's mother; she was baptised on 31 May 1927 and confirmed at Westminster Cathedral by Cardinal Francis Bourne on 5 June.\nIn 1928, when they learned that Reville was pregnant, the Hitchcocks purchased \"Winter's Grace\", a Tudor farmhouse set in eleven acres on Stroud Lane, Shamley Green, Surrey, for \u00a32,500. Their daughter and only child, Patricia (Pat) Alma Hitchcock, was born on 7 July that year. Pat died on 9 August 2021 at the age of 93.\nReville became her husband's closest collaborator; Charles Champlin wrote in 1982: \"The Hitchcock touch had four hands, and two were Alma's.\" When Hitchcock accepted the AFI Life Achievement Award in 1979, he said that he wanted to mention \"four people who have given me the most affection, appreciation and encouragement, and constant collaboration. The first of the four is a film editor, the second is a scriptwriter, the third is the mother of my daughter, Pat, and the fourth is as fine a cook as ever performed miracles in a domestic kitchen. And their names are Alma Reville.\" Reville wrote or co-wrote on many of Hitchcock's films, including Shadow of a Doubt, Suspicion and The 39 Steps.\n\nEarly sound films\nHitchcock began work on his tenth film, Blackmail (1929), when its production company, British International Pictures (BIP), converted its Elstree studios to sound. The film was the first British \"talkie\"; this followed the rapid development of sound films in the United States, from the use of brief sound segments in The Jazz Singer (1927) to the first full sound feature Lights of New York (1928). Blackmail began the Hitchcock tradition of using famous landmarks as a backdrop for suspense sequences, with the climax taking place on the dome of the British Museum. It also features one of his longest cameo appearances, which shows him being bothered by a small boy as he reads a book on the London Underground. In the PBS series The Men Who Made The Movies, Hitchcock explained how he used early sound recording as a special element of the film to create tension, with a gossipy woman (Phyllis Monkman) stressing the word \"knife\" in her conversation with the woman suspected of murder. During this period, Hitchcock directed segments for a BIP revue, Elstree Calling (1930), and directed a short film, An Elastic Affair (1930), featuring two Film Weekly scholarship winners. An Elastic Affair is one of the lost films.\n\nIn 1933, Hitchcock signed a multi-film contract with Gaumont-British, once again working for Michael Balcon. His first film for the company, The Man Who Knew Too Much (1934), was a success; his second, The 39 Steps (1935), was acclaimed in the UK, and gained him recognition in the US. It also established the quintessential English \"Hitchcock blonde\" (Madeleine Carroll) as the template for his succession of ice-cold, elegant leading ladies. Screenwriter Robert Towne remarked: \"It's not much of an exaggeration to say that all contemporary escapist entertainment begins with The 39 Steps\". John Buchan, author of The Thirty-Nine Steps on which the film is loosely based, met with Hitchcock on set, and attended the high-profile premiere at the New Gallery Cinema in London. Upon viewing the film, the author said it had improved on the book. This film was one of the first to introduce the \"MacGuffin\" plot device, a term coined by the English screenwriter and Hitchcock collaborator Angus MacPhail. The MacGuffin is an item or goal the protagonist is pursuing, one that otherwise has no narrative value; in The 39 Steps, the MacGuffin is a stolen set of design plans.\nHitchcock released two spy thrillers in 1936. Sabotage was loosely based on Joseph Conrad's novel, The Secret Agent (1907), about a woman who discovers that her husband is a terrorist, and Secret Agent, based on two stories in Ashenden: Or the British Agent (1928) by W. Somerset Maugham.\n\nAt this time, Hitchcock also became notorious for pranks against the cast and crew. These jokes ranged from simple and innocent to crazy and maniacal. For instance, he hosted a dinner party where he dyed all the food blue because he claimed there weren't enough blue foods. He also had a horse delivered to the dressing room of his friend, actor Gerald du Maurier.\nHitchcock followed up with Young and Innocent in 1937, a crime thriller based on the 1936 novel A Shilling for Candles by Josephine Tey. Starring Nova Pilbeam and Derrick De Marney, the film was relatively enjoyable for the cast and crew to make. To meet distribution purposes in America, the film's runtime was cut and this included removal of one of Hitchcock's favourite scenes: a children's tea party which becomes menacing to the protagonists.\nHitchcock's next major success was The Lady Vanishes (1938), \"one of the greatest train movies from the genre's golden era\", according to Philip French, in which Miss Froy (May Whitty), a British spy posing as a governess, disappears on a train journey through the fictional European country of Bandrika. The film saw Hitchcock receive the 1938 New York Film Critics Circle Award for Best Director. Benjamin Crisler of The New York Times wrote in June 1938: \"Three unique and valuable institutions the British have that we in America have not: Magna Carta, the Tower Bridge and Alfred Hitchcock, the greatest director of screen melodramas in the world.\" The film was based on the novel The Wheel Spins (1936) written by Ethel Lina White, and starred Michael Redgrave (in his film debut) and Margaret Lockwood.\nBy 1938, Hitchcock was aware that he had reached his peak in Britain. He had received numerous offers from producers in the United States, but he turned them all down because he disliked the contractual obligations or thought the projects were repellent. However, producer David O. Selznick offered him a concrete proposal to make a film based on the sinking of RMS Titanic, which was eventually shelved, but Selznick persuaded Hitchcock to come to Hollywood. In July 1938, Hitchcock flew to New York, and found that he was already a celebrity; he was featured in magazines and gave interviews to radio stations. In Hollywood, Hitchcock met Selznick for the first time. Selznick offered him a four-film contract, approximately $40,000 for each picture (equivalent to $870,000 in 2023).\n\nEarly Hollywood years: 1939\u20131945\nSelznick contract\nSelznick signed Hitchcock to a seven-year contract beginning in April 1939, and the Hitchcocks moved to Hollywood. The Hitchcocks lived in a spacious flat on Wilshire Boulevard, and slowly acclimatised themselves to the Los Angeles area. He and his wife Alma kept a low profile, and were not interested in attending parties or being celebrities. Hitchcock discovered his taste for fine food in West Hollywood, but still carried on his way of life from England. He was impressed with Hollywood's filmmaking culture, expansive budgets and efficiency, compared to the limits that he had often faced in Britain. In June that year, Life called him the \"greatest master of melodrama in screen history\".\nAlthough Hitchcock and Selznick respected each other, their working arrangements were sometimes difficult. Selznick suffered from constant financial problems, and Hitchcock was often unhappy about Selznick's creative control and interference over his films. Selznick was also displeased with Hitchcock's method of shooting just what was in the script, and nothing more, which meant that the film could not be cut and remade differently at a later time. As well as complaining about Hitchcock's \"goddamn jigsaw cutting\", their personalities were mismatched: Hitchcock was reserved whereas Selznick was flamboyant. Eventually, Selznick generously lent Hitchcock to the larger film studios. Selznick made only a few films each year, as did fellow independent producer Samuel Goldwyn, so he did not always have projects for Hitchcock to direct. Goldwyn had also negotiated with Hitchcock on a possible contract, only to be outbid by Selznick. In a later interview, Hitchcock said: \"[Selznick] was the Big Producer. ... Producer was king. The most flattering thing Mr. Selznick ever said about me\u2014and it shows you the amount of control\u2014he said I was the 'only director' he'd 'trust with a film'.\"\n\nHitchcock approached American cinema cautiously; his first American film was set in England in which the \"Americanness\" of the characters was incidental: Rebecca (1940) was set in a Hollywood version of England's Cornwall and based on a novel by English novelist Daphne du Maurier. Selznick insisted on a faithful adaptation of the book, and disagreed with Hitchcock with the use of humour. The film, starring Laurence Olivier and Joan Fontaine, concerns an unnamed na\u00efve young woman who marries a widowed aristocrat. She lives in his large English country house, and struggles with the lingering reputation of his elegant and worldly first wife Rebecca, who died under mysterious circumstances. The film won Best Picture at the 13th Academy Awards; the statuette was given to producer Selznick. Hitchcock received his first nomination for Best Director, his first of five such nominations.\nHitchcock's second American film was the thriller Foreign Correspondent (1940), set in Europe, based on Vincent Sheean's book Personal History (1935) and produced by Walter Wanger. It was nominated for Best Picture that year. Hitchcock felt uneasy living and working in Hollywood while Britain was at war; his concern resulted in a film that overtly supported the British war effort. Filmed in 1939, it was inspired by the rapidly changing events in Europe, as covered by an American newspaper reporter played by Joel McCrea. By mixing footage of European scenes with scenes filmed on a Hollywood backlot, the film avoided direct references to Nazism, Nazi Germany and Germans, to comply with the Motion Picture Production Code at the time.\n\nEarly war years\nIn September 1940, the Hitchcocks bought the 200-acre (0.81 km2) Cornwall Ranch near Scotts Valley, California, in the Santa Cruz Mountains. Their primary residence was an English-style home in Bel Air, purchased in 1942. Hitchcock's films were diverse during this period, ranging from the romantic comedy Mr. & Mrs. Smith (1941) to the bleak film noir Shadow of a Doubt (1943).\n\nSuspicion (1941) marked Hitchcock's first film as a producer and director. It is set in England; Hitchcock used the north coast of Santa Cruz for the English coastline sequence. The film is the first of four in which Cary Grant was cast by Hitchcock, and it is one of the rare occasions that Grant plays a sinister character. Grant plays Johnnie Aysgarth, an English conman whose actions raise suspicion and anxiety in his shy young English wife, Lina McLaidlaw (Joan Fontaine). In one scene, Hitchcock placed a light inside a glass of milk, perhaps poisoned, that Grant is bringing to his wife; the light ensures that the audience's attention is on the glass. Grant's character is actually a killer, according to the book, Before the Fact by Francis Iles, but the studio felt that Grant's image would be tarnished by that. Hitchcock would have preferred to end with the wife's murder. Instead, the actions that she found suspicious are a reflection of his own despair and his plan to commit suicide. Fontaine won Best Actress for her performance.\nSaboteur (1942) is the first of two films that Hitchcock made for Universal Studios during the decade. Hitchcock wanted Gary Cooper and Barbara Stanwyck or Henry Fonda and Gene Tierney to star, but was forced by Universal to use Universal contract player Robert Cummings and Priscilla Lane, a freelancer who signed a one-picture deal with the studio, both known for their work in comedies and light dramas. The story depicts a confrontation between a suspected saboteur (Cummings) and a real saboteur (Norman Lloyd) atop the Statue of Liberty. Hitchcock took a three-day tour of New York City to scout for Saboteur's filming locations. He also directed Have You Heard? (1942), a photographic dramatisation for Life magazine of the dangers of rumours during wartime. In 1943, he wrote a mystery story for Look, \"The Murder of Monty Woolley\", a sequence of captioned photographs inviting the reader to find clues to the murderer's identity; Hitchcock cast the performers as themselves, such as Woolley, Doris Merrick and make-up man Guy Pearce.\n\nBack in England, Hitchcock's mother Emma was severely ill; she died on 26 September 1942 at age 79. Hitchcock never spoke publicly about his mother, but his assistant said that he admired her. Four months later, on 4 January 1943, his brother William died of an overdose at age 52. Hitchcock was not very close to William, but his death made Hitchcock conscious about his own eating and drinking habits. He was overweight and suffering from back aches. His New Year's resolution in 1943 was to take his diet seriously with the help of a physician. In January that year, Shadow of a Doubt was released, which Hitchcock had fond memories of making. In the film, Charlotte \"Charlie\" Newton (Teresa Wright) suspects her beloved uncle Charlie Oakley (Joseph Cotten) of being a serial killer. Hitchcock filmed extensively on location, this time in the Northern California city of Santa Rosa.\nAt 20th Century Fox, Hitchcock approached John Steinbeck with an idea for a film, which recorded the experiences of the survivors of a German U-boat attack. Steinbeck began work on the script for what would become Lifeboat (1944). However, Steinbeck was unhappy with the film and asked that his name be removed from the credits, to no avail. The idea was rewritten as a short story by Harry Sylvester and published in Collier's in 1943. The action sequences were shot in a small boat in the studio water tank. The locale posed problems for Hitchcock's traditional cameo appearance; it was solved by having Hitchcock's image appear in a newspaper that William Bendix is reading in the boat, showing the director in a before-and-after advertisement for \"Reduco-Obesity Slayer\". He told Truffaut in 1962:\n\nAt the time, I was on a strenuous diet, painfully working my way from three hundred to two hundred pounds. So I decided to immortalize my loss and get my bit part by posing for \"before\" and \"after\" pictures. ... I was literally submerged by letters from fat people who wanted to know where and how they could get Reduco.\nHitchcock's typical dinner before his weight loss had been a roast chicken, boiled ham, potatoes, bread, vegetables, relishes, salad, dessert, a bottle of wine and some brandy. To lose weight, his diet consisted of black coffee for breakfast and lunch, and steak and salad for dinner, but it was hard to maintain; Donald Spoto wrote that his weight fluctuated considerably over the next 40 years. At the end of 1943, despite the weight loss, the Occidental Insurance Company of Los Angeles refused his application for life insurance.\n\nWartime non-fiction films\nHitchcock returned to the UK for an extended visit in late 1943 and early 1944. While there he made two short propaganda films, Bon Voyage (1944) and Aventure Malgache (1944), for the Ministry of Information. In June and July 1945, Hitchcock served as \"treatment advisor\" on a Holocaust documentary that used Allied Forces footage of the liberation of Nazi concentration camps. The film was assembled in London and produced by Sidney Bernstein of the Ministry of Information, who brought Hitchcock (a friend of his) on board. It was originally intended to be broadcast to the Germans, but the British government deemed it too traumatic to be shown to a shocked post-war population. Instead, it was transferred in 1952 from the British War Office film vaults to London's Imperial War Museum and remained unreleased until 1985, when an edited version was broadcast as an episode of PBS Frontline, under the title the Imperial War Museum had given it: Memory of the Camps. The full-length version of the film, German Concentration Camps Factual Survey, was restored in 2014 by scholars at the Imperial War Museum.\n\nPost-war Hollywood years: 1945\u20131953\nLater Selznick films\nHitchcock worked for David Selznick again when he directed Spellbound (1945), which explores psychoanalysis and features a dream sequence designed by Salvador Dal\u00ed. The dream sequence as it appears in the film is ten minutes shorter than was originally envisioned; Selznick edited it to make it \"play\" more effectively. Gregory Peck plays amnesiac Dr. Anthony Edwardes under the treatment of analyst Dr. Peterson (Ingrid Bergman), who falls in love with him while trying to unlock his repressed past. Two point-of-view shots were achieved by building a large wooden hand (which would appear to belong to the character whose point of view the camera took) and out-sized props for it to hold: a bucket-sized glass of milk and a large wooden gun. For added novelty and impact, the climactic gunshot was hand-coloured red on some copies of the black-and-white film. The original musical score by Mikl\u00f3s R\u00f3zsa makes use of the theremin, and some of it was later adapted by the composer into Rozsa's Piano Concerto Op. 31 (1967) for piano and orchestra.\nThe spy film Notorious followed next in 1946. Hitchcock told Fran\u00e7ois Truffaut that Selznick sold him, Ingrid Bergman, Cary Grant and Ben Hecht's screenplay, to RKO Radio Pictures as a \"package\" for $500,000 (equivalent to $7.8 million in 2023) because of cost overruns on Selznick's Duel in the Sun (1946). Notorious stars Bergman and Grant, both Hitchcock collaborators, and features a plot about Nazis, uranium and South America. His prescient use of uranium as a plot device led to him being briefly placed under surveillance by the Federal Bureau of Investigation. According to Patrick McGilligan, in or around March 1945, Hitchcock and Hecht consulted Robert Millikan of the California Institute of Technology about the development of a uranium bomb. Selznick complained that the notion was \"science fiction\", only to be confronted by the news of the detonation of two atomic bombs on Hiroshima and Nagasaki in Japan in August 1945.\n\nTransatlantic Pictures\nHitchcock formed an independent production company, Transatlantic Pictures, with his friend Sidney Bernstein. He made two films with Transatlantic, one of which was his first colour film. With Rope (1948), Hitchcock experimented with marshalling suspense in a confined environment, as he had done earlier with Lifeboat. The film appears as a very limited number of continuous shots, but it was actually shot in 10 ranging from 4+1\u20442 to 10 minutes each; a 10-minute length of film was the most that a camera's film magazine could hold at the time. Some transitions between reels were hidden by having a dark object fill the entire screen for a moment. Hitchcock used those points to hide the cut, and began the next take with the camera in the same place. The film features James Stewart in the leading role, and was the first of four films that Stewart made with Hitchcock. It was inspired by the Leopold and Loeb case of the 1920s. Critical response at the time was mixed.\nUnder Capricorn (1949), set in 19th-century Australia, also uses the short-lived technique of long takes, but to a more limited extent. He again used Technicolor in this production, then returned to black-and-white for several years. Transatlantic Pictures became inactive after the last two films. Hitchcock filmed Stage Fright (1950) at Elstree Studios in England, where he had worked during his British International Pictures contract many years before. He paired one of Warner Bros.' most popular stars, Jane Wyman, with the expatriate German actor Marlene Dietrich and used several prominent British actors, including Michael Wilding, Richard Todd and Alastair Sim. This was Hitchcock's first proper production for Warner Bros., which had distributed Rope and Under Capricorn, because Transatlantic Pictures was experiencing financial difficulties.\nHis thriller Strangers on a Train (1951) was based on the novel of the same name by Patricia Highsmith. Hitchcock combined many elements from his preceding films. He approached Dashiell Hammett to write the dialogue, but Raymond Chandler took over, then left over disagreements with the director. In the film, two men casually meet, one of whom speculates on a foolproof method to murder; he suggests that two people, each wishing to do away with someone, should each perform the other's murder. Farley Granger's role was as the innocent victim of the scheme, while Robert Walker, previously known for \"boy-next-door\" roles, played the villain. I Confess (1953) was set in Quebec with Montgomery Clift as a Catholic priest.\n\nPeak years: 1954\u20131964\nDial M for Murder and Rear Window\nI Confess was followed by three colour films starring Grace Kelly: Dial M for Murder (1954), Rear Window (1954) and To Catch a Thief (1955). In Dial M for Murder, Ray Milland plays the villain who tries to murder his unfaithful wife (Kelly) for her money. She kills the hired assassin in self-defence, so Milland manipulates the evidence to make it look like murder. Her lover, Mark Halliday (Robert Cummings), and Police Inspector Hubbard (John Williams) save her from execution. Hitchcock experimented with 3D cinematography for Dial M for Murder.\nHitchcock moved to Paramount Pictures and filmed Rear Window (1954), starring James Stewart and Grace Kelly, as well as Thelma Ritter and Raymond Burr. Stewart's character is a photographer named Jeff (based on Robert Capa) who must temporarily use a wheelchair. Out of boredom, he begins observing his neighbours across the courtyard, then becomes convinced that one of them (Raymond Burr) has murdered his wife. Jeff eventually manages to convince his policeman buddy (Wendell Corey) and his girlfriend (Kelly). As with Lifeboat and Rope, the principal characters are depicted in confined or cramped quarters, in this case Stewart's studio apartment. Hitchcock uses close-ups of Stewart's face to show his character's reactions, \"from the comic voyeurism directed at his neighbours to his helpless terror watching Kelly and Burr in the villain's apartment\".\n\nAlfred Hitchcock Presents\nFrom 1955 to 1965, Hitchcock was the host of the television series Alfred Hitchcock Presents. With his droll delivery, gallows humour and iconic image, the series made Hitchcock a celebrity. The title-sequence of the show pictured a minimalist caricature of his profile (he drew it himself; it is composed of only nine strokes), which his real silhouette then filled. The series theme tune was Funeral March of a Marionette by the French composer Charles Gounod (1818\u20131893).\nHis introductions always included some sort of wry humour, such as the description of a recent multi-person execution hampered by having only one electric chair, while two are shown with a sign \"Two chairs\u2014no waiting!\" He directed 18 episodes of the series, which aired from 1955 to 1965. It became The Alfred Hitchcock Hour in 1962, and NBC broadcast the final episode on 10 May 1965. In the 1980s, a new version of Alfred Hitchcock Presents was produced for television, making use of Hitchcock's original introductions in a colourised form.\nHitchcock's success in television spawned a set of short-story collections in his name; these included Alfred Hitchcock's Anthology, Stories They Wouldn't Let Me Do on TV, and Tales My Mother Never Told Me. In 1956, HSD Publications also licensed the director's name to create Alfred Hitchcock's Mystery Magazine, a monthly digest specialising in crime and detective fiction. Hitchcock's television series were very profitable, and his foreign-language versions of books were bringing revenues of up to $100,000 a year (equivalent to $1,030,000 in 2023).\n\nFrom To Catch a Thief to Vertigo\nIn 1955, Hitchcock became a United States citizen. In the same year, his third Grace Kelly film, To Catch a Thief, was released; it is set in the French Riviera, and stars Kelly and Cary Grant. Grant plays retired thief John Robie, who becomes the prime suspect for a spate of robberies in the Riviera. A thrill-seeking American heiress played by Kelly surmises his true identity and tries to seduce him. \"Despite the obvious age disparity between Grant and Kelly and a lightweight plot, the witty script (loaded with double entendres) and the good-natured acting proved a commercial success.\" It was Hitchcock's last film with Kelly; she married Prince Rainier of Monaco in 1956, and ended her film career afterward. Hitchcock then remade his own 1934 film The Man Who Knew Too Much in 1956. This time, the film starred James Stewart and Doris Day, who sang the theme song \"Que Sera, Sera\", which won the Academy Award for Best Original Song and became a big hit. They play a couple whose son is kidnapped to prevent them from interfering with an assassination. As in the 1934 film, the climax takes place at the Royal Albert Hall.\n\nThe Wrong Man (1956), Hitchcock's final film for Warner Bros., is a low-key black-and-white production based on a real-life case of mistaken identity reported in Life magazine in 1953. This was the only film of Hitchcock to star Henry Fonda, playing a Stork Club musician mistaken for a liquor store thief, who is arrested and tried for robbery while his wife (Vera Miles) emotionally collapses under the strain. Hitchcock told Truffaut that his lifelong fear of the police attracted him to the subject and was embedded in many scenes.\nWhile directing episodes for Alfred Hitchcock Presents during the summer of 1957, Hitchcock was admitted to hospital for hernia and gallstones, and had to have his gallbladder removed. Following a successful surgery, he immediately returned to work to prepare for his next project. Vertigo (1958) again starred James Stewart, with Kim Novak and Barbara Bel Geddes. He had wanted Vera Miles to play the lead, but she was pregnant. He told Oriana Fallaci: \"I was offering her a big part, the chance to become a beautiful sophisticated blonde, a real actress. We'd have spent a heap of dollars on it, and she has the bad taste to get pregnant. I hate pregnant women, because then they have children.\"\nIn Vertigo, Stewart plays Scottie, a former police investigator suffering from acrophobia, who becomes obsessed with a woman he has been hired to shadow (Novak). Scottie's obsession leads to tragedy, and this time Hitchcock did not opt for a happy ending. Some critics, including Donald Spoto and Roger Ebert, agree that Vertigo is the director's most personal and revealing film, dealing with the Pygmalion-like obsessions of a man who moulds a woman into the person he desires. Vertigo explores more frankly and at greater length his interest in the relation between sex and death, than any other work in his filmography.\nVertigo contains a camera technique developed by Irmin Roberts, commonly referred to as a dolly zoom, which has been copied by many filmmakers. The film premiered at the San Sebasti\u00e1n International Film Festival, and Hitchcock won the Silver Seashell prize. Vertigo is considered a classic, but it attracted mixed reviews and poor box-office receipts at the time; the critic from Variety opined that the film was \"too slow and too long\". Bosley Crowther of the New York Times thought it was \"devilishly far-fetched\", but praised the cast performances and Hitchcock's direction. The picture was also the last collaboration between Stewart and Hitchcock. In the 2002 Sight & Sound polls, it ranked just behind Citizen Kane (1941); ten years later, in the same magazine, critics chose it as the best film ever made.\n\nNorth by Northwest and Psycho\nAfter Vertigo, the rest of 1958 was a difficult year for Hitchcock. During pre-production of North by Northwest (1959), which was a \"slow\" and \"agonising\" process, his wife Alma was diagnosed with cancer. While she was in hospital, Hitchcock kept himself occupied with his television work and would visit her every day. Alma underwent surgery and made a full recovery, but it caused Hitchcock to imagine, for the first time, life without her.\n\nHitchcock followed up with three more successful films, which are also recognised as among his best: North by Northwest, Psycho (1960) and The Birds (1963). In North by Northwest, Cary Grant portrays Roger Thornhill, a Madison Avenue advertising executive who is mistaken for a government secret agent. He is pursued across the United States by enemy agents, including Eve Kendall (Eva Marie Saint). At first, Thornhill believes Kendall is helping him, but then realises that she is an enemy agent; he later learns that she is working undercover for the CIA. During its opening two-week run at Radio City Music Hall, the film grossed $404,056 (equivalent to $4.2 million in 2023), setting a non-holiday gross record for that theatre. Time magazine called the film \"smoothly troweled and thoroughly entertaining\".\nPsycho (1960) is arguably Hitchcock's best-known film. Based on Robert Bloch's 1959 novel Psycho, which was inspired by the case of Ed Gein, the film was produced on a tight budget of $800,000 (equivalent to $8.2 million in 2023) and shot in black-and-white on a spare set using crew members from Alfred Hitchcock Presents. The unprecedented violence of the shower scene, the early death of the heroine, and the innocent lives extinguished by a disturbed murderer became the hallmarks of a new horror-film genre. The film proved popular with audiences, with lines stretching outside theatres as viewers waited for the next showing. It broke box-office records in the United Kingdom, France, South America, the United States and Canada, and was a moderate success in Australia for a brief period.\nPsycho was the most profitable of Hitchcock's career, and he personally earned in excess of $15 million (equivalent to $150 million in 2023). He subsequently swapped his rights to Psycho and his TV anthology for 150,000 shares of MCA, making him the third largest shareholder and his own boss at Universal, in theory at least, although that did not stop studio interference. Following the first film, Psycho became an American horror franchise: Psycho II, Psycho III, Bates Motel, Psycho IV: The Beginning and a colour 1998 remake of the original.\n\nTruffaut interview\nOn 13 August 1962, Hitchcock's 63rd birthday, the French director Fran\u00e7ois Truffaut began a 50-hour interview of Hitchcock, filmed over eight days at Universal Studios, during which Hitchcock agreed to answer 500 questions. It took four years to transcribe the tapes and organise the images; it was published as a book in 1967, which Truffaut nicknamed the \"Hitchbook\". The audio tapes were used as the basis of a documentary in 2015. Truffaut sought the interview because it was clear to him that Hitchcock was not simply the mass-market entertainer the American media made him out to be. It was obvious from his films, Truffaut wrote, that Hitchcock had \"given more thought to the potential of his art than any of his colleagues\". He compared the interview to \"Oedipus' consultation of the oracle\".\n\nThe Birds\nThe film scholar Peter William Evans wrote that The Birds (1963) and Marnie (1964) are regarded as \"undisputed masterpieces\". Hitchcock had intended to film Marnie first, and in March 1962 it was announced that Grace Kelly, Princess Grace of Monaco since 1956, would come out of retirement to star in it. When Kelly asked Hitchcock to postpone Marnie until 1963 or 1964, he recruited Evan Hunter, author of The Blackboard Jungle (1954), to develop a screenplay based on a Daphne du Maurier short story, \"The Birds\" (1952), which Hitchcock had republished in his My Favorites in Suspense (1959). He hired Tippi Hedren to play the lead role. It was her first role; she had been a model in New York when Hitchcock saw her, in October 1961, in an NBC television advert for Sego, a diet drink: \"I signed her because she is a classic beauty. Movies don't have them any more. Grace Kelly was the last.\" He insisted, without explanation, that her first name be written in single quotation marks: 'Tippi'.\nIn The Birds, Melanie Daniels, a young socialite, meets lawyer Mitch Brenner (Rod Taylor) in a bird shop; Jessica Tandy plays his possessive mother. Hedren visits him in Bodega Bay (where The Birds was filmed) carrying a pair of lovebirds as a gift. Suddenly waves of birds start gathering, watching, and attacking. The question: \"What do the birds want?\" is left unanswered. Hitchcock made the film with equipment from the Revue Studio, which made Alfred Hitchcock Presents. He said it was his most technically challenging film, using a combination of trained and mechanical birds against a backdrop of wild ones. Every shot was sketched in advance.\nAn HBO\/BBC television film, The Girl (2012), depicted Hedren's experiences on set; she said that Hitchcock became obsessed with her and sexually harassed her. He reportedly isolated her from the rest of the crew, had her followed, whispered obscenities to her, had her handwriting analysed and had a ramp built from his private office directly into her trailer. Diane Baker, her co-star in Marnie, said: \"[N]othing could have been more horrible for me than to arrive on that movie set and to see her being treated the way she was.\" While filming the attack scene in the attic \u2014 which took a week to film \u2014 she was placed in a caged room while two men wearing elbow-length protective gloves threw live birds at her. Toward the end of the week, to stop the birds' flying away from her too soon, one leg of each bird was attached by nylon thread to elastic bands sewn inside her clothes. She broke down after a bird cut her lower eyelid, and filming was halted on doctor's orders.\n\nMarnie\nIn June 1962, Grace Kelly announced that she had decided against appearing in Marnie (1964). Hedren had signed an exclusive seven-year, $500-a-week contract with Hitchcock in October 1961, and he decided to cast her in the lead role opposite Sean Connery. In 2016, describing Hedren's performance as \"one of the greatest in the history of cinema\", Richard Brody called the film a \"story of sexual violence\" inflicted on the character played by Hedren: \"The film is, to put it simply, sick, and it's so because Hitchcock was sick. He suffered all his life from furious sexual desire, suffered from the lack of its gratification, suffered from the inability to transform fantasy into reality, and then went ahead and did so virtually, by way of his art.\" A 1964 New York Times review called it Hitchcock's \"most disappointing film in years\", citing Hedren's and Connery's lack of experience, an amateurish script and \"glaringly fake cardboard backdrops\".\nIn the film, Marnie Edgar (Hedren) steals $10,000 from her employer and goes on the run. She applies for a job at Mark Rutland's (Connery) company in Philadelphia and steals from there too. Earlier, she is shown having a panic attack during a thunderstorm and fearing the colour red. Mark tracks her down and blackmails her into marrying him. She explains that she does not want to be touched, but during the \"honeymoon\", Mark rapes her. Marnie and Mark discover that Marnie's mother had been a prostitute when Marnie was a child, and that, while the mother was fighting with a client during a thunderstorm \u2014 the mother believed the client had tried to molest Marnie \u2014 Marnie had killed the client to save her mother. Cured of her fears when she remembers what happened, she decides to stay with Mark.\n\nHitchcock told cinematographer Robert Burks that the camera had to be placed as close as possible to Hedren when he filmed her face. Evan Hunter, the screenwriter of The Birds who was writing Marnie too, explained to Hitchcock that, if Mark loved Marnie, he would comfort her, not rape her. Hitchcock reportedly replied: \"Evan, when he sticks it in her, I want that camera right on her face!\" When Hunter submitted two versions of the script, one without the rape scene, Hitchcock replaced him with Jay Presson Allen.\n\nLater years: 1966\u20131980\nFinal films\nFailing health reduced Hitchcock's output during the last two decades of his life. Biographer Stephen Rebello claimed Universal imposed two films on him, Torn Curtain (1966) and Topaz (1969), the latter of which is based on a Leon Uris novel, partly set in Cuba. Both were spy thrillers with Cold War-related themes. Torn Curtain, with Paul Newman and Julie Andrews, precipitated the bitter end of the twelve-year collaboration between Hitchcock and composer Bernard Herrmann. Hitchcock was unhappy with Herrmann's score and replaced him with John Addison, Jay Livingston and Ray Evans. Upon release, Torn Curtain was a box office disappointment, and Topaz was disliked by both critics and the studio.\n\nHitchcock returned to Britain to make his penultimate film, Frenzy (1972), based on the novel Goodbye Piccadilly, Farewell Leicester Square (1966). After two espionage films, the plot marked a return to the murder-thriller genre. Richard Blaney (Jon Finch), a volatile barman with a history of explosive anger, becomes the prime suspect in the investigation into the \"Necktie Murders\", which are actually committed by his friend Bob Rusk (Barry Foster). This time, Hitchcock makes the victim and villain kindreds, rather than opposites, as in Strangers on a Train.\nIn Frenzy, Hitchcock allowed nudity for the first time. Two scenes show naked women, one of whom is being raped and strangled; Donald Spoto called the latter \"one of the most repellent examples of a detailed murder in the history of film\". Both actors, Barbara Leigh-Hunt and Anna Massey, refused to do the scenes, so models were used instead. Biographers have noted that Hitchcock had always pushed the limits of film censorship, often managing to fool Joseph Breen, the head of the Motion Picture Production Code. Hitchcock would add subtle hints of improprieties forbidden by censorship until the mid-1960s. Yet, Patrick McGilligan wrote that Breen and others often realised that Hitchcock was inserting such material and were actually amused, as well as alarmed by Hitchcock's \"inescapable inferences\".\nFamily Plot (1976) was Hitchcock's last film. It relates the escapades of \"Madam\" Blanche Tyler, played by Barbara Harris, a fraudulent spiritualist, and her taxi-driver lover Bruce Dern, making a living from her phony powers. While Family Plot was based on the Victor Canning novel The Rainbird Pattern (1972), the novel's tone is more sinister. Screenwriter Ernest Lehman originally wrote the film, under the working title Deception, with a dark tone but was pushed to a lighter, more comical tone by Hitchcock where it took the name Deceit, then finally, Family Plot.\n\nKnighthood and death\nToward the end of his life, Hitchcock was working on the script for a spy thriller, The Short Night, collaborating with James Costigan, Ernest Lehman and David Freeman. Despite preliminary work, it was never filmed. Hitchcock's health was declining and he was worried about his wife, who had suffered a stroke. The screenplay was eventually published in Freeman's book The Last Days of Alfred Hitchcock (1999).\nHaving refused a CBE in 1962, Hitchcock was appointed a Knight Commander of the Most Excellent Order of the British Empire (KBE) in the 1980 New Year Honours. He was too ill to travel to London\u2014he had a pacemaker and was being given cortisone injections for his arthritis\u2014so on 3 January 1980 the British consul general presented him with the papers at Universal Studios. Asked by a reporter after the ceremony why it had taken the Queen so long, Hitchcock quipped, \"I suppose it was a matter of carelessness.\" Cary Grant, Janet Leigh and others attended a luncheon afterwards.\nHis last public appearance was on 16 March 1980, when he introduced the next year's winner of the American Film Institute award. He died of kidney failure the following month, on 29 April, in his Bel Air home. Donald Spoto, one of Hitchcock's biographers, wrote that Hitchcock had declined to see a priest, but according to Jesuit priest Mark Henninger, he and another priest, Tom Sullivan, celebrated Mass at the filmmaker's home, and Sullivan heard his confession. Hitchcock was survived by his wife and daughter. His funeral was held at Good Shepherd Catholic Church in Beverly Hills on 30 April, after which his body was cremated. His remains were scattered over the Pacific Ocean on 10 May 1980.\n\nFilmmaking\nStyle and themes\nThe \"Hitchcockian\" style includes the use of editing and camera movement to mimic a person's gaze, thereby turning viewers into voyeurs, and framing shots to maximise anxiety and fear. The film critic Robin Wood wrote that the meaning of a Hitchcock film \"is there in the method, in the progression from shot to shot. A Hitchcock film is an organism, with the whole implied in every detail and every detail related to the whole.\"\nHitchcock's film production career evolved from small-scale silent films to financially significant sound films. Hitchcock remarked that he was influenced by early filmmakers George M\u00e9li\u00e8s, D. W. Griffith and Alice Guy-Blach\u00e9. His silent films between 1925 and 1929 were in the crime and suspense genres, but also included melodramas and comedies. Whilst visual storytelling was pertinent during the silent era, even after the arrival of sound, Hitchcock still relied on visuals in cinema; he referred to this emphasis on visual storytelling as \"pure cinema\". In Britain, he honed his craft so that by the time he moved to Hollywood, the director had perfected his style and camera techniques. Hitchcock later said that his British work was the \"sensation of cinema\", whereas the American phase was when his \"ideas were fertilised\". Scholar Robin Wood writes that the director's first two films, The Pleasure Garden and The Mountain Eagle, were influenced by German Expressionism. Afterward, he discovered Soviet cinema, and Sergei Eisenstein's and Vsevolod Pudovkin's theories of montage. 1926's The Lodger was inspired by both German and Soviet aesthetics, styles which solidified the rest of his career. Although Hitchcock's work in the 1920s found some success, several British reviewers criticised Hitchcock's films for being unoriginal and conceited. Raymond Durgnat opined that Hitchcock's films were carefully and intelligently constructed, but thought they can be shallow and rarely present a \"coherent worldview\".\n\nEarning the title \"Master of Suspense\", the director experimented with ways to generate tension in his work. He said, \"My suspense work comes out of creating nightmares for the audience. And I play with an audience. I make them gasp and surprise them and shock them. When you have a nightmare, it's awfully vivid if you're dreaming that you're being led to the electric chair. Then you're as happy as can be when you wake up because you're relieved.\" During filming of North by Northwest, Hitchcock explained his reasons for recreating the set of Mount Rushmore: \"The audience responds in proportion to how realistic you make it. One of the dramatic reasons for this type of photography is to get it looking so natural that the audience gets involved and believes, for the time being, what's going on up there on the screen.\" In a 1963 interview with Italian journalist Oriana Fallaci, Hitchcock was asked how in spite of appearing to be a pleasant, innocuous man, he seemed to enjoy making films involving suspense and terrifying crime. He responded:I'm English. The English use a lot of imagination with their crimes. I don't get such a kick out of anything as much as out of imagining a crime. When I'm writing a story and I come to a crime, I think happily: now wouldn't it be nice to have him die like this? And then, even more happily, I think: at this point people will start yelling. It must be because I spent three years studying with the Jesuits. They used to terrify me to death, with everything, and now I'm getting my own back by terrifying other people.\n\nHitchcock's films, from the silent to the sound era, contained a number of recurring themes that he is famous for. His films explored audience as a voyeur, notably in Rear Window, Marnie and Psycho. He understood that human beings enjoy voyeuristic activities and made the audience participate in it through the character's actions. Of his fifty-three films, eleven revolved around stories of mistaken identity, where an innocent protagonist is accused of a crime and is pursued by police. In most cases, it is an ordinary, everyday person who finds themselves in a dangerous situation. Hitchcock told Truffaut: \"That's because the theme of the innocent man being accused, I feel, provides the audience with a greater sense of danger. It's easier for them to identify with him than with a guilty man on the run.\" One of his constant themes was the struggle of a personality torn between \"order and chaos\"; known as the notion of \"double\", which is a comparison or contrast between two characters or objects: the double representing a dark or evil side.\nAccording to Robin Wood, Hitchcock retained a feeling of ambivalence towards homosexuality, despite working with gay actors throughout his career. Donald Spoto suggests that Hitchcock's sexually repressive childhood may have contributed to his exploration of deviancy. During the 1950s, the Motion Picture Production Code prohibited direct references to homosexuality but the director was known for his subtle references, and pushing the boundaries of the censors. Moreover, Shadow of a Doubt has a double incest theme through the storyline, expressed implicitly through images. Author Jane Sloan argues that Hitchcock was drawn to both conventional and unconventional sexual expression in his work, and the theme of marriage was usually presented in a \"bleak and skeptical\" manner. It was also not until after his mother's death in 1942, that Hitchcock portrayed motherly figures as \"notorious monster-mothers\". The espionage backdrop, and murders committed by characters with psychopathic tendencies were common themes too. In Hitchcock's depiction of villains and murderers, they were usually charming and friendly, forcing viewers to identify with them. The director's strict childhood and Jesuit education may have led to his distrust of authority figures such as policemen and politicians; a theme which he has explored. Also, he used the \"MacGuffin\"\u2014the use of an object, person or event to keep the plot moving along even if it was non-essential to the story.\nHitchcock appears briefly in most of his own films. For example, he is seen struggling to get a double bass onto a train (Strangers on a Train), walking dogs out of a pet shop (The Birds), fixing a neighbour's clock (Rear Window), as a shadow (Family Plot), sitting at a table in a photograph (Dial M for Murder), and riding a bus (North by Northwest, To Catch a Thief).\n\nRepresentation of women\nHitchcock's portrayal of women has been the subject of much scholarly debate. Bidisha wrote in The Guardian in 2010: \"There's the vamp, the tramp, the snitch, the witch, the slink, the double-crosser and, best of all, the demon mommy. Don't worry, they all get punished in the end.\" In a widely cited essay in 1975, Laura Mulvey introduced the idea of the male gaze; the view of the spectator in Hitchcock's films, she argued, is that of the heterosexual male protagonist. \"The female characters in his films reflected the same qualities over and over again\", Roger Ebert wrote in 1996: \"They were blonde. They were icy and remote. They were imprisoned in costumes that subtly combined fashion with fetishism. They mesmerised the men, who often had physical or psychological handicaps. Sooner or later, every Hitchcock woman was humiliated.\"\n\nHitchcock's films often feature characters struggling in their relationships with their mothers, such as Norman Bates in Psycho. In North by Northwest, Roger Thornhill (Cary Grant) is an innocent man ridiculed by his mother for insisting that shadowy, murderous men are after him. In The Birds, the Rod Taylor character, an innocent man, finds his world under attack by vicious birds, and struggles to free himself from a clinging mother (Jessica Tandy). The killer in Frenzy has a loathing of women but idolises his mother. The villain Bruno in Strangers on a Train hates his father, but has an incredibly close relationship with his mother (played by Marion Lorne). Sebastian (Claude Rains) in Notorious has a clearly conflicting relationship with his mother, who is (rightly) suspicious of his new bride, Alicia Huberman (Ingrid Bergman).\n\nRelationship with actors\nHitchcock became known for having remarked that \"actors should be treated like cattle\". During the filming of Mr. & Mrs. Smith (1941), Carole Lombard brought three cows onto the set wearing the name tags of Lombard, Robert Montgomery, and Gene Raymond, the stars of the film, to surprise him. In an episode of The Dick Cavett Show, originally broadcast on 8 June 1972,  Dick Cavett stated as fact that Hitchcock had once called actors cattle. Hitchcock responded by saying that, at one time, he had been accused of calling actors cattle. \"I said that I would never say such an unfeeling, rude thing about actors at all. What I probably said, was that all actors should be treated like cattle...In a nice way of course.\" He then described Carole Lombard's joke, with a smile.\nHitchcock believed that actors should concentrate on their performances and leave work on script and character to the directors and screenwriters. He told Bryan Forbes in 1967: \"I remember discussing with a method actor how he was taught and so forth. He said, 'We're taught using improvisation. We are given an idea and then we are turned loose to develop in any way we want to.' I said, 'That's not acting. That's writing.'\"\nRecalling their experiences on Lifeboat for Charles Chandler, author of It's Only a Movie: Alfred Hitchcock A Personal Biography, Walter Slezak said that Hitchcock \"knew more about how to help an actor than any director I ever worked with\", and Hume Cronyn dismissed the idea that Hitchcock was not concerned with his actors as \"utterly fallacious\", describing at length the process of rehearsing and filming Lifeboat.\nCritics observed that, despite his reputation as a man who disliked actors, actors who worked with him often gave brilliant performances. He used the same actors in many of his films; Cary Grant and James Stewart both worked with Hitchcock four times, and Ingrid Bergman and Grace Kelly three. James Mason said that Hitchcock regarded actors as \"animated props\". For Hitchcock, the actors were part of the film's setting. He told Fran\u00e7ois Truffaut: \"The chief requisite for an actor is the ability to do nothing well, which is by no means as easy as it sounds. He should be willing to be used and wholly integrated into the picture by the director and the camera. He must allow the camera to determine the proper emphasis and the most effective dramatic highlights.\"\n\nWriting, storyboards and production\nHitchcock planned his scripts in detail with his writers. In Writing with Hitchcock (2001), Steven DeRosa noted that Hitchcock supervised them through every draft, asking that they tell the story visually. Hitchcock told Roger Ebert in 1969:\n\nOnce the screenplay is finished, I'd just as soon not make the film at all. All the fun is over. I have a strongly visual mind. I visualize a picture right down to the final cuts. I write all this out in the greatest detail in the script, and then I don't look at the script while I'm shooting. I know it off by heart, just as an orchestra conductor needs not look at the score. It's melancholy to shoot a picture. When you finish the script, the film is perfect. But in shooting it you lose perhaps 40 per cent of your original conception.\nHitchcock's films were extensively storyboarded to the finest detail. He was reported to have never even bothered looking through the viewfinder, since he did not need to, although in publicity photos he was shown doing so. He also used this as an excuse to never have to change his films from his initial vision. If a studio asked him to change a film, he would claim that it was already shot in a single way, and that there were no alternative takes to consider.\n\nThis view of Hitchcock as a director who relied more on pre-production than on the actual production itself has been challenged by Bill Krohn, the American correspondent of French film magazine Cahiers du Cin\u00e9ma, in his book Hitchcock at Work. After investigating script revisions, notes to other production personnel written by or to Hitchcock, and other production material, Krohn observed that Hitchcock's work often deviated from how the screenplay was written or how the film was originally envisioned. He noted that the myth of storyboards in relation to Hitchcock, often regurgitated by generations of commentators on his films, was to a great degree perpetuated by Hitchcock himself or the publicity arm of the studios. For example, the celebrated crop-spraying sequence of North by Northwest was not storyboarded at all. After the scene was filmed, the publicity department asked Hitchcock to make storyboards to promote the film, and Hitchcock in turn hired an artist to match the scenes in detail.\nEven when storyboards were made, scenes that were shot differed from them significantly. Krohn's analysis of the production of Hitchcock classics like Notorious reveals that Hitchcock was flexible enough to change a film's conception during its production. Another example Krohn notes is the American remake of The Man Who Knew Too Much, whose shooting schedule commenced without a finished script and moreover went over schedule, something that, as Krohn notes, was not an uncommon occurrence on many of Hitchcock's films, including Strangers on a Train and Topaz. While Hitchcock did do a great deal of preparation for all his films, he was fully cognisant that the actual film-making process often deviated from the best-laid plans and was flexible to adapt to the changes and needs of production as his films were not free from the normal hassles faced and common routines used during many other film productions.\n\nKrohn's work also sheds light on Hitchcock's practice of generally shooting in chronological order, which he notes sent many films over budget and over schedule and, more importantly, differed from the standard operating procedure of Hollywood in the Studio System Era. Equally important is Hitchcock's tendency to shoot alternative takes of scenes. This differed from coverage in that the films were not necessarily shot from varying angles so as to give the editor options to shape the film how they chose (often under the producer's aegis). Rather they represented Hitchcock's tendency to give himself options in the editing room, where he would provide advice to his editors after viewing a rough cut of the work.\nAccording to Krohn, this and a great deal of other information revealed through his research of Hitchcock's personal papers, script revisions and the like refute the notion of Hitchcock as a director who was always in control of his films, whose vision of his films did not change during production, which Krohn notes has remained the central long-standing myth of Alfred Hitchcock. Both his fastidiousness and attention to detail also found their way into each film poster for his films. Hitchcock preferred to work with the best talent of his day\u2014film poster designers such as Bill Gold and Saul Bass\u2014who would produce posters that accurately represented his films.\n\nLegacy\nAwards and honours\nHitchcock was inducted into the Hollywood Walk of Fame on 8 February 1960 with two stars: one for television and a second for motion pictures. In 1978, John Russell Taylor described him as \"the most universally recognizable person in the world\" and \"a straightforward middle-class Englishman who just happened to be an artistic genius\". In 2002, MovieMaker named him the most influential director of all time, and a 2007 The Daily Telegraph critics' poll ranked him Britain's greatest director. David Gritten, the newspaper's film critic, wrote: \"Unquestionably the greatest filmmaker to emerge from these islands, Hitchcock did more than any director to shape modern cinema, which would be utterly different without him. His flair was for narrative, cruelly withholding crucial information (from his characters and from us) and engaging the emotions of the audience like no one else.\" In 1992, the Sight & Sound Critics' Poll ranked Hitchcock at No. 4 in its list of \"Top 10 Directors\" of all time. In 2002, Hitchcock was ranked second in the critics' top ten poll and fifth in the directors' top ten poll in the list of \"The Greatest Directors of All Time\" compiled by Sight & Sound. Hitchcock was voted the \"Greatest Director of 20th Century\" in a poll conducted by Japanese film magazine kinema Junpo. In 1996, Entertainment Weekly ranked Hitchcock at No. 1 in its \"50 Greatest Directors\" list. Hitchcock was ranked at No. 2 on Empire's \"Top 40 Greatest Directors of All-Time\" list in 2005. In 2007, Total Film ranked Hitchcock at No. 1 on its \"100 Greatest Film Directors Ever\" list.\n\nHe won two Golden Globes, eight Laurel Awards, and five lifetime achievement awards, including the first BAFTA Academy Fellowship Award in 1971, and, in 1979, an AFI Life Achievement Award. He was nominated five times for an Academy Award for Best Director. Rebecca, nominated for eleven Oscars, won the Academy Award for Best Picture of 1940; another Hitchcock film, Foreign Correspondent, was also nominated that year. By 2021, nine of his films had been selected for preservation by the US National Film Registry: Rebecca (1940; inducted 2018), Shadow of a Doubt (1943; inducted 1991), Notorious (1946; inducted 2006), Strangers on a Train (1951; inducted 2021), Rear Window (1954; inducted 1997), Vertigo (1958; inducted 1989), North by Northwest (1959; inducted 1995), Psycho (1960; inducted 1992) and The Birds (1963; inducted 2016).\nIn 2001, a series of 17 mosaics of Hitchcock's life and work, which are located in Leytonstone tube station in the London Underground, was commissioned by the London Borough of Waltham Forest. In 2012, Hitchcock was selected by artist Sir Peter Blake, author of the Beatles' Sgt. Pepper's Lonely Hearts Club Band album cover, to appear in a new version of the cover, along with other British cultural figures, and he was featured that year in a BBC Radio 4 series, The New Elizabethans, as someone \"whose actions during the reign of Elizabeth II have had a significant impact on lives in these islands and given the age its character\". In June 2013 nine restored versions of Hitchcock's early silent films, including The Pleasure Garden (1925), were shown at the Brooklyn Academy of Music's Harvey Theatre; known as \"The Hitchcock 9\", the travelling tribute was organised by the British Film Institute.\n\nArchives\nThe Alfred Hitchcock Collection is housed at the Academy Film Archive in Hollywood, California. It includes home movies, 16mm film shot on the set of Blackmail (1929) and Frenzy (1972), and the earliest known colour footage of Hitchcock. The Academy Film Archive has preserved many of his home movies. In 1984, Pat Hitchcock donated her father's papers to the Academy's Margaret Herrick Library. The David O. Selznick and the Ernest Lehman collections housed at the Harry Ransom Humanities Research Center in Austin, Texas, contain material related to Hitchcock's work on the production of The Paradine Case, Rebecca, Spellbound, North by Northwest and Family Plot.\n\nHitchcock portrayals\nAnthony Hopkins in Hitchcock (2012)\nToby Jones in The Girl (2012)\nRoger Ashton-Griffiths in Grace of Monaco (2014)\nEpicLLOYD in the YouTube comedy series Epic Rap Battles of History (2014)\n\nFilmography\nFilms\nSilent films\n\nSound films\n\nSee also\nAlfred Hitchcock's unrealized projects\nList of cameo appearances by Alfred Hitchcock\nList of film director and actor collaborations\nRemakes of films by Alfred Hitchcock\n\nNotes and sources\nNotes\nReferences\nWorks cited\nBiographies (chronological)\n\nMiscellaneous\n\nFurther reading\nArticles\nHitchcock's Style \u2013 BFI Screenonline\nAlfred Hitchcock: England's Biggest and Best Director Goes to Hollywood \u2013 Life, 20 November 1939, p. 33-43\nAlfred Hitchcock Now Says Actors Are Children, Not Cattle \u2013 Boston Globe, 1 June 1958, p. A-11\n'Twas Alfred Hitchcock Week in London \u2013 Variety, 17 August 1966, p. 16\n\nBooks\nExternal links\n\nAlfred Hitchcock at IMDb \nAlfred Hitchcock at AllMovie \nAlfred Hitchcock at the BFI's Screenonline\nAlfred Hitchcock at the British Film Institute\nAlfred Hitchcock at the TCM Movie Database \nPortraits of Alfred Hitchcock at the National Portrait Gallery, London \nTalking About Alfred Hitchcock at The Interviews: An Oral History of Television","6":"Anticholinergics (anticholinergic agents) are substances that block the action of the acetylcholine (ACh) neurotransmitter at synapses in the central and peripheral nervous system.\nThese agents inhibit the parasympathetic nervous system by selectively blocking the binding of ACh to its receptor in nerve cells. The nerve fibers of the parasympathetic system are responsible for the involuntary movement of smooth muscles present in the gastrointestinal tract, urinary tract, lungs, sweat glands, and many other parts of the body.\nIn broad terms, anticholinergics are divided into two categories in accordance with their specific targets in the central and peripheral nervous system and at the neuromuscular junction: antimuscarinic agents and antinicotinic agents (ganglionic blockers, neuromuscular blockers).\nThe term \"anticholinergic\" is typically used to refer to antimuscarinics which competitively inhibit the binding of ACh to muscarinic acetylcholine receptors; such agents do not antagonize the binding at nicotinic acetylcholine receptors at the neuromuscular junction, although the term is sometimes used to refer to agents which do so.\n\nMedical uses\nAnticholinergic drugs are used to treat a variety of conditions:\n\nDizziness (including vertigo and motion sickness-related symptoms)\nExtrapyramidal symptoms, a potential side-effect of antipsychotic medications\nGastrointestinal disorders (e.g., peptic ulcers, diarrhea, pylorospasm, diverticulitis, ulcerative colitis, nausea, and vomiting)\nGenitourinary disorders (e.g., cystitis, urethritis, and prostatitis)\nInsomnia, although usually only on a short-term basis\nRespiratory disorders (e.g., asthma, chronic bronchitis, and chronic obstructive pulmonary disease [COPD])\nSinus bradycardia due to a hypersensitive vagus nerve\nOrganophosphate based nerve agent poisoning, such as VX, sarin, tabun, and soman (atropine is favoured in conjunction with an oxime, usually pralidoxime)\nAnticholinergics generally have antisialagogue effects (decreasing saliva production), and most produce some level of sedation, both being advantageous in surgical procedures.\nUntil the beginning of the 20th century, anticholinergic drugs were widely used to treat psychiatric disorders.\n\nPhysiological effects\nEffects of anticholinergic drugs include:\n\nDelirium (often with hallucinations and delusions indistinguishable from reality)\nOcular symptoms (from eye drops): mydriasis, pupil dilation, and acute angle-closure glaucoma in those with shallow anterior chamber\nAnhidrosis, dry mouth, dry skin\nFever\nConstipation\nTachycardia\nUrinary retention\nCutaneous vasodilation\nClinically the most significant feature is delirium, particularly in the elderly, who are most likely to be affected by the toxidrome.\n\nSide effects\nLong-term use may increase the risk of both cognitive and physical decline. It is unclear whether they affect the risk of death generally. However, in older adults they do appear to increase the risk of death.\nPossible effects of anticholinergics include:\n\nPossible effects in the central nervous system resemble those associated with delirium, and may include:\n\nOlder patients are at a higher risk of experiencing CNS side effects. The link possible between anticholinergic medication use and cognitive decline\/dementia has been noted in weaker observational studies. Although there is no strong evidence from randomized controlled trials to suggest that these medications should be avoided, clinical guidelines suggest that a consideration be made to decrease the use of these medications if safe to do so and the use of these medications be carefully considered to reduce any possible adverse effects including cognitive decline.\n\nToxicity\nAn acute anticholinergic syndrome is reversible and subsides once all of the causative agents have been excreted. Reversible acetylcholinesterase inhibitor agents such as physostigmine can be used as an antidote in life-threatening cases. Wider use is discouraged due to the significant side effects related to cholinergic excess including seizures, muscle weakness, bradycardia, bronchoconstriction, lacrimation, salivation, bronchorrhea, vomiting, and diarrhea. Even in documented cases of anticholinergic toxicity, seizures have been reported after the rapid administration of physostigmine. Asystole has occurred after physostigmine administration for tricyclic antidepressant overdose, so a conduction delay (QRS > 0.10 second) or suggestion of tricyclic antidepressant ingestion is generally considered a contraindication to physostigmine administration.\n\nPharmacology\nAnticholinergics are classified according to the receptors that are affected:\n\nAntimuscarinic agents operate on the muscarinic acetylcholine receptors. The majority of anticholinergic drugs are antimuscarinics.\nAntinicotinic agents operate on the nicotinic acetylcholine receptors. The majority of these are non-depolarising skeletal muscle relaxants for surgical use that are structurally related to curare. Several are depolarizing agents.\n\nExamples\nExamples of common anticholinergics:\n\nAntidotes\nPhysostigmine is one of only a few drugs that can be used as an antidote for anticholinergic poisoning. Nicotine also counteracts anticholinergics by activating nicotinic acetylcholine receptors. Caffeine (although an adenosine receptor antagonist) can counteract the anticholinergic symptoms by reducing sedation and increasing acetylcholine activity, thereby causing alertness and arousal.\n\nPsychoactive uses\nWhen a significant amount of an anticholinergic is taken into the body, a toxic reaction known as acute anticholinergic syndrome may result. This may happen accidentally or intentionally as a consequence of either recreational or entheogenic drug use, though many users find the side effects to be exceedingly unpleasant and not worth the recreational effects they experience. In the context of recreational use, anticholinergics are often called deliriants.\n\nPlant sources\nThe most common plants containing anticholinergic alkaloids (including atropine, scopolamine, and hyoscyamine among others) are:\n\nAtropa belladonna (deadly nightshade)\nBrugmansia species\nDatura species\nGarrya species\nHyoscyamus niger (henbane)\nMandragora officinarum (mandrake)\n\nUse as a deterrent\nSeveral narcotic and opiate-containing drug preparations, such as those containing  hydrocodone and codeine are combined with an anticholinergic agent to deter intentional misuse. Examples include hydrocodone\/homatropine (Tussigon, Hydromet, Hycodan), diphenoxylate\/atropine (Lomotil), and hydrocodone polistirex\/chlorpheniramine polistirex (Tussionex Pennkinetic, TussiCaps). However, it is noted that opioid\/antihistamine combinations are used clinically for their synergistic effect in the management of pain and maintenance of dissociative anesthesia (sedation) in such preparations as meperidine\/promethazine (Mepergan) and dipipanone\/cyclizine (Diconal), which act as strong anticholinergic agents.\n\n\n== References ==","7":"An antiemetic is a drug that is effective against vomiting and nausea. Antiemetics are typically used to treat motion sickness and the side effects of opioid analgesics, general anaesthetics, and chemotherapy directed against cancer. They may be used for severe cases of gastroenteritis, especially if the patient is dehydrated.\nSome antiemetics previously thought to cause birth defects appear safe for use by pregnant women in the treatment of morning sickness and the more serious hyperemesis gravidarum.\n\nTypes\n5-HT3 receptor antagonists block serotonin receptors in the central nervous system and gastrointestinal tract. As such, they can be used to treat post-operative and cytotoxic drug nausea & vomiting.  However, they can also cause constipation or diarrhea, dry mouth, and fatigue.\nDolasetron (Anzemet) can be administered in tablet form or in an injection.\nGranisetron (Kytril, Sancuso) can be administered in tablet (Kytril), oral solution (Kytril), injection (Kytril), or in a single transdermal patch to the upper arm (SANCUSO).\nOndansetron (Zofran) is administered in an oral tablet form, orally dissolving tablet form, orally dissolving film, sublingual, or in an IV\/IM injection.\nTropisetron (Setrovel, Navoban) can be administered in oral capsules or in injection form.\nPalonosetron (Aloxi) can be administered in an injection or in oral capsules.\nDopamine antagonists act on the brainstem and are used to treat nausea and vomiting associated with cancer, radiation sickness, opioids, cytotoxic drugs and general anaesthetics.  Side effects include muscle spasms and restlessness.\nAmisulpride (Barhemsys), administered by intravenous injection.\nDomperidone (Motilium)\nDroperidol\nOlanzapine (Zyprexa)\nHaloperidol (limited in usefulness by extra-pyramidal and sedative side-effects)\nAlizapride\nProchlorperazine (Compazine, Stemzine, Buccastem, Stemetil, Phenotil)\nChlorpromazine (Use limited by sedating properties)\nMetoclopramide\nNK1 receptor antagonist\nAprepitant (Emend) is a commercially available NK1 receptor antagonist\nCasopitant is an investigational NK1 receptor antagonist\nRolapitant (Varubi) another recently approved drug from this class\nAntihistamines (H1 histamine receptor antagonists) are effective in many conditions, including motion sickness, morning sickness in pregnancy, and to combat opioid nausea. H1 receptors in central areas include area postrema and vomiting center in the vestibular nucleus. Also, many of the antihistamines listed here also block muscarinic acetylcholine receptors.\nCinnarizine (UK only)\nCyclizine\nDiphenhydramine (Benadryl)\nDimenhydrinate (Gravol, Dramamine)\nDoxylamine (Bonjesta, Unisom)\nMirtazapine (Remeron) is an antidepressant that also has antiemetic effects. It is a potent histamine H1 receptor antagonist, Ki=1.6 nM, and also exhibits notable 5-HT3 antagonism.\nMeclizine (Bonine, Antivert)\nPromethazine (Pentazine, Phenergan, Promacot) can be administered via a rectal suppository, intravenous injection, oral tablet or oral suspension for adults and children over 2 years of age.\nHydroxyzine (Vistaril)\nCannabinoids are used in patients with cachexia, cytotoxic nausea, and vomiting, or who are unresponsive to other agents. These may cause changes in perception, dizziness, and loss of coordination.\nCannabis, also known as medical marijuana in the United States, is a Schedule I drug.\nNabilone\nDronabinol (Marinol\/Syndros) is a Schedule II drug in the U.S. when in an oral solution (Syndros), and Schedule III when in sesame oil and encapsulated in a soft gelatin capsule (Marinol).\nSome synthetic cannabinoids such as Nabilone (Cesamet) or the JWH series.\nSativex is an oral spray containing THC and CBD.  It is currently legal in Canada and a few countries in Europe and the US as of June 25, 2018 \nBenzodiazepines (GABA receptor positive allosteric modulators)\nMidazolam (Versed) is given at the onset of anesthesia and has been shown in recent trials to be as effective as ondansetron, but most effective when used in combination with ondansetron.\nLorazepam (Ativan) is said to be very good as an adjunct treatment for nausea along with first line medications such as Compazine. \nAnticholinergics\nHyoscine (also known as scopolamine)\nAtropine\nSteroids\nDexamethasone (Decadron) is given in low dose at the onset of a general anesthetic as an effective antiemetic. It is also used in chemotherapy as a single drug as well as with other antiemetics such as 5-HT3 receptor antagonists and NK1 receptor antagonist, but the specific mechanism of action is not fully understood.\nOther\nTrimethobenzamide is thought to work on the CTZ\nGinger contains 5-HT3 antagonists gingerols, shogaols, and galanolactone. Preliminary clinical data suggests ginger may be effective for treatment of nausea and\/or vomiting in a number of settings.\nEmetrol is also claimed to be an effective antiemetic.\nPropofol is given intravenously. It has been used in an acute care setting in hospital as a rescue therapy for emesis.\nPeppermint is claimed to help nausea or stomach pain when added into a tea or peppermint candies.\nMuscimol is purported to have antiemetic activity.\nAjwain is purported to be antiemetic. It is a popular spice in India, Ethiopia and Eritrea.\n\nSee also\nCancer and nausea\nEmetic \u2013 substances that induce nausea and vomiting\n\n\n== References ==","8":"Antihistamines are drugs which treat allergic rhinitis, common cold, influenza, and other allergies.  Typically, people take antihistamines as an inexpensive, generic (not patented) drug that can be bought without a prescription and provides relief from nasal congestion, sneezing, or hives caused by pollen, dust mites, or animal allergy with few side effects. Antihistamines are usually for short-term treatment. Chronic allergies increase the risk of health problems which antihistamines might not treat, including asthma, sinusitis, and lower respiratory tract infection. Consultation of a medical professional is recommended for those who intend to take antihistamines for longer-term use.\nAlthough the general public typically uses the word \"antihistamine\" to describe drugs for treating allergies, physicians and scientists use the term to describe a class of drug that opposes the activity of histamine receptors in the body. In this sense of the word, antihistamines are subclassified according to the histamine receptor that they act upon. The two largest classes of antihistamines are H1-antihistamines and H2-antihistamines.\nH1-antihistamines work by binding to histamine H1 receptors in mast cells, smooth muscle, and endothelium in the body as well as in the tuberomammillary nucleus in the brain. Antihistamines that target the histamine H1-receptor are used to treat allergic reactions in the nose (e.g., itching, runny nose, and sneezing). In addition, they may be used to treat insomnia, motion sickness, or vertigo caused by problems with the inner ear. H2-antihistamines bind to histamine H2 receptors in the upper gastrointestinal tract, primarily in the stomach. Antihistamines that target the histamine H2-receptor are used to treat gastric acid conditions (e.g., peptic ulcers and acid reflux). Other antihistamines also target H3 receptors and H4 receptors.\nHistamine receptors exhibit constitutive activity, so antihistamines can function as either a neutral receptor antagonist or an inverse agonist at histamine receptors. Only a few currently marketed H1-antihistamines are known to function as antagonists.\n\nMedical uses\nHistamine makes blood vessels more permeable (vascular permeability), causing fluid to escape from capillaries into tissues, which leads to the classic symptoms of an allergic reaction \u2014 a runny nose and watery eyes.  Histamine also promotes angiogenesis.\nAntihistamines suppress the histamine-induced wheal response (swelling) and flare response (vasodilation) by blocking the binding of histamine to its receptors or reducing histamine receptor activity on nerves, vascular smooth muscle, glandular cells, endothelium, and mast cells. Antihistamines can also help correct Eustachian Tube dysfunction, thereby helping correct problems such as muffled hearing, fullness in the ear and even tinnitus.\nItching, sneezing, and inflammatory responses are suppressed by antihistamines that act on H1-receptors. In 2014, antihistamines such as desloratadine were found to be effective to complement standardized treatment of acne due to their anti-inflammatory properties and their ability to suppress sebum production.\n\nTypes\nH1-antihistamines\nH1-antihistamines refer to compounds that inhibit the activity of the H1 receptor. Since the H1 receptor exhibits constitutive activity, H1-antihistamines can be either neutral receptor antagonists or inverse agonists.  Normally, histamine binds to the H1 receptor and heightens the receptor's activity; the receptor antagonists work by binding to the receptor and blocking the activation of the receptor by histamine; by comparison, the inverse agonists bind to the receptor and both block the binding of histamine, and reduce its constitutive activity, an effect which is opposite to histamine's. Most antihistamines are inverse agonists at the H1 receptor, but it was previously thought that they were antagonists.\nClinically, H1-antihistamines are used to treat allergic reactions and mast cell-related disorders. Sedation is a common side effect of H1-antihistamines that readily cross the blood\u2013brain barrier; some of these drugs, such as diphenhydramine and doxylamine, may therefore be used to treat insomnia. H1-antihistamines can also reduce inflammation, since the expression of NF-\u03baB, the transcription factor the regulates inflammatory processes, is promoted by both the receptor's constitutive activity and agonist (i.e., histamine) binding at the H1 receptor.\nA combination of these effects, and in some cases metabolic ones as well, lead to most first-generation antihistamines having analgesic-sparing (potentiating) effects on opioid analgesics and to some extent with non-opioid ones as well.  The most common antihistamines utilized for this purpose include hydroxyzine, promethazine (enzyme induction especially helps with codeine and similar prodrug opioids), phenyltoloxamine, orphenadrine, and tripelennamine; some may also have intrinsic analgesic properties of their own, orphenadrine being an example.\nSecond-generation antihistamines cross the blood\u2013brain barrier to a much lesser extent than the first-generation antihistamines. They minimize sedatory effects due to their focused effect on peripheral histamine receptors. However, upon high doses second-generation antihistamines will begin to act on the central nervous system and thus can induce drowsiness when ingested in higher quantity.\n\nList of H1 antagonists\/inverse agonists\nH2-antihistamines\nH2-antihistamines, like H1-antihistamines, exist as inverse agonists and neutral antagonists. They act on H2 histamine receptors found mainly in the parietal cells of the gastric mucosa, which are part of the endogenous signaling pathway for gastric acid secretion. Normally, histamine acts on H2 to stimulate acid secretion; drugs that inhibit H2 signaling thus reduce the secretion of gastric acid.\nH2-antihistamines are among first-line therapy to treat gastrointestinal conditions including peptic ulcers and gastroesophageal reflux disease.  Some formulations are available over the counter. Most side effects are due to cross-reactivity with unintended receptors. Cimetidine, for example, is notorious for antagonizing androgenic testosterone and DHT receptors at high doses.\nExamples include:\n\nH3-antihistamines\nAn H3-antihistamine is a classification of drugs used to inhibit the action of histamine at the H3 receptor. H3 receptors are primarily found in the brain and are inhibitory autoreceptors located on histaminergic nerve terminals, which modulate the release of histamine. Histamine release in the brain triggers secondary release of excitatory neurotransmitters such as glutamate and acetylcholine via stimulation of H1 receptors in the cerebral cortex. Consequently, unlike the H1-antihistamines which are sedating, H3-antihistamines have stimulant and cognition-modulating effects.\nExamples of selective H3-antihistamines include:\n\nH4-antihistamines\nH4-antihistamines inhibit the activity of the H4 receptor. Examples include:\n\nAtypical antihistamines\nHistidine decarboxylase inhibitors\nInhibit the action of histidine decarboxylase:\n\nMast cell stabilizers\nMast cell stabilizers are drugs which prevent mast cell degranulation. Examples include:\n\nHistory\nThe first H1 receptor antagonists were discovered in the 1930s and were marketed in the 1940s. Piperoxan was discovered in 1933 and was the first compound with antihistamine effects to be identified. Piperoxan and its analogues were too toxic to be used in humans. Phenbenzamine (Antergan) was the first clinically useful antihistamine and was introduced for medical use in 1942. Subsequently, many other antihistamines were developed and marketed. Diphenhydramine (Benadryl) was synthesized in 1943, tripelennamine (Pyribenzamine) was patented in 1946, and promethazine (Phenergan) was synthesized in 1947 and launched in 1949. By 1950, at least 20 antihistamines had been marketed. Chlorphenamine (Piriton), a less sedating antihistamine, was synthesized in 1951, and hydroxyzine (Atarax, Vistaril), an antihistamine used specifically as a sedative and tranquilizer, was developed in 1956. The first non-sedating antihistamine was terfenadine (Seldane) and was developed in 1973. Subsequently, other non-sedating antihistamines like loratadine (Claritin), cetirizine (Zyrtec), and fexofenadine (Allegra) were developed and introduced.\nThe introduction of the first-generation antihistamines marked the beginning of medical treatment of nasal allergies. Research into these drugs led to the discovery that they were H1 receptor antagonists and also to the development of H2 receptor antagonists, where H1-antihistamines affected the nose and the H2-antihistamines affected the stomach. This history has led to contemporary research into drugs which are H3 receptor antagonists and which affect the H4 receptor antagonists. Most people who use an H1 receptor antagonist to treat allergies use a second-generation drug.\n\nSociety and culture\nThe United States government removed two second generation antihistamines, terfenadine and astemizole, from the market based on evidence that they could cause heart problems.\n\nResearch\nNot much published research exists which compares the efficacy and safety of the various antihistamines available. The research which does exist is mostly short-term studies or studies which look at too few people to make general assumptions. Another gap in the research is in information reporting the health effects for individuals with long-term allergies who take antihistamines for a long period of time. Newer antihistamines have been demonstrated to be effective in treating hives. However, there is no research comparing the relative efficacy of these drugs.\n\nSpecial populations\nIn 2020, the UK National Health Service wrote that \"[m]ost people can safely take antihistamines\" but that \"[s]ome antihistamines may not be suitable\" for young children, the pregnant or breastfeeding, for those taking other medicines, or people with conditions \"such as heart disease, liver disease, kidney disease or epilepsy\".\nMost studies of antihistamines reported on people who are younger, so the effects on people over age 65 are not as well understood. Older people are more likely to experience drowsiness from antihistamine use than younger people. Continuous and\/or cumulative use of anticholinergic medications, including first-generation antihistamines, is associated with higher risk for cognitive decline and dementia in older people.\nAlso, most of the research has been on caucasians and other ethnic groups are not as represented in the research. The evidence does not report how antihistamines affect women differently than men. Different studies have reported on antihistamine use in children, with various studies finding evidence that certain antihistamines could be used by children 2 years of age, and other drugs being safer for younger or older children.\n\nPotential uses studied\nResearch regarding the effects of commonly used medications upon certain cancer therapies has suggested that when consumed in conjunction with immune checkpoint inhibitors some may influence the response of subjects to that particular treatment whose T-cell functions were failing in anti-tumor activity. Upon study of records in mouse studies associated with 40 common medications ranging from antibiotics, antihistamines, aspirin, and hydrocortisone, that for subjects with melanoma and lung cancers, fexofenadine, one of three medications,  along with loratadine, and cetirizine, that target histamine receptor H1 (HRH1), demonstrated significantly higher survival rates and had experienced restored T-cell anti-tumor activity, ultimately inhibiting tumor growth in the subject animals. Such results encourage further study in order to see whether results in humans is similar in combating resistance to immunotherapy.\n\nSee also\nAntileukotriene\nImmunotherapy\n\nReferences\nExternal links\nHistamine+antagonist at the U.S. National Library of Medicine Medical Subject Headings (MeSH)\nAntihistamine Archived 22 April 2017 at the Wayback Machine information at Allergy UK","9":"Anticonvulsants (also known as antiepileptic drugs, antiseizure drugs, or anti-seizure medications (ASM)) are a diverse group of pharmacological agents used in the treatment of epileptic seizures. Anticonvulsants are also increasingly being used in the treatment of bipolar disorder and borderline personality disorder, since many seem to act as mood stabilizers, and for the treatment of neuropathic pain. Anticonvulsants suppress the excessive rapid firing of neurons during seizures. Anticonvulsants also prevent the spread of the seizure within the brain.\nConventional antiepileptic drugs may block sodium channels or enhance \u03b3-aminobutyric acid (GABA) function. Several antiepileptic drugs have multiple or uncertain mechanisms of action. Next to the voltage-gated sodium channels and components of the GABA system, their targets include GABAA receptors, the GABA transporter type 1, and GABA transaminase. Additional targets include voltage-gated calcium channels, SV2A, and \u03b12\u03b4. By blocking sodium or calcium channels, antiepileptic drugs reduce the release of excitatory glutamate, whose release is considered to be elevated in epilepsy, but also that of GABA. This is probably a side effect or even the actual mechanism of action for some antiepileptic drugs, since GABA can itself, directly or indirectly, act proconvulsively. Another potential target of antiepileptic drugs is the peroxisome proliferator-activated receptor alpha.\nSome anticonvulsants have shown antiepileptogenic effects in animal models of epilepsy. That is, they either prevent the development of epilepsy or can halt or reverse the progression of epilepsy. However, no drug has been shown in human trials to prevent epileptogenesis (the development of epilepsy in an individual at risk, such as after a head injury).\n\nTerminology\nAnticonvulsants are more accurately called antiepileptic drugs (AEDs) because not every epileptic seizure involves convulsion, and vice versa, not every convulsion is caused by an epileptic seizure. They are also often referred to as antiseizure drugs because they provide symptomatic treatment only and have not been demonstrated to alter the course of epilepsy.\n\nApproval\nThe usual method of achieving approval for a drug is to show it is effective when compared against placebo, or that it is more effective than an existing drug. In monotherapy (where only one drug is taken) it is considered unethical by most to conduct a trial with placebo on a new drug of uncertain efficacy. This is because untreated epilepsy leaves the patient at significant risk of death. Therefore, almost all new epilepsy drugs are initially approved only as adjunctive (add-on) therapies. Patients whose epilepsy is uncontrolled by their medication (i.e., it is refractory to treatment) are selected to see if supplementing the medication with the new drug leads to an improvement in seizure control. Any reduction in the frequency of seizures is compared against a placebo. The lack of superiority over existing treatment, combined with lacking placebo-controlled trials, means that few modern drugs have earned FDA approval as initial monotherapy. In contrast, Europe only requires equivalence to existing treatments and has approved many more. Despite their lack of FDA approval, the American Academy of Neurology and the American Epilepsy Society still recommend a number of these new drugs as initial monotherapy.\n\nDrugs\nIn the following list, the dates in parentheses are the earliest approved use of the drug.\n\nAldehydes\nParaldehyde (1882). One of the earliest anticonvulsants. It is still used to treat status epilepticus, particularly where there are no resuscitation facilities.\n\nAromatic allylic alcohols\nStiripentol (2007). Indicated for the treatment of Dravet syndrome.\n\nBarbiturates\nBarbiturates are drugs that act as central nervous system (CNS) depressants, and by virtue of this they produce a wide spectrum of effects, from mild sedation to anesthesia. The following are classified as anticonvulsants:\n\nPhenobarbital (1912). See also the related drug primidone.\nMethylphenobarbital (1935). Known as mephobarbital in the US. No longer marketed in the UK.\nBarbexaclone (1982). Only available in some European countries.\n\nBenzodiazepines\nThe benzodiazepines are a class of drugs with hypnotic, anxiolytic, anticonvulsive, amnestic and muscle relaxant properties. Benzodiazepines act as a central nervous system depressant. The relative strength of each of these properties in any given benzodiazepine varies greatly and influences the indications for which it is prescribed. Long-term use can be problematic due to the development of tolerance to the anticonvulsant effects and dependency. Of many drugs in this class, only a few are used to treat epilepsy:\n\nClobazam (1979). Notably, used on a short-term basis around menstruation in women with catamenial epilepsy.\nClonazepam (1974).\nClorazepate (1972).\nThe following benzodiazepines are used to treat status epilepticus:\n\nDiazepam (1963). Can be given rectally by trained care-givers.\nMidazolam (N\/A). Increasingly being used as an alternative to diazepam. This water-soluble drug is squirted into the side of the mouth but not swallowed. It is rapidly absorbed by the buccal mucosa.\nLorazepam (1972). Given by injection in hospital.\nNitrazepam, temazepam, and especially nimetazepam are powerful anticonvulsant agents, however their use is rare due to an increased incidence of side effects and strong sedative and motor-impairing properties.\n\nBromides\nPotassium bromide (1857). The earliest effective treatment for epilepsy. There would not be a better drug until phenobarbital in 1912. It is still used as an anticonvulsant for dogs and cats but is no longer used in humans.\n\nCarbamates\nFelbamate (1993). This effective anticonvulsant has had its usage severely restricted due to rare but life-threatening side effects.\nCenobamate (2019).\n\nCarboxamides\nThe following are carboxamides:\n\nCarbamazepine (1963). A popular anticonvulsant that is available in generic formulations.\nOxcarbazepine (1990). A derivative of carbamazepine that has similar efficacy and is better tolerated and is also available generically.\nEslicarbazepine acetate (2009).\nPhotoswitchable analogues of carbamazepine (2024) are research compounds developed to control its pharmacological activity locally and on demand using light, with the purpose to reduce adverse systemic effects. One of these compounds (carbadiazocine, based on a bridged azobenzene) has been shown to produce analgesia with noninvasive illumination in a rat model of neuropathic pain.\n\nFatty acids\nThe following are fatty-acids:\n\nThe valproates \u2014 valproic acid, sodium valproate, and divalproex sodium (1967).\nVigabatrin (1989).\nProgabide (1987).\nTiagabine (1996).\nVigabatrin and progabide are also analogs of GABA.\n\nFructose derivatives\nTopiramate (1995).\n\nGabapentinoids\nGabapentinoids are used in epilepsy, neuropathic pain, fibromyalgia, restless leg syndrome, opioid withdrawal and generalized anxiety disorder (GAD). Gabapentinoids block voltage-gated calcium channels, mainly the N-Type, and P\/Q-type calcium channels. The following are gabapentinoids:\n\nPregabalin (2004)\nMirogabalin (2019) (Japan only)\nGabapentin (1993)\nGabapentin enacarbil (Horizant) (2011)\nGabapentin extended release (Gralise) (1996)\nGabapentinoids are analogs of GABA, but they do not act on GABA receptors. They have analgesic, anticonvulsant, and anxiolytic effects.\n\nHydantoins\nThe following are hydantoins:\n\nEthotoin (1957).\nPhenytoin (1938).\nMephenytoin.\nFosphenytoin (1996).\n\nOxazolidinediones\nThe following are oxazolidinediones:\n\nParamethadione.\nTrimethadione (1946).\nEthadione.\n\nPropionates\nBeclamide.\n\nPyrimidinediones\nPrimidone (1952).\n\nPyrrolidines\nBrivaracetam (2016).\nEtiracetam.\nLevetiracetam (1999).\nSeletracetam.\n\nSuccinimides\nThe following are succinimides:\n\nEthosuximide (1955).\nPhensuximide.\nMesuximide.\n\nSulfonamides\nAcetazolamide (1953).\nSultiame.\nMethazolamide.\nZonisamide (2000).\n\nTriazines\nLamotrigine (1990).\n\nUreas\nPheneturide.\nPhenacemide.\n\nValproylamides\nValpromide.\nValnoctamide.\n\nOther\nPerampanel.\nStiripentol.\nPyridoxine (1939).\n\nNon-pharmaceutical anticonvulsants\nThe ketogenic diet and vagus nerve stimulation are alternative treatments for epilepsy without the involvement of pharmaceuticals. The ketogenic diet consists of a high-fat, low-carbohydrate diet, and has shown good results in patients whose epilepsy has not responded to medications and who cannot receive surgery. The vagus nerve stimulator is a device that can be implanted into patients with epilepsy, especially that which originates from a specific part of the brain. However, both of these treatment options can cause severe adverse effects. Additionally, while seizure frequency typically decreases, they often do not stop entirely.\n\nTreatment guidelines\nAccording to guidelines by the American Academy of Neurology and American Epilepsy Society, mainly based on a major article review in 2004, patients with newly diagnosed epilepsy who require treatment can be initiated on standard anticonvulsants such as carbamazepine, phenytoin, valproic acid\/valproate semisodium, phenobarbital, or on the newer anticonvulsants gabapentin, lamotrigine, oxcarbazepine or topiramate. The choice of anticonvulsants depends on individual patient characteristics. Both newer and older drugs are generally equally effective in new onset epilepsy. The newer drugs tend to have fewer side effects. For newly diagnosed partial or mixed seizures, there is evidence for using gabapentin, lamotrigine, oxcarbazepine or topiramate as monotherapy. Lamotrigine can be included in the options for children with newly diagnosed absence seizures.\n\nHistory\nThe first anticonvulsant was bromide, suggested in 1857 by the British gynecologist Charles Locock who used it to treat women with \"hysterical epilepsy\" (probably catamenial epilepsy). Bromides are effective against epilepsy, and also cause impotence, which is not related to its anti-epileptic effects. Bromide also suffered from the way it affected behaviour, introducing the idea of the \"epileptic personality\" which was actually a result of medication. Phenobarbital was first used in 1912 for both its sedative and antiepileptic properties. By the 1930s, the development of animal models in epilepsy research led to the development of phenytoin by Tracy Putnam and H. Houston Merritt, which had the distinct advantage of treating epileptic seizures with less sedation. By the 1970s, a National Institutes of Health initiative, the Anticonvulsant Screening Program, headed by J. Kiffin Penry, served as a mechanism for drawing the interest and abilities of pharmaceutical companies in the development of new anticonvulsant medications.\n\nMarketing approval history\nThe following table lists anticonvulsant drugs together with the date their marketing was approved in the US, UK and France. Data for the UK and France are incomplete. The European Medicines Agency approves drugs throughout the European Union. Some of the drugs are no longer marketed.\n\nPregnancy\nMany of the commonly used anticonvulsant\/anti-seizure medications (ASMs), such as valproate, phenytoin, carbamazepine, phenobarbitol, gabapentin have been reported to cause an increased risk of birth defects including major congenital malformations such as neural tube defects. The risk of birth defects associated with taking these medications while pregnant may be dependent on the dose and on the timing of gestation (how well developed the baby is). While trying to conceive a child and during pregnancy, medical advice should be followed to optimize the management of the person's epilepsy in order to keep the person and the unborn baby safe from epileptic seizures and also ensure that the risk of birth defects due to in utero exposure of anticonvulsants is as low as possible. Use of anticonvulsant medications should be carefully monitored during use in pregnancy. For example, since the first trimester is the most susceptible period for fetal development, planning a routine antiepileptic drug dose that is safer for the first trimester could be beneficial to prevent pregnancy complications.\nValproic acid, and its derivatives such as sodium valproate and divalproex sodium, causes cognitive deficit in the child, with an increased dose causing decreased intelligence quotient and use is associated with adverse neurodevelopmental outcomes (cognitive and behavioral)  in children. On the other hand, evidence is conflicting for carbamazepine regarding any increased risk of congenital physical anomalies or neurodevelopmental disorders by intrauterine exposure. Similarly, children exposed lamotrigine or phenytoin in the womb do not seem to differ in their skills compared to those who were exposed to carbamazepine. \nThere is inadequate evidence to determine if newborns of women with epilepsy taking anticonvulsants have a substantially increased risk of hemorrhagic disease of the newborn.\nThere is little evidence to suggest that anticonvulsant\/ASM exposure through breastmilk has clinical effects on newborns. The Maternal Outcomes and Neurodevelopmental Effects of Antiepileptic Drugs (MONEAD) study showed that most blood concentrations in breastfed infants of mothers taking carbamazepine, oxcarbazepine, valproate, levetiracetam, and topiramate were quite low, especially in relationship to the mother's level and what the fetal level would have been during pregnancy. (Note: valproic acid is NOT a recommended ASM for people with epilepsy who are considering having children.) \nInfant exposure to newer ASMs (cenobamate, perampanel, brivaracetam, eslicarbazepine, rufinamide, levetiracetam, topiramate, gabapentin, oxcarbazepine, lamotrigine, and vigabatrin) via breastmilk was not associated with negative neurodevelopment (such as lower IQ and autism spectrum disorder) at 36 months.\nSeveral studies that followed children exposed to ASMs during pregnancy showed that a number of widely used ones (including lamotrigine and levetiracetam) carried a low risk of adverse neurodevelopmental outcomes (cognitive and behavioral) in children when compared to children born to mothers without epilepsy and children born to mothers taking other anti-seizure medications. Data from several pregnancy registries showed that children exposed to levetiracetam or lamotrigine during pregnancy had the lowest risk of developing major congenital malformations compared to those exposed to other ASMs. The risk of major congenital malformations for children exposed to these ASMs were within the range for children who were not exposed to any ASMs during pregnancy.\nPeople with epilepsy can have healthy pregnancies and healthy babies. However, proper planning and care is essential to minimize the risk of congenital malformations or adverse neurocognitive outcomes for the fetus while maintaining seizure control for the pregnant person with epilepsy. If possible, when planning pregnancy, people with epilepsy should switch to ASMs with the lowest teratogenic risk for major congenital malformations as well as the least risk of adverse neurodevelopmental outcomes (e.g., lower IQ or autism spectrum disorder). They should also work with their healthcare providers to identify the lowest effective ASM dosage that will maintain their seizure control while regularly checking medication levels throughout pregnancy.\nData from studies conducted on women taking antiepileptic drugs for non-epileptic reasons, including depression and bipolar disorder, show that if high doses of the drugs are taken during the first trimester of pregnancy then there is the potential of an increased risk of congenital malformations.\n\nResearch\nThe mechanism of how anticonvulsants cause birth defects is not entirely clear. During pregnancy, the metabolism of many anticonvulsants is affected. There may be an increase in the clearance and resultant decrease in the blood concentration of lamotrigine, phenytoin, and to a lesser extent carbamazepine, and possibly decreases the level of levetiracetam and the active oxcarbazepine metabolite, the monohydroxy derivative. In animal models, several anticonvulsant drugs have been demonstrated to induce neuronal apoptosis in the developing brain.\n\nReferences\nFurther reading\nAnti epileptic activity of novel substituted fluorothiazole derivatives by Devid Chutia, RGUHS\n\nExternal links\nDrug Reference for FDA Approved Epilepsy Drugs","10":"Audiometry (from Latin  aud\u012bre 'to hear' and  metria 'to measure') is a branch of audiology and the science of measuring hearing acuity for variations in sound intensity and pitch and for tonal purity, involving thresholds and differing frequencies. Typically, audiometric tests determine a subject's hearing levels with the help of an audiometer, but may also measure ability to discriminate between different sound intensities, recognize pitch, or distinguish speech from background noise. Acoustic reflex and otoacoustic emissions may also be measured. Results of audiometric tests are used to diagnose hearing loss or diseases of the ear, and often make use of an audiogram.\n\nHistory\nThe basic requirements of the field were to be able to produce a repeating sound, some way to attenuate the amplitude, a way to transmit the sound to the subject, and a means to record and interpret the subject's responses to the test.\n\nMechanical \"acuity meters\" and tuning forks\nFor many years there was desultory use of various devices capable of producing sounds of controlled intensity. The first types were clock-like, giving off air-borne sound to the tubes of a stethoscope; the sound distributor head had a valve that could be gradually closed. Another model used a tripped hammer to strike a metal rod and produce the testing sound; in another, a tuning fork was struck.\nThe first such measurement device for testing hearing was described by Wolke (1802).\n\nPure tone audiometry and audiograms\nFollowing development of the induction coil in 1849 and audio transducers (telephone) in 1876, a variety of audiometers were invented in United States and overseas. These early audiometers were known as induction-coil audiometers due to...\n\nHughes 1879\nHartmann 1878\nIn 1885, Arthur Hartmann designed an \"Auditory Chart\" which included left and right ear tuning fork representation on the x -axis and percent of hearing on the y-axis.\nIn 1899, Carl E. Seashore Prof. of Psychology at U. Iowa, United States, introduced the audiometer as an instrument to measure the \"keenness of hearing\" whether in the laboratory, schoolroom, or office of the psychologist or aurist. The instrument operated on a battery and presented a tone or a click; it had an attenuator set in a scale of 40 steps. His machine became the basis of the audiometers later manufactured at Western Electric.\n\nCordia C. Bunch 1919\nThe concept of a frequency versus sensitivity (amplitude) audiogram plot of human hearing sensitivity was conceived by German physicist Max Wien in 1903. The first vacuum tube implementations, November 1919, two groups of researchers \u2014 K.L. Schaefer and G. Gruschke, B. Griessmann and H. Schwarzkopf \u2014 demonstrated before the Berlin Oto-logical Society two instruments designed to test hearing acuity. Both were built with vacuum tubes. Their designs were characteristic of the two basic types of electronic circuits used in most electronic audio devices for the next two decades. Neither of the two devices was developed commercially for some time, although the second was to be manufactured under the name \"Otaudion.\"\nThe Western Electric 1A, developed by <who> was built in 1922 in the United States. It was not until 1922 that otolaryngologist Dr. Edmund P. Fowler, and physicists Dr. Harvey Fletcher and Robert Wegel of Western Electric Co. first employed frequency at octave intervals plotted along the x axis and intensity downward along the y-axis as a degree of hearing loss. Fletcher et al. also coined the term \"audiogram\" at that time.\nWith further technologic advances, bone conduction testing capabilities became a standard component of all Western Electric audiometers by 1928.\n\nElectrophysiologic audiometry\nIn 1967, Sohmer and Feinmesser were the first to publish auditory brainstem responses (ABR), recorded with surface electrodes in humans which showed that cochlear potentials could be obtained non-invasively.\n\nOtoacoustic audiometry\nIn 1978, David Kemp reported that sound energy produced by the ear could be detected in the ear canal\u2014otoacoustic emissions. The first commercial system for detecting and measuring otoacoustic emissions was produced in 1988.\n\nAuditory system\nComponents\nThe auditory system is composed of epithelial, osseous, vascular, neural and neocortical tissues. The anatomical divisions\nare external ear canal and tympanic membrane, middle ear, inner ear, VIII auditory nerve, and central auditory processing portions of the neocortex.\n\nHearing process\nSound waves enter the outer ear and travel through the external auditory canal until they reach the tympanic membrane, causing the membrane and the attached chain of auditory ossicles to vibrate. The motion of the stapes against the oval window sets up waves in the fluids of the cochlea, causing the basilar membrane to vibrate. This stimulates the sensory cells of the organ of Corti, atop the basilar membrane, to send nerve impulses to the central auditory processing areas of the brain, the auditory cortex, where sound is perceived and interpreted.\n\nSensory and psychodynamics of human hearing\nCocktail party effect\nUnderstanding speech\nNon-linearity\nTemporal synchronization \u2013 sound localization and echo location\nParameters of human hearing\nFrequency range\nAmplitude sensitivity\nAudiometric testing\nobjectives: integrity, structure, function, freedom from infirmity.\n\nNormative standards\nISO 7029:2000 and BS 6951\n\nTypes of audiometry\nSubjective audiometry\nSubjective audiometry requires the cooperation of the subject, and relies upon subjective responses which may both qualitative and quantitative, and involve attention (focus), reaction time, etc. \n\nDifferential testing is conducted with a low frequency (usually 512 Hz) tuning fork. They are used to assess asymmetrical hearing and air\/bone conduction differences. They are simple manual physical tests and do not result in an audiogram.\nWeber test\nBing test\nRinne test\nSchwabach test, a variant of the Rinne test\nPure tone audiometry is a standardized hearing test in which air conduction hearing thresholds in decibels (db) for a set of fixed frequencies between 250 Hz and 8,000 Hz are plotted on an audiogram for each ear independently. A separate set of measurements is made for bone conduction. There is also high frequency Pure Tone Audiometry covering the frequency range above 8000 Hz to 16,000 Hz.\nThreshold equalizing noise (TEN) test\nMasking level difference (MLD) test\nPsychoacoustic (or psychophysical) tuning curve test\nSpeech audiometry is a diagnostic hearing test designed to test word or speech recognition. It has become a fundamental tool in hearing-loss assessment. In conjunction with pure-tone audiometry, it can aid in determining the degree and type of hearing loss. Speech audiometry also provides information regarding discomfort or tolerance to speech stimuli and information on word recognition abilities. In addition, information gained by speech audiometry can help determine proper gain and maximum output of hearing aids and other amplifying devices for patients with significant hearing losses and help assess how well they hear in noise. Speech audiometry also facilitates audiological rehabilitation management.\nSpeech audiometry may include:\n\nSpeech awareness threshold\nSpeech recognition threshold\nSuprathreshold word-recognition\nSentence testing\nDichotic listening test\nLoudness levels determination\nB\u00e9k\u00e9sy audiometry, also called decay audiometry - audiometry in which the subject controls increases and decreases in intensity as the frequency of the stimulus is gradually changed so that the subject traces back and forth across the threshold of hearing over the frequency range of the test. The test is quick and reliable, so was frequently used in military and industrial contexts.\nAudiometry of children\nConditioned play audiometry\nBehavioral observation audiometry\nVisual reinforcement audiometry\n\nObjective audiometry\nObjective audiometry is based on physical, acoustic or electrophysiologic measurements and does not depend on the cooperation or subjective responses of the subject.\n\nCaloric stimulation\/reflex test uses temperature difference between hot and cold water or air delivered into the ear to test for neural damage. Caloric stimulation of the ear results in rapid side-to-side eye movements called nystagmus. Absence of nystagmus may indicate auditory nerve damage. This test will often be done as part of another test called electronystagmography.\nElectronystagmography (ENG) uses skin electrodes and an electronic recording device to measure nystagmus evoked by procedures such as caloric stimulation of the ear\nAcoustic immittance audiometry - Immittance audiometry is an objective technique which evaluates middle ear function by three procedures: static immittance, tympanometry, and the measurement of acoustic reflex threshold sensitivity. Immittance audiometry is superior to pure tone audiometry in detecting middle ear pathology.\nTympanometry\nAcoustic reflex thresholds\nAcoustic reflectometry\nwide-band absorbance audiometry also called 3D tympanometry\nEvoked potential audiometry\nN1-P2 cortical audio evoked potential (CAEP) audiometry\nABR is a neurologic tests of auditory brainstem function in response to auditory (click) stimuli.\nElectrocochleography a variant of ABR, tests the impulse transmission function of the cochlea in response to auditory (click) stimuli. It is most often used to detect endolymphatic hydrops in the diagnosis\/assessment of Meniere's disease.\nAudio steady state response (ASSR) audiometry\nVestibular evoked myogenic potential (VEMP) test, a variant of ABR that tests the integrity of the saccule\nOtoacoustic emission audiometry - this test can differentiate between the sensory and neural components of sensorineural hearing loss.\nDistortion product otoacoustic emissions (DPOAE) audiometry\nTransient evoked otoacoustic emissions (TEOAE) audiometry\nSustained frequency otoacoustic emissions (SFOAE) audiometry - At present, SFOAEs are not used clinically.\nIn situ audiometry: a technique for measuring not only the condition of the person's auditory system, but also the characteristics of sound reproduction devices, in-the-canal hearing aids, vents and sound tubes of hearing aids.\n\nAudiograms\nThe result of most audiometry is an audiogram plotting some measured dimension of hearing, either graphically or tabularly.\nThe most common type of audiogram is the result of a pure tone audiometry hearing test which plots frequency versus amplitude sensitivity thresholds for each ear along with bone conduction thresholds at 8 standard frequencies from 250 Hz to 8000 Hz. A pure tone audiometry hearing test is the gold standard for evaluation of hearing loss or disability. Other types of hearing tests also generate graphs or tables of results that may be loosely called 'audiograms', but the term is universally used to refer to the result of a pure tone audiometry hearing test.\n\nHearing assessment\nApart from testing hearing, part of the function of audiometry is in assessing or evaluating hearing from the test results. The most commonly used assessment of hearing is the determination of the threshold of audibility, i.e. the level of sound required to be just audible. This level can vary for an individual over a range of up to 5 decibels from day to day and from determination to determination, but it provides an additional and useful tool in monitoring the potential ill effects of exposure to noise. Hearing loss may be unilateral or bilateral, and bilateral hearing loss may not be symmetrical. The most common types of hearing loss, due to age and noise exposure, are usually bilateral and symmetrical.\nIn addition to the traditional audiometry, hearing assessment can be performed using a standard set of frequencies (audiogram) with mobile applications to detect possible hearing impairments.\n\nHearing loss classification\nThe primary focus of audiometry is assessment of hearing status and hearing loss, including extent, type and configuration.\n\nThere are four defined degrees of hearing loss: mild, moderate, severe and profound.\nHearing loss may be divided into four types: conductive hearing loss, sensorineural hearing loss, central auditory processing disorders, and mixed types.\nHearing loss may be unilateral or bilateral, of sudden onset or progressive, and temporary or permanent.\nHearing loss may be caused by a number of factors including heredity, congenital conditions, age-related (presbycusis) and acquired factors like noise-induced hearing loss, ototoxic chemicals and drugs, infections, and physical trauma.\n\nClinical practice\nAudiometric testing may be performed by a general practitioner medical doctor, an otolaryngologist (a specialized MD also called an ENT), a CCC-A (Certificate of Clinical Competence in Audiology) audiologist, a certified school audiometrist (a practitioner analogous to an optometrist who tests eyes), and sometimes other trained practitioners. Practitioners are certified by American Board of Audiology (ABA). Practitioners are licensed by various state boards\nregulating workplace health & safety, occupational professions, or ...\n\nSchools\nOccupational testing\nNoise-induced hearing loss\nWorkplace and environmental noise is the most prevalent cause of hearing loss in the United States and elsewhere.\n\nResearch\nComputer modeling of patterns of hearing deficit\n3D longitudinal profiles of hearing loss including age axis (presbycusis study)\n\nSee also\nHearing test\nAmerican Academy of Audiology\nInternational Society of Audiology\nWeber test\nRinne Test\nVisual reinforcement audiometry\n\n\n== References ==","11":"Aspirin, also known as acetylsalicylic acid (ASA), is a nonsteroidal anti-inflammatory drug (NSAID) used to reduce pain, fever, and\/or inflammation, and as an antithrombotic. Specific inflammatory conditions which aspirin is used to treat include Kawasaki disease, pericarditis, and rheumatic fever.\nAspirin is also used long-term to help prevent further heart attacks, ischaemic strokes, and blood clots in people at high risk. For pain or fever, effects typically begin within 30 minutes. Aspirin works similarly to other NSAIDs but also suppresses the normal functioning of platelets.\nOne common adverse effect is an upset stomach. More significant side effects include stomach ulcers, stomach bleeding, and worsening asthma. Bleeding risk is greater among those who are older, drink alcohol, take other NSAIDs, or are on other blood thinners. Aspirin is not recommended in the last part of pregnancy. It is not generally recommended in children with infections because of the risk of Reye syndrome. High doses may result in ringing in the ears.\nA precursor to aspirin found in the bark of the willow tree (genus Salix) has been used for its health effects for at least 2,400 years. In 1853, chemist Charles Fr\u00e9d\u00e9ric Gerhardt treated the medicine sodium salicylate with acetyl chloride to produce acetylsalicylic acid for the first time. Over the next 50 years, other chemists, mostly of the German company Bayer, established the chemical structure and devised more efficient production methods.:\u200a69\u201375\u200a\nAspirin is available without medical prescription as a proprietary or generic medication in most jurisdictions. It is one of the most widely used medications globally, with an estimated 40,000 tonnes (44,000 tons) (50 to 120 billion pills) consumed each year, and is on the World Health Organization's List of Essential Medicines. In 2022, it was the 36th most commonly prescribed medication in the United States, with more than 16 million prescriptions.\n\nBrand vs. generic name\nIn 1897, scientists at the Bayer company began studying acetylsalicylic acid as a less-irritating replacement medication for common salicylate medicines.:\u200a69\u201375\u200a By 1899, Bayer had named it \"Aspirin\" and was selling it around the world.\nAspirin's popularity grew over the first half of the 20th century, leading to competition between many brands and formulations. The word Aspirin was Bayer's brand name; however, their rights to the trademark were lost or sold in many countries. The name is ultimately a blend of the prefix a(cetyl) + spir Spiraea, the meadowsweet plant genus from which the acetylsalicylic acid was originally derived at Bayer + -in, the common chemical suffix.\n\nChemical properties\nAspirin decomposes rapidly in solutions of ammonium acetate or the acetates, carbonates, citrates, or hydroxides of the alkali metals. It is stable in dry air, but gradually hydrolyses in contact with moisture to acetic and salicylic acids. In solution with alkalis, the hydrolysis proceeds rapidly and the clear solutions formed may consist entirely of acetate and salicylate.\nLike flour mills, factories producing aspirin tablets must control the amount of the powder that becomes airborne inside the building, because the powder-air mixture can be explosive. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit in the United States of 5 mg\/m3 (time-weighted average). In 1989, the Occupational Safety and Health Administration (OSHA) set a legal permissible exposure limit for aspirin of 5 mg\/m3, but this was vacated by the AFL-CIO v. OSHA decision in 1993.\n\nSynthesis\nThe synthesis of aspirin is classified as an esterification reaction. Salicylic acid is treated with acetic anhydride, an acid derivative, causing a chemical reaction that turns salicylic acid's hydroxyl group into an ester group (R-OH \u2192 R-OCOCH3). This process yields aspirin and acetic acid, which is considered a byproduct of this reaction. Small amounts of sulfuric acid (and occasionally phosphoric acid) are almost always used as a catalyst. This method is commonly demonstrated in undergraduate teaching labs.\n\nReaction between acetic acid and salicylic acid can also form aspirin but this esterification reaction is reversible and the presence of water can lead to hydrolysis of the aspirin. So, an anhydrous reagent is preferred.\nReaction mechanism\n\nFormulations containing high concentrations of aspirin often smell like vinegar because aspirin can decompose through hydrolysis in moist conditions, yielding salicylic and acetic acids.\n\nPhysical properties\nAspirin, an acetyl derivative of salicylic acid, is a white, crystalline, weakly acidic substance, which melts at 136 \u00b0C (277 \u00b0F), and decomposes around 140 \u00b0C (284 \u00b0F). Its acid dissociation constant (pKa) is 3.5 at 25 \u00b0C (77 \u00b0F).\n\nPolymorphism\nPolymorphism, or the ability of a substance to form more than one crystal structure, is important in the development of pharmaceutical ingredients. Many drugs receive regulatory approval for only a single crystal form or polymorph.\nThere was only one proven polymorph Form I of aspirin, though the existence of another polymorph was debated since the 1960s, and one report from 1981 reported that when crystallized in the presence of aspirin anhydride, the diffractogram of aspirin has weak additional peaks. Though at the time it was dismissed as mere impurity, it was, in retrospect, Form II aspirin.\nForm II was reported in 2005, found after attempted co-crystallization of aspirin and levetiracetam from hot acetonitrile.\nIn form I, pairs of aspirin molecules form centrosymmetric dimers through the acetyl groups with the (acidic) methyl proton to carbonyl hydrogen bonds. In form II, each aspirin molecule forms the same hydrogen bonds, but with two neighbouring molecules instead of one. With respect to the hydrogen bonds formed by the carboxylic acid groups, both polymorphs form identical dimer structures. The aspirin polymorphs contain identical 2-dimensional sections and are therefore more precisely described as polytypes.\nPure Form II aspirin could be prepared by seeding the batch with aspirin anhydrate in 15% weight.\n\nMechanism of action\nDiscovery of the mechanism\nIn 1971, British pharmacologist John Robert Vane, then employed by the Royal College of Surgeons in London, showed aspirin suppressed the production of prostaglandins and thromboxanes. For this discovery he was awarded the 1982 Nobel Prize in Physiology or Medicine, jointly with Sune Bergstr\u00f6m and Bengt Ingemar Samuelsson.\n\nProstaglandins and thromboxanes\nAspirin's ability to suppress the production of prostaglandins and thromboxanes is due to its irreversible inactivation of the cyclooxygenase (COX; officially known as prostaglandin-endoperoxide synthase, PTGS) enzyme required for prostaglandin and thromboxane synthesis. Aspirin acts as an acetylating agent where an acetyl group is covalently attached to a serine residue in the active site of the COX enzyme (Suicide inhibition). This makes aspirin different from other NSAIDs (such as diclofenac and ibuprofen), which are reversible inhibitors.\nLow-dose aspirin use irreversibly blocks the formation of thromboxane A2 in platelets, producing an inhibitory effect on platelet aggregation during the lifetime of the affected platelet (8\u20139 days). This antithrombotic property makes aspirin useful for reducing the incidence of heart attacks in people who have had a heart attack, unstable angina, ischemic stroke or transient ischemic attack. 40 mg of aspirin a day is able to inhibit a large proportion of maximum thromboxane A2 release provoked acutely, with the prostaglandin I2 synthesis being little affected; however, higher doses of aspirin are required to attain further inhibition.\nProstaglandins, local hormones produced in the body, have diverse effects, including the transmission of pain information to the brain, modulation of the hypothalamic thermostat, and inflammation. Thromboxanes are responsible for the aggregation of platelets that form blood clots. Heart attacks are caused primarily by blood clots, and low doses of aspirin are seen as an effective medical intervention to prevent a second acute myocardial infarction.\n\nCOX-1 and COX-2 inhibition\nAt least two different types of cyclooxygenases, COX-1 and COX-2, are acted on by aspirin. Aspirin irreversibly inhibits COX-1 and modifies the enzymatic activity of COX-2. COX-2 normally produces prostanoids, most of which are proinflammatory. Aspirin-modified COX-2 (aka prostaglandin-endoperoxide synthase 2 or PTGS2) produces lipoxins, most of which are anti-inflammatory. Newer NSAID drugs, COX-2 inhibitors (coxibs), have been developed to inhibit only COX-2, with the intent to reduce the incidence of gastrointestinal side effects.\nSeveral COX-2 inhibitors, such as rofecoxib (Vioxx), have been withdrawn from the market, after evidence emerged that COX-2 inhibitors increase the risk of heart attack and stroke. Endothelial cells lining the microvasculature in the body are proposed to express COX-2, and, by selectively inhibiting COX-2, prostaglandin production (specifically, PGI2; prostacyclin) is downregulated with respect to thromboxane levels, as COX-1 in platelets is unaffected. Thus, the protective anticoagulative effect of PGI2 is removed, increasing the risk of thrombus and associated heart attacks and other circulatory problems. Since platelets have no DNA, they are unable to synthesize new COX once aspirin has irreversibly inhibited the enzyme, an important difference as compared with reversible inhibitors.\nFurthermore, aspirin, while inhibiting the ability of COX-2 to form pro-inflammatory products such as the prostaglandins, converts this enzyme's activity from a prostaglandin-forming cyclooxygenase to a lipoxygenase-like enzyme: aspirin-treated COX-2 metabolizes a variety of polyunsaturated fatty acids to hydroperoxy products which are then further metabolized to specialized proresolving mediators such as the aspirin-triggered lipoxins, aspirin-triggered resolvins, and aspirin-triggered maresins. These mediators possess potent anti-inflammatory activity. It is proposed that this aspirin-triggered transition of COX-2 from cyclooxygenase to lipoxygenase activity and the consequential formation of specialized proresolving mediators contributes to the anti-inflammatory effects of aspirin.\n\nAdditional mechanisms\nAspirin has been shown to have at least three additional modes of action. It uncouples oxidative phosphorylation in cartilaginous (and hepatic) mitochondria, by diffusing from the inner membrane space as a proton carrier back into the mitochondrial matrix, where it ionizes once again to release protons. Aspirin buffers and transports the protons. When high doses are given, it may actually cause fever, owing to the heat released from the electron transport chain, as opposed to the antipyretic action of aspirin seen with lower doses. In addition, aspirin induces the formation of NO-radicals in the body, which have been shown in mice to have an independent mechanism of reducing inflammation. This reduced leukocyte adhesion is an important step in the immune response to infection; however, evidence is insufficient to show aspirin helps to fight infection. More recent data also suggest salicylic acid and its derivatives modulate signalling through NF-\u03baB. NF-\u03baB, a transcription factor complex, plays a central role in many biological processes, including inflammation.\nAspirin is readily broken down in the body to salicylic acid, which itself has anti-inflammatory, antipyretic, and analgesic effects. In 2012, salicylic acid was found to activate AMP-activated protein kinase, which has been suggested as a possible explanation for some of the effects of both salicylic acid and aspirin. The acetyl portion of the aspirin molecule has its own targets. Acetylation of cellular proteins is a well-established phenomenon in the regulation of protein function at the post-translational level. Aspirin is able to acetylate several other targets in addition to COX isoenzymes. These acetylation reactions may explain many hitherto unexplained effects of aspirin.\n\nFormulations\nAspirin is produced in many formulations, with some differences in effect. In particular, aspirin can cause gastrointestinal bleeding, and formulations are sought which deliver the benefits of aspirin while mitigating harmful bleeding. Formulations may be combined (e.g., buffered + vitamin C).\n\nTablets, typically of about 75\u2013100 mg and 300\u2013320 mg of immediate-release aspirin (IR-ASA).\nDispersible tablets.\nEnteric-coated tablets.\nBuffered formulations containing aspirin with one of many buffering agents.\nFormulations of aspirin with vitamin C (ASA-VitC)\nA phospholipid-aspirin complex liquid formulation, PL-ASA. As of 2023 the phospholipid coating was being trialled to determine if it caused less gastrointestinal damage.\n\nPharmacokinetics\nAcetylsalicylic acid is a weak acid, and very little of it is ionized in the stomach after oral administration. Acetylsalicylic acid is quickly absorbed through the cell membrane in the acidic conditions of the stomach. The increased pH and larger surface area of the small intestine causes aspirin to be absorbed more slowly there, as more of it is ionized. Owing to the formation of concretions, aspirin is absorbed much more slowly during overdose, and plasma concentrations can continue to rise for up to 24 hours after ingestion.\nAbout 50\u201380% of salicylate in the blood is bound to human serum albumin, while the rest remains in the active, ionized state; protein binding is concentration-dependent. Saturation of binding sites leads to more free salicylate and increased toxicity. The volume of distribution is 0.1\u20130.2 L\/kg. Acidosis increases the volume of distribution because of enhancement of tissue penetration of salicylates.\nAs much as 80% of therapeutic doses of salicylic acid is metabolized in the liver. Conjugation with glycine forms salicyluric acid, and with glucuronic acid to form two different glucuronide esters. The conjugate with the acetyl group intact is referred to as the acyl glucuronide; the deacetylated conjugate is the phenolic glucuronide. These metabolic pathways have only a limited capacity. Small amounts of salicylic acid are also hydroxylated to gentisic acid. With large salicylate doses, the kinetics switch from first-order to zero-order, as metabolic pathways become saturated and renal excretion becomes increasingly important.\nSalicylates are excreted mainly by the kidneys as salicyluric acid (75%), free salicylic acid (10%), salicylic phenol (10%), and acyl glucuronides (5%), gentisic acid (< 1%), and 2,3-dihydroxybenzoic acid. When small doses (less than 250 mg in an adult) are ingested, all pathways proceed by first-order kinetics, with an elimination half-life of about 2.0 h to 4.5 h. When higher doses of salicylate are ingested (more than 4 g), the half-life becomes much longer (15 h to 30 h), because the biotransformation pathways concerned with the formation of salicyluric acid and salicyl phenolic glucuronide become saturated. Renal excretion of salicylic acid becomes increasingly important as the metabolic pathways become saturated, because it is extremely sensitive to changes in urinary pH. A 10- to 20-fold increase in renal clearance occurs when urine pH is increased from 5 to 8. The use of urinary alkalinization exploits this particular aspect of salicylate elimination. It was found that short-term aspirin use in therapeutic doses might precipitate reversible acute kidney injury when the patient was ill with glomerulonephritis or cirrhosis. Aspirin for some patients with chronic kidney disease and some children with congestive heart failure was contraindicated.\n\nHistory\nMedicines made from willow and other salicylate-rich plants appear in clay tablets from ancient Sumer as well as the Ebers Papyrus from ancient Egypt.:\u200a8\u201313\u200a Hippocrates referred to the use of salicylic tea to reduce fevers around 400 BC, and willow bark preparations were part of the pharmacopoeia of Western medicine in classical antiquity and the Middle Ages. Willow bark extract became recognized for its specific effects on fever, pain, and inflammation in the mid-eighteenth century. By the nineteenth century, pharmacists were experimenting with and prescribing a variety of chemicals related to salicylic acid, the active component of willow extract.:\u200a46\u201355\u200a\n\nIn 1853, chemist Charles Fr\u00e9d\u00e9ric Gerhardt treated sodium salicylate with acetyl chloride to produce acetylsalicylic acid for the first time;:\u200a46\u201348\u200a in the second half of the 19th century, other academic chemists established the compound's chemical structure and devised more efficient methods of synthesis. In 1897, scientists at the drug and dye firm Bayer began investigating acetylsalicylic acid as a less-irritating replacement for standard common salicylate medicines, and identified a new way to synthesize it.:\u200a69\u201375\u200a By 1899, Bayer had dubbed this drug Aspirin and was selling it globally.:\u200a27\u200a The word Aspirin was Bayer's brand name, rather than the generic name of the drug; however, Bayer's rights to the trademark were lost or sold in many countries. Aspirin's popularity grew over the first half of the 20th century leading to fierce competition with the proliferation of aspirin brands and products.\nAspirin's popularity declined after the development of acetaminophen\/paracetamol in 1956 and ibuprofen in 1962. In the 1960s and 1970s, John Vane and others discovered the basic mechanism of aspirin's effects,:\u200a226\u2013231\u200a while clinical trials and other studies from the 1960s to the 1980s established aspirin's efficacy as an anti-clotting agent that reduces the risk of clotting diseases.:\u200a247\u2013257\u200a The initial large studies on the use of low-dose aspirin to prevent heart attacks that were published in the 1970s and 1980s helped spur reform in clinical research ethics and guidelines for human subject research and US federal law, and are often cited as examples of clinical trials that included only men, but from which people drew general conclusions that did not hold true for women.\nAspirin sales revived considerably in the last decades of the 20th century, and remain strong in the 21st century with widespread use as a preventive treatment for heart attacks and strokes.:\u200a267\u2013269\n\nTrademark\nBayer lost its trademark for Aspirin in the United States and some other countries in actions taken between 1918 and 1921 because it had failed to use the name for its own product correctly and had for years allowed the use of \"Aspirin\" by other manufacturers without defending the intellectual property rights. Today, aspirin is a generic trademark in many countries. Aspirin, with a capital \"A\", remains a registered trademark of Bayer in Germany, Canada, Mexico, and in over 80 other countries, for acetylsalicylic acid in all markets, but using different packaging and physical aspects for each.\n\nCompendial status\nUnited States Pharmacopeia\nBritish Pharmacopoeia\n\nMedical use\nAspirin is used in the treatment of a number of conditions, including fever, pain, rheumatic fever, and inflammatory conditions, such as rheumatoid arthritis, pericarditis, and Kawasaki disease. Lower doses of aspirin have also been shown to reduce the risk of death from a heart attack, or the risk of stroke in people who are at high risk or who have cardiovascular disease, but not in elderly people who are otherwise healthy. There is evidence that aspirin is effective at preventing colorectal cancer, though the mechanisms of this effect are unclear.\n\nPain\nAspirin is an effective analgesic for acute pain, although it is generally considered inferior to ibuprofen because aspirin is more likely to cause gastrointestinal bleeding. Aspirin is generally ineffective for those pains caused by muscle cramps, bloating, gastric distension, or acute skin irritation. As with other NSAIDs, combinations of aspirin and caffeine provide slightly greater pain relief than aspirin alone. Effervescent formulations of aspirin relieve pain faster than aspirin in tablets, which makes them useful for the treatment of migraines. Topical aspirin may be effective for treating some types of neuropathic pain.\nAspirin, either by itself or in a combined formulation, effectively treats certain types of a headache, but its efficacy may be questionable for others. Secondary headaches, meaning those caused by another disorder or trauma, should be promptly treated by a medical provider. Among primary headaches, the International Classification of Headache Disorders distinguishes between tension headache (the most common), migraine, and cluster headache. Aspirin or other over-the-counter analgesics are widely recognized as effective for the treatment of tension headaches. Aspirin, especially as a component of an aspirin\/paracetamol\/caffeine combination, is considered a first-line therapy in the treatment of migraine, and comparable to lower doses of sumatriptan. It is most effective at stopping migraines when they are first beginning.\n\nFever\nLike its ability to control pain, aspirin's ability to control fever is due to its action on the prostaglandin system through its irreversible inhibition of COX. Although aspirin's use as an antipyretic in adults is well established, many medical societies and regulatory agencies, including the American Academy of Family Physicians, the American Academy of Pediatrics, and the Food and Drug Administration, strongly advise against using aspirin for the treatment of fever in children because of the risk of Reye's syndrome, a rare but often fatal illness associated with the use of aspirin or other salicylates in children during episodes of viral or bacterial infection. Because of the risk of Reye's syndrome in children, in 1986, the US Food and Drug Administration (FDA) required labeling on all aspirin-containing medications advising against its use in children and teenagers.\n\nInflammation\nAspirin is used as an anti-inflammatory agent for both acute and long-term inflammation, as well as for the treatment of inflammatory diseases, such as rheumatoid arthritis.\n\nHeart attacks and strokes\nAspirin is an important part of the treatment of those who have had a heart attack. It is generally not recommended for routine use by people with no other health problems, including those over the age of 70.\nThe 2009 Antithrombotic Trialists' Collaboration published in Lancet evaluated the efficacy and safety of low dose aspirin in secondary prevention. In those with prior ischaemic stroke or acute myocardial infarction, daily low dose aspirin was associated with a 19% relative risk reduction of serious cardiovascular events (non-fatal myocardial infarction, non-fatal stroke, or vascular death). This did come at the expense of a 0.19% absolute risk increase in gastrointestinal bleeding; however, the benefits outweigh the hazard risk in this case. Data from previous trials have suggested that weight-based dosing of aspirin has greater benefits in primary prevention of cardiovascular outcomes. However, more recent trials were not able to replicate similar outcomes using low dose aspirin in low body weight (<70 kg) in specific subset of population studied i.e. elderly and diabetic population, and more evidence is required to study the effect of high dose aspirin in high body weight (\u226570 kg).\nAfter percutaneous coronary interventions (PCIs), such as the placement of a coronary artery stent, a U.S. Agency for Healthcare Research and Quality guideline recommends that aspirin be taken indefinitely. Frequently, aspirin is combined with an ADP receptor inhibitor, such as clopidogrel, prasugrel, or ticagrelor to prevent blood clots. This is called dual antiplatelet therapy (DAPT). Duration of DAPT was advised in the United States and European Union guidelines after the CURE and PRODIGY studies. In 2020, the systematic review and network meta-analysis from Khan et al. showed promising benefits of short-term (< 6 months) DAPT followed by P2Y12 inhibitors in selected patients, as well as the benefits of extended-term (> 12 months) DAPT in high risk patients. In conclusion, the optimal duration of DAPT after PCIs should be personalized after outweighing each patient's risks of ischemic events and risks of bleeding events with consideration of multiple patient-related and procedure-related factors. Moreover, aspirin should be continued indefinitely after DAPT is complete.\nThe status of the use of aspirin for the primary prevention in cardiovascular disease is conflicting and inconsistent, with recent changes from previously recommending it widely decades ago, and that some referenced newer trials in clinical guidelines show less of benefit of adding aspirin alongside other anti-hypertensive and cholesterol lowering therapies. The ASCEND study demonstrated that in high-bleeding risk diabetics with no prior cardiovascular disease, there is no overall clinical benefit (12% decrease in risk of ischaemic events v\/s 29% increase in GI bleeding) of low dose aspirin in preventing the serious vascular events over a period of 7.4 years. Similarly, the results of the ARRIVE study also showed no benefit of same dose of aspirin in reducing the time to first cardiovascular outcome in patients with moderate risk of cardiovascular disease over a period of five years. Aspirin has also been suggested as a component of a polypill for prevention of cardiovascular disease. Complicating the use of aspirin for prevention is the phenomenon of aspirin resistance. For people who are resistant, aspirin's efficacy is reduced. Some authors have suggested testing regimens to identify people who are resistant to aspirin.\nAs of April 2022, the United States Preventive Services Task Force (USPSTF) determined that there was a \"small net benefit\" for patients aged 40\u201359 with a 10% or greater 10-year cardiovascular disease (CVD) risk, and \"no net benefit\" for patients aged over 60. Determining the net benefit was based on balancing the risk reduction of taking aspirin for heart attacks and ischaemic strokes, with the increased risk of gastrointestinal bleeding, intracranial bleeding, and hemorrhagic strokes. Their recommendations state that age changes the risk of the medicine, with the magnitude of the benefit of aspirin coming from starting at a younger age, while the risk of bleeding, while small, increases with age, particular for adults over 60, and can be compounded by other risk factors such as diabetes and a history of gastrointestinal bleeding. As a result, the USPSTF suggests that \"people ages 40 to 59 who are at higher risk for CVD should decide with their clinician whether to start taking aspirin; people 60 or older should not start taking aspirin to prevent a first heart attack or stroke.\" Primary prevention guidelines from September 2019 made by the American College of Cardiology and the American Heart Association state they might consider aspirin for patients aged 40\u201369 with a higher risk of atherosclerotic CVD, without an increased bleeding risk, while stating they would not recommend aspirin for patients aged over 70 or adults of any age with an increased bleeding risk. They state a CVD risk estimation and a risk discussion should be done before starting on aspirin, while stating aspirin should be used \"infrequently in the routine primary prevention of (atherosclerotic CVD) because of lack of net benefit\". As of August 2021, the European Society of Cardiology made similar recommendations; considering aspirin specifically to patients aged less than 70 at high or very high CVD risk, without any clear contraindications, on a case-by-case basis considering both ischemic risk and bleeding risk.\n\nCancer prevention\nAspirin may reduce the overall risk of both getting cancer and dying from cancer. There is substantial evidence for lowering the risk of colorectal cancer (CRC), but aspirin must be taken for at least 10\u201320 years to see this benefit. It may also slightly reduce the risk of endometrial cancer and prostate cancer.\nSome conclude the benefits are greater than the risks due to bleeding in those at average risk. Others are unclear if the benefits are greater than the risk. Given this uncertainty, the 2007 United States Preventive Services Task Force (USPSTF) guidelines on this topic recommended against the use of aspirin for prevention of CRC in people with average risk. Nine years later however, the USPSTF issued a grade B recommendation for the use of low-dose aspirin (75 to 100 mg\/day) \"for the primary prevention of CVD [cardiovascular disease] and CRC in adults 50 to 59 years of age who have a 10% or greater 10-year CVD risk, are not at increased risk for bleeding, have a life expectancy of at least 10 years, and are willing to take low-dose aspirin daily for at least 10 years\".\nA meta-analysis through 2019 said that there was an association between taking aspirin and lower risk of cancer of the colorectum, esophagus, and stomach.\nIn 2021, the U.S. Preventive services Task Force raised questions about the use of aspirin in cancer prevention. It notes the results of the 2018 ASPREE (Aspirin in Reducing Events in the Elderly) Trial, in which the risk of cancer-related death was higher in the aspirin-treated group than in the placebo group.\n\nPsychiatry\nBipolar disorder\nAspirin, along with several other agents with anti-inflammatory properties, has been repurposed as an add-on treatment for depressive episodes in subjects with bipolar disorder in light of the possible role of inflammation in the pathogenesis of severe mental disorders. A 2022 systematic review concluded that aspirin exposure reduced the risk of depression in a pooled cohort of three studies (HR 0.624, 95% CI: 0.0503, 1.198, P=0.033). However, further high-quality, longer-duration, double-blind randomized controlled trials (RCTs) are needed to determine whether aspirin is an effective add-on treatment for bipolar depression. Thus, notwithstanding the biological rationale, the clinical perspectives of aspirin and anti-inflammatory agents in the treatment of bipolar depression remain uncertain.\n\nDementia\nAlthough cohort and longitudinal studies have shown low-dose aspirin has a greater likelihood of reducing the incidence of dementia, numerous randomized controlled trials have not validated this.\n\nSchizophrenia\nSome researchers have speculated the anti-inflammatory effects of aspirin may be beneficial for schizophrenia. Small trials have been conducted but evidence remains lacking.\n\nOther uses\nAspirin is a first-line treatment for the fever and joint-pain symptoms of acute rheumatic fever. The therapy often lasts for one to two weeks, and is rarely indicated for longer periods. After fever and pain have subsided, the aspirin is no longer necessary, since it does not decrease the incidence of heart complications and residual rheumatic heart disease. Naproxen has been shown to be as effective as aspirin and less toxic, but due to the limited clinical experience, naproxen is recommended only as a second-line treatment.\nAlong with rheumatic fever, Kawasaki disease remains one of the few indications for aspirin use in children in spite of a lack of high quality evidence for its effectiveness.\nLow-dose aspirin supplementation has moderate benefits when used for prevention of pre-eclampsia. This benefit is greater when started in early pregnancy.\nAspirin has also demonstrated anti-tumoral effects, via inhibition of the PTTG1 gene, which is often overexpressed in tumors.\n\nResistance\nFor some people, aspirin does not have as strong an effect on platelets as for others, an effect known as aspirin-resistance or insensitivity. One study has suggested women are more likely to be resistant than men, and a different, aggregate study of 2,930 people found 28% were resistant.\nA study in 100 Italian people found, of the apparent 31% aspirin-resistant subjects, only 5% were truly resistant, and the others were noncompliant.\nAnother study of 400 healthy volunteers found no subjects who were truly resistant, but some had \"pseudoresistance, reflecting delayed and reduced drug absorption\".\n\nMeta-analysis and systematic reviews have concluded that laboratory confirmed aspirin resistance confers increased rates of poorer outcomes in cardiovascular and neurovascular diseases. Although the majority of research conducted has surrounded cardiovascular and neurovascular, there is emerging research into the risk of aspirin resistance after orthopaedic surgery where aspirin is used for venous thromboembolism prophylaxis. Aspirin resistance in orthopaedic surgery, specifically after total hip and knee arthroplasties, is of interest as risk factors for aspirin resistance are also risk factors for venous thromboembolisms and osteoarthritis; the sequelae of requiring a total hip or knee arthroplasty. Some of these risk factors include obesity, advancing age, diabetes mellitus, dyslipidemia and inflammatory diseases.\n\nDosages\nAdult aspirin tablets are produced in standardised sizes, which vary slightly from country to country, for example 300 mg in Britain and 325 mg in the United States. Smaller doses are based on these standards, e.g., 75 mg and 81 mg tablets. The 81 mg tablets are commonly called \"baby aspirin\" or \"baby-strength\", because they were originally \u2013 but no longer \u2013 intended to be administered to infants and children. No medical significance occurs due to the slight difference in dosage between the 75 mg and the 81 mg tablets. The dose required for benefit appears to depend on a person's weight. For those weighing less than 70 kilograms (154 lb), low dose is effective for preventing cardiovascular disease; for patients above this weight, higher doses are required.\nIn general, for adults, doses are taken four times a day for fever or arthritis, with doses near the maximal daily dose used historically for the treatment of rheumatic fever. For the prevention of myocardial infarction (MI) in someone with documented or suspected coronary artery disease, much lower doses are taken once daily.\nMarch 2009 recommendations from the USPSTF on the use of aspirin for the primary prevention of coronary heart disease encourage men aged 45\u201379 and women aged 55\u201379 to use aspirin when the potential benefit of a reduction in MI for men or stroke for women outweighs the potential harm of an increase in gastrointestinal hemorrhage. The WHI study of postmenopausal women found that aspirin resulted in a 25% lower risk of death from cardiovascular disease and a 14% lower risk of death from any cause, though there was no significant difference between 81 mg and 325 mg aspirin doses. The 2021 ADAPTABLE study also showed no significant difference in cardiovascular events or major bleeding between 81 mg and 325 mg doses of aspirin in patients (both men and women) with established cardiovascular disease.\nLow-dose aspirin use was also associated with a trend toward lower risk of cardiovascular events, and lower aspirin doses (75 or 81 mg\/day) may optimize efficacy and safety for people requiring aspirin for long-term prevention.\nIn children with Kawasaki disease, aspirin is taken at dosages based on body weight, initially four times a day for up to two weeks and then at a lower dose once daily for a further six to eight weeks.\n\nAdverse effects\nIn October 2020, the US Food and Drug Administration (FDA) required the drug label to be updated for all nonsteroidal anti-inflammatory medications to describe the risk of kidney problems in unborn babies that result in low amniotic fluid. They recommend avoiding NSAIDs in pregnant women at 20 weeks or later in pregnancy. One exception to the recommendation is the use of low-dose 81 mg aspirin at any point in pregnancy under the direction of a health care professional.\n\nContraindications\nAspirin should not be taken by people who are allergic to ibuprofen or naproxen, or who have salicylate intolerance or a more generalized drug intolerance to NSAIDs, and caution should be exercised in those with asthma or NSAID-precipitated bronchospasm. Owing to its effect on the stomach lining, manufacturers recommend people with peptic ulcers, mild diabetes, or gastritis seek medical advice before using aspirin. Even if none of these conditions is present, the risk of stomach bleeding is still increased when aspirin is taken with alcohol or warfarin. People with hemophilia or other bleeding tendencies should not take aspirin or other salicylates. Aspirin is known to cause hemolytic anemia in people who have the genetic disease glucose-6-phosphate dehydrogenase deficiency, particularly in large doses and depending on the severity of the disease. Use of aspirin during dengue fever is not recommended owing to increased bleeding tendency. Aspirin taken at doses of \u2264325 mg and \u2264100 mg per day for \u22652 days can increase the odds of suffering a gout attack by 81% and 91% respectively. This effect may potentially be worsened by high purine diets, diuretics, and kidney disease, but is eliminated by the urate lowering drug allopurinol. Daily low dose aspirin does not appear to worsen kidney function. Aspirin may reduce cardiovascular risk in those without established cardiovascular disease in people with moderate CKD, without significantly increasing the risk of bleeding. Aspirin should not be given to children or adolescents under the age of 16 to control cold or influenza symptoms, as this has been linked with Reye's syndrome.\n\nGastrointestinal\nAspirin increases the risk of upper gastrointestinal bleeding. Enteric coating on aspirin may be used in manufacturing to prevent release of aspirin into the stomach to reduce gastric harm, but enteric coating does not reduce gastrointestinal bleeding risk. Enteric-coated aspirin may not be as effective at reducing blood clot risk. Combining aspirin with other NSAIDs has been shown to further increase the risk of gastrointestinal bleeding. Using aspirin in combination with clopidogrel or warfarin also increases the risk of upper gastrointestinal bleeding.\nBlockade of COX-1 by aspirin apparently results in the upregulation of COX-2 as part of a gastric defense. There is no clear evidence that simultaneous use of a COX-2 inhibitor with aspirin may increase the risk of gastrointestinal injury.\n\"Buffering\" is an additional method used with the intent to mitigate  gastrointestinal bleeding, such as by preventing aspirin from concentrating in the walls of the stomach, although the benefits of buffered aspirin are disputed. Almost any buffering agent used in antacids can be used; Bufferin, for example, uses magnesium oxide. Other preparations use calcium carbonate. Gas-forming agents in effervescent tablet and powder formulations can also double as a buffering agent, one example being sodium bicarbonate, used in Alka-Seltzer.\nTaking vitamin C with aspirin has been investigated as a method of protecting the stomach lining. In trials vitamin C-releasing aspirin (ASA-VitC) or a buffered aspirin formulation containing vitamin C was found to cause less stomach damage than aspirin alone.\n\nRetinal vein occlusion\nIt is a widespread habit among eye specialists (ophthalmologists) to prescribe aspirin as an add-on medication for patients with retinal vein occlusion (RVO), such as central retinal vein occlusion (CRVO) and branch retinal vein occlusion (BRVO). The reason of this widespread use is the evidence of its proven effectiveness in major systemic venous thrombotic disorders, and it has been assumed that may be similarly beneficial in various types of retinal vein occlusion.\nHowever, a large-scale investigation based on data of nearly 700 patients showed \"that aspirin or other antiplatelet aggregating agents or anticoagulants adversely influence the visual outcome in patients with CRVO and hemi-CRVO, without any evidence of protective or beneficial effect\". Several expert groups, including the Royal College of Ophthalmologists, recommended against the use of antithrombotic drugs (incl. aspirin) for patients with RVO.\n\nCentral effects\nLarge doses of salicylate, a metabolite of aspirin, cause temporary tinnitus (ringing in the ears) based on experiments in rats, via the action on arachidonic acid and NMDA receptors cascade.\n\nReye's syndrome\nReye's syndrome, a rare but severe illness characterized by acute encephalopathy and fatty liver, can occur when children or adolescents are given aspirin for a fever or other illness or infection. From 1981 to 1997, 1207 cases of Reye's syndrome in people younger than 18 were reported to the US Centers for Disease Control and Prevention (CDC). Of these, 93% reported being ill in the three weeks preceding the onset of Reye's syndrome, most commonly with a respiratory infection, chickenpox, or diarrhea. Salicylates were detectable in 81.9% of children for whom test results were reported. After the association between Reye's syndrome and aspirin was reported, and safety measures to prevent it (including a Surgeon General's warning, and changes to the labeling of aspirin-containing drugs) were implemented, aspirin taken by children declined considerably in the United States, as did the number of reported cases of Reye's syndrome; a similar decline was found in the United Kingdom after warnings against pediatric aspirin use were issued. The US Food and Drug Administration recommends aspirin (or aspirin-containing products) should not be given to anyone under the age of 12 who has a fever, and the UK National Health Service recommends children who are under 16 years of age should not take aspirin, unless it is on the advice of a doctor.\n\nSkin\nFor a small number of people, taking aspirin can result in symptoms including hives, swelling, and headache. Aspirin can exacerbate symptoms among those with chronic hives, or create acute symptoms of hives. These responses can be due to allergic reactions to aspirin, or more often due to its effect of inhibiting the COX-1 enzyme. Skin reactions may also tie to systemic contraindications, seen with NSAID-precipitated bronchospasm, or those with atopy.\nAspirin and other NSAIDs, such as ibuprofen, may delay the healing of skin wounds. Earlier findings from two small, low-quality trials suggested a benefit with aspirin (alongside compression therapy) on venous leg ulcer healing time and leg ulcer size, however larger, more recent studies of higher quality have been unable to corroborate these outcomes. As such, further research is required to clarify the role of aspirin in this context.\n\nOther adverse effects\nAspirin can induce swelling of skin tissues in some people. In one study, angioedema appeared one to six hours after ingesting aspirin in some of the people. However, when the aspirin was taken alone, it did not cause angioedema in these people; the aspirin had been taken in combination with another NSAID-induced drug when angioedema appeared.\nAspirin causes an increased risk of cerebral microbleeds, having the appearance on MRI scans of 5 to 10 mm or smaller, hypointense (dark holes) patches.\nA study of a group with a mean dosage of aspirin of 270 mg per day estimated an average absolute risk increase in intracerebral hemorrhage (ICH) of 12 events per 10,000 persons. In comparison, the estimated absolute risk reduction in myocardial infarction was 137 events per 10,000 persons, and a reduction of 39 events per 10,000 persons in ischemic stroke. In cases where ICH already has occurred, aspirin use results in higher mortality, with a dose of about 250 mg per day resulting in a relative risk of death within three months after the ICH around 2.5 (95% confidence interval 1.3 to 4.6).\nAspirin and other NSAIDs can cause abnormally high blood levels of potassium by inducing a hyporeninemic hypoaldosteronism state via inhibition of prostaglandin synthesis; however, these agents do not typically cause hyperkalemia by themselves in the setting of normal renal function and euvolemic state.\nUse of low-dose aspirin before a surgical procedure has been associated with an increased risk of bleeding events in some patients, however, ceasing aspirin prior to surgery has also been associated with an increase in major adverse cardiac events. An analysis of multiple studies found a three-fold increase in adverse events such as myocardial infarction in patients who ceased aspirin prior to surgery. The analysis found that the risk is dependent on the type of surgery being performed and the patient indication for aspirin use.\nOn 9 July 2015, the US Food and Drug Administration (FDA) toughened warnings of increased heart attack and stroke risk associated with nonsteroidal anti-inflammatory drugs (NSAID). Aspirin is an NSAID but is not affected by the new warnings.\n\nOverdose\nAspirin overdose can be acute or chronic. In acute poisoning, a single large dose is taken; in chronic poisoning, higher than normal doses are taken over a period of time. Acute overdose has a mortality rate of 2%. Chronic overdose is more commonly lethal, with a mortality rate of 25%; chronic overdose may be especially severe in children. Toxicity is managed with a number of potential treatments, including activated charcoal, intravenous dextrose and normal saline, sodium bicarbonate, and dialysis. The diagnosis of poisoning usually involves measurement of plasma salicylate, the active metabolite of aspirin, by automated spectrophotometric methods. Plasma salicylate levels in general range from 30 to 100 mg\/L after usual therapeutic doses, 50\u2013300 mg\/L in people taking high doses and 700\u20131400 mg\/L following acute overdose. Salicylate is also produced as a result of exposure to bismuth subsalicylate, methyl salicylate, and sodium salicylate.\n\nInteractions\nAspirin is known to interact with other drugs. For example, acetazolamide and ammonium chloride are known to enhance the intoxicating effect of salicylates, and alcohol also increases the gastrointestinal bleeding associated with these types of drugs. Aspirin is known to displace a number of drugs from protein-binding sites in the blood, including the antidiabetic drugs tolbutamide and chlorpropamide, warfarin, methotrexate, phenytoin, probenecid, valproic acid (as well as interfering with beta oxidation, an important part of valproate metabolism), and other NSAIDs. Corticosteroids may also reduce the concentration of aspirin. Other NSAIDs, such as ibuprofen and naproxen, may reduce the antiplatelet effect of aspirin. Although limited evidence suggests this may not result in a reduced cardioprotective effect of aspirin. Analgesic doses of aspirin decrease sodium loss induced by spironolactone in the urine, however this does not reduce the antihypertensive effects of spironolactone. Furthermore, antiplatelet doses of aspirin are deemed too small to produce an interaction with spironolactone. Aspirin is known to compete with penicillin G for renal tubular secretion. Aspirin may also inhibit the absorption of vitamin C.\n\nResearch\nThe ISIS-2 trial demonstrated that aspirin at doses of 160 mg daily for one month, decreased the mortality by 21% of participants with a suspected myocardial infarction in the first five weeks. A single daily dose of 324 mg of aspirin for 12 weeks has a highly protective effect against acute myocardial infarction and death in men with unstable angina.\n\nBipolar disorder\nAspirin has been repurposed as an add-on treatment for depressive episodes in subjects with bipolar disorder. However, meta-analytic evidence is based on very few studies and does not suggest any efficacy of aspirin in the treatment of bipolar depression. Thus, notwithstanding the biological rationale, the clinical perspectives of aspirin and anti-inflammatory agents in the treatment of bipolar depression remain uncertain.\n\nInfectious diseases\nSeveral studies investigated the anti-infective properties of aspirin for bacterial, viral and parasitic infections. Aspirin was demonstrated to limit platelet activation induced by Staphylococcus aureus and Enterococcus faecalis and to reduce streptococcal adhesion to heart valves. In patients with tuberculous meningitis, the addition of aspirin reduced the risk of new cerebral infarction [RR = 0.52 (0.29-0.92)]. A role of aspirin on bacterial and fungal biofilm is also being supported by growing evidence.\n\nCancer prevention\nEvidence from observational studies were conflicting on the effect of aspirin in breast cancer prevention, a randomized controlled trial showed that aspirin had no significant effect in reducing breast cancer  thus further studies are needed to clarify aspirin effect in cancer prevention.\n\nIn gardening\nThere are many anecdotal reportings that aspirin can improve plant's growth and resistance though most research involved salicylic acid instead of aspirin.\n\nVeterinary medicine\nAspirin is sometimes used in veterinary medicine as an anticoagulant or to relieve pain associated with musculoskeletal inflammation or osteoarthritis. Aspirin should be given to animals only under the direct supervision of a veterinarian, as adverse effects\u2014including gastrointestinal issues\u2014are common. An aspirin overdose in any species may result in salicylate poisoning, characterized by hemorrhaging, seizures, coma, and even death.\nDogs are better able to tolerate aspirin than cats are. Cats metabolize aspirin slowly because they lack the glucuronide conjugates that aid in the excretion of aspirin, making it potentially toxic if dosing is not spaced out properly. No clinical signs of toxicosis occurred when cats were given 25 mg\/kg of aspirin every 48 hours for 4 weeks, but the recommended dose for relief of pain and fever and for treating blood clotting diseases in cats is 10 mg\/kg every 48 hours to allow for metabolization.\n\nReferences\nFurther reading\n\n\n== External links ==","12":"The auditory brainstem response (ABR), also called brainstem evoked response audiometry (BERA) or brainstem auditory evoked potentials (BAEPs) or brainstem auditory evoked responses (BAERs) is an auditory evoked potential extracted from ongoing electrical activity in the brain and recorded via electrodes placed on the scalp. The measured recording is a series of six to seven vertex positive waves of which I through V are evaluated. These waves, labeled with Roman numerals in Jewett and Williston convention, occur in the first 10 milliseconds after onset of an auditory stimulus. The ABR is considered an exogenous response because it is dependent upon external factors.\nThe auditory structures that generate the auditory brainstem response are believed to be as follows:\n\nWave I through III \u2013 generated by the auditory branch of cranial nerve VIII and lower\nWave IV and V \u2013 generated by the upper brainstem\nMore in depth location \u2013 wave I and II originates from the distal and proximal auditory nerve fibers, wave III from the cochlear nucleus, IV showing activity in the superior olivary complex, and wave V is associated with the lateral lemniscus.\n\nHistory of research\nIn 1967, Sohmer and Feinmesser were the first to publish ABRs recorded with surface electrodes in humans which showed that cochlear potentials could be obtained non-invasively. In 1971, Jewett and Williston gave a clear description of the human ABR and correctly interpreted the later waves as arriving from the brainstem. In 1977, Selters and Brackman published landmark findings on prolonged inter-peak latencies in tumor cases (greater than 1 cm). In 1974, Hecox and Galambos showed that the ABR could be used for threshold estimation in adults and infants. In 1975, Starr and Achor were the first to report the effects on the ABR of CNS pathology in the brainstem.\nLong and Allen were the first to report the abnormal brainstem auditory evoked potentials (BAEPs) in an alcoholic woman who recovered from acquired central hypoventilation syndrome. These investigators hypothesized that their patient's brainstem was poisoned, but not destroyed, by her chronic alcoholism.\n\nMeasurement techniques\nRecording parameters\nElectrode montage: most performed with a vertical montage (high forehead [active or positive], earlobes or mastoids [reference right & left or negative], low forehead [ground]\nImpedance: 5 k\u03a9 or less (also equal between electrodes)\nFilter settings: 30\u20131500 Hz bandwidth\nTime window: 10ms (minimum)\nSampling rate: usually high sampling rate of ca 20 kHz\nIntensity: usually start at 70 dBnHL\nStimulus type: click (100 us long), chirp or toneburst\nTransducer type: insert, bone vibrator, sound field, headphones\nStimulation or repetition rate: 21.1 (for example)\nAmplification: 100\u2013150K\nn (# of averages\/ sweeps): 1000 minimum (1500 recommended)\nPolarity: rarefaction or alternating recommended\n\nUse\nThe ABR is used for newborn hearing screening, auditory threshold estimation, intraoperative monitoring, determining hearing loss type and degree, and auditory nerve and brainstem lesion detection, and in development of cochlear implants.\n\nAdvanced techniques\nStacked ABR\nHistory\nOne use of the traditional ABR is site-of-lesion testing and it has been shown to be sensitive to large acoustic tumors. However, it has poor sensitivity to tumors smaller than 1 centimeter in diameter. In the 1990s, there were several studies that concluded that the use of ABRs to detect acoustic tumors should be abandoned.  As a result, many practitioners only use MRI for this purpose now.\nThe reason the ABR does not identify small tumors can be explained by the fact that ABRs rely on latency changes of peak V. Peak V is primarily influenced by high-frequency fibers, and tumors will be missed if those fibers aren't affected. Although the click stimulates a wide frequency region on the cochlea, phase cancellation of the lower-frequency responses occurs as a result of time delays along the basilar membrane.  If a tumor is small, it is possible those fibers won't be sufficiently affected to be detected by the traditional ABR measure.\nPrimary reasons why it is not practical to simply send every patient in for an MRI are the high cost of an MRI, its impact on patient comfort, and limited availability in rural areas and third-world countries. In 1997, Dr. Manuel Don and colleagues published on the Stacked ABR as a way to enhance the sensitivity of the ABR in detecting smaller tumors. Their hypothesis was that the new ABR-stacked derived-band ABR amplitude could detect small acoustic tumors missed by standard ABR measures. In 2005, he stated that it would be clinically valuable to have available an ABR test to screen for small tumors.  In a 2005 interview in Audiology Online, Dr. Don of House Ear Institute defined the Stacked ABR as \"...an attempt to record the sum of the neural activity across the entire frequency region of the cochlea in response to a click stimuli.\"\n\nStacked ABR defined\nThe stacked ABR is the sum of the synchronous neural activity generated from five frequency regions across the cochlea in response to click stimulation and high-pass pink noise masking. The development of this technique was based on the 8th cranial nerve compound action potential work done by Teas, Eldredge, and Davis in 1962.\n\nMethodology\nThe stacked ABR is a composite of activity from ALL frequency regions of the cochlea \u2013 not just high frequency.\n\nStep 1: obtain Click-evoked ABR responses to clicks and high-pass pink masking noise (ipsilateral masking)\nStep 2: obtain derived-band ABRs (DBR)\nStep 3: shift & align the wave V peaks of the DBR \u2013 thus, \"stacking\" the waveforms with wave V lined up\nStep 4: add the waveforms together\nStep 5: compare the amplitude of the Stacked ABR with the click-evoked ABR from the same ear\nWhen the derived waveforms are representing activity from more apical regions along the basilar membrane, wave V latencies are prolonged because of the nature of the traveling wave. In order to compensate for these latency shifts, the wave V component for each derived waveform is stacked (aligned), added together, and then the resulting amplitude is measured.\nIn 2005, Don explains that in a normal ear, the sum of the Stacked ABR will have the same amplitude as the Click-evoked ABR. But, the presence of even a small tumor results in a reduction in the amplitude of the Stacked ABR in comparison with the Click-evoked ABR.\n\nApplication and effectiveness\nWith the intent of screening for and detecting the presence of small (less than or equal to 1 cm) acoustic tumors, the Stacked ABR is:\n\n95% Sensitivity\n83% Specificity\n(Note: 100% sensitivity was obtained at 50% specificity)\nIn a 2007 comparative study of ABR abnormalities in acoustic tumor patients, Montaguti and colleagues mention the promise of and great scientific interest in the Stacked ABR. The article suggests that the Stacked ABR could make it possible to identify small acoustic neuromas missed by traditional ABRs.\nThe Stacked ABR is a valuable screening tool for the detection of small acoustic tumors because it is sensitive, specific, widely available, comfortable, and cost-effective.\n\nTone-burst ABR\nTone-burst ABR is used to obtain thresholds for children who are too young to otherwise reliably respond behaviorally to frequency-specific sound stimuli.  The most common frequencies tested at 500, 1000, 2000, and 4000 Hz, as these frequencies are generally thought to be necessary for hearing aid programming.\n\nAuditory steady-state response (ASSR)\nASSR defined\nAuditory steady-state response is an auditory evoked potential, elicited with modulated tones that can be used to predict hearing sensitivity in patients of all ages. It is an electrophysiologic response to rapid auditory stimuli and creates a statistically valid estimated audiogram (evoked potential used to predict hearing thresholds for normal hearing individuals and those with hearing loss). The ASSR uses statistical measures to determine if and when a threshold is present and is a \"cross-check\" for verification purposes prior to arriving at a differential diagnosis.\n\nHistory\nIn 1981, Galambos and colleagues reported on the \"40 Hz auditory potential\" which is a continuous 400 Hz tone sinusoidally 'amplitude modulated' at  40 Hz and at 70 dB SPL. This produced a very frequency specific response, but the response was very susceptible to state of arousal. In 1991, Cohen and colleagues learned that by presenting at a higher rate of stimulation than 40 Hz (>70 Hz), the response was smaller but less affected by sleep. In 1994, Rickards and colleagues showed that it was possible to obtain responses in newborns. In 1995, Lins and Picton found that simultaneous stimuli presented at rates in the 80 to 100 Hz range made it possible to obtain auditory thresholds.\n\nMethodology\nThe same or similar to traditional recording montages used for ABR recordings are used for the ASSR. Two active electrodes are placed at or near vertex and at ipsilateral earlobe\/mastoid with ground at low forehead. If collecting from both ears simultaneously, a two-channel pre-amplifier is used. When single channel recording system is used to detect activity from a binaural presentation, a common reference electrode may be located at the nape of the neck. Transducers can be insert earphones, headphones, a bone oscillator, or sound field and it is preferable if patient is asleep. Unlike ABR settings, the high pass filter might be approximately 40 to 90 Hz and low pass filter might be between 320 and 720 Hz with typical filter slopes of 6 dB per octave. Gain settings of 10,000 are common, artifact reject is left \"on\", and it is thought to be advantageous to have manual \"override\" to allow the clinician to make decisions during test and apply course corrections as needed.\n\nABR vs. ASSR\nSimilarities:\n\nBoth record bioelectric activity from electrodes arranged in similar recording arrays.\nBoth are auditory evoked potentials.\nBoth use acoustic stimuli delivered through inserts (preferably).\nBoth can be used to estimate threshold for patients who cannot or will not participate in traditional behavioral measures.\nDifferences:\n\nASSR looks at amplitude and phases in the spectral (frequency) domain rather than at amplitude and latency.\nASSR depends on peak detection across a spectrum rather than across a time vs. amplitude waveform.\nASSR is evoked using repeated sound stimuli presented at a high rep rate rather than an abrupt sound at a relatively low rep rate.\nABR typically uses click or tone-burst stimuli in one ear at a time, but ASSR can be used binaurally while evaluating broad bands or four frequencies (500, 1k, 2k, & 4k) simultaneously.\nABR estimates thresholds basically from 1-4k in typical mild-moderate-severe hearing losses. ASSR can also estimate thresholds in the same range, but offers more frequency specific info more quickly and can estimate hearing in the severe-to-profound hearing loss ranges.\nABR depends highly upon a subjective analysis of the amplitude\/latency function. The ASSR uses a statistical analysis of the probability of a response (usually at a 95% confidence interval).\nABR is measured in microvolts (millionths of a volt) and the ASSR is measured in nanovolts (billionths of a volt).\n\nAnalysis, normative data, and general trends\nAnalysis is mathematically based and dependent upon the fact that related bioelectric events coincide with the stimulus rep rate. The specific method of analysis is based on the manufacturer's statistical detection algorithm. It occurs in the spectral domain and is composed of specific frequency components that are harmonics of the stimulus repetition rate. Early ASSR systems considered the first harmonic only, but newer systems also incorporate higher harmonics in their detection algorithms.\nMost equipment provides correction tables for converting ASSR thresholds to estimated HL audiograms and are found to be within 10 dB to 15 dB of audiometric thresholds. Although there are variances across studies. Correction data depends on variables such as: equipment used, frequencies collected, collection time, age of subject, sleep state of subject, stimulus parameters.\n\nHearing aid fittings\nIn certain cases where behavioral thresholds cannot be attained, ABR thresholds can be used for hearing aid fittings. New fitting formulas such as DSL v5.0 allow the user to base the settings in the hearing aid on the ABR thresholds. Correction factors do exist for converting ABR thresholds to behavioral thresholds, but vary greatly. For example, one set of correction factors involves lowering ABR thresholds from 1000 to 4000 Hz by 10 dB and lowering the ABR threshold at 500 Hz by 15 to 20 dB. Previously, brainstem audiometry has been used for hearing aid selection by using normal and pathological intensity-amplitude functions to determine appropriate amplification. The principal idea of the selection and fitting of the hearing instrument was based on the assumption that amplitudes of the brainstem potentials were directly related to loudness perception. Under this assumption, the amplitudes of brainstem potentials stimulated by the hearing devices should exhibit close-to-normal values. ABR thresholds do not necessarily improve in the aided condition. ABR can be an inaccurate indicator of hearing aid benefit due to difficulty processing the appropriate amount of fidelity of the transient stimuli used to evoke a response. Bone conduction ABR thresholds can be used if other limitations are present, but thresholds are not as accurate as ABR thresholds recorded through air conduction.\nAdvantages of hearing aid selection by brainstem audiometry include the following applications:\n\nevaluation of loudness perception in the dynamic range of hearing (recruitment)\ndetermination of basic hearing aid properties (gain, compression factor, compression onset level)\ncases with middle ear impairment (contrary to acoustic reflex methods)\nnon-cooperative subjects even in sleep\nsedation or anesthesia without influence of age and vigilance (contrary to cortical evoked responses).\nDisadvantages of hearing aid selection by brainstem audiometry include the following applications:\n\nin cases of severe hearing impairment including no or only poor information as to loudness perception\nno control of compression setting\nno frequency-specific compensation of hearing impairment\n\nCochlear implantation and central auditory development\nThere are about 188,000 people around the world who have received cochlear implants. In the United States alone, there are about 30,000 adults and over 30,000 children who are recipients of cochlear implants. This number continues to grow as cochlear implantation is becoming more and more accepted. In 1961, Dr. William House began work on the predecessor for today's cochlear implant. William House is an Otologist and is the founder of House ear institute in Los Angeles, California. This groundbreaking device, which was manufactured by 3M company was approved by the FDA in 1984. Although this was a single channel device, it paved the way for future multi channel cochlear implants. Currently, as of 2007, the three cochlear implant devices approved for use in the U.S. are manufactured by Cochlear, Med-El, and Advanced Bionics. The way a cochlear implant works is sound is received by the cochlear implant's microphone, which picks up input that needs to be processed to determine how the electrodes will receive the signal. This is done on the external component of the cochlear implant called the sound processor. The transmitting coil, also an external component transmits the information from the speech processor through the skin using frequency modulated radio waves. The signal is never turned back into an acoustic stimulus, unlike a hearing aid. This information is then received by the cochlear implant's internal components. The receiver stimulator delivers the correct amount of electrical stimulation to the appropriate electrodes on the array to represent the sound signal that was detected. The electrode array stimulates the remaining auditory nerve fibers in the cochlea, which carry the signal on to the brain, where it is processed.\nOne way to measure the developmental status and limits of plasticity of the auditory cortical pathways is to study the latency of cortical auditory evoked potentials (CAEP). In particular, the latency of the first positive peak (P1) of the CAEP is of interest to researchers. P1 in children is considered a marker for maturation of the auditory cortical areas (Eggermont & Ponton, 2003; Sharma & Dorman, 2006; Sharma, Gilley, Dorman, & Baldwin, 2007).  The P1 is a robust positive wave occurring at around 100 to 300 ms in children. P1 latency represents the synaptic delays throughout the peripheral and central auditory pathways (Eggermont, Ponton, Don, Waring, & Kwong, 1997).\nP1 latency changes as a function of age, and is considered an index of cortical auditory maturation (Ceponiene, Cheour, & Naatanen, 1998). P1 latency and age has a strong negative correlation, decrease in P1 latency with increasing age. This is most likely due to more efficient synaptic transmission over time.  The P1 waveform also becomes broader as we age. The P1 neural generators are thought to originate from the thalamo-cortical portion of the auditory cortex. Researchers believe that P1 may be the first recurrent activity in the auditory cortex (Kral & Eggermont, 2007). The negative component following P1 is called N1. N1 is not consistently seen in children until 12 years or age.\nIn 2006 Sharma & Dorman measured the P1 response in deaf children who received cochlear implants at different ages to examine the limits of plasticity in the central auditory system.  Those who received cochlear implant stimulation in early childhood (younger than 3.5 years) had normal P1 latencies. Children who received cochlear implant stimulation late in childhood (younger than seven years) had abnormal cortical responses latencies. However, children who received cochlear implant stimulation between the ages 3.5 and 7 years revealed variable latencies of the P1. Sharma also studied the waveform morphology of the P1 response in 2005   and 2007.  She found that in early implanted children the P1 waveform morphology was normal. For late implanted children, the P1 waveforms were abnormal and had lower amplitudes when compared to normal waveform morphology. In 2008 Gilley and colleagues used source reconstruction and dipole source analysis derived from high density EEG recordings to estimate generators for the P1 in three groups of children: normal hearing children, children receiving a cochlear implant before the age of four, and children receiving a cochlear implant after the age of seven. Findings concluded that the waveform morphology of normal hearing children and early implanted children were very similar.\n\nSedation protocols\nCommon sedative used\nTo achieve the highest-quality recordings for any recording potential, good patient relaxation is generally necessary. However, many recordings can be filled and contaminated with myogenic and movement artifacts. Patient restlessness and movement will contribute to threshold overestimation and inaccurate test results. In most cases, an adult is usually more than capable to provide a good extratympanic recording. In transtympanic recordings, a sedative can be used when time-consuming events need to take place. Most patients (especially infants) are given light anesthesia when test transtympanically.\nChloral hydrate is a commonly prescribed sedative, and most common for inducing sleep in young children and infants for AEP recordings. It uses alcohol to depress the central nervous system, specifically the cerebral cortex. Side effects of chloral hydrate include vomiting, nausea, gastric irritation, delirium, disorientation, allergic reactions and occasionally excitement \u2013 a high level of activity rather than becoming tired and falling asleep. Chloral hydrate is readily available in three forms \u2013 syrup, capsule and suppository. Syrup is most successful for those 4 months and older, proper dosage is poured in an oral syringe or cup. The syringe is used to squirt in the back of the mouth and then the child is encouraged to swallow. To induce sleep, dosages range anywhere from 500 mg to 2g, the recommended pediatric dose is equal to 50 mg per kg of body weight. A second dose no greater than the first dose, and an overall dose not exceeding 100 mg\/kg of body weight can be used if the child does not fall asleep after the first dose. Sedation personnel should include a physician and a registered or practical nurse. Documentation and monitoring of physiologic parameters is required throughout the entire process. Sedatives should only be administered in the presence of those who are knowledgeable and skilled in airway management and cardiopulmonary resuscitation (CPR).\nIncreasingly, Propofol is used intravenously via infusion pump for sedation.\n\nProcedures\nA consent form must be signed and received from the patient or guardian indicating the conscious sedation and the procedure being performed. Documented medical evaluation for pre-sedation purposes including a focused airway examination either on the same day as the sedation process or within recent days that will include but not limited to:\n\nAge and weight\nA complete and thorough medical history including all current medications, drug allergies, relevant disease, adverse drug reactions (especially relevant if any previous reaction to sedatives) and all relevant family history\nVerify any airway or respiratory problems\nAll medications taken (including dosage and history of specific drug use) on the day of the procedure\nFood and fluid intake within the 8 hours prior to sedation \u2013 light breakfast or lunch 1\u20132 hours prior to testing reduces likelihood of gastric irritation (common with chloral hydrate).\nAll vital signs\nAll orders for conscious sedation for patients must be written. Prescriptions or orders received from areas outside of the conscious sedation area are not acceptable. There has to be a single individual assigned to monitor the sedated patient's cardiorespiratory status before, during and after sedation.\nIf patient is deeply sedated, the individual's only job should be to verify and record vital signs no less than every five minutes. All age and size appropriate equipment and medications used to sustain life should be verified before sedation and should be readily available at any time during and after sedation.\nThe medication should be administered by a physician or nurse and documented (dosage, name, time, etc.). Children should not receive the sedative without supervision of a skilled and knowledgeable medical personnel (at home, technician). Emergency equipment including crash cart must be readily available and respiration monitoring should be done visually or with stethoscope. Family member needs to remain in room with patient, especially if tester steps out. In this scenario, respiration can be monitored acoustically with a talk-back system microphone placed near patient's head. Medical personnel should be notified of slow respiration state.\nAfter procedure is over, patient must be continuously observed in the facility that is appropriately equipped and staffed because patient's typically \"floppy\" and have poor motor control. Patients shouldn't stand on their own for the first few hours. No other medications with alcohol should be administered until patient is back to normal state. Drinking fluids is encouraged to reduce stomach irritation. Each facility should create and use their own discharge criteria. Verbal and written instructions should be provided on the topics of limitations of activity and anticipated changes in behavior. All discharge criteria must be met and documented before the patient leaves the facility.\nSome criteria prior to discharge should include:\n\nStable vital signs similar to those taken pre-procedure\nPatient is at the level of consciousness pre-procedure\nPatient has received post-procedure care instructions.\n\nSee also\nAuditory system\nBone conduction auditory brainstem response\nCochlea\nEEG\nEvoked potential\nOtoacoustic emission\nInternational Society of Audiology\n\nReferences\nFurther reading\nDon M, Kwong B, Tanaka C (2012). \"Interaural stacked auditory brainstem response measures for detecting small unilateral acoustic tumors\". Audiol. Neurootol. 17 (1): 54\u201368. doi:10.1159\/000329364. PMC 3169358. PMID 21829011.\n\nExternal links\nEmedicine article on Auditory Brainstem Response Audiometry\nBiological Psychology, PDF file describing research of related speech and hearing problems\nAuditory Neuroscience Laboratory at Northwestern University\nAmerican Academy of Audiology","13":"Auditory processing disorder (APD), rarely known as King-Kopetzky syndrome or auditory disability with normal hearing (ADN), is a neurodevelopmental disorder affecting the way the brain processes sounds. Individuals with APD usually have normal structure and function of the ear, but cannot process the information they hear in the same way as others do, which leads to difficulties in recognizing and interpreting sounds, especially the sounds composing speech. It is thought that these difficulties arise from dysfunction in the central nervous system. \nThe American Academy of Audiology notes that APD is diagnosed by difficulties in one or more auditory processes known to reflect the function of the central auditory nervous system. It can affect both children and adults, and may continue to affect children into adulthood. Although the actual prevalence is currently unknown, it has been estimated to impact 2\u20137% of children in US and UK populations. Males are twice as likely to be affected by the disorder as females.\nNeurodevelopmental forms of APD are different than aphasia because aphasia is by definition caused by acquired brain injury. However, acquired epileptic aphasia has been viewed as a form of APD.\n\nSigns and symptoms\nIndividuals with this disorder may experience the following signs and symptoms:\n\nspeaking louder or softer than is situationally appropriate;\ndifficulty remembering lists or sequences;\nneeding words or sentences to be repeated;\nimpaired ability to memorize information learned by listening;\ninterpreting words too literally;\nneeding assistance to hear clearly in noisy environments;\nrelying on accommodation and modification strategies;\nfinding or requesting a quiet work space away from others;\nrequesting written material when attending oral presentations; and\nasking for directions to be given one step at a time.\n\nRelation to attention deficit hyperactivity disorder\nAPD and attention deficit hyperactivity disorder (ADHD) can present with overlapping symptoms. Below is a ranked order of behavioral symptoms that are most frequently observed in each disorder. Professionals evaluated the overlap of symptoms between the two disorders; the order below is of symptoms that are almost always observed. Although the symptoms listed have differences, there are many similarities in how they may present in an individual, which can make it difficult to differentiate between the two conditions.\n\nThere is a co-occurrence between ADHD and APD. A systematic review published in 2018 detailed one study that showed 10% of children with APD have confirmed or suspected ADHD. It also stated that it is sometimes difficult to distinguish the two, since characteristics and symptoms between APD and ADHD tend to overlap. The systematic review also described this overlap between APD and other behavioral disorders and whether or not it was easy to distinguish those children that solely had auditory processing disorder.\n\nRelation to specific language impairment and developmental dyslexia\nThere has been considerable debate over the relationship between APD and specific language impairment (SLI).\nSLI is diagnosed when a child has difficulties with understanding or producing spoken language, and the cause of these difficulties is not obvious (and specifically cannot be explained by peripheral hearing loss). The child is typically late in their language development and may struggle to produce clear speech sounds and produce or understand complex sentences. Some theorize that SLI is the result of auditory processing problems. However, this theory is not universally accepted; others theorize that the main difficulties associated with SLI stem from problems with the higher-level aspects of language processing. Where a child has both auditory and language problems, it can be difficult to sort out the causality at play.\nSimilarly with developmental dyslexia, researchers continue to explore the hypothesis that reading problems emerge as a downstream consequence of difficulties in rapid auditory processing. Again, cause and effect can be hard to unravel. This is one reason why some experts have recommended using non-verbal auditory tests to diagnose APD. Specifically regarding neurological factors, dyslexia has been linked to polymicrogyria which causes cell migrational problems. Children that have polymicrogyri almost always present with deficits on APD testing. It has also been suggested that APD may be related to cluttering, a fluency disorder marked by word and phrase repetitions.\nSome studies found that a higher than expected proportion of individuals diagnosed with SLI and dyslexia on the basis of language and reading tests also perform poorly on tests in which auditory processing skills are tested. APD can be assessed using tests that involve identifying, repeating, or discriminating speech, and a child may perform poorly because of primary language problems. In a study comparing children with a diagnosis of dyslexia and those with a diagnosis of APD, they found the two groups could not be distinguished. Analogous results were observed in studies comparing children diagnosed with SLI or APD, the two groups presenting with similar diagnostic criteria. As such, the diagnosis a child receives may depend on which specialist they consult: the same child who might be diagnosed with APD by an audiologist may instead be diagnosed with SLI by a speech-language therapist, or with dyslexia by a psychologist.\n\nCauses\nAcquired\nAcquired APD can be caused by any damage to, or dysfunction of, the central auditory nervous system and can cause auditory processing problems. For an overview of neurological aspects of APD, see T. D. Griffiths's 2002 article \"Central Auditory Pathologies\".\n\nGenetics\nSome studies have indicated an increased prevalence of a family history of hearing impairment in these patients. The pattern of results is suggestive that auditory processing disorder may be related to conditions of autosomal dominant inheritance. In other words, the ability to listen to and comprehend multiple messages at the same time is a trait that is heavily influenced by our genes. These \"short circuits in the wiring\" sometimes run in families or result from a difficult birth, just like any learning disability. Inheritance of auditory processing disorder refers to whether an individual inherits the condition from their parents, or whether it \"runs\" in families.  Central auditory processing disorder may be hereditary neurological traits from the mother or the father.\n\nDevelopmental\nIn the majority of cases of developmental APD, the cause is unknown. An exception is acquired epileptic aphasia or Landau\u2013Kleffner syndrome, where a child's development regresses, with language comprehension severely affected. The child is often thought to be deaf, but testing reveals normal peripheral hearing. In other cases, suspected or known causes of APD in children include delay in myelin maturation, ectopic (misplaced) cells in the auditory cortical areas, or genetic predisposition. In one family with autosomal dominant epilepsy, seizures which affected the left temporal lobe seemed to cause problems with auditory processing. In another extended family with a high rate of APD, genetic analysis showed a haplotype in chromosome 12 that fully co-segregated with language impairment.\nHearing begins in utero, but the central auditory system continues to develop for at least the first decade after birth. There is considerable interest in the idea that disruption to hearing during a sensitive period may have long-term consequences for auditory development. One study showed thalamocortical connectivity in vitro was associated with a time sensitive developmental window and required a specific cell adhesion molecule (lcam5) for proper brain plasticity to occur. This points to connectivity between the thalamus and cortex shortly after being able to hear (in vitro) as at least one critical period for auditory processing. Another study showed that rats reared in a single tone environment during critical periods of development had permanently impaired auditory processing. In rats, \"bad\" auditory experiences, such as temporary deafness by cochlear removal, leads to neuron shrinkage. In a study looking at attention in APD patients, children with one ear blocked developed a strong right-ear advantage but were not able to modulate that advantage during directed-attention tasks.\nIn the 1980s and 1990s, there was considerable interest in the role of chronic otitis media (also called middle ear disease or \"glue ear\") in causing APD and related language and literacy problems. Otitis media with effusion is a very common childhood disease that causes a fluctuating conductive hearing loss, and there was concern this may disrupt auditory development if it occurred during a sensitive period. Consistent with this, in a sample of young children with chronic ear infections recruited from a hospital otorhinolaryngology department, increased rates of auditory difficulties were found later in childhood. However, this kind of study will have sampling bias because children with otitis media will be more likely to be referred to hospital departments if they are experiencing developmental difficulties. Compared with hospital studies, epidemiological studies, which assesses a whole population for otitis media and then evaluate outcomes, found much weaker evidence for long-term impacts of otitis media on language outcomes.\n\nSomatic\nIt seems that somatic anxiety (that is, physical symptoms of anxiety such as butterflies in the stomach or cotton mouth) and situations of stress may be determinants of speech-hearing disability.\n\nDiagnosis\nQuestionnaires which address common listening problems can be used to identify individuals who may have auditory processing disorder, and can help in the decision to pursue clinical evaluation. \nOne of the most common listening problems is speech recognition in the presence of background noise. \nAccording to the respondents who participated in a study by Neijenhuis, de Wit, and Luinge (2017), symptoms of APD which are characteristic in children with listening difficulties, and are typically problematic with adolescents and adults, include:\n\nDifficulty hearing in noisy environments\nAuditory attention problems\nUnderstanding speech more easily in one-on-one situations\nDifficulties in noise localization\nDifficulties in remembering oral information\nAccording to the New Zealand Guidelines on Auditory Processing Disorders (2017), the following checklist of key symptoms of APD or comorbidities can be used to identify individuals who should be referred for audiological and APD assessment:\n\nDifficulty following spoken directions unless they are brief and simple\nDifficulty attending to and remembering spoken information\nSlowness in processing spoken information\nDifficulty understanding in the presence of other sounds\nOverwhelmed by complex or \"busy\" auditory environments e.g. classrooms, shopping malls\nPoor listening skills\nInsensitivity to tone of voice or other nuances of speech\nAcquired brain injury\nHistory of frequent or persistent middle ear disease (otitis media, \"glue ear\").\nDifficulty with language, reading, or spelling\nSuspicion or diagnosis of dyslexia\nSuspicion or diagnosis of language disorder or delay\nFinally, the New Zealand guidelines state that behavioral checklists and questionnaires should only be used to provide guidance for referrals, for information gathering (for example, prior to assessment or as outcome measures for interventions), and as measures to describe the functional impact of auditory processing disorder. They are not designed for the purpose of diagnosing auditory processing disorders. The New Zealand guidelines indicate that a number of questionnaires have been developed to identify children who might benefit from evaluation of their problems in listening. Examples of available questionnaires include the Fisher's Auditory Problems Checklist, the Children's Auditory Performance Scale, the Screening Instrument for Targeting Educational Risk, and the Auditory Processing Domains Questionnaire among others. All of the previous questionnaires were designed for children and none are useful for adolescents and adults.\nThe University of Cincinnati Auditory Processing Inventory (UCAPI) was designed for use with adolescents and adults seeking testing for evaluation of problems with listening and\/or to be used following diagnosis of an auditory processing disorder to determine the subject's status. Following a model described by Zoppo et al. (2015), a 34-item questionnaire was developed that investigates auditory processing abilities in each of the six common areas of complaint in APD (listening and concentration, understanding speech, following spoken instructions, attention, and other.) The final questionnaire was standardized on normally-achieving young adults ranging from 18 to 27 years of age. Validation data was acquired from subjects with language-learning or auditory processing disorders who were either self-reported or confirmed by diagnostic testing. A UCAPI total score is calculated by combining the totals from the six listening conditions and provides an overall value to categorize listening abilities. Additionally, analysis of the scores from the six listening conditions provides an auditory profile for the subject. Each listening condition can then be utilized by the professional in making recommendation for diagnosing problem of learning through listening and treatment decisions. The UCAPI provides information on listening problems in various populations that can aid examiners in making recommendations for assessment and management.\nAPD has been defined anatomically in terms of the integrity of the auditory areas of the nervous system. However, children with symptoms of APD typically have no evidence of neurological disease, so the diagnosis is made based on how the child performs behavioral auditory tests. Auditory processing is \"what we do with what we hear\", and in APD there is a mismatch between peripheral hearing ability (which is typically normal) and ability to interpret or discriminate sounds. Thus in those with no signs of neurological impairment, APD is diagnosed on the basis of auditory tests. There is, however, no consensus as to which tests should be used for diagnosis, as evidenced by the succession of task force reports that have appeared in recent years.\nThe first of these occurred in 1996. This was followed by a conference organized by the American Academy of Audiology.\nExperts attempting to define diagnostic criteria have to grapple with the problem that a child may do poorly on an auditory test for reasons other than poor auditory perception: for instance, failure could be due to inattention, difficulty in coping with task demands, or limited language ability. In an attempt to rule out at least some of these factors, the American Academy of Audiology conference explicitly advocated that for APD to be diagnosed, the child must have a modality-specific problem, i.e. affecting auditory but not visual processing. However, a committee of the American Speech-Language-Hearing Association subsequently rejected modality-specificity as a defining characteristic of auditory processing disorders.\n\nDefinitions\nin 2005 the American Speech\u2013Language\u2013Hearing Association published \"Central Auditory Processing Disorders\" as an update to the 1996 pulication, \"Central Auditory Processing: Current Status of Research and Implications for Clinical Practice\". The American Academy of Audiology has released more current practice guidelines related to the disorder. ASHA formally defines APD as \"a difficulty in the efficiency and effectiveness by which the central nervous system (CNS) utilizes auditory information.\"\nIn 2018, the British Society of Audiology published a \"position statement and practice guidance\" on auditory processing disorder and updated its definition of APD. According to the Society, APD refers to the inability to process speech and on-speech sounds.\nAuditory processing disorder can be developmental or acquired. It may result from ear infections, head injuries, or neurodevelopmental delays that affect processing of auditory information. This can include problems with: \"...sound localization and lateralization (see also binaural fusion); auditory discrimination; auditory pattern recognition; temporal aspects of audition, including temporal integration, temporal discrimination (e.g., temporal gap detection), temporal ordering, and temporal masking; auditory performance in competing acoustic signals (including dichotic listening); and auditory performance with degraded acoustic signals\".\nThe Committee of UK Medical Professionals Steering the UK Auditory Processing Disorder Research Program have developed the following working definition of auditory processing disorder: \"APD results from impaired neural function and is characterized by poor recognition, discrimination, separation, grouping, localization, or ordering of speech sounds. It does not solely result from a deficit in general attention, language or other cognitive processes.\"\n\nTypes of testing\nThe SCAN-C for children and SCAN-A for adolescents and adults are the most common tools for screening and diagnosing APD in the USA. Both tests are standardized on a large number of subjects and include validation data on subjects with auditory processing disorders. The SCAN test batteries include screening tests: norm-based criterion-referenced scores; diagnostic tests: scaled scores, percentile ranks and ear advantage scores for all tests except the Gap Detection test. The four tests include four subsets on which the subject scores are derived include: discrimination of monaurally presented single words against background noise (speech in noise), acoustically degraded single words (filtered words), dichotically presented single words and sentences.\nRandom Gap Detection Test (RGDT) is also a standardized test. It assesses an individual's gap detection threshold of tones and white noise. The exam includes stimuli at four different frequencies (500, 1000, 2000, and 4000 Hz) and white noise clicks of 50 ms duration. This test provides an index of auditory temporal resolution. In children, an overall gap detection threshold greater than 20 ms means they have failed and may have an auditory processing disorder based on abnormal perception of sound in the time domain.\nGaps in Noise Test (GIN) also measures temporal resolution by testing the patient's gap detection threshold in white noise.\nPitch Patterns Sequence Test (PPT) and Duration Patterns Sequence Test (DPT) measure auditory pattern identification. The PPS has s series of three tones presented at either of two pitches (high or low). Meanwhile, the DPS has a series of three tones that vary in duration rather than pitch (long or short). Patients are then asked to describe the pattern of pitches presented.\nMasking Level Difference (MLD) at 500 Hz measures overlapping temporal processing, binaural processing, and low-redundancy by measuring the difference in threshold of an auditory stimulus when a masking noise is presented in and out of phase.\nThe Staggered Spondaic Word Test (SSW) is one of the oldest tests for APD developed by Jack Katz. Although it has fallen into some disuse by audiologists as it is complicated to score, it is one of the quickest and most sensitive tests to determine APD.\n\nModality-specificity and controversies\nThe issue of modality-specificity has led to considerable debate among experts in this field. Cacace and McFarland have argued that APD should be defined as a modality-specific perceptual dysfunction that is not due to peripheral hearing loss. They criticize more inclusive conceptualizations of APD as lacking diagnostic specificity. A requirement for modality-specificity could potentially avoid including children whose poor auditory performance is due to general factors such as poor attention or memory. Others, however, have argued that a modality-specific approach is too narrow, and that it would miss children who had genuine perceptual problems affecting both visual and auditory processing. It is also impractical, as audiologists do not have access to standardized tests that are visual analogs of auditory tests. The debate over this issue remains unresolved between modality-specific researchers such as Cacace, and associations such as the American Speech-Language-Hearing Association (among others). It is clear, however, that a modality-specific approach will diagnose fewer children with APD than a modality-general one, and that the latter approach runs a risk of including children who fail auditory tests for reasons other than poor auditory processing. Although modality-specific testing has been advocated for well over a decade, the visual analog of APD testing has met with sustained resistance from the fields of optometry and ophthalmology.\nAnother controversy concerns the fact that most traditional tests of APD use verbal materials. The British Society of Audiology has embraced Moore's (2006) recommendation that tests for APD should assess processing of non-speech sounds. The concern is that if verbal materials are used to test for APD, then children may fail because of limited language ability. An analogy may be drawn with trying to listen to sounds in a foreign language. It is much harder to distinguish between sounds or to remember a sequence of words in a language you do not know well: the problem is not an auditory one, but rather due to lack of expertise in the language.\nIn recent years there have been additional criticisms of some popular tests for diagnosis of APD. Tests that use tape-recorded American English have been shown to over-identify APD in speakers of other forms of English. Performance on a battery of non-verbal auditory tests devised by the Medical Research Council's Institute of Hearing Research was found to be heavily influenced by non-sensory task demands, and indices of APD had low reliability when this was controlled for. This research undermines the validity of APD as a distinct entity in its own right and suggests that the use of the term \"disorder\" itself is unwarranted. In a recent review of such diagnostic issues, it was recommended that children with suspected auditory processing impairments receive a holistic psychometric assessment including general intellectual ability, auditory memory, and attention, phonological processing, language, and literacy. The authors state that \"a clearer understanding of the relative contributions of perceptual and non-sensory, unimodal and supramodal factors to performance on psychoacoustic tests may well be the key to unraveling the clinical presentation of these individuals.\"\nDepending on how it is defined, APD may share common symptoms with ADD\/ADHD, specific language impairment, and autism spectrum disorders. A review showed substantial evidence for atypical processing of auditory information in children with autism. Dawes and Bishop noted how specialists in audiology and speech-language pathology often adopted different approaches to child assessment, and they concluded their review as follows: \"We regard it as crucial that these different professional groups work together in carrying out assessment, treatment and management of children and undertaking cross-disciplinary research.\" In practice, this seems rare.\nTo ensure that APD is correctly diagnosed, the examiners must differentiate APD from other disorders with similar symptoms. Factors that should be taken into account during the diagnosis are: attention, auditory neuropathy, fatigue, hearing and sensitivity, intellectual and developmental age, medications, motivation, motor skills, native language and language experience, response strategies and decision-making style, and visual acuity.\nIt should also be noted that children under the age of seven cannot be evaluated correctly because their language and auditory processes are still developing. In addition, the presence of APD cannot be evaluated when a child's primary language is not English.\n\nCharacteristics\nThe American Speech-Language-Hearing Association state that children with (central) auditory processing disorder often:\n\nhave trouble paying attention to and remembering information presented orally, and may cope better with visually acquired information\nhave problems carrying out multi-step directions given orally; need to hear only one direction at a time\nhave poor listening skills\nneed more time to process information\nhave difficulty learning a new language\nhave difficulty understanding jokes, sarcasm, and learning songs or nursery rhymes\nhave language difficulties (e.g., they confuse syllable sequences and have problems developing vocabulary and understanding language)\nhave difficulty with reading, comprehension, spelling, and vocabulary\nAPD can manifest as problems determining the direction of sounds, difficulty perceiving differences between speech sounds and the sequencing of these sounds into meaningful words, confusing similar sounds such as \"hat\" with \"bat\", \"there\" with \"where\", etc.  Fewer words may be perceived than were actually said, as there can be problems detecting the gaps between words, creating the sense that someone is speaking unfamiliar or nonsense words. In addition, it is common for APD to cause speech errors involving the distortion and substitution of consonant sounds. Those with APD may have problems relating what has been said with its meaning, despite obvious recognition that a word has been said, as well as repetition of the word.  Background noise, such as the sound of a radio, television or a noisy bar can make it difficult to impossible to understand speech, since spoken words may sound distorted either into irrelevant words or words that do not exist, depending on the severity of the auditory processing disorder. Using a telephone can be problematic for someone with auditory processing disorder, in comparison with someone with normal auditory processing, due to low quality audio, poor signal, intermittent sounds, and the chopping of words. Many who have auditory processing disorder subconsciously develop visual coping strategies, such as lip reading, reading body language, and eye contact, to compensate for their auditory deficit, and these coping strategies are not available when using a telephone.\nAs noted above, the status of APD as a distinct disorder has been queried, especially by speech-language pathologists and psychologists, who note the overlap between clinical profiles of children diagnosed with APD and those with other forms of specific learning disability. Many audiologists, however, would dispute that APD is just an alternative label for dyslexia, SLI, or ADHD, noting that although it often co-occurs with these conditions, it can be found in isolation.\n\nSubcategories\nBased on sensitized measures of auditory dysfunction and on psychological assessment, patients can be subdivided into seven subcategories:\n\nmiddle ear dysfunction\nmild cochlear pathology\ncentral\/medial olivocochlear efferent system (MOCS) auditory dysfunction\npurely psychological problems\nmultiple auditory pathologies\ncombined auditory dysfunction and psychological problems\nunknown\nDifferent subgroups may represent different pathogenic and etiological factors. Thus, subcategorization provides further understanding of the basis of auditory processing disorder, and hence may guide the rehabilitative management of these patients. This was suggested by Professor Dafydd Stephens and F Zhao at the Welsh Hearing Institute, Cardiff University.\n\nTreatment\nTreatment of APD typically focuses on three primary areas: changing learning environment, developing higher-order skills to compensate for the disorder, and remediation of the auditory deficit itself. However, there is a lack of well-conducted evaluations of intervention using randomized controlled trial methodology. Most evidence for effectiveness adopts weaker standards of evidence, such as showing that performance improves after training. This does not control for possible influences of practice, maturation, or placebo effects. Recent research has shown that practice with basic auditory processing tasks (i.e. auditory training) may improve performance on auditory processing measures and phonemic awareness measures.  Changes after auditory training have also been recorded at the physiological level. Many of these tasks are incorporated into computer-based auditory training programs such as Earobics and Fast ForWord, an adaptive software available at home and in clinics worldwide,  but overall, evidence for effectiveness of these computerized interventions in improving language and literacy is not impressive. One small-scale uncontrolled study reported successful outcomes for children with APD using auditory training software.\nTreating additional issues related to APD can result in success. For example, treatment for phonological disorders (difficulty in speech) can result in success in terms of both the phonological disorder as well as APD. In one study, speech therapy improved auditory evoked potentials (a measure of brain activity in the auditory portions of the brain).\nWhile there is evidence that language training is effective for improving APD, there is no current research supporting the following APD treatments:\n\nAuditory Integration Training typically involves a child attending two 30-minute sessions per day for ten days.\nLindamood-Bell Learning Processes (particularly, the Visualizing and Verbalizing program)\nPhysical activities that require frequent crossing of the midline (e.g., occupational therapy)\nSound Field Amplification\nNeuro-Sensory Educational Therapy\nNeurofeedback\nThe use of an individual FM transmitter\/receiver system by teachers and students has nevertheless been shown to produce significant improvements with children over time.\n\nHistory\nSamuel J. Kopetzky first described the condition in 1948. P. F. King, first discussed the etiological factors behind it in 1954. Helmer Rudolph Myklebust's 1954 study, \"Auditory Disorders in Children\". suggested auditory processing disorder was separate from language learning difficulties. His work sparked interest in auditory deficits after acquired brain lesions affecting the temporal lobes  and led to additional work looking at the physiological basis of auditory processing, but it was not until the late seventies and early eighties that research began on APD in depth.\nIn 1977, the first conference on the topic of APD was organized by Robert W. Keith, Ph.D. at the University of Cincinnati.  The proceedings of that conference was published by Grune and Stratton under the title \"Central Auditory Dysfunction\" (Keith RW Ed.)  That conference started a new series of studies focusing on APD in children. Virtually all tests currently used to diagnose APD originate from this work. These early researchers also invented many of the auditory training approaches, including interhemispheric transfer training and interaural intensity difference training. This period gave us a rough understanding of the causes and possible treatment options for APD.\nMuch of the work in the late nineties and 2000s has been looking to refining testing, developing more sophisticated treatment options, and looking for genetic risk factors for APD. Scientists have worked on improving behavioral tests of auditory function, neuroimaging, electroacoustic, and electrophysiologic testing. Working with new technology has led to a number of software programs for auditory training. With global awareness of mental disorders and increasing understanding of neuroscience, auditory processing is more in the public and academic consciousness than in years past.\n\nSee also\nAmblyaudia\nAuditory verbal agnosia\nCocktail party effect\nCortical deafness\nDafydd Stephens\nEchoic memory\nHearing loss\nLanguage processing\nList of eponymous diseases\nMusic-specific disorders\nSelective auditory attention\nSelective mutism\nSensory processing disorders\nSpatial hearing loss\n\nReferences\nExternal links\nAuditory processing disorder: An overview for the clinician\nAmerican Speech-Language-Hearing Association (ASHA)","14":"The auditory system is the sensory system for the sense of hearing.  It includes both the sensory organs (the ears) and the auditory parts of the sensory system.\n\nSystem overview\nThe outer ear funnels sound vibrations to the eardrum, increasing the sound pressure in the middle frequency range. The middle-ear ossicles further amplify the vibration pressure roughly 20 times. The base of the stapes couples vibrations into the cochlea via the oval window, which vibrates the perilymph liquid (present throughout the inner ear) and causes the round window to bulb out as the oval window bulges in.\nVestibular and tympanic ducts are filled with perilymph, and the smaller cochlear duct between them is filled with endolymph, a fluid with a very different ion concentration and voltage. Vestibular duct perilymph vibrations bend organ of Corti outer cells (4 lines) causing prestin to be released in cell tips. This causes the cells to be chemically elongated and shrunk (somatic motor), and hair bundles to shift which, in turn, electrically affects the basilar membrane's movement (hair-bundle motor). These motors (outer hair cells) amplify the traveling wave amplitudes over 40-fold. The outer hair cells (OHC) are minimally innervated by spiral ganglion in slow (unmyelinated) reciprocal communicative bundles (30+ hairs per nerve fiber); this contrasts inner hair cells (IHC) that have only afferent innervation (30+ nerve fibers per one hair) but are heavily connected. There are three to four times as many OHCs as IHCs. The basilar membrane (BM) is a barrier between scalae, along the edge of which the IHCs and OHCs sit. Basilar membrane width and stiffness vary to control the frequencies best sensed by the IHC. At the cochlear base the BM is at its narrowest and most stiff (high-frequencies), while at the cochlear apex it is at its widest and least stiff (low-frequencies).  The tectorial membrane (TM) helps facilitate cochlear amplification by stimulating OHC (direct) and IHC (via endolymph vibrations). TM width and stiffness parallels BM's and similarly aids in frequency differentiation.\nThe superior olivary complex (SOC), in the pons, is the first convergence of the left and right cochlear pulses. SOC has 14 described nuclei; their abbreviation are used here (see Superior olivary complex for their full names). MSO determines the angle the sound came from by measuring time differences in left and right info. LSO normalizes sound levels between the ears; it uses the sound intensities to help determine sound angle. LSO innervates the IHC. VNTB innervate OHC. MNTB inhibit LSO via glycine. LNTB are glycine-immune, used for fast signalling. DPO are high-frequency and tonotopical. DLPO are low-frequency and tonotopical. VLPO have the same function as DPO, but act in a different area. PVO, CPO, RPO, VMPO, ALPO and SPON (inhibited by glycine) are various signalling and inhibiting nuclei.\nThe trapezoid body is where most of the cochlear nucleus (CN) fibers decussate (cross left to right and vice versa); this cross aids in sound localization. The CN breaks into ventral (VCN) and dorsal (DCN) regions. The VCN has three nuclei. Bushy cells transmit timing info, their shape averages timing jitters.  Stellate (chopper) cells encode sound spectra (peaks and valleys) by spatial neural firing rates based on auditory input strength (rather than frequency). Octopus cells have close to the best temporal precision while firing, they decode the auditory timing code. The DCN has 2 nuclei. DCN also receives info from VCN. Fusiform cells integrate information to determine spectral cues to locations (for example, whether a sound originated from in front or behind). Cochlear nerve fibers (30,000+) each have a most sensitive frequency and respond over a wide range of levels.\nSimplified, nerve fibers' signals are transported by bushy cells to the binaural areas in the olivary complex, while signal peaks and valleys are noted by stellate cells, and signal timing is extracted by octopus cells. The lateral lemniscus has three nuclei: dorsal nuclei respond best to bilateral input and have complexity tuned responses; intermediate nuclei have broad tuning responses; and ventral nuclei have broad and moderately complex tuning curves. Ventral nuclei of lateral lemniscus help the inferior colliculus (IC) decode amplitude modulated sounds by giving both phasic and tonic responses (short and long notes, respectively). IC receives inputs not shown, including visual (pretectal area: moves eyes to sound. superior colliculus: orientation and behavior toward objects, as well as eye movements (saccade)) areas, pons (superior cerebellar peduncle: thalamus to cerebellum connection\/hear sound and learn behavioral response), spinal cord (periaqueductal grey: hear sound and instinctually move), and thalamus. The above are what implicate IC in the 'startle response' and ocular reflexes. Beyond multi-sensory integration IC responds to specific amplitude modulation frequencies, allowing for the detection of pitch. IC also determines time differences in binaural hearing. The medial geniculate nucleus divides into ventral (relay and relay-inhibitory cells: frequency, intensity, and binaural info topographically relayed), dorsal (broad and complex tuned nuclei: connection to somatosensory info), and medial (broad, complex, and narrow tuned nuclei: relay intensity and sound duration). The auditory cortex (AC) brings sound into awareness\/perception. AC identifies sounds (sound-name recognition) and also identifies the sound's origin location. AC is a topographical frequency map with bundles reacting to different harmonies, timing and pitch. Right-hand-side AC is more sensitive to tonality, left-hand-side AC is more sensitive to minute sequential differences in sound. Rostromedial and ventrolateral prefrontal cortices are involved in activation during tonal space and storing short-term memories, respectively. The Heschl's gyrus\/transverse temporal gyrus includes Wernicke's area and functionality, it is heavily involved in emotion-sound, emotion-facial-expression, and sound-memory processes. The entorhinal cortex is the part of the 'hippocampus system' that aids and stores visual and auditory memories. The supramarginal gyrus (SMG) aids in language comprehension and is responsible for compassionate responses. SMG links sounds to words with the angular gyrus and aids in word choice. SMG integrates tactile, visual, and auditory info.\n\nStructure\nOuter ear\nThe folds of cartilage surrounding the ear canal are called the auricle. Sound waves are reflected and attenuated when they hit the auricle, and these changes provide additional information that will help the brain determine the sound direction.\nThe sound waves enter the auditory canal, a deceptively simple tube.  The ear canal amplifies sounds that are between 3 and 12 kHz. The tympanic membrane, at the far end of the ear canal marks the beginning of the middle ear.\n\nMiddle ear\nSound waves travel through the ear canal and hit the tympanic membrane, or eardrum. This wave information travels across the air-filled middle ear cavity via a series of delicate bones: the malleus (hammer), incus (anvil) and stapes (stirrup). These ossicles act as a lever, converting the lower-pressure eardrum sound vibrations into higher-pressure sound vibrations at another, smaller membrane called the oval window or vestibular window. The manubrium (handle) of the malleus articulates with the tympanic membrane, while the footplate (base) of the stapes articulates with the oval window. Higher pressure is necessary at the oval window than at the tympanic membrane because the inner ear beyond the oval window contains liquid rather than air. The stapedius reflex of the middle ear muscles helps protect the inner ear from damage by reducing the transmission of sound energy when the stapedius muscle is activated in response to sound.  The middle ear still contains the sound information in wave form; it is converted to nerve impulses in the cochlea.\n\nInner ear\nThe inner ear consists of the cochlea and several non-auditory structures.  The cochlea has three fluid-filled sections (i.e. the scala media, scala tympani and scala vestibuli), and supports a fluid wave driven by pressure across the basilar membrane separating two of the sections. Strikingly, one section, called the cochlear duct or scala media, contains endolymph. The organ of Corti is located in this duct on the basilar membrane, and transforms mechanical waves to electric signals in neurons. The other two sections are known as the scala tympani and the scala vestibuli. These are located within the bony labyrinth, which is filled with fluid called perilymph, similar in composition to cerebrospinal fluid. The chemical difference between the fluids endolymph and perilymph fluids is important for the function of the inner ear due to electrical potential differences between potassium and calcium ions.\nThe plan view of the human cochlea (typical of all mammalian and most vertebrates) shows where specific frequencies occur along its length. The frequency is an approximately exponential function of the length of the cochlea within the Organ of Corti. In some species, such as bats and dolphins, the relationship is expanded in specific areas to support their active sonar capability.\n\nOrgan of Corti\nThe organ of Corti forms a ribbon of sensory epithelium which runs lengthwise down the cochlea's entire scala media. Its hair cells transform the fluid waves into nerve signals. The journey of countless nerves begins with this first step; from here, further processing leads to a panoply of auditory reactions and sensations.\n\nHair cell\nHair cells are columnar cells, each with a \"hair bundle\" of 100\u2013200 specialized stereocilia at the top, for which they are named. There are two types of hair cells specific to the auditory system; inner and outer hair cells. Inner hair cells are the mechanoreceptors for hearing: they transduce the vibration of sound into electrical activity in nerve fibers, which is transmitted to the brain. Outer hair cells are a motor structure. Sound energy causes changes in the shape of these cells, which serves to amplify sound vibrations in a frequency specific manner. Lightly resting atop the longest cilia of the inner hair cells is the tectorial membrane, which moves back and forth with each cycle of sound, tilting the cilia, which is what elicits the hair cells' electrical responses.\nInner hair cells, like the photoreceptor cells of the eye, show a  graded response, instead of the spikes typical of other neurons.  These graded potentials are not bound by the \"all or none\" properties of an action potential.\nAt this point, one may ask how such a wiggle of a hair bundle triggers a difference in membrane potential. The current model is that cilia are attached to one another by \"tip links\", structures which link the tips of one cilium to another. Stretching and compressing, the tip links may open an ion channel and produce the receptor potential in the hair cell. Recently it has been shown that cadherin-23 CDH23 and protocadherin-15 PCDH15 are the adhesion molecules associated with these tip links. It is thought that a calcium driven motor causes a  shortening of these links to regenerate tensions. This regeneration of tension allows for apprehension of prolonged auditory stimulation.\n\nNeurons\nAfferent neurons innervate cochlear inner hair cells, at synapses where the neurotransmitter glutamate communicates signals from the hair cells to the dendrites of the primary auditory neurons.\nThere are far fewer inner hair cells in the cochlea than afferent nerve fibers \u2013 many auditory nerve fibers innervate each hair cell. The neural dendrites belong to neurons of the auditory nerve, which in turn joins the vestibular nerve to form the vestibulocochlear nerve, or cranial nerve number VIII.\nThe region of the basilar membrane supplying the inputs to a particular afferent nerve fibre can be considered to be its receptive field.\nEfferent projections from the brain to the cochlea also play a role in the perception of sound, although this is not well understood. Efferent synapses occur on outer hair cells and on afferent (towards the brain) dendrites under inner hair cells\n\nNeuronal structure\nCochlear nucleus\nThe cochlear nucleus is the first site of the neuronal processing of the newly converted \"digital\" data from the inner ear (see also binaural fusion).  In mammals, this region is anatomically and physiologically split into two regions, the dorsal cochlear nucleus (DCN), and ventral cochlear nucleus (VCN).  The VCN is further divided by the nerve root into the posteroventral cochlear nucleus (PVCN) and the anteroventral cochlear nucleus (AVCN).\n\nTrapezoid body\nThe trapezoid body is a bundle of decussating fibers in the ventral pons that carry information used for binaural computations in the brainstem. Some of these axons come from the cochlear nucleus and cross over to the other side before traveling on to the superior olivary nucleus.  This is believed to help with localization of sound.\n\nSuperior olivary complex\nThe superior olivary complex is located in the pons, and receives projections predominantly from the ventral cochlear nucleus, although the dorsal cochlear nucleus projects there as well, via the ventral acoustic stria.  Within the superior olivary complex lies the lateral superior olive (LSO) and the medial superior olive (MSO).  The former is important in detecting interaural level differences while the latter is important in distinguishing interaural time difference.\n\nLateral lemniscus\nThe lateral lemniscus is a tract of axons in the brainstem that carries information about sound from the cochlear nucleus to various brainstem nuclei and ultimately the contralateral inferior colliculus of the midbrain.\n\nInferior colliculi\nThe inferior colliculi (IC) are located just below the visual processing centers known as the superior colliculi.  The central nucleus of the IC is a nearly obligatory relay in the ascending auditory system, and most likely acts to integrate information (specifically regarding sound source localization from the superior olivary complex and dorsal cochlear nucleus) before sending it to the thalamus and cortex.  The inferior colliculus also receives descending inputs from the auditory cortex and auditory thalamus (or medial geniculate nucleus).\n\nMedial geniculate nucleus\nThe medial geniculate nucleus is part of the thalamic relay system.\n\nPrimary auditory cortex\nThe primary auditory cortex is the first region of cerebral cortex to receive auditory input.\nPerception of sound is associated with the left posterior superior temporal gyrus (STG). The superior temporal gyrus contains several important structures of the brain, including Brodmann areas 41 and 42, marking the location of the primary auditory cortex, the cortical region responsible for the sensation of basic characteristics of sound such as pitch and rhythm. We know from research in nonhuman primates that the primary auditory cortex can probably be divided further into functionally differentiable subregions.\n\nThe neurons of the primary auditory cortex can be considered to have receptive fields covering a range of auditory frequencies and have selective responses to harmonic pitches. Neurons integrating information from the two ears have receptive fields covering a particular region of auditory space.\nThe primary auditory cortex is surrounded by secondary auditory cortex, and interconnects with it. These secondary areas interconnect with further processing areas in the superior temporal gyrus, in the dorsal bank of the superior temporal sulcus, and in the frontal lobe. In humans, connections of these regions with the middle temporal gyrus are probably important for speech perception.  The frontotemporal system underlying auditory perception allows us to distinguish sounds as speech, music, or noise.\n\nThe auditory ventral and dorsal streams\nFrom the primary auditory cortex emerge two separate pathways: the auditory ventral stream and auditory dorsal stream. The auditory ventral stream includes the anterior superior temporal gyrus, anterior superior temporal sulcus, middle temporal gyrus and temporal pole. Neurons in these areas are responsible for sound recognition, and extraction of meaning from sentences. The auditory dorsal stream includes the posterior superior temporal gyrus and sulcus, inferior parietal lobule and intra-parietal sulcus. Both pathways project in humans to the inferior frontal gyrus. The most established role of the auditory dorsal stream in primates is sound localization. In humans, the auditory dorsal stream in the left hemisphere is also responsible for speech repetition and articulation, phonological long-term encoding of word names, and verbal working memory.\n\nClinical significance\nProper function of the auditory system is required to able to sense, process, and understand sound from the surroundings. Difficulty in sensing, processing and understanding sound input has the potential to adversely impact an individual's ability to communicate, learn and effectively complete routine tasks on a daily basis.\nIn children, early diagnosis and treatment of impaired auditory system function is an important factor in ensuring that key social, academic and speech\/language developmental milestones are met.\nImpairment of the auditory system can include any of the following:\n\nAuditory brainstem response and ABR audiometry test for newborn hearing\nAuditory processing disorder\nHyperacusis\nHealth effects due to noise\nTinnitus\nEndaural phenomena\n\nSee also\nLanguage processing in the brain\nNeuroscience of music\nSelective auditory attention\n\nReferences\nFurther reading\nKandel, Eric R. (2012). Principles of Neural Science. New York: McGraw-Hill. ISBN 978-0-07-139011-8. OCLC 795553723.\n\nExternal links\n\nPromenade 'round the cochlea\nAuditory system \u2013 Washington University Neuroscience Tutorial\nLincoln Gray. \"Chapter 13: Auditory System: Pathways and Reflexes\". Neuroscience Online, the Open-Access Neuroscience Electronic Textbook. The University of Texas Health Science Center at Houston (UTHealth). Archived from the original on 2016-11-12. Retrieved 27 April 2014.","15":"Benign paroxysmal positional vertigo (BPPV) is a disorder arising from a problem in the inner ear. Symptoms are repeated, brief periods of vertigo with movement, characterized by a spinning sensation upon changes in the position of the head. This can occur with turning in bed or changing position. Each episode of vertigo typically lasts less than one minute. Nausea is commonly associated. BPPV is one of the most common causes of vertigo.\nBPPV is a type of balance disorder along with labyrinthitis and M\u00e9ni\u00e8re's disease. It can result from a head injury or simply occur among those who are older. Often, a specific cause is not identified. When found, the underlying mechanism typically involves a small calcified otolith moving around loose in the inner ear. Diagnosis is typically made when the Dix\u2013Hallpike test results in nystagmus (a specific movement pattern of the eyes) and other possible causes have been ruled out. In typical cases, medical imaging is not needed.\nBPPV is easily treated with a number of simple movements such as the Epley maneuver or Half Somersault Maneuver (in case of diagonal\/rotational nystagmus), the Lempert maneuver (in case of horizontal nystagmus), the deep head hanging maneuver (in case of vertical nystagmus) or sometimes the less effective Brandt\u2013Daroff exercises. Medications, including antihistamines such as meclizine, may be used to help with nausea. There is tentative evidence that betahistine may help with vertigo, but its use is not generally needed. BPPV is not a serious medical condition, but may present serious risks of injury through falling or other spatial disorientation-induced accidents.\nWhen untreated, it might resolve in days to months; however, it may recur in some people. One can needlessly suffer from BPPV for years despite there being a simple and very effective cure. Short-term self-resolution of BPPV is unlikely because the effective cure maneuvers induce strong vertigo which the patient will naturally resist and not accidentally perform.\nThe first medical description of the condition occurred in 1921 by R\u00f3bert B\u00e1r\u00e1ny. Approximately 2.4% of people are affected at some point in time. Among those who live until their 80s, 10% have been affected. BPPV affects females twice as often as males. Onset is typically in people between the ages of 50 and 70.\n\nSigns and symptoms\nSymptoms:\n\nParoxysmal\u2014appears suddenly, and in episodes of short duration: lasts only seconds to minutes\nPositional\u2014is induced by a change in position, even slight\nVertigo\u2014a spinning dizziness, which must have a rotational component\nTorsional nystagmus\u2014a diagnostic symptom where the top of the eye rotates toward the affected ear in a beating or twitching fashion, which has a latency and can be fatigued (vertigo should lessen with deliberate repetition of the provoking maneuver): nystagmus should only last for 30 seconds to one minute\nPre-syncope\u2014(feeling faint) or syncope (fainting) is unusual, but possible\nVisual disturbance\u2014due to associated nystagmus, making it difficult to read or see during an attack\nNausea\u2014is often associated\nVomiting\u2014is common, depending on the strength of vertigo itself and the causes for this illness\nMany people will report a history of vertigo as a result of fast head movements. Many are also capable of describing the exact head movements that provoke their vertigo. Purely horizontal nystagmus and symptoms of vertigo lasting more than one minute can also indicate BPPV occurring in the horizontal semicircular canal.\nThe spinning sensation experienced from BPPV is usually triggered by movement of the head, will have a sudden onset, and can last anywhere from a few seconds to several minutes. The most common movements people report triggering a spinning sensation are tilting their heads upward in order to look at something and when rolling over in bed.\nPeople with BPPV do not experience other neurological deficits such as numbness or weakness. If those symptoms are present, a more serious etiology, such as posterior circulation stroke or ischemia, must be considered.\nThe most significant symptom is nystagmus as it is essential to determine the kind of nystagmus ( horizontal, vertical, or diagonal ) to select the correct cure maneuver.\n\nCause\nWithin the labyrinth of the inner ear lie collections of calcium crystals known as otoconia or otoliths. In people with BPPV, the otoconia are dislodged from their usual position within the utricle, and over time, migrate into one of the three semicircular canals (the posterior canal is most commonly affected due to its anatomical position). When the head is reoriented relative to gravity, the gravity-dependent movement of the heavier otoconial debris (colloquially \"ear rocks\") within the affected semicircular canal causes abnormal (pathological) endolymph fluid displacement and a resultant sensation of vertigo. This more common condition is known as canalithiasis. There is a direct link between the kind of nystagmus and which of the three semicircular canals is affected. With horizontal nystagmus (left-right eye movement) the horizontal (also called lateral) canal is affected, with vertical nystagmus (up-down eye movement) the superior (also called anterior) canal is affected, and with diagonal nystagmus (diagonal or rotational eye movement) the posterior canal is affected. Diagonal eye movement is easily confused with horizontal movement. This is important since it might result in selecting a wrong and thus ineffective cure maneuver.\nIn rare cases, the crystals themselves can adhere to a semicircular canal cupula, rendering it heavier than the surrounding endolymph. Upon reorientation of the head relative to gravity, the cupula is weighted down by the dense particles, thereby inducing an immediate and sustained excitation of semicircular canal afferent nerves. This condition is termed cupulolithiasis.\nThere is evidence in the dental literature that malleting of an osteotome during closed sinus floor elevation, otherwise known as osteotome sinus elevation or lift, transmits percussive and vibratory forces capable of detaching otoliths from their normal location and thereby leading to the symptoms of BPPV.\nBPPV can be triggered by any action that stimulates the posterior semi-circular canal including:\n\nLooking up or down\nFollowing head injury\nSudden head movement\nRolling over in bed\nTilting the head\nBPPV may be made worse by any number of modifiers which may vary among individuals:\n\nChanges in barometric pressure \u2013 people may feel increased symptoms up to two days before rain or snow\nLack of sleep (required amounts of sleep may vary widely)\nStress\nAn episode of BPPV may be triggered by dehydration, such as that caused by diarrhea.\nBPPV is one of the most common vestibular disorders in people presenting with dizziness; a migraine is implicated in idiopathic cases. Proposed mechanisms linking the two are genetic factors and vascular damage to the labyrinth.\nAlthough BPPV can occur at any age, it is most often seen in people older than the age of 60. Besides aging, there are no major risk factors known for BPPV, although previous episodes of head trauma, preexisting disorders, or the inner ear infection labyrinthitis, may predispose to the future development of BPPV.\n\nMechanism\nThe inside of the ear is composed of an organ called the vestibular labyrinth. The vestibular labyrinth includes three semicircular canals, which contain fluids and fine hairlike sensors that act as a monitor to the rotations of the head. Other important structures in the inner ear includes the otolith organs, the utricle and saccule, that contain calcium carbonate crystals(otoconia) that are sensitive to gravity.\nThe crystals may dislodge from the utricle (an otolith organ) and settle within the semicircular canals. When there is motion, the displaced otoconia shift within the endolymph of semicircular canals, causing an unbalanced (with respect to the opposite ear) stimulus, causing symptoms of BPPV.\n\nDiagnosis\nThe condition is diagnosed by the person's history, and by performing the Dix\u2013Hallpike test or the roll test, or both. The patient can also be asked to induce vertigo by performing a movement that the patient knows to induce vertigo. The eyes of the patient can then easily be observed for which kind (horizontal, vertical, or diagonal) of nystagmus is present, to determine which semicircular canal (horizontal, superior, or posterior) is affected.\nThe Dix\u2013Hallpike test is a common test performed by examiners to determine whether the posterior semicircular canal is involved. It involves a reorientation of the head to align the posterior semicircular canal (at its entrance to the ampulla) with the direction of gravity. This test will reproduce vertigo and nystagmus characteristic of posterior canal BPPV.\nWhen performing the Dix\u2013Hallpike test, people are lowered quickly to a supine position, with the neck extended by the person performing the maneuver. For some people, this maneuver may not be indicated, and a modification may be needed that also targets the posterior semicircular canal. Such people include those who are too anxious about eliciting the uncomfortable symptoms of vertigo, and those who may not have the range of motion necessary to comfortably be in a supine position. The modification involves the person moving from a seated position to side-lying without their head extending off the examination table, such as with Dix\u2013Hallpike. The head is rotated 45 degrees away from the side being tested, and the eyes are examined for nystagmus. A positive test is indicated by the patient report of a reproduction of vertigo and clinician observation of nystagmus. Both the Dix\u2013Hallpike and the side-lying testing position have yielded similar results, and as such the side-lying position can be used if the Dix\u2013Hallpike cannot be performed easily.\nThe roll test can determine whether the horizontal semicircular canal is involved. The roll test requires the person to be in a supine position with their head in 30\u00b0 of cervical flexion. Then the examiner quickly rotates the head 90\u00b0 to the left side, and checks for vertigo and nystagmus. This is followed by gently bringing the head back to the starting position. The examiner then quickly rotates the head 90\u00b0 to the right side and checks again for vertigo and nystagmus. In this roll test, the person may experience vertigo and nystagmus on both sides, but rotating toward the affected side will trigger a more intense vertigo. Similarly, when the head is rotated toward the affected side, the nystagmus will beat toward the ground and be more intense.\nAs mentioned above, both the Dix\u2013Hallpike and roll test provoke the signs and symptoms in subjects with archetypal BPPV. The signs and symptoms people with BPPV experience are typically a short-lived vertigo and observed nystagmus. In some people, although rarely, vertigo can persist for years. Assessment of BPPV is best done by a medical health professional skilled in the management of dizziness disorders, commonly a physiotherapist, audiologist, or other physician.\nThe nystagmus associated with BPPV has several important characteristics that differentiate it from other types of nystagmus.\n\nLatency of onset: there is a 5\u201310 second delay prior to onset of nystagmus\nNystagmus lasts for 5\u201360 seconds\nPositional: the nystagmus occurs only in certain positions\nRepeated stimulation, including via Dix\u2013Hallpike maneuvers, cause the nystagmus to fatigue or disappear temporarily\nRotatory\/torsional component is present, or (in the case of lateral canal involvement) the nystagmus beats in either a geotropic (toward the ground) or ageotropic (away from the ground) fashion\nVisual fixation suppresses nystagmus due to BPPV\nAlthough rare, disorders of the central nervous system can sometimes present as BPPV. A practitioner should be aware that if a person whose symptoms are consistent with BPPV, but does not show improvement or resolution after undergoing different particle repositioning maneuvers \u2014 detailed in the Treatment section below \u2014 need to have a detailed neurological assessment and imaging performed to help identify the pathological condition.\n\nDifferential diagnosis\nVertigo, a distinct process sometimes confused with the broader term, dizziness, accounts for about six million clinic visits in the United States every year; between 17 and 42% of these people are eventually diagnosed with BPPV.\nOther causes of vertigo include:\n\nMotion sickness\/motion intolerance: a disjunction between visual stimulation, vestibular stimulation, and\/or proprioception\nVisual exposure to nearby moving objects (examples of optokinetic stimuli include passing cars and falling snow)\nOther diseases: (labyrinthitis, M\u00e9ni\u00e8re's disease, and migraine, etc.)\n\nTreatment\nRepositioning maneuvers\nA number of maneuvers have been found to be effective including Canalith Repositioning Procedures (CRP) such as the Epley maneuver, the Half Somersault Maneuver (HSM), the Semont maneuver, and to a lesser degree the non-CRP Brandt\u2013Daroff exercises. Both the Epley and the Semont maneuvers are equally effective. The HSM can have better long-term success than the Epley, is more comfortable to experience, and has less risk of causing subsequent horizontal canal BPPV (H-BPPV).\nNone of these maneuvers addresses the presence of the particles (otoconia); rather it changes their location. The maneuvers aim to move these particles from some locations in the inner ear that cause symptoms such as vertigo and reposition them to where they do not cause these problems. These maneuvers are easily performed at home and online resources are available to patients.\nThe Epley maneuver is popular because it is designed to address posterior canal BPPV (PC-BPPV), which is caused by particles in the posterior semicircular canal, the most common cause of BPPV. This might give the wrong impression that PC-BPPV is the only kind of BPPV. Misdiagnosing which semicircular canal is affected, typically by confusing horizontal and diagonal nystagmus, or simply ignoring the identification of the affected canal, and then using the wrong treatment maneuver, regularly results in no cure.\nUsing the appropriate maneuver for the affected canal is essential. The maneuvers may be uncomfortable for the patient as they might induce strong vertigo and the patient might then resist performing them. Though some treatments, such as the Epley, are much more uncomfortable than others, such as the HSM. If the maneuver is not uncomfortable then it is possible the wrong maneuver has been selected by a misdiagnosis of the affected semicircular canal.\nAll the maneuvers consist of a series of steps in which the head is held in a specific position, typically for 30 to 60 seconds until any nystagmus stops. Movement from one position to the position of the next step has to be done fluently to give the particles enough momentum to move. A position has to be held until any nystagmus has completely resided, which indicates that the particles have stopped moving, before one proceeds to the next step.\n\nEpley maneuver\nThe Epley maneuver employs gravity to move the calcium crystal build-up from the posterior semicircular canal (resulting in diagonal nystagmus) that causes the condition. This maneuver can be performed during a clinic visit by health professionals, or taught to people to perform at home, or both. Postural restriction after the Epley maneuver increases its effect somewhat.\nWhen practiced at home, the Epley maneuver is more effective than the Semont maneuver. An effective repositioning treatment for posterior canal BPPV is the therapist-performed Epley combined with home-practiced Epley maneuvers. Devices such as the DizzyFIX can help users conduct the Epley maneuver at home, and are available for the treatment of BPPV.\n\nHalf Somersault Maneuver\nThe Half Somersault Maneuver (HSM) is a patient-performed alternative to the Epley for posterior canal BPPV (PC-BPPV). Like the Epley, it uses gravity to move the calcium crystal build-up from the posterior semicircular canal that causes the condition. Compared to the Epley, HSM has better long-term success, with less discomfort, and less risk of causing subsequent horizontal canal BPPV (H-BPPV).\n\nLempert maneuver or Roll maneuver\nFor the lateral (horizontal) canal, resulting in horizontal nystagmus, the Lempert maneuver has been used for productive results. It is unusual for the lateral canal to respond to the canalith repositioning procedure used for the posterior canal BPPV. Treatment is therefore geared toward moving the canalith from the lateral canal into the vestibule.\nThe roll maneuver or its variations are used, and involve rolling the person 360 degrees in a series of steps to reposition the particles. This maneuver is generally performed by a trained clinician who begins seated at the head of the examination table with the person supine. There are four stages, each a minute apart, and at the third position the horizontal canal is oriented in a vertical position with the person's neck flexed and on forearm and elbows. When all four stages are completed, the head roll test is repeated, and if negative, treatment ceases.\n\nDeep head hanging maneuver\nFor the superior (also called anterior) semicircular canal, resulting in vertical nystagmus, the Deep head hanging maneuver is used. The patient lays down on their back on a bed with their head overhanging the bed.\nIn the first step the head is turned as backward (hanging) as possible. In the following step, the patient remains lying but lifts their head with the chin close to the chest. In the last step, the patient sits upright with the head in the normal position. Before going to the next step, one has to wait until the nystagmus fully resides ( typical 30 to 60 seconds ) and progression from one step to the next has to happen in a fluid movement.\n\nSemont maneuver\nThe Semont maneuver has a cure rate of 90.3%. It is performed as follows:\n\nThe person is seated on a treatment table with their legs hanging off the side of the table. The therapist then turns the person's head 45 degrees toward the unaffected side.\nThe therapist then quickly tilts the person so they are lying on the affected side. The head position is maintained, so their head is turned up 45 degrees. This position is maintained for 3 minutes. The purpose is to allow the debris to move to the apex of the semicircular duct.\nThe person is then quickly moved so they are lying on the unaffected side with their head in the same position (now facing downward 45 degrees). This position is also held for 3 minutes. The purpose of this position is to allow the debris to move toward the exit of the semicircular duct.\nFinally, the person is slowly brought back to an upright seated position. The debris should then fall into the utricle of the canal and the symptoms of vertigo should decrease or end completely.\nSome people will only need one treatment, but others may need multiple treatments, depending on the severity of their BPPV. In the Semont maneuver, as with the Epley maneuver, people are able to achieve canalith repositioning by themselves.\n\nBrandt\u2013Daroff exercises\nThe Brandt\u2013Daroff exercises may be prescribed by the clinician as a home treatment method, usually in conjunction with particle-repositioning maneuvers or in lieu of the particle-repositioning maneuver. The exercise is a form of habituation exercise, designed to allow the person to become accustomed to the position that causes the vertigo symptoms. The Brandt\u2013Daroff exercises are performed in a similar fashion to the Semont maneuver; however, as the person rolls onto the unaffected side, the head is rotated toward the affected side. The exercise is typically performed 3 times a day with 5\u201310 repetitions each time, until symptoms of vertigo have resolved for at least 2 days.\n\nMedications\nMedical treatment with anti-vertigo medications may be considered in acute, severe exacerbation of BPPV, but in most cases are not indicated. These primarily include drugs of the antihistamine and anticholinergic class, such as meclizine and hyoscine butylbromide (scopolamine), respectively. The medical management of vestibular syndromes has become increasingly popular over the last decade, and numerous novel drug therapies (including existing drugs with new indications) have emerged for the treatment of vertigo\/dizziness syndromes. These drugs vary considerably in their mechanisms of action, with many of them being receptor- or ion channel-specific. Among them are betahistine or dexamethasone\/gentamicin for the treatment of M\u00e9ni\u00e8re's disease, carbamazepine\/oxcarbazepine for the treatment of paroxysmal dysarthria and ataxia in multiple sclerosis, metoprolol\/topiramate or valproic acid\/tricyclic antidepressant for the treatment of vestibular migraine, and 4-aminopyridine for the treatment of episodic ataxia type 2 and both downbeat and upbeat nystagmus. Calcium channel blockers such as verapamil may also be of value. These drug therapies offer symptomatic treatment, and do not affect the disease process or resolution rate. Medications may be used to suppress symptoms during the positioning maneuvers if the person's symptoms are severe and intolerable. More dose-specific studies are required, however, in order to determine the most-effective drug(s) for both acute symptom relief and long-term remission of the condition.\n\nSurgery\nSurgical treatments, such as a semi-circular canal occlusion, exist for severe and persistent cases that fail vestibular rehabilitation (including particle repositioning and habituation therapy). As they carry the same risks as any neurosurgical procedure, they are reserved as last resorts.\n\nReferences\n\n\n== External links ==","16":"A balance disorder is a disturbance that causes an individual to feel unsteady, for example when standing or walking. It may be accompanied by feelings of giddiness, or wooziness, or having a sensation of movement, spinning, or floating. Balance is the result of several body systems working together: the visual system (eyes), vestibular system (ears) and proprioception (the body's sense of where it is in space). Degeneration or loss of function in any of these systems can lead to balance deficits.\n\nSigns and symptoms\nCognitive dysfunction (disorientation) may occur with vestibular disorders.  Cognitive deficits are not just spatial in nature, but also include non-spatial functions such as object recognition memory. Vestibular dysfunction has been shown to adversely affect processes of attention and increased demands of attention can worsen the postural sway associated with vestibular disorders. Recent MRI studies also show that humans with bilateral vestibular damage (damage to both inner ears) undergo atrophy of the hippocampus which correlates with their degree of impairment on spatial memory tasks.\n\nCauses\nProblems with balance can occur when there is a disruption in any of the vestibular, visual, or proprioceptive systems. Abnormalities in balance function may indicate a wide range of pathologies from causes like inner ear disorders, low blood pressure, brain tumors, and brain injury including stroke.\n\nRelated to the ear\nCauses of dizziness related to the ear are often characterized by vertigo (spinning) and nausea. Nystagmus (flickering of the eye, related to the Vestibulo-ocular reflex [VOR]) is often seen in patients with an acute peripheral cause of dizziness.\n\nBenign paroxysmal positional vertigo (BPPV) \u2013 The most common cause of vertigo.  It is typically described as a brief, intense sensation of spinning that occurs when there are changes in the position of the head with respect to gravity. An individual may experience BPPV when rolling over to the left or right, upon getting out of bed in the morning, or when looking up for an object on a high shelf. The cause of BPPV is the presence of normal but misplaced calcium crystals called otoconia, which are normally found in the utricle and saccule (the otolith organs) and are used to sense movement. If they fall from the utricle and become loose in the semicircular canals, they can distort the sense of movement and cause a mismatch between actual head movement and the information sent to the brain by the inner ear, causing a spinning sensation.\n\nMigraine\nMigraine headaches are a common neurological disease. Although typical migraines are characterized by moderate to severe throbbing headaches, vestibular migraines may be accompanied by symptoms of vestibular disorders such as dizziness, disequilibrium, nausea, and vomiting.\n\nPresyncope\nPresyncope is a feeling of lightheadedness or simply feeling faint.  Syncope, by contrast, is actually fainting. A circulatory system deficiency, such as low blood pressure, can contribute to a feeling of dizziness when one suddenly stands up.\n\nDiagnosis\nThe difficulty of making the right vestibular diagnosis is reflected in the fact that in some populations, more than one-third of the patients with a vestibular disease consult more than one physician \u2013 in some cases up to more than fifteen.\n\nTreatment\nThere are various options for treating balance disorders. One option includes treatment for a disease or disorder that may be contributing to the balance problem, such as ear infection, stroke, multiple sclerosis, spinal cord injury, Parkinson's, neuromuscular conditions, acquired brain injury, cerebellar dysfunctions and\/or ataxia, or some tumors, such as acoustic neuroma. Individual treatment will vary and will be based upon assessment results including symptoms, medical history, general health, and the results of medical tests. Additionally, tai chi may be a cost-effective method to prevent falls in the elderly.\n\nVestibular rehabilitation\nMany types of balance disorders will require balance training, prescribed by an occupational therapist or physiotherapist. Physiotherapists often administer standardized outcome measures as part of their assessment in order to gain useful information and data about a patient's current status. Some standardized balance assessments or outcome measures include but are not limited to the Functional Reach Test, Clinical Test for Sensory Integration in Balance (CTSIB), Berg Balance Scale and\/or Timed Up and Go  The data and information collected can further help the physiotherapist develop an intervention program that is specific to the individual assessed. Intervention programs may include training activities that can be used to improve static and dynamic postural control, body alignment, weight distribution, ambulation, fall prevention and sensory function.\n\nBilateral vestibular loss\nDysequilibrium arising from bilateral loss of vestibular function \u2013 such as can occur from ototoxic drugs such as gentamicin \u2013 can also be treated with balance retraining exercises (vestibular rehabilitation) although the improvement is not likely to be full recovery.\n\nResearch\nScientists at the National Institute on Deafness and Other Communication Disorders (NIDCD) are working to understand the various balance disorders and the complex interactions between the labyrinth, other balance-sensing organs, and the brain. NIDCD scientists are studying eye movement to understand the changes that occur in aging, disease, and injury, as well as collecting data about eye movement and posture to improve diagnosis and treatment of balance disorders. They are also studying the effectiveness of certain exercises as a treatment option. Recently, a study published in JAMA Otolaryngology-Head & Neck Surgery found that balance problems are an indicator of mortality potentially due to altered metabolism of vestibular system.\n\nSee also\nHypokinesia\nTremor\nHuntington's disease\n\nReferences\n\n\n== External links ==","17":"Benign paroxysmal positional vertigo (BPPV) is a disorder arising from a problem in the inner ear. Symptoms are repeated, brief periods of vertigo with movement, characterized by a spinning sensation upon changes in the position of the head. This can occur with turning in bed or changing position. Each episode of vertigo typically lasts less than one minute. Nausea is commonly associated. BPPV is one of the most common causes of vertigo.\nBPPV is a type of balance disorder along with labyrinthitis and M\u00e9ni\u00e8re's disease. It can result from a head injury or simply occur among those who are older. Often, a specific cause is not identified. When found, the underlying mechanism typically involves a small calcified otolith moving around loose in the inner ear. Diagnosis is typically made when the Dix\u2013Hallpike test results in nystagmus (a specific movement pattern of the eyes) and other possible causes have been ruled out. In typical cases, medical imaging is not needed.\nBPPV is easily treated with a number of simple movements such as the Epley maneuver or Half Somersault Maneuver (in case of diagonal\/rotational nystagmus), the Lempert maneuver (in case of horizontal nystagmus), the deep head hanging maneuver (in case of vertical nystagmus) or sometimes the less effective Brandt\u2013Daroff exercises. Medications, including antihistamines such as meclizine, may be used to help with nausea. There is tentative evidence that betahistine may help with vertigo, but its use is not generally needed. BPPV is not a serious medical condition, but may present serious risks of injury through falling or other spatial disorientation-induced accidents.\nWhen untreated, it might resolve in days to months; however, it may recur in some people. One can needlessly suffer from BPPV for years despite there being a simple and very effective cure. Short-term self-resolution of BPPV is unlikely because the effective cure maneuvers induce strong vertigo which the patient will naturally resist and not accidentally perform.\nThe first medical description of the condition occurred in 1921 by R\u00f3bert B\u00e1r\u00e1ny. Approximately 2.4% of people are affected at some point in time. Among those who live until their 80s, 10% have been affected. BPPV affects females twice as often as males. Onset is typically in people between the ages of 50 and 70.\n\nSigns and symptoms\nSymptoms:\n\nParoxysmal\u2014appears suddenly, and in episodes of short duration: lasts only seconds to minutes\nPositional\u2014is induced by a change in position, even slight\nVertigo\u2014a spinning dizziness, which must have a rotational component\nTorsional nystagmus\u2014a diagnostic symptom where the top of the eye rotates toward the affected ear in a beating or twitching fashion, which has a latency and can be fatigued (vertigo should lessen with deliberate repetition of the provoking maneuver): nystagmus should only last for 30 seconds to one minute\nPre-syncope\u2014(feeling faint) or syncope (fainting) is unusual, but possible\nVisual disturbance\u2014due to associated nystagmus, making it difficult to read or see during an attack\nNausea\u2014is often associated\nVomiting\u2014is common, depending on the strength of vertigo itself and the causes for this illness\nMany people will report a history of vertigo as a result of fast head movements. Many are also capable of describing the exact head movements that provoke their vertigo. Purely horizontal nystagmus and symptoms of vertigo lasting more than one minute can also indicate BPPV occurring in the horizontal semicircular canal.\nThe spinning sensation experienced from BPPV is usually triggered by movement of the head, will have a sudden onset, and can last anywhere from a few seconds to several minutes. The most common movements people report triggering a spinning sensation are tilting their heads upward in order to look at something and when rolling over in bed.\nPeople with BPPV do not experience other neurological deficits such as numbness or weakness. If those symptoms are present, a more serious etiology, such as posterior circulation stroke or ischemia, must be considered.\nThe most significant symptom is nystagmus as it is essential to determine the kind of nystagmus ( horizontal, vertical, or diagonal ) to select the correct cure maneuver.\n\nCause\nWithin the labyrinth of the inner ear lie collections of calcium crystals known as otoconia or otoliths. In people with BPPV, the otoconia are dislodged from their usual position within the utricle, and over time, migrate into one of the three semicircular canals (the posterior canal is most commonly affected due to its anatomical position). When the head is reoriented relative to gravity, the gravity-dependent movement of the heavier otoconial debris (colloquially \"ear rocks\") within the affected semicircular canal causes abnormal (pathological) endolymph fluid displacement and a resultant sensation of vertigo. This more common condition is known as canalithiasis. There is a direct link between the kind of nystagmus and which of the three semicircular canals is affected. With horizontal nystagmus (left-right eye movement) the horizontal (also called lateral) canal is affected, with vertical nystagmus (up-down eye movement) the superior (also called anterior) canal is affected, and with diagonal nystagmus (diagonal or rotational eye movement) the posterior canal is affected. Diagonal eye movement is easily confused with horizontal movement. This is important since it might result in selecting a wrong and thus ineffective cure maneuver.\nIn rare cases, the crystals themselves can adhere to a semicircular canal cupula, rendering it heavier than the surrounding endolymph. Upon reorientation of the head relative to gravity, the cupula is weighted down by the dense particles, thereby inducing an immediate and sustained excitation of semicircular canal afferent nerves. This condition is termed cupulolithiasis.\nThere is evidence in the dental literature that malleting of an osteotome during closed sinus floor elevation, otherwise known as osteotome sinus elevation or lift, transmits percussive and vibratory forces capable of detaching otoliths from their normal location and thereby leading to the symptoms of BPPV.\nBPPV can be triggered by any action that stimulates the posterior semi-circular canal including:\n\nLooking up or down\nFollowing head injury\nSudden head movement\nRolling over in bed\nTilting the head\nBPPV may be made worse by any number of modifiers which may vary among individuals:\n\nChanges in barometric pressure \u2013 people may feel increased symptoms up to two days before rain or snow\nLack of sleep (required amounts of sleep may vary widely)\nStress\nAn episode of BPPV may be triggered by dehydration, such as that caused by diarrhea.\nBPPV is one of the most common vestibular disorders in people presenting with dizziness; a migraine is implicated in idiopathic cases. Proposed mechanisms linking the two are genetic factors and vascular damage to the labyrinth.\nAlthough BPPV can occur at any age, it is most often seen in people older than the age of 60. Besides aging, there are no major risk factors known for BPPV, although previous episodes of head trauma, preexisting disorders, or the inner ear infection labyrinthitis, may predispose to the future development of BPPV.\n\nMechanism\nThe inside of the ear is composed of an organ called the vestibular labyrinth. The vestibular labyrinth includes three semicircular canals, which contain fluids and fine hairlike sensors that act as a monitor to the rotations of the head. Other important structures in the inner ear includes the otolith organs, the utricle and saccule, that contain calcium carbonate crystals(otoconia) that are sensitive to gravity.\nThe crystals may dislodge from the utricle (an otolith organ) and settle within the semicircular canals. When there is motion, the displaced otoconia shift within the endolymph of semicircular canals, causing an unbalanced (with respect to the opposite ear) stimulus, causing symptoms of BPPV.\n\nDiagnosis\nThe condition is diagnosed by the person's history, and by performing the Dix\u2013Hallpike test or the roll test, or both. The patient can also be asked to induce vertigo by performing a movement that the patient knows to induce vertigo. The eyes of the patient can then easily be observed for which kind (horizontal, vertical, or diagonal) of nystagmus is present, to determine which semicircular canal (horizontal, superior, or posterior) is affected.\nThe Dix\u2013Hallpike test is a common test performed by examiners to determine whether the posterior semicircular canal is involved. It involves a reorientation of the head to align the posterior semicircular canal (at its entrance to the ampulla) with the direction of gravity. This test will reproduce vertigo and nystagmus characteristic of posterior canal BPPV.\nWhen performing the Dix\u2013Hallpike test, people are lowered quickly to a supine position, with the neck extended by the person performing the maneuver. For some people, this maneuver may not be indicated, and a modification may be needed that also targets the posterior semicircular canal. Such people include those who are too anxious about eliciting the uncomfortable symptoms of vertigo, and those who may not have the range of motion necessary to comfortably be in a supine position. The modification involves the person moving from a seated position to side-lying without their head extending off the examination table, such as with Dix\u2013Hallpike. The head is rotated 45 degrees away from the side being tested, and the eyes are examined for nystagmus. A positive test is indicated by the patient report of a reproduction of vertigo and clinician observation of nystagmus. Both the Dix\u2013Hallpike and the side-lying testing position have yielded similar results, and as such the side-lying position can be used if the Dix\u2013Hallpike cannot be performed easily.\nThe roll test can determine whether the horizontal semicircular canal is involved. The roll test requires the person to be in a supine position with their head in 30\u00b0 of cervical flexion. Then the examiner quickly rotates the head 90\u00b0 to the left side, and checks for vertigo and nystagmus. This is followed by gently bringing the head back to the starting position. The examiner then quickly rotates the head 90\u00b0 to the right side and checks again for vertigo and nystagmus. In this roll test, the person may experience vertigo and nystagmus on both sides, but rotating toward the affected side will trigger a more intense vertigo. Similarly, when the head is rotated toward the affected side, the nystagmus will beat toward the ground and be more intense.\nAs mentioned above, both the Dix\u2013Hallpike and roll test provoke the signs and symptoms in subjects with archetypal BPPV. The signs and symptoms people with BPPV experience are typically a short-lived vertigo and observed nystagmus. In some people, although rarely, vertigo can persist for years. Assessment of BPPV is best done by a medical health professional skilled in the management of dizziness disorders, commonly a physiotherapist, audiologist, or other physician.\nThe nystagmus associated with BPPV has several important characteristics that differentiate it from other types of nystagmus.\n\nLatency of onset: there is a 5\u201310 second delay prior to onset of nystagmus\nNystagmus lasts for 5\u201360 seconds\nPositional: the nystagmus occurs only in certain positions\nRepeated stimulation, including via Dix\u2013Hallpike maneuvers, cause the nystagmus to fatigue or disappear temporarily\nRotatory\/torsional component is present, or (in the case of lateral canal involvement) the nystagmus beats in either a geotropic (toward the ground) or ageotropic (away from the ground) fashion\nVisual fixation suppresses nystagmus due to BPPV\nAlthough rare, disorders of the central nervous system can sometimes present as BPPV. A practitioner should be aware that if a person whose symptoms are consistent with BPPV, but does not show improvement or resolution after undergoing different particle repositioning maneuvers \u2014 detailed in the Treatment section below \u2014 need to have a detailed neurological assessment and imaging performed to help identify the pathological condition.\n\nDifferential diagnosis\nVertigo, a distinct process sometimes confused with the broader term, dizziness, accounts for about six million clinic visits in the United States every year; between 17 and 42% of these people are eventually diagnosed with BPPV.\nOther causes of vertigo include:\n\nMotion sickness\/motion intolerance: a disjunction between visual stimulation, vestibular stimulation, and\/or proprioception\nVisual exposure to nearby moving objects (examples of optokinetic stimuli include passing cars and falling snow)\nOther diseases: (labyrinthitis, M\u00e9ni\u00e8re's disease, and migraine, etc.)\n\nTreatment\nRepositioning maneuvers\nA number of maneuvers have been found to be effective including Canalith Repositioning Procedures (CRP) such as the Epley maneuver, the Half Somersault Maneuver (HSM), the Semont maneuver, and to a lesser degree the non-CRP Brandt\u2013Daroff exercises. Both the Epley and the Semont maneuvers are equally effective. The HSM can have better long-term success than the Epley, is more comfortable to experience, and has less risk of causing subsequent horizontal canal BPPV (H-BPPV).\nNone of these maneuvers addresses the presence of the particles (otoconia); rather it changes their location. The maneuvers aim to move these particles from some locations in the inner ear that cause symptoms such as vertigo and reposition them to where they do not cause these problems. These maneuvers are easily performed at home and online resources are available to patients.\nThe Epley maneuver is popular because it is designed to address posterior canal BPPV (PC-BPPV), which is caused by particles in the posterior semicircular canal, the most common cause of BPPV. This might give the wrong impression that PC-BPPV is the only kind of BPPV. Misdiagnosing which semicircular canal is affected, typically by confusing horizontal and diagonal nystagmus, or simply ignoring the identification of the affected canal, and then using the wrong treatment maneuver, regularly results in no cure.\nUsing the appropriate maneuver for the affected canal is essential. The maneuvers may be uncomfortable for the patient as they might induce strong vertigo and the patient might then resist performing them. Though some treatments, such as the Epley, are much more uncomfortable than others, such as the HSM. If the maneuver is not uncomfortable then it is possible the wrong maneuver has been selected by a misdiagnosis of the affected semicircular canal.\nAll the maneuvers consist of a series of steps in which the head is held in a specific position, typically for 30 to 60 seconds until any nystagmus stops. Movement from one position to the position of the next step has to be done fluently to give the particles enough momentum to move. A position has to be held until any nystagmus has completely resided, which indicates that the particles have stopped moving, before one proceeds to the next step.\n\nEpley maneuver\nThe Epley maneuver employs gravity to move the calcium crystal build-up from the posterior semicircular canal (resulting in diagonal nystagmus) that causes the condition. This maneuver can be performed during a clinic visit by health professionals, or taught to people to perform at home, or both. Postural restriction after the Epley maneuver increases its effect somewhat.\nWhen practiced at home, the Epley maneuver is more effective than the Semont maneuver. An effective repositioning treatment for posterior canal BPPV is the therapist-performed Epley combined with home-practiced Epley maneuvers. Devices such as the DizzyFIX can help users conduct the Epley maneuver at home, and are available for the treatment of BPPV.\n\nHalf Somersault Maneuver\nThe Half Somersault Maneuver (HSM) is a patient-performed alternative to the Epley for posterior canal BPPV (PC-BPPV). Like the Epley, it uses gravity to move the calcium crystal build-up from the posterior semicircular canal that causes the condition. Compared to the Epley, HSM has better long-term success, with less discomfort, and less risk of causing subsequent horizontal canal BPPV (H-BPPV).\n\nLempert maneuver or Roll maneuver\nFor the lateral (horizontal) canal, resulting in horizontal nystagmus, the Lempert maneuver has been used for productive results. It is unusual for the lateral canal to respond to the canalith repositioning procedure used for the posterior canal BPPV. Treatment is therefore geared toward moving the canalith from the lateral canal into the vestibule.\nThe roll maneuver or its variations are used, and involve rolling the person 360 degrees in a series of steps to reposition the particles. This maneuver is generally performed by a trained clinician who begins seated at the head of the examination table with the person supine. There are four stages, each a minute apart, and at the third position the horizontal canal is oriented in a vertical position with the person's neck flexed and on forearm and elbows. When all four stages are completed, the head roll test is repeated, and if negative, treatment ceases.\n\nDeep head hanging maneuver\nFor the superior (also called anterior) semicircular canal, resulting in vertical nystagmus, the Deep head hanging maneuver is used. The patient lays down on their back on a bed with their head overhanging the bed.\nIn the first step the head is turned as backward (hanging) as possible. In the following step, the patient remains lying but lifts their head with the chin close to the chest. In the last step, the patient sits upright with the head in the normal position. Before going to the next step, one has to wait until the nystagmus fully resides ( typical 30 to 60 seconds ) and progression from one step to the next has to happen in a fluid movement.\n\nSemont maneuver\nThe Semont maneuver has a cure rate of 90.3%. It is performed as follows:\n\nThe person is seated on a treatment table with their legs hanging off the side of the table. The therapist then turns the person's head 45 degrees toward the unaffected side.\nThe therapist then quickly tilts the person so they are lying on the affected side. The head position is maintained, so their head is turned up 45 degrees. This position is maintained for 3 minutes. The purpose is to allow the debris to move to the apex of the semicircular duct.\nThe person is then quickly moved so they are lying on the unaffected side with their head in the same position (now facing downward 45 degrees). This position is also held for 3 minutes. The purpose of this position is to allow the debris to move toward the exit of the semicircular duct.\nFinally, the person is slowly brought back to an upright seated position. The debris should then fall into the utricle of the canal and the symptoms of vertigo should decrease or end completely.\nSome people will only need one treatment, but others may need multiple treatments, depending on the severity of their BPPV. In the Semont maneuver, as with the Epley maneuver, people are able to achieve canalith repositioning by themselves.\n\nBrandt\u2013Daroff exercises\nThe Brandt\u2013Daroff exercises may be prescribed by the clinician as a home treatment method, usually in conjunction with particle-repositioning maneuvers or in lieu of the particle-repositioning maneuver. The exercise is a form of habituation exercise, designed to allow the person to become accustomed to the position that causes the vertigo symptoms. The Brandt\u2013Daroff exercises are performed in a similar fashion to the Semont maneuver; however, as the person rolls onto the unaffected side, the head is rotated toward the affected side. The exercise is typically performed 3 times a day with 5\u201310 repetitions each time, until symptoms of vertigo have resolved for at least 2 days.\n\nMedications\nMedical treatment with anti-vertigo medications may be considered in acute, severe exacerbation of BPPV, but in most cases are not indicated. These primarily include drugs of the antihistamine and anticholinergic class, such as meclizine and hyoscine butylbromide (scopolamine), respectively. The medical management of vestibular syndromes has become increasingly popular over the last decade, and numerous novel drug therapies (including existing drugs with new indications) have emerged for the treatment of vertigo\/dizziness syndromes. These drugs vary considerably in their mechanisms of action, with many of them being receptor- or ion channel-specific. Among them are betahistine or dexamethasone\/gentamicin for the treatment of M\u00e9ni\u00e8re's disease, carbamazepine\/oxcarbazepine for the treatment of paroxysmal dysarthria and ataxia in multiple sclerosis, metoprolol\/topiramate or valproic acid\/tricyclic antidepressant for the treatment of vestibular migraine, and 4-aminopyridine for the treatment of episodic ataxia type 2 and both downbeat and upbeat nystagmus. Calcium channel blockers such as verapamil may also be of value. These drug therapies offer symptomatic treatment, and do not affect the disease process or resolution rate. Medications may be used to suppress symptoms during the positioning maneuvers if the person's symptoms are severe and intolerable. More dose-specific studies are required, however, in order to determine the most-effective drug(s) for both acute symptom relief and long-term remission of the condition.\n\nSurgery\nSurgical treatments, such as a semi-circular canal occlusion, exist for severe and persistent cases that fail vestibular rehabilitation (including particle repositioning and habituation therapy). As they carry the same risks as any neurosurgical procedure, they are reserved as last resorts.\n\nReferences\n\n\n== External links ==","18":"Balance in biomechanics, is an ability to maintain the line of gravity (vertical line from centre of mass) of a body within the base of support with minimal postural sway. Sway is the horizontal movement of the centre of gravity even when a person is standing still. A certain amount of sway is essential and inevitable due to small perturbations within the body (e.g., breathing, shifting body weight from one foot to the other or from forefoot to rearfoot) or from external triggers (e.g., visual distortions, floor translations). An increase in sway is not necessarily an indicator of dysfunctional balance so much as it is an indicator of decreased sensorimotor control.\n\nMaintaining balance\nMaintaining balance requires coordination of input from multiple sensory systems including the vestibular, somatosensory, and visual systems.\n\nVestibular system: sense organs that regulate equilibrium (equilibrioception); directional information as it relates to head position (internal gravitational, linear, and angular acceleration)\nSomatosensory system: senses of proprioception and kinesthesia of joints; information from skin and joints (pressure and vibratory senses); spatial position and movement relative to the support surface; movement and position of different body parts relative to each other\nVisual system: Reference to verticality of body and head motion; spatial location relative to objects\nThe senses must detect changes of spatial orientation with respect to the base of support, regardless of whether the body moves or the base is altered. There are environmental factors that can affect balance such as light conditions, floor surface changes, alcohol, drugs, and ear infection.\n\nBalance impairments\nThere are balance impairments associated with aging. Age-related decline in the ability of the above systems to receive and integrate sensory information contributes to poor balance in older adults. As a result, the elderly are at an increased risk of falls. In fact, one in three adults aged 65 and over will fall each year.\nIn the case of an individual standing quietly upright, the limit of stability is defined as the amount of postural sway at which balance is lost and corrective action is required.\nBody sway can occur in all planes of motion, which make it an increasingly difficult ability to rehabilitate. There is strong evidence in research showing that deficits in postural balance is related to the control of medial-lateral stability and an increased risk of falling. To remain balanced, a person standing must be able to keep the vertical projection of their center of mass within their base of support, resulting in little medial-lateral or anterior-posterior sway. Ankle sprains are one of the most frequently occurring injuries among athletes and physically active people. The most common residual disability post ankle sprain is instability along with body sway. Mechanical instability includes insufficient stabilizing structures and mobility that exceed physiological limits. Functional instability involves recurrent sprains or a feeling of giving way of the ankle. Nearly 40% of patients with ankle sprains suffer from instability and an increase in body sway.  Injury to the ankle causes a proprioceptive deficit and impaired postural control. Individuals with muscular weakness, occult instability, and decreased postural control are more susceptible to ankle injury than those with better postural control.\nBalance can be severely affected in individuals with neurological conditions. People who suffer a stroke or spinal cord injury for example, can struggle with this ability. Impaired balance is strongly associated with future function and recovery after a stroke, and is the strongest predictor of falls.\nAnother population where balance is severely affected is Parkinson's disease patients. A study done by Nardone and Schieppati (2006) showed that individuals with Parkinson's disease problems in balance have been related to a reduced limit of stability and an impaired production of anticipatory motor strategies and abnormal calibration.\nBalance can also be negatively affected in a normal population through fatigue in the musculature surrounding the ankles, knees, and hips. Studies have found, however, that muscle fatigue around the hips (gluteals and lumbar extensors) and knees have a greater effect on postural stability (sway). It is thought that muscle fatigue leads to a decreased ability to contract with the correct amount of force or accuracy. As a result, proprioception and kinesthetic feedback from joints are altered so that conscious joint awareness may be negatively effected.\n\nBalance training\nSince balance is a key predictor of recovery and is required in many activities of daily living, it is often introduced into treatment plans by physiotherapists and occupational therapists when dealing with geriatrics, patients with neurological conditions, or others for whom balance training has been determined to be beneficial.\nBalance training in stroke patients has been supported in the literature. Methods commonly used and proven to be effective for this population include sitting or standing balance practice with various progressions including reaching, variations in base of support, use of tilt boards, gait training varying speed, and stair climbing exercises. Another method to improve balance is perturbation training, which is an external force applied to a person's center of mass in an attempt to move it from the base of support. The type of training should be determined by a physiotherapist and will depend on the nature and severity of the stroke, stage of recovery, and the patient's abilities and impairments after the stroke.\nPopulations such as the elderly, children with neuromuscular diseases, and those with motor deficits such as chronic ankle instability have all been studied and balance training has been shown to result in improvements in postural sway and improved \"one-legged stance balance\" in these groups. The effects of balance training can be measured by more varied means, but typical quantitative outcomes are centre of pressure (CoP), postural sway, and static\/dynamic balance, which are measured by the subject's ability to maintain a set body position while undergoing some type of instability.\nStudies have suggested, higher level of physical activity have shown to reduce the morbidity and mortality along with risk of fall up to 30% to 50%. Some types of exercise  (gait, balance, co-ordination and functional tasks; strengthening exercise; 3D exercise and multiple exercise types) improve clinical balance outcomes in older people, and are seemingly safe. A study has shown to be effective in improving ability to balance after undergoing aerobic exercises along with resistance exercises. There is still insufficient evidence supporting general physical activity, computerized balance programs or vibration plates.\n\nFunctional balance assessments\nFunctional tests of balance focus on maintenance of both static and dynamic balance, whether it involves a type of perturbation\/change of center of mass or during quiet stance. Standardized tests of balance are available to allow allied health care professionals to assess an individual's postural control. Some functional balance tests that are available are:\n\nRomberg Test: used to determine proprioceptive contributions to upright balance. Subject remains in quiet standing while eyes are open. If this test is not difficult enough, there is a Sharpened Romberg's test. Subjects would have to have their arms crossed, feet together and eyes closed. This decreases the base of support, raises the subject's center of mass, and prevents them from using their arms to help balance.\nFunctional Reach Test: measures the maximal distance one can reach forward beyond arm's length while maintaining feet planted in a standing position.\nBerg Balance Scale: measures static and dynamic balance abilities using functional tasks commonly performed in everyday life. One study reports that the Berg Balance Scale is the most commonly used assessment tool throughout stroke rehabilitation, and found it to be a sound measure of balance impairment in patients following a stroke. Berg balance scale is known to be the golden test. BBS was first published in 1989 and to this day in 2022, it's still effective which is pretty remarkable.  Not every test and every study that was made stuck around this long so its truly a golden test.\nPerformance-Oriented Mobility Assessment (POMA): measures both static and dynamic balance using tasks testing balance and gait.\nTimed Up and Go Test: measures dynamic balance and mobility.\nBalance Efficacy Scale: self-report measure that examines an individual's confidence while performing daily tasks with or without assistance.\nStar Excursion Test: A dynamic balance test that measures single stance maximal reach in multiple directions.\nBalance Evaluation Systems Test (BESTest): Tests for 6 unique balance control methods to create a specialized rehabilitation protocol by identifying specific balance deficits.\nThe Mini-Balance Evaluation Systems Test (Mini-BESTest): Is a short form of the Balance Evaluation System Test that is used widely in both clinical practice and research. The test is used to assess balance impairments and includes 14 items of dynamic balance task, divided in to four subcomponents: anticipatory postural adjustments, reactive postural control, sensory orientation and dynamic gait. Mini-BESTest has been tested for mainly neurological diseases, but also other diseases. A review of psychometric properties of the test support the reliability, validity and responsiveness, and according to the review, it can be considered a standard balance measure.\nBESS: The BESS (Balance Error Scoring System) is a commonly used way to assess balance. It is known as a  simple and affordable way to get an accurate assessment of balance, although the validity of the BESS protocol has been questioned. The BESS is often used in sports settings to assess the effects of mild to moderate head injury on one's postural stability. The BESS tests three separate stances (double leg, single leg, tandem) on two different surfaces (firm surface and medium density foam) for a total of six tests. Each test is 20 seconds long, with the entire time of the assessment approximately 5\u20137 minutes. The first stance is the double leg stance. The participant is instructed to stand on a firm surface with feet side by side with hands on hips and eyes closed. The second stance is the single leg stance. In this stance the participant is instructed to stand on their non-dominant foot on a firm surface with hands on hips and eyes closed. The third stance is the tandem stance. The participant stands heel to toe on a firm surface with hands on hips and eyes closed. The fourth, fifth, and sixth stances repeat in order stances one, two, and three except the participant performs these stances on a medium density foam surface. The BESS is scored by an examiner who looks for deviations from the proper stances. A deviation is noted when any of the following occurs in the participant during testing: opening the eyes, removing hands from the hips, stumbling forward or falling, lifting the forefoot or heel off the testing surface, abduction or flexion of the hip beyond 30 degrees, or remaining out of the proper testing position for more than 5 seconds.\n\nConcussion (or mild traumatic brain injury) have been associated with imbalance among sports participants and military personnel. Some of the standard balance tests may be too easy or time-consuming for application to these high-functioning groups, s. Expert recommendations have been gathered concerning balance assessments appropriate to military service-members.\n\nQuantitative (computerized) assessments\nDue to recent technological advances, a growing trend in balance assessments has become the monitoring of center of pressure (terrestrial locomotion) (CoP), the reaction vector of center of mass on the ground, path length for a specified duration. With quantitative assessments, minimal CoP path length is suggestive of good balance. Laboratory-grade force plates are considered the \"gold-standard\" of measuring CoP. The NeuroCom Balance Manager (NeuroCom, Clackamas, OR, United States) is a commercially available dynamic posturography system that uses computerized software to track CoP during different tasks. These different assessments range from the sensory organization test looking at the different systems that contribute through sensory receptor input to the limits of stability test observing a participant's ankle range of motion, velocity, and reaction time. While the NeuroCom is considered the industry standard for balance assessments, it does come at a steep price (about $250,000).\nWithin the past 5 years research has headed toward inexpensive and portable devices capable of measuring CoP accurately. Recently, Nintendo's Wii balance board (Nintendo, Kyoto, Japan) has been validated against a force plate and found to be an accurate tool to measure CoP  This is very exciting as the price difference in technology ($25 vs $10,000) makes the Wii balance board a suitable alternative for clinicians to use quantitative balance assessments. Other inexpensive, custom-built force plates are being integrated into this new dynamic to create a growing field of research and clinical assessment that will benefit many populations.\n\nFatigue's effect on balance\nThe complexity of balance allows for many confounding variables to affect a person's ability to stay upright. Fatigue, causing central nervous system (CNS) dysfunction, can indirectly result in the inability to remain upright. This is seen repeatedly in clinical populations (e.g. Parkinson's disease, multiple sclerosis). Another major concern regarding fatigue's effect on balance is in the athletic population. Balance testing has become a standard measure to help diagnose concussions in athletes, but due to the fact that athletes can be extremely fatigued has made it hard for clinicians to accurately determine how long the athletes need to rest before fatigue is gone, and they can measure balance to determine if the athlete is concussed. So far, researchers have only been able to estimate that athletes need anywhere from 8\u201320 minutes of rest before testing balance That can be a huge difference depending on the circumstances.\n\nOther factors influencing balance\nAge, gender, and height have all been shown to impact an individual's ability to balance and the assessment of that balance. Typically, older adults have more body sway with all testing conditions. Tests have shown that older adults demonstrate shorter functional reach and larger body sway path lengths. Height also influences body sway in that as height increases, functional reach typically decreases.  However, this test is only a measure of anterior and posterior sway.  This is done to create a repeatable and reliable clinical balance assessment tool. A 2011 Cochrane Review found that specific types of exercise (such as gait, balance, co-ordination and functional tasks; strengthening exercises; 3D exercises [e.g. Tai Chi] and combinations of these) can help improve balance in older adults. However, there was no or limited evidence on the effectiveness of general physical activities, such as walking and cycling, computer-based balance games and vibration plates.\n\nVoluntary control of balance\nWhile balance is mostly an automatic process, voluntary control is common. Active control usually takes place when a person is in a situation where balance is compromised. This can have the counter-intuitive effect of increasing postural sway during basic activities such as standing. One explanation for this effect is that conscious control results in over-correcting an instability and \"may inadvertently disrupt relatively automatic control processes.\" While concentration on an external task \"promotes the utilization of more automatic control processes.\"\n\nBalance and dual-tasking\nSupra-postural tasks are those activities that rely on postural control while completing another behavioral goal, such as walking or creating a text message while standing upright. Research has demonstrated that postural stability operates to permit the achievement of other activities.  In other words, standing in a stable upright position is not at all beneficial if one falls as soon as any task is attempted.  In a healthy individual, it is believed that postural control acts to minimize the amount of effort required (not necessarily to minimize sway), while successfully accomplishing the supra-postural task.  Research has shown that spontaneous reductions in postural sway occur in response to the addition of a secondary goal.\nMcNevin and Wulf (2002) found an increase in postural performance when directing an individual's attention externally compared to directing attention internally  That is, focusing attention on the effects of one's movements rather than on the movement itself will boost performance.  This results from the use of more automatic and reflexive control processes.  When one is focused on their movements (internal focus), they may inadvertently interfere with these automatic processes, decreasing their performance.  Externally focusing attention improves postural stability, despite increasing postural sway at times.  It is believed that utilizing automatic control processes by focusing attention externally enhances both performance and learning.  Adopting an external focus of attention subsequently improves the performance of supra-postural tasks, while increasing postural stability.\n\nReferences\n\n\n== Further reading ==","19":"Beta blockers, also spelled \u03b2-blockers, are a class of medications that are predominantly used to manage abnormal heart rhythms (arrhythmia), and to protect the heart from a second heart attack after a first heart attack (secondary prevention). They are also widely used to treat high blood pressure, although they are no longer the first choice for initial treatment of most patients.\nBeta blockers are competitive antagonists that block the receptor sites for the endogenous catecholamines epinephrine (adrenaline) and norepinephrine (noradrenaline) on adrenergic beta receptors, of the sympathetic nervous system, which mediates the fight-or-flight response.:\u200a152\u200a Some block activation of all types of \u03b2-adrenergic receptors and others are selective for one of the three known types of beta receptors, designated \u03b21, \u03b22 and \u03b23 receptors.:\u200a153\u200a \u03b21-adrenergic receptors are located mainly in the heart and in the kidneys. \u03b22-adrenergic receptors are located mainly in the lungs, gastrointestinal tract, liver, uterus, vascular smooth muscle, and skeletal muscle. \u03b23-adrenergic receptors are located in fat cells.\nBeta receptors are found on cells of the heart muscles, smooth muscles, airways, arteries, kidneys, and other tissues that are part of the sympathetic nervous system and lead to stress responses, especially when they are stimulated by epinephrine (adrenaline). Beta blockers interfere with the binding to the receptor of epinephrine and other stress hormones and weaken the effects of stress hormones.\nIn 1964, James Black synthesized the first clinically significant beta blockers\u2014propranolol and pronethalol; it revolutionized the medical management of angina pectoris and is considered by many to be one of the most important contributions to clinical medicine and pharmacology of the 20th century.\nFor the treatment of primary hypertension, meta-analyses of studies which mostly used atenolol have shown that although beta blockers are more effective than placebo in preventing stroke and total cardiovascular events, they are not as effective as diuretics, medications inhibiting the renin\u2013angiotensin system (e.g., ACE inhibitors), or calcium channel blockers.\n\nMedical uses\nBeta blockers are utilized in the treatment of various conditions related to the heart and vascular system, as well as several other medical conditions. Common heart-related conditions for which beta blockers are well-established include angina pectoris, acute coronary syndromes, hypertension, and arrhythmias such as atrial fibrillation and heart failure. They are also used in the management of other heart diseases, such as hypertrophic obstructive cardiomyopathy, mitral valve stenosis or prolapse, and dissecting aneurysm. Additionally, beta blockers find applications in vascular surgery, the treatment of anxiety states, cases of thyrotoxicosis, glaucoma, migraines, and esophageal varices.\n\nCongestive heart failure\nAlthough beta blockers were once contraindicated in congestive heart failure, as they have the potential to worsen the condition due to their effect of decreasing cardiac contractility, studies in the late 1990s showed their efficacy at reducing morbidity and mortality. Bisoprolol, carvedilol, and sustained-release metoprolol are specifically indicated as adjuncts to standard ACE inhibitor and diuretic therapy in congestive heart failure, although at doses typically much lower than those indicated for other conditions. Beta blockers are only indicated in cases of compensated, stable congestive heart failure; in cases of acute decompensated heart failure, beta blockers will cause a further decrease in ejection fraction, worsening the patient's current symptoms.\nBeta blockers are known primarily for their reductive effect on heart rate, although this is not the only mechanism of action of importance in congestive heart failure. Beta blockers, in addition to their sympatholytic \u03b21 activity in the heart, influence the renin\u2013angiotensin system at the kidneys. Beta blockers cause a decrease in renin secretion, which in turn reduces the heart oxygen demand by lowering the extracellular volume and increasing the oxygen-carrying capacity of the blood. Heart failure characteristically involves increased catecholamine activity on the heart, which is responsible for several deleterious effects, including increased oxygen demand, propagation of inflammatory mediators, and abnormal cardiac tissue remodeling, all of which decrease the efficiency of cardiac contraction and contribute to the low ejection fraction. Beta blockers counter this inappropriately high sympathetic activity, eventually leading to an improved ejection fraction, despite an initial reduction in ejection fraction.\nTrials have shown beta blockers reduce the absolute risk of death by 4.5% over a 13-month period. In addition to reducing the risk of mortality, the numbers of hospital visits and hospitalizations were also reduced in the trials. A 2020 Cochrane review found minimal evidence to support the use of beta blockers in congestive heart failure in children, however did identify that from the data available, that they may be of benefit.\nTherapeutic administration of beta blockers for congestive heart failure ought to begin at very low doses (1\u20448 of target) with a gradual escalation of the dose. The heart of the patient must adjust to decreasing stimulation by catecholamines and find a new equilibrium at a lower adrenergic drive.\n\nAcute myocardial infarction\nBeta blockers are indicated for the treatment of acute myocardial infarctions. During a myocardial infarction, systemic stress causes an increase in circulating catecholamines. This results an increase in heart rate and blood pressure, therefore increasing myocardial oxygen demand. Beta blockers competitively inhibit catecholamines acting on the \u03b21-adrenergic receptors, thus reducing these detrimental effects and resulting in reduced myocardial oxygen consumption and demand.\nA 2019 Cochrane review compared beta blockers with placebo or no intervention, it found that beta blockers probably reduced the short-term risk of reinfarction and the long-term risk of all-cause mortality and cardiovascular mortality. The review identified that beta blockers likely had little to no impact on short-term all-cause mortality and cardiovascular mortality.\n\nHypertension\nBeta blockers are widely used for the treatment of hypertension.\nA 2014 Cochrane review found that in individuals with mild-to-moderate hypertension, non-selective beta blockers led to a reduction of -10\/-7mmHg (systolic\/diastolic) without increased rates of adverse events. At higher doses, it was found to increase the rate of adverse effects such as a reduction in heart rate, without a corresponding reduction in blood pressure.\nA 2017 Cochrane review on the use of beta blockers in hypertension found a modest reduction in cardiovascular disease but little to no change in mortality It suggested that the effects of beta blockers are inferior to other anti-hypertensive medications.\n\nAnxiety\nOfficially, beta blockers are not approved for anxiolytic use by the U.S. Food and Drug Administration. However, many controlled trials in the past 25 years indicate beta blockers are effective in anxiety disorders, though the mechanism of action is not known. The physiological symptoms of the fight-or-flight response (pounding heart, cold\/clammy hands, increased respiration, sweating, etc.) are significantly reduced, thus enabling anxious individuals to concentrate on the task at hand.\nMusicians, public speakers, actors, and professional dancers have been known to use beta blockers to avoid performance anxiety, stage fright, and tremor during both auditions and public performances. The application to stage fright was first recognized in The Lancet in 1976, and by 1987, a survey conducted by the International Conference of Symphony Orchestra Musicians, representing the 51 largest orchestras in the United States, revealed 27% of its musicians had used beta blockers and 70% obtained them from friends, not physicians. Beta blockers are inexpensive, said to be relatively safe, and on one hand, seem to improve musicians' performances on a technical level, while some, such as Barry Green, the author of \"The Inner Game of Music\" and Don Greene, a former Olympic diving coach who teaches Juilliard students to overcome their stage fright naturally, say the performances may be perceived as \"soulless and inauthentic\".\n\nSurgery\nLow certainty evidence indicates that the use of beta blockers around the time of cardiac surgery may decrease the risk of heart dysrhythmias and atrial fibrillation. Starting them around the time of other types of surgery, however, may worsen outcomes. For non-cardiac surgery, the use of beta blockers to prevent adverse effects may reduce the risk of atrial fibrillation and myocardial infarctions (very low certainty evidence), however, there is moderate certainty evidence that this approach may increase the risk of hypotension. Low-certainty evidence suggests that beta blockers used perioperatively in non-cardiac surgeries may increase the risk of bradycardia.\n\nOther\nA 2014 Cochrane review investigated the use of beta blockers in the maintenance of chronic type B thoracic aortic aneurysm in comparison to other anti hypertensive medications. The review found no suitable evidence to support the current guidelines recommending its use.\nA 2017 Cochrane review on the use of beta blockers to prevent aortic dissections in people with Marfan syndrome was unable to draw definitive conclusions due to lack of evidence.\n\nMedical uses\nAdrenergic antagonists are mostly used for cardiovascular disease. The adrenergic antagonists are widely used for lowering blood pressure and relieving hypertension. These antagonists have a been proven to relieve the pain caused by myocardial infarction, and also the infarction size, which correlates with heart rate.\nThere are few non-cardiovascular uses for adrenergic antagonists. Alpha-adrenergic antagonists are also used for treatment of ureteric stones, pain and panic disorders, withdrawal, and anesthesia.\nBeta blockers are used to treat acute cardiovascular toxicity (e.g. in overdose) caused by sympathomimetics, for instance caused by amphetamine, methamphetamine, cocaine, ephedrine, and other drugs. Combined \u03b11 and beta blockers like labetalol and carvedilol may be more favorable for such purposes due to the possibility of \"unopposed \u03b1-stimulation\" with selective beta blockers.\n\nPerformance-enhancing use\nBecause they promote lower heart rates and reduce tremors, beta blockers have been used in professional sports where high accuracy is required, including archery, shooting, golf and snooker. Beta blockers are banned in some sports by the International Olympic Committee. In the 2008 Summer Olympics, 50-metre pistol silver medalist and 10-metre air pistol bronze medalist Kim Jong-su tested positive for propranolol and was stripped of his medals.\nFor similar reasons, beta blockers have also been used by surgeons.\nClassical musicians have commonly used beta blockers since the 1970s to reduce stage fright.\n\nAdverse effects\nAdverse drug reactions  associated with the use of beta blockers include: nausea, diarrhea, bronchospasm, dyspnea, cold extremities, exacerbation of Raynaud's syndrome, bradycardia, hypotension, heart failure, heart block, fatigue, dizziness, alopecia (hair loss), abnormal vision, hallucinations, insomnia, nightmares, sexual dysfunction, erectile dysfunction, alteration of glucose and lipid metabolism. Mixed \u03b11\/\u03b2-antagonist therapy is also commonly associated with orthostatic hypotension. \nCarvedilol therapy is commonly associated with edema. \nDue to the high penetration across the blood\u2013brain barrier, lipophilic beta blockers, such as propranolol and metoprolol, are more likely than other less lipophilic beta blockers to cause sleep disturbances, such as insomnia, vivid dreams and nightmares.\nAdverse effects associated with \u03b22-adrenergic receptor antagonist activity (bronchospasm, peripheral vasoconstriction, alteration of glucose and lipid metabolism) are less common with \u03b21-selective (often termed \"cardioselective\") agents, but receptor selectivity diminishes at higher doses. Beta blockade, especially of the beta-1 receptor at the macula densa, inhibits renin release, thus decreasing the release of aldosterone. This causes hyponatremia and hyperkalemia.\nHypoglycemia can occur with beta blockade because \u03b22-adrenoceptors normally stimulate glycogen breakdown (glycogenolysis) in the liver and pancreatic release of the hormone glucagon, which work together to increase plasma glucose. Therefore, blocking \u03b22-adrenoceptors lowers plasma glucose. \u03b21-blockers have fewer metabolic side effects in diabetic patients; however, the fast heart rate that serves as a warning sign for insulin-induced low blood sugar may be masked, resulting in hypoglycemia unawareness. This is termed beta blocker-induced hypoglycemia unawareness. Therefore, beta blockers are to be used cautiously in diabetics.\nA 2007 study revealed diuretics and beta blockers used for hypertension increase a patient's risk of developing diabetes mellitus, while ACE inhibitors and angiotensin II receptor antagonists (angiotensin receptor blockers) actually decrease the risk of diabetes. Clinical guidelines in Great Britain, but not in the United States, call for avoiding diuretics and beta blockers as first-line treatment of hypertension due to the risk of diabetes.\nBeta blockers must not be used in the treatment of selective alpha-adrenergic agonist overdose. The blockade of only beta receptors increases blood pressure, reduces coronary blood flow, left ventricular function, and cardiac output and tissue perfusion by means of leaving the alpha-adrenergic system stimulation unopposed. Beta blockers with lipophilic properties and CNS penetration such as metoprolol and labetalol may be useful for treating CNS and cardiovascular toxicity from a methamphetamine overdose. The mixed alpha- and beta blocker labetalol is especially useful for treatment of concomitant tachycardia and hypertension induced by methamphetamine. The phenomenon of \"unopposed alpha stimulation\" has not been reported with the use of beta blockers for treatment of methamphetamine toxicity. Other appropriate antihypertensive drugs to administer during hypertensive crisis resulting from stimulant overdose are vasodilators such as nitroglycerin, diuretics such as furosemide, and alpha blockers such as phentolamine.\n\nContraindications and cautions\nAsthma\nThe 2007 National Heart, Lung, and Blood Institute (NHLBI) asthma guidelines recommend against the use of non-selective beta blockers in asthmatics, while allowing for the use of cardio selective beta blockers.:\u200a182\u200a\nCardio selective beta blocker (\u03b21 blockers) can be prescribed at the least possible dose to those with mild to moderate respiratory symptoms. \u03b22-agonists can somewhat mitigate \u03b2-blocker-induced bronchospasm where it exerts greater efficacy on reversing selective \u03b2-blocker-induced bronchospasm than the nonselective \u03b2-blocker-induced worsening asthma and\/or COPD.\n\nDiabetes mellitus\nEpinephrine signals early warning of the upcoming hypoglycemia.\nBeta blockers' inhibition on epinephrine's effect can somewhat exacerbate hypoglycemia by interfering with glycogenolysis and mask signs of hypoglycemia such as tachycardia, palpitations, diaphoresis, and tremors. Diligent blood glucose level monitoring is necessary for a patient with diabetes mellitus on beta blocker.\n\nHyperthyroidism\nAbrupt withdrawal can result in a thyroid storm.\n\nBradycardia or AV block\nUnless a pacemaker is present, beta blockers can severely depress conduction in the AV node, resulting in a reduction of heart rate and cardiac output. One should be very cautious with the use of beta blockers in tachycardia patients with Wolff-Parkinson-White Syndrome, as it can result in life-threatening arrhythmia in certain patients. By slowing the conduction through the AV node, preferential conduction through the accessory pathway is favored. If the patient happens to develop atrial flutter, this could lead to a 1:1 conduction with very fast ventricular rate, or worse, ventricular fibrillation in the case of atrial fibrillation.\n\nToxicity\nGlucagon, used in the treatment of overdose, increases the strength of heart contractions, increases intracellular cAMP, and decreases renal vascular resistance. It is, therefore, useful in patients with beta blocker cardiotoxicity. Cardiac pacing is usually reserved for patients unresponsive to pharmacological therapy.\nPeople experiencing bronchospasm due to the \u03b22 receptor-blocking effects of nonselective beta blockers may be treated with anticholinergic drugs, such as ipratropium, which are safer than beta agonists in patients with cardiovascular disease. Other antidotes for beta blocker poisoning are salbutamol and isoprenaline.\n\nPharmacology\nIntrinsic sympathomimetic activity\nAlso referred to as intrinsic sympathomimetic effect, this term is used particularly with beta blockers that can show both agonism and antagonism at a given beta receptor, depending on the concentration of the agent (beta blocker) and the concentration of the antagonized agent (usually an endogenous compound, such as norepinephrine). See partial agonist for a more general description.\nSome beta blockers (e.g. oxprenolol, pindolol, penbutolol, labetalol and acebutolol) exhibit intrinsic sympathomimetic activity (ISA). These agents are capable of exerting low-level agonist activity at the \u03b2-adrenergic receptor while simultaneously acting as a receptor site antagonist. These agents, therefore, may be useful in individuals exhibiting excessive bradycardia with sustained beta blocker therapy.\nAgents with ISA should not be used for patients with any kind of angina as it can aggravate or after myocardial infarctions. They may also be less effective than other beta blockers in the management of angina and tachyarrhythmia.\n\n\u03b2-Adrenergic receptor antagonism\nStimulation of \u03b21 receptors by epinephrine and norepinephrine induces a positive chronotropic and inotropic effect on the heart and increases cardiac conduction velocity and automaticity. Stimulation of \u03b21 receptors on the kidney causes renin release. Stimulation of \u03b22 receptors induces smooth muscle relaxation, induces tremor in skeletal muscle, and increases glycogenolysis in the liver and skeletal muscle. Stimulation of \u03b23 receptors induces lipolysis.\nBeta blockers inhibit these normal epinephrine- and norepinephrine-mediated sympathetic actions, but have minimal effect on resting subjects.\nThat is, they reduce the effect of excitement or physical exertion on heart rate and force of contraction, and also tremor, and breakdown of glycogen. Beta blockers can have a constricting effect on the bronchi of the lungs, possibly worsening or causing asthma symptoms.\nSince \u03b22 adrenergic receptors can cause vascular smooth muscle dilation, beta blockers may cause some vasoconstriction. However, this effect tends to be small because the activity of \u03b22 receptors is overshadowed by the more dominant vasoconstricting \u03b11 receptors. By far the greatest effect of beta blockers remains in the heart. Newer, third-generation beta blockers can cause vasodilation through blockade of alpha-adrenergic receptors.\nAccordingly, nonselective beta blockers are expected to have antihypertensive effects. The primary antihypertensive mechanism of beta blockers is unclear, but may involve reduction in cardiac output (due to negative chronotropic and inotropic effects). It may also be due to reduction in renin release from the kidneys, and a central nervous system effect to reduce sympathetic activity (for those beta blockers that do cross the blood\u2013brain barrier, e.g. propranolol).\nAntianginal effects result from negative chronotropic and inotropic effects, which decrease cardiac workload and oxygen demand. Negative chronotropic properties of beta blockers allow the lifesaving property of heart rate control. Beta blockers are readily titrated to optimal rate control in many pathologic states.\nThe antiarrhythmic effects of beta blockers arise from sympathetic nervous system blockade\u2014resulting in depression of sinus node function and atrioventricular node conduction, and prolonged atrial refractory periods. Sotalol, in particular, has additional antiarrhythmic properties and prolongs action potential duration through potassium channel blockade.\nBlockade of the sympathetic nervous system on renin release leads to reduced aldosterone via the renin\u2013angiotensin\u2013aldosterone system, with a resultant decrease in blood pressure due to decreased sodium and water retention.\n\n\u03b11-Adrenergic receptor antagonism\nSome beta blockers (e.g., labetalol and carvedilol) exhibit mixed antagonism of both \u03b2- and \u03b11-adrenergic receptors, which provides additional arteriolar vasodilating action.\n\nBlood\u2013brain barrier permeability\nBeta blockers vary in their lipophilicity (fat solubility) and in turn in their ability to cross the blood\u2013brain barrier and exert effects in the central nervous system. Beta blockers with greater blood\u2013brain barrier permeability can have both neuropsychiatric therapeutic benefits and side effects, as well as adverse cognitive effects. Central nervous system-related side effects and risks of beta blockers may include fatigue, depression, sleep disorders (namely insomnia) and nightmares, visual hallucinations, delirium, psychosis, Parkinson's disease, and falling. Conversely, central nervous system-related benefits of beta blockers may include prevention and treatment of migraine, essential tremor, akathisia, anxiety, post-traumatic stress disorder, aggression, and obsessive\u2013compulsive disorder.\nMost beta blockers are lipophilic and can cross into the brain, but there are a number of exceptions. Highly lipophilic beta blockers include penbutolol, pindolol, propranolol, and timolol, moderately lipophilic beta blockers include acebutolol, betaxolol, bisoprolol, carvedilol, metoprolol, and nebivolol, and low lipophilicity or hydrophilic beta blockers include atenolol, carteolol, esmolol, labetalol, nadolol, and sotalol. It is thought that highly lipophilic beta blockers are able to readily cross into the brain, moderately lipophilic beta blockers are able to cross to a lesser degree, and low lipophilicity or hydrophilic beta blockers are minimally able to cross. More lipophilic beta-blockers are known to suppress melatonin release by 50-80%. The preceding beta blockers also vary in their intrinsic sympathomimetic activity and \u03b21-adrenergic receptor selectivity (or cardioselectivity), resulting in further differences in pharmacological profiles and suitability in different contexts between them.\n\nAgents\nNonselective agents\nNonselective beta blockers display both \u03b21 and \u03b22 antagonism.\n\nPropranolol\nBucindolol (has additional \u03b11-blocking activity)\nCarteolol\nCarvedilol (has additional \u03b11-blocking activity)\nLabetalol (has intrinsic sympathomimetic activity and additional \u03b11-blocking activity)\nNadolol\nOxprenolol (has intrinsic sympathomimetic activity)\nPenbutolol (has intrinsic sympathomimetic activity)\nPindolol (has intrinsic sympathomimetic activity)\nSotalol (not considered a \"typical beta blocker\")\nTimolol\n\n\u03b21-selective agents\n\u03b21-selective beta blockers are also known as cardioselective beta blockers. Pharmacologically, the beta-blockade of the \u03b21 receptors in the heart will act on cAMP. The function of cAMP as a second messenger in the cardiac cell is that it phosphorylates the LTCC and the ryanodine receptor to increase intracellular calcium levels and cause contraction. Beta-blockade of the \u03b21 receptor will inhibit cAMP from phosphorylating, and it will decrease the ionotrophic and chronotropic effect. Note that drugs may be cardioselective, or act on \u03b21 receptors in the heart only, but still have instrinsic sympathomimetic activity.\n\nAcebutolol (has intrinsic sympathomimetic activity, ISA)\nAtenolol\nBetaxolol\nBisoprolol\nCeliprolol (has intrinsic sympathomimetic activity)\nMetoprolol\nNebivolol\nEsmolol\nNebivolol and bisoprolol are the most \u03b21 cardioselective beta blockers.\n\n\u03b22-selective agents\nButaxamine\nICI-118,551\n\n\u03b23-selective agents\nSR 59230A\n\n\u03b21 selective antagonist and \u03b23 agonist agents\nNebivolol\n\nComparative information\nPharmacological differences\nAgents with intrinsic sympathomimetic action (ISA)\nAcebutolol, pindolol, labetalol, mepindolol, oxprenolol, celiprolol, penbutolol\nAgents organized by lipid solubility (lipophilicity)\nHigh lipophilicity: propranolol, labetalol\nIntermediate lipophilicity: metoprolol, bisoprolol, carvedilol, acebutolol, timolol, pindolol\nLow lipophilicity (also known as hydrophilic beta blockers): atenolol, nadolol, and sotalol\nAgents with membrane stabilizing effect\nCarvedilol, propranolol > oxprenolol > labetalol, metoprolol, timolol\n\nIndication differences\nAgents specifically labeled for cardiac arrhythmia\nEsmolol, sotalol, landiolol (Japan)\nAgents specifically labeled for congestive heart failure\nBisoprolol, carvedilol, sustained-release metoprolol\nAgents specifically labeled for glaucoma\nBetaxolol, carteolol, levobunolol, timolol, metipranolol\nAgents specifically labeled for myocardial infarction\nAtenolol, metoprolol (immediate release), propranolol (immediate release), timolol, carvedilol (after left ventricular dysfunction), bisoprolol (preventive treatment before and primary treatment after heart attacks)\nAgents specifically labeled for migraine prophylaxis\nTimolol, propranolol\nPropranolol is the only agent indicated for the control of tremor, portal hypertension, and esophageal variceal bleeding, and used in conjunction with \u03b1-blocker therapy in phaeochromocytoma.\n\nOther effects\nBeta blockers, due to their antagonism at beta-1 adrenergic receptors, inhibit both the synthesis of new melatonin and its secretion by the pineal gland. The neuropsychiatric side effects of some beta blockers (e.g. sleep disruption, insomnia) may be due to this effect.\nSome pre-clinical and clinical research suggests that some beta blockers may be beneficial for cancer treatment. However, other studies do not show a correlation between cancer survival and beta blocker usage. Also, a 2017 meta-analysis failed to show any benefit for the use of beta blockers in breast cancer.\nBeta blockers have also been used for the treatment of schizoid personality disorder. However, there is limited evidence supporting the efficacy of supplemental beta blocker use in addition to antipsychotic drugs for treating schizophrenia.\nContrast agents are not contraindicated in those receiving beta blockers.\n\nSee also\nAlpha blockers\nDiscovery and development of beta-blockers\n\nReferences\nExternal links\nMusicians and beta-blockers by Gerald Klickstein, March 11, 2010 (A blog post that considers \"whether beta-blockers are safe, effective, and appropriate for performers to use.\")\nBetter Playing Through Chemistry by Blair Tindall, The New York Times, October 17, 2004. (Discusses the use of beta blockers among professional musicians)\nMusicians using beta blockers by Blair Tindall. A condensed version of the above article.\nIn Defense of the Beta Blocker by Carl Elliott, The Atlantic, August 20, 2008. (Discusses the use of propranolol by a North Korean pistol shooter in the 2008 Olympics)\nbeta-Adrenergic+Blockers at the U.S. National Library of Medicine Medical Subject Headings (MeSH)","20":"Betahistine, sold under the brand name Serc among others, is an anti-vertigo medication. It is commonly prescribed for balance disorders or to alleviate vertigo symptoms. It was first registered in Europe in 1970 for the treatment of M\u00e9ni\u00e8re's disease, but current evidence does not support its efficacy in treating it.\n\nMedical uses\nBetahistine was once believed to have some positive effects in the treatment of M\u00e9ni\u00e8re's disease and vertigo, but more recent evidence casts doubt on its efficacy.  Studies of the use of betahistine have shown a reduction in symptoms of vertigo and, to a lesser extent, tinnitus, but conclusive evidence is lacking at present.\nOral betahistine has been approved for the treatment of M\u00e9ni\u00e8re's disease and vestibular vertigo in more than 80 countries worldwide, and has been reportedly prescribed for more than 130 million patients. However, betahistine has not been approved for marketing in the United States for the past few decades, and there is disagreement about its efficacy.\nThe Cochrane Library concluded in 2001 that \"Most trials suggested a reduction of vertigo with betahistine and some suggested a reduction in tinnitus but all these effects may have been caused by bias in the methods. One trial with good methods showed no effect of betahistine on tinnitus compared with placebo in 35 patients. None of the trials showed any effect of betahistine on hearing loss. No serious adverse effects were found with betahistine.\"\nBetahistine is also undergoing clinical trials for the treatment of attention deficit hyperactivity disorder (ADHD).\n\nContraindications\nBetahistine is contraindicated for patients with pheochromocytoma. Patients with bronchial asthma or a history of peptic ulcer need to be closely monitored.\n\nAdverse effects\nPatients taking betahistine may experience the following adverse effects:\n\nHeadache\nLow level of gastric adverse effects\nNausea can be an adverse effect, but patients are often already experiencing nausea owing to vertigo, so it goes largely unnoticed.\nPatients taking betahistine may experience hypersensitivity and allergic reactions. In the November 2006 issue of \"Drug Safety\", Dr. Sabine Jeck-Thole and Dr. Wolfgang Wagner reported that betahistine may cause allergic and skin-related adverse effects. These include rashes in several areas of the body; itching and urticaria (hives); and swelling of the face, tongue, and mouth. Other hypersensitivity reactions reported include tingling, numbness, burning sensations, shortness of breath, and laboured breathing. The study authors suggested that hypersensitivity reactions may be a direct result of betahistine's role in increasing histamine concentrations throughout the body. Hypersensitivity reactions quickly subside after betahistine has been discontinued.\n\nDigestive\nBetahistine may also cause several digestive-related adverse effects. The package insert for Serc, a trade name for betahistine, states that patients may experience several gastrointestinal side effects. These may include nausea, upset stomach, vomiting, diarrhea, dry mouth, and stomach cramping. These symptoms are usually not serious and subside between doses. Patients experiencing chronic digestive problems may lower their dose to the minimum effective and may mitigate the effects by taking betahistine with meals. Additional digestive problems may require that patients consult their physician in order to find a possible suitable alternative.\n\nOthers\nPeople taking betahistine may experience several other adverse effects ranging from mild to serious. The package insert for Serc states that patients may experience nervous-system side effects, including headache. Some nervous system events may also partly be attributable to the underlying condition, rather than the medication used to treat it. Jeck-Thole and Wagner also reported that patients may experience headache and liver problems, including increased liver enzymes and bile-flow disturbances. Any adverse effects that persist or outweigh the relief of symptoms of the original condition may warrant that the patient consult their physician to adjust or change the medication.\n\nPharmacology\nPharmacodynamics\nBetahistine is a strong antagonist at histamine H3 receptors and a weak agonist at histamine H1 receptors.\nBetahistine has two mechanisms of action. Primarily, it is a weak agonist at histamine H1 receptors located on blood vessels in the inner ear. This gives rise to local vasodilatation and increased permeability, which helps to reverse the underlying problem of endolymphatic hydrops.\nMore importantly, betahistine has a powerful antagonistic effect at histamine H3 receptors, thereby increasing the amounts of the neurotransmitters histamine, acetylcholine, norepinephrine, serotonin, and GABA released from nerve endings. The increased amounts of histamine released from histaminergic nerve endings can stimulate histamine receptors. This stimulation explains the potent vasodilatory effects of betahistine in the inner ear, which are well documented.\nBetahistine seems to dilate blood vessels in the inner ear, which can relieve pressure from excess fluid and act on the smooth muscle.\nIt is postulated that the increase in the amount of serotonin in the brainstem caused  by betahistine inhibits the activity of vestibular nuclei.\n\nPharmacokinetics\nBetahistine comes in both a tablet form and as an oral solution, and is taken orally. It is rapidly and completely absorbed. The mean plasma elimination half-life is 3 to 4 hours, and excretion is virtually complete in the urine within 24 hours. Plasma protein binding is very low. Betahistine is converted to aminoethylpyridine and hydroxyethylpyridine and excreted in the urine as pyridylacetic acid. There is some evidence that one of these metabolites, aminoethylpyridine, may be active and have effects similar to those of betahistine on ampullar receptors.\n\nChemistry\nBetahistine chemically is 2-[2-(methylamino)ethyl]pyridine, and is formulated as the dihydrochloride salt. Its chemical structure closely resembles those of phenethylamine and histamine.\n\nSociety and culture\nBrand names\nBetahistine is marketed under a number of brand names, including Veserc, Serc, Hiserk, Betaserc, and Vergo.\n\nAvailability\nBetahistine is widely used and available in Europe, including in the United Kingdom. It was approved by the US Food and Drug Administration in the early 1970s for M\u00e9ni\u00e8re's disease, but approval was later withdrawn because of lack of evidence of efficacy. The withdrawal was upheld by a US court of appeal in 1968.\n\nSee also\n2-Pyridylethylamine\n\n\n== References ==","21":"Bezold's abscess is an abscess deep to the sternocleidomastoid muscle where pus from mastoiditis erodes through the cortex of the mastoid part of the temporal bone, medial to the attachment of sternocleidomastoid, extends into the infratemporal fossa, and deep to the investing layer of the deep cervical fascia. It is a rare complication of acute otitis media.\n\nSymptoms\nSymptoms may include:\n\nSevere pain in perimastoid region\nDifficulty in swallowing (dysphagia)\nSore throat\nDifficulty in breathing (dyspnoea)\nNuchal rigidity\nFever\n\nDiagnosis\nCT scan of mastoid and swelling of the neck.\nDifferential diagnosis should include:\n\nAcute upper jugular lymphadenitis\nAbscess or mass in Lower part of parotid\nInfected branchial cyst\nParapharyngeal abscess\nJugular vein thrombosis\n\nTreatment\nCortical mastoidectomy for mastoiditis. Exploration of fistulous opening into the soft tissues of neck. Drainage of the neck abscess from a separate incision and insertion of a drain. Administration of intravenous antibiotics guided by the culture and sensitivity of the pus.\n\nEponym\nIt is named after Friedrich Bezold (German otologist, 1842\u20131908).\n\nReferences\n\n\n== External links ==","22":"Blurred vision is an ocular symptom where vision becomes less precise and there is added difficulty to resolve fine details.\nTemporary blurred vision may involve dry eyes, eye infections, alcohol poisoning, hypoglycemia, or low blood pressure. Other medical conditions may include refractive errors such as myopia, high hypermetropia, and astigmatism, amblyopia, presbyopia, pseudomyopia, diabetes, cataract, pernicious anemia, vitamin B12 deficiency, thiamine deficiency, glaucoma, retinopathy, hypervitaminosis A, migraine, sj\u00f6gren's syndrome, floater, macular degeneration, and can be a sign of stroke or brain tumor.\n\nCauses\nThere are many causes of blurred vision:\n\nRefractive errors: Uncorrected refractive errors like myopia, high hypermetropia, and astigmatism will cause distance vision blurring. It is one of the leading cause of visual impairment worldwide. Unless there is no associated amblyopia, visual blur due to refractive errors can be corrected to normal using corrective lenses or refractive surgeries.\nPresbyopia due to physiological insufficiency of accommodation (accommodation tends to decrease with age) is the main cause of defective near vision in the elderly. Other causes of defective near vision include accommodative insufficiency, paralysis of accommodation etc.\nPseudomyopia due to accommodation anomalies like accommodative excess, accommodative spasm etc. cause distance vision blurring.\nAlcohol intoxication can cause blurred vision.\nUse of cycloplegic drugs like atropine or other anticholinergics cause visual blur due to paralysis of accommodation.\nCataracts: Cloudiness over the eye's lens, cause blurring of vision, halos around lights, and sensitivity to glare. It is also the main cause of blindness worldwide.\nGlaucoma: Increased intraocular pressure (pressure in the eye) cause progressive optic neuropathy that leads to optic nerve damage, visual field defects and blindness. Sometimes glaucoma may occur without increased intraocular pressure also. Some glaucomas (e.g. open angle glaucoma) cause gradual loss of vision and some others (e.g. angle closure glaucoma) cause sudden loss of vision. It is one of the leading cause of blindness worldwide.\nDiabetes: Poorly controlled blood sugar can lead to temporary swelling of the lens of the eye, resulting in blurred vision.  While it resolves if blood sugar control is reestablished, it is believed repeated occurrences promote the formation of cataracts (which are not temporary).\nRetinopathy: If left untreated, any type of retinopathy (including diabetic retinopathy, hypertensive retinopathy, sickle cell retinopathy, anemic retinopathy, etc.) can damage retina and lead to visual field defects and blindness.\nHypervitaminosis A: Excess consumption of vitamin A can cause blurred vision.\nMacular degeneration: Macular degeneration cause loss of central vision, blurred vision (especially while reading), metamorphopsia (seeing straight lines as wavy), and colors appearing faded. Macular degeneration is the third main cause of blindness worldwide, and is the main cause of blindness in industrialised countries.\nEye infection, inflammation, or injury.\nSj\u00f6gren's syndrome, a chronic autoimmune inflammatory disease that destroys moisture producing glands, including lacrimal gland and leads to dry eye and visual blur.\nFloaters: Tiny particles drifting across the eye. Although often brief and harmless, they may be a sign of retinal detachment.\nRetinal detachment: Symptoms include floaters, flashes of light across your visual field, or a sensation of a shade or curtain hanging on one side of your visual field.\nOptic neuritis: Inflammation of the optic nerve from infection or multiple sclerosis may cause blurring of vision. There may be pain while moving the eye or touching it through the eyelid.\nStroke or transient ischemic attack\nBrain tumor\nToxocara: A parasitic roundworm that can cause blurred vision.\nBleeding into the eye\nTemporal arteritis: Inflammation of an artery in the brain that supplies blood to the optic nerve.\nMigraine headaches: Spots of light, halos, or zigzag patterns are common symptoms prior to the start of the headache. A retinal migraine is when you have only visual symptoms without a headache.\nReduced blinking: Lid closure that occurs too infrequently often leads to irregularities of the tear film due to prolonged evaporation, thus resulting in disruptions in visual perception.\nCarbon monoxide poisoning: Reduced oxygen delivery can affect many areas of the body including vision. Other symptoms caused by CO include vertigo, hallucination and sensitivity to light.\n\nSee also\nFunctional visual loss\n\n\n== References ==","23":"A brain tumor occurs when abnormal cells form within the brain. There are two main types of tumors: malignant (cancerous) tumors and benign (non-cancerous) tumors. These can be further classified as primary tumors, which start within the brain, and secondary tumors, which most commonly have spread from tumors located outside the brain, known as brain metastasis tumors. All types of brain tumors may produce symptoms that vary depending on the size of the tumor and the part of the brain that is involved. Where symptoms exist, they may include headaches, seizures, problems with vision, vomiting and mental changes. Other symptoms may include difficulty walking, speaking, with sensations, or unconsciousness.\nThe cause of most brain tumors is unknown, though up to 4% of brain cancers may be caused by CT scan radiation. Uncommon risk factors include exposure to vinyl chloride, Epstein\u2013Barr virus, ionizing radiation, and inherited syndromes such as neurofibromatosis, tuberous sclerosis, and von Hippel-Lindau Disease. Studies on mobile phone exposure have not shown a clear risk. The most common types of primary tumors in adults are meningiomas (usually benign) and astrocytomas such as glioblastomas. In children, the most common type is a malignant medulloblastoma. Diagnosis is usually by medical examination along with computed tomography (CT) or magnetic resonance imaging (MRI). The result is then often confirmed by a biopsy. Based on the findings, the tumors are divided into different grades of severity.\nTreatment may include some combination of surgery, radiation therapy and chemotherapy. If seizures occur, anticonvulsant medication may be needed. Dexamethasone and furosemide are medications that may be used to decrease swelling around the tumor. Some tumors grow gradually, requiring only monitoring and possibly needing no further intervention. Treatments that use a person's immune system are being studied. Outcomes for malignant tumors vary considerably depending on the type of tumor and how far it has spread at diagnosis. Although benign tumors only grow in one area, they may still be life-threatening depending on their size and location. Malignant glioblastomas usually have very poor outcomes, while benign meningiomas usually have good outcomes. The average five-year survival rate for all (malignant) brain cancers in the United States is 33%.\nSecondary, or metastatic, brain tumors are about four times as common as primary brain tumors, with about half of metastases coming from lung cancer. Primary brain tumors occur in around 250,000 people a year globally, and make up less than 2% of cancers. In children younger than 15, brain tumors are second only to acute lymphoblastic leukemia as the most common form of cancer. In NSW Australia in 2005, the average lifetime economic cost of a case of brain cancer was AU$1.9 million, the greatest of any type of cancer.\n\nSigns and symptoms\nThe signs and symptoms of brain tumors are broad. People may experience symptoms regardless of whether the tumor is benign (not cancerous) or cancerous. Primary and secondary brain tumors present with similar symptoms, depending on the location, size, and rate of growth of the tumor. For example, larger tumors in the frontal lobe can cause changes in the ability to think. However, a smaller tumor in an area such as Wernicke's area (small area responsible for language comprehension) can result in a greater loss of function.\n\nHeadaches\nHeadaches as a result of raised intracranial pressure can be an early symptom of brain cancer. However, isolated headache without other symptoms is rare, and other symptoms including visual abnormalities may occur before headaches become common. Certain warning signs for headache exist which make the headache more likely to be associated with brain cancer. These are defined as \"abnormal neurological examination, headache worsened by Valsalva maneuver, headache causing awakening from sleep, new headache in the older population, progressively worsening headache, atypical headache features, or patients who do not fulfill the strict definition of migraine\". Other associated signs are headaches that are worse in the morning or that subside after vomiting.\n\nLocation-specific symptoms\nThe brain is divided into lobes and each lobe or area has its own function. A tumor in any of these lobes may affect the area's performance. The symptoms experienced are often linked to the location of the tumor, but each person may experience something different.\n\nFrontal lobe: Tumors may contribute to poor reasoning, inappropriate social behavior, personality changes, poor planning, lower inhibition, and decreased production of speech (Broca's area).\nTemporal lobe: Tumors in this lobe may contribute to poor memory, loss of hearing, and difficulty in language comprehension (Wernicke's area is located in this lobe).\nParietal lobe: Tumors here may result in poor interpretation of languages, difficulty with speaking, writing, drawing, naming, and recognizing, and poor spatial and visual perception.\nOccipital lobe: Damage to this lobe may result in poor vision or loss of vision.\nCerebellum: Tumors in this area may cause poor balance, muscle movement, and posture.\nBrain stem: Tumors on the brainstem can cause seizures, endocrine problems, respiratory changes, visual changes, headaches and partial paralysis.\n\nBehavior changes\nA person's personality may be altered due to the tumor-damaging lobes of the brain. Since the frontal, temporal, and parietal lobes control inhibition, emotions, mood, judgement, reasoning, and behavior, a tumor in those regions can cause inappropriate social behavior, temper tantrums, laughing at things which merit no laughter, and even psychological symptoms such as depression and anxiety. More research is needed into the effectiveness and safety of medication for depression in people with brain tumors.\nPersonality changes can have damaging effects such as unemployment, unstable relationships, and a lack of control.\n\nCause\nThe best-known cause of brain cancers is ionizing radiation. Approximately 4% of brain cancers in the general population are caused by CT-scan radiation. For brain cancers that follow a CT scan at lags of 2 years or more, it has been estimated that 40% are attributable to CT-scan radiation. The relationship between ionizing radiation and brain cancers can be best explained by radiation carcinogenesis, and by traditional models of oncogenesis. The stochastic effects of ionizing radiation demonstrate a dose-response relationship to the probability of occurrence, but no dose-response relationship to severity of disease. The majority of radiation-induced brain cancers are caused by ionizing radiation from medical sources such as CT scans.\nMutations and deletions of tumor suppressor genes, such as P53, are thought to be the cause of some forms of brain tumor. Inherited conditions, such as Von Hippel\u2013Lindau disease, tuberous sclerosis, multiple endocrine neoplasia, and neurofibromatosis type 2 carry a high risk for the development of brain tumors. People with celiac disease have a slightly increased risk of developing brain tumors. Smoking may increase the risk, but evidence of this remains unclear.\nAlthough studies have not shown any link between cell-phone or mobile-phone radiation and the occurrence of brain tumors, the World Health Organization has classified mobile-phone radiation on the IARC scale into Group 2B \u2013 possibly carcinogenic.\nThe claim that cell-phone usage may cause brain cancer is likely based on epidemiological studies which observed a slight increase in glioma risk among heavy users of wireless phones. When those studies were conducted, GSM (2G) phones were in use. Modern, third-generation (3G) phones emit, on average, about 1% of the energy emitted by those GSM (2G) phones, and therefore the finding of an association between cell-phone usage and increased risk of brain cancer is not based upon current phone usage.\n\nPathophysiology\nMeninges\nHuman brains are surrounded by a system of connective tissue membranes called meninges that separate the brain from the skull. This three-layered covering is composed of (from the outside in) the dura mater, arachnoid mater, and pia mater. The arachnoid and pia are physically connected and thus often considered as a single layer, the leptomeninges. Between the arachnoid mater and the pia mater is the subarachnoid space which contains cerebrospinal fluid (CSF). This fluid circulates in the narrow spaces between cells and through the cavities in the brain called ventricles, to support and protect the brain tissue. Blood vessels enter the central nervous system through the perivascular space above the pia mater. The cells in the blood vessel walls are joined tightly, forming the blood\u2013brain barrier which protects the brain from toxins that might enter through the blood.\nTumors of the meninges are meningiomas and are often benign. Though not technically a tumor of brain tissue, they are often considered brain tumors since they protrude into the space where the brain is, causing symptoms. Since they are usually slow-growing tumors, meningiomas can be quite large by the time symptoms appear.\n\nBrain matter\nThe brains of humans and other vertebrates are composed of very soft tissue and have a gelatin-like texture. Living brain tissue has a pink tint in color on the outside (gray matter), and nearly complete white on the inside (white matter), with subtle variations in color. The three largest divisions of the brain are:\n\nCerebral cortex\nBrainstem\nCerebellum\nThese areas are composed of two broad classes of cells: neurons and glia. These two types are equally numerous in the brain as a whole, although glial cells outnumber neurons roughly 4 to 1 in the cerebral cortex. Glia come in several types, which perform a number of critical functions, including structural support, metabolic support, insulation, and guidance of development. Primary tumors of the glial cells are called gliomas and often are malignant by the time they are diagnosed.\nThe thalamus and hypothalamus are major divisions of the diencephalon, with the pituitary gland and pineal gland attached at the bottom; tumors of the pituitary and pineal gland are often benign.\nThe brainstem lies between the large cerebral cortex and the spinal cord. It is divided into the midbrain, pons, and medulla oblongata.\n\nSpinal cord\nThe spinal cord is considered a part of the central nervous system. It is made up of the same cells as the brain: neurons and glial cells.\n\nDiagnosis\nAlthough there is no specific or singular symptom or sign, the presence of a combination of symptoms and the lack of corresponding indications of other causes can be an indicator for investigation towards the possibility of a brain tumor. Brain tumors have similar characteristics and obstacles when it comes to diagnosis and therapy with tumors located elsewhere in the body. However, they create specific issues that follow closely to the properties of the organ they are in.\nThe diagnosis will often start by taking a medical history noting medical antecedents, and current symptoms. Clinical and laboratory investigations will serve to exclude infections as the cause of the symptoms. Examinations in this stage may include the eyes, otolaryngological (or ENT) and electrophysiological exams. The use of electroencephalography (EEG) often plays a role in the diagnosis of brain tumors.\nBrain tumors, when compared to tumors in other areas of the body, pose a challenge for diagnosis. Commonly, radioactive tracers are uptaken in large volumes in tumors due to the high activity of tumor cells, allowing for radioactive imaging of the tumor. However, most of the brain is separated from the blood by the blood\u2013brain barrier (BBB), a membrane that exerts a strict control over what substances are allowed to pass into the brain. Therefore, many tracers that may reach tumors in other areas of the body easily would be unable to reach brain tumors until there was a disruption of the BBB by the tumor. Disruption of the BBB is well imaged via MRI or CT scan, and is therefore regarded as the main diagnostic indicator for malignant gliomas, meningiomas, and brain metastases.\nSwelling or obstruction of the passage of cerebrospinal fluid (CSF) from the brain may cause (early) signs of increased intracranial pressure which translates clinically into headaches, vomiting, or an altered state of consciousness, and in children changes to the diameter of the skull and bulging of the fontanelles. More complex symptoms such as endocrine dysfunctions should alarm doctors not to exclude brain tumors.\nA bilateral temporal visual field defect (due to compression of the optic chiasm) or dilation of the pupil, and the occurrence of either slowly evolving or the sudden onset of focal neurologic symptoms, such as cognitive and behavioral impairment (including impaired judgment, memory loss, lack of recognition, spatial orientation disorders), personality or emotional changes, hemiparesis, hypoesthesia, aphasia, ataxia, visual field impairment, impaired sense of smell, impaired hearing, facial paralysis, double vision, or more severe symptoms such as tremors, paralysis on one side of the body hemiplegia, or (epileptic) seizures in a patient with a negative history for epilepsy, should raise the possibility of a brain tumor.\n\nImaging\nMedical imaging plays a central role in the diagnosis of brain tumors. Early imaging methods \u2013 invasive and sometimes dangerous \u2013 such as pneumoencephalography and cerebral angiography have been abandoned in favor of non-invasive, high-resolution techniques, especially magnetic resonance imaging (MRI) and computed tomography (CT) scans, though MRI is typically the reference standard used. Neoplasms will often show as differently colored masses (also referred to as processes) in CT or MRI results.\n\nBenign brain tumors often show up as hypodense (darker than brain tissue) mass lesions on CT scans. On MRI, they appear either hypodense or isointense (same intensity as brain tissue) on T1-weighted scans, or hyperintense (brighter than brain tissue) on T2-weighted MRI, although the appearance is variable.\nContrast agent uptake, sometimes in characteristic patterns, can be demonstrated on either CT or MRI scans in most malignant primary and metastatic brain tumors.\nPressure areas where the brain tissue has been compressed by a tumor also appear hyperintense on T2-weighted scans and might indicate the presence of a diffuse neoplasm due to an unclear outline. Swelling around the tumor known as peritumoral edema can also show a similar result. This is because these tumors disrupt the normal functioning of the BBB and lead to an increase in its permeability.\nMore recently, advancements have been made to increase the utility of MRI in providing physiological data that can help to inform diagnosis and prognosis. MRI itself is sufficient in identifying the brain tumor's location and morphology, but other types of MRI may be used on top of that, such as MRA, MRS, pMRI, fMRI, and DWI. These imaging techniques help doctors and surgeons to diagnose the type of tumor, plan for surgery, and to assess treatment and radiation\/chemotherapy. Treatment with radiation and chemotherapy can lead to treatment induced changes in the brain, visible on conventional imaging and which can be difficult to differentiate from tumor recurrence.\n\nDifferent Types of MRI Scans\nMagnetic Resonance Angiography (MRA) \u2013  looks at the blood vessels in the brain. In the diagnosis of brain tumor, MRAs are typically carried out before surgery to help surgeons get a better understanding of the tumor vasculature. For example, a study was done where surgeons were able to separate benign brain tumors from malignant ones by analyzing the shapes of the blood vessels that were extracted from MRA. Although not required, some MRA may inject contrast agent, gadolinium, into the patient to get an enhanced image\nMagnetic Resonance Spectroscopy (MRS) \u2013  measures the metabolic changes or chemical changes inside the tumor. The most common MRS is proton spectroscopy with its frequency measured in parts per million (ppm). Gliomas or malignant brain tumors have different spectra from normal brain tissue in that they have greater choline levels and lower N-acetyl aspartate (NAA) signals. Using MRS in brain tumor diagnosis can help doctors identify the type of tumor and its aggressiveness. For example, benign brain tumors or meningioma have increased alanine levels. It can also help to distinguish brain tumors from scar tissues or dead tissues caused by previous radiation treatment, which does not have increased choline levels that brain tumors have, and from tumor-mimicking lesions such as abscesses or infarcts.\nPerfusion Magnetic Resonance Imaging (pMRI) \u2013  assess the blood volume and blood flow of different parts of the brain and brain tumors. pMRI requires the injection of contrast agent, usually gadopentetate dimeglumine (Gd-DTPA) into the veins in order to enhance the contrast. pMRI provides a cerebral blood volume map that shows the tumor vascularity and angiogenesis. Brain tumors would require a larger blood supply and thus, would show a high cerebral blood volume on the pMRI map. The vascular morphology and degree of angiogenesis from pMRI help to determine the grade and malignancy of brain tumors. For brain tumor diagnosis, pMRI is useful in determining the best site to perform biopsy and to help reduce sampling error. pMRI is also valuable for after treatment to determine if the abnormal area is a remaining tumor or a scar tissue. For patients that are undergoing anti-angiogenesis cancer therapy, pMRI can give the doctors a better sense of efficacy of the treatment by monitoring tumor cerebral blood volume.\nFunctional MRI (fMRI) \u2013  measures blood flow changes in active parts of the brain while the patient is performing tasks and provides specific locations of the brain that are responsible for certain functions. Before performing a brain tumor surgery on patients, neurosurgeons would use fMRI to avoid damage to structures of the brain that correspond with important brain functions while resecting the tumor at the same time. Preoperative fMRI is important because it is often difficult to distinguish the anatomy near the tumor as it distorts its surrounding regions. Neurosurgeons would use fMRI to plan whether to perform a resection where tumor is surgically removed as much as possible, a biopsy where they take a surgical sampling amount to provide a diagnosis, or to not undergo surgery at all. For example, a neurosurgeon may be opposed to resecting a tumor near the motor cortex as that would affect the patient's movements. Without preoperative fMRI, the neurosurgeon would have to perform an awake-craniotomy where the patient would have to interact during open surgery to see if tumor removal would affect important brain functions.\nDiffusion Weighted Imaging (DWI) \u2013  a form of MRI that measures random Brownian motion of water molecules along a magnetic field gradient. For brain tumor diagnosis, measurement of apparent diffusion coefficient (ADC) in brain tumors allow doctors to categorize tumor type. Most brain tumors have higher ADC than normal brain tissues and doctors can match the observed ADC of the patient's brain tumor with a list of accepted ADC to identify tumor type. DWI is also useful for treatment and therapy purposes where changes in diffusion can be analyzed in response to drug, radiation, or gene therapy. Successful response results in apoptosis and increase in diffusion while failed treatment results in unchanged diffusion values.\n\nOther Types of Imaging Techniques\nComputed Tomography (CT) Scan \u2013  uses x-rays to take pictures from different angles and computer processing to combine the pictures into a 3D image. A CT scan usually serves as an alternative to MRI in cases where the patient cannot have an MRI due to claustrophobia or pacemaker. Compared to MRI, a CT scan shows a more detailed image of the bone structures near the tumor and can be used to measure the tumor's size. Like an MRI, a contrast dye may also be injected into the veins or ingested by mouth before a CT scan to better outline any tumors that may be present. CT scans use contrast materials that are iodine-based and barium sulfate compounds. The downside of using CT scans as opposed to MRI is that some brain tumors do not show up well on CT scans because some intra-axial masses are faint and resemble normal brain tissue. In some scenarios, brain tumors in CT scans may be mistaken for infarction, infection, and demyelination. To suspect that an intra-axial mass is a brain tumor instead of other possibilities, there must be unexplained calcifications in the brain, preservation of the cortex, and disproportionate mass effect.\nCT Angiography (CTA) \u2013  provides information about the blood vessels in the brain using X-rays. A contrast agent is always required to be injected into the patient in the CT scanner. CTA serves as an alternative to MRA.\nPositron Emission Tomography (PET) Scan \u2013  uses radioactive substances, with the most common one being a sugar known as FDG, while more specific tracers for glioma are emerging. This injected substance is taken up by cells that are actively dividing. Tumor cells are more active in dividing so they would absorb more of the radioactive substance. After injection, a scanner would be used to create an image of the radioactive areas in the brain. PET scans are used more often for high-grade tumors than for low-grade tumors. It is useful after treatment to help doctors determine if the abnormal area on an MRI image is a remaining tumor or a scar tissue. Scar tissues will not show up on PET scans while tumors would.\nHowever, these techniques cannot alone diagnose high- versus low-grade gliomas, and thus the definitive diagnosis of brain tumor should only be confirmed by histological examination of tumor tissue samples obtained either by means of brain biopsy or open surgery. The histological examination is essential for determining the appropriate treatment and the correct prognosis. This examination, performed by a pathologist, typically has three stages: interoperative examination of fresh tissue, preliminary microscopic examination of prepared tissues, and follow-up examination of prepared tissues after immunohistochemical staining or genetic analysis.\n\nPathology\nTumors have characteristics that allow the determination of malignancy and how they will evolve, and determining these characteristics will allow the medical team to determine the management plan.\nAnaplasia or dedifferentiation: loss of differentiation of cells and of their orientation to one another and blood vessels, a characteristic of anaplastic tumor tissue. Anaplastic cells have lost total control of their normal functions and many have deteriorated cell structures. Anaplastic cells often have abnormally high nuclear-to-cytoplasmic ratios, and many are multinucleated. Additionally, the nucleus of anaplastic cells is usually unnaturally shaped or oversized. Cells can become anaplastic in two ways: neoplastic tumor cells can dedifferentiate to become anaplasias (the dedifferentiation causes the cells to lose all of their normal structure\/function), or cancer stem cells can increase their capacity to multiply (i.e., uncontrollable growth due to failure of differentiation).\nAtypia: an indication of abnormality of a cell (which may be indicative of malignancy). Significance of the abnormality is highly dependent on context.\nNeoplasia: the (uncontrolled) division of cells. As such, neoplasia is not problematic but its consequences are: the uncontrolled division of cells means that the mass of a neoplasm increases in size, and in a confined space such as the intracranial cavity this quickly becomes problematic because the mass invades the space of the brain pushing it aside, leading to compression of the brain tissue and increased intracranial pressure and destruction of brain parenchyma. Increased intracranial pressure (ICP) may be attributable to the direct mass effect of the tumor, increased blood volume, or increased cerebrospinal fluid (CSF) volume, which may, in turn, have secondary symptoms.\nNecrosis: the (premature) death of cells, caused by external factors such as infection, toxin or trauma. Necrotic cells send the wrong chemical signals which prevent phagocytes from disposing of the dead cells, leading to a buildup of dead tissue, cell debris and toxins at or near the site of the necrotic cells\nArterial and venous hypoxia, or the deprivation of adequate oxygen supply to certain areas of the brain, occurs when a tumor makes use of nearby blood vessels for its supply of blood and the neoplasm enters into competition for nutrients with the surrounding brain tissue. More generally a neoplasm may cause release of metabolic end products (e.g., free radicals, altered electrolytes, neurotransmitters), and release and recruitment of cellular mediators (e.g., cytokines) that disrupt normal parenchymal function.\n\nClassification\nTumors can be benign or malignant, can occur in different parts of the brain, and may be classified as primary or secondary. A primary tumor is one that has started in the brain, as opposed to a metastatic tumor, which is one that has spread to the brain from another area of the body. The incidence of metastatic tumors is approximately four times greater than primary tumors. Tumors may or may not be symptomatic: some tumors are discovered because the patient has symptoms, others show up incidentally on an imaging scan, or at an autopsy.\nGrading of the tumors of the central nervous system commonly occurs on a 4-point scale (I-IV) created by the World Health Organization in 1993. Grade I tumors are the least severe and commonly associated with long-term survival, with severity and prognosis worsening as the grade increases. Low-grade tumors are often benign, while higher grades are aggressively malignant and\/or metastatic. Other grading scales do exist, many based upon the same criteria as the WHO scale and graded from I-IV.\n\nPrimary\nThe most common primary brain tumors are:\n\nGliomas (50.4%)\nMeningiomas (20.8%)\nPituitary adenomas (15%)\nNerve sheath tumors (10%)\nThese common tumors can also be organized according to tissue of origin as shown below:\n\nSecondary\nSecondary tumors of the brain are metastatic and have invaded the brain from cancers originating in other organs. This means that a cancerous neoplasm has developed in another organ elsewhere in the body and that cancer cells have leaked from that primary tumor and then entered the lymphatic system and blood vessels.  They then circulate through the bloodstream, and are deposited in the brain. There, these cells continue growing and dividing, becoming another invasive neoplasm of primary cancer's tissue. Secondary tumors of the brain are very common in the terminal phases of patients with an incurable metastasized cancer; the most common types of cancers that bring about secondary tumors of the brain are lung cancer, breast cancer, malignant melanoma, kidney cancer, and colon cancer (in decreasing order of frequency).\nSecondary brain tumors are more common than primary ones; in the United States, there are about 170,000 new cases every year. Secondary brain tumors are the most common cause of tumors in the intracranial cavity. The skull bone structure can also be subject to a neoplasm that by its very nature reduces the volume of the intracranial cavity, and can damage the brain.\n\nBy behavior\nBrain tumors or intracranial neoplasms can be cancerous (malignant) or non-cancerous (benign). However, the definitions of malignant or benign neoplasms differ from those commonly used in other types of cancerous or non-cancerous neoplasms in the body.\nIn cancers elsewhere in the body, three malignant properties differentiate benign tumors from malignant forms of cancer: benign tumors are self-limited and do not invade or metastasize. Characteristics of malignant tumors include:\n\nuncontrolled mitosis (growth by division beyond the normal limits)\nanaplasia: the cells in the neoplasm have an obviously different form (in size and shape). Anaplastic cells display marked pleomorphism. The cell nuclei are characteristically extremely hyperchromatic (darkly stained) and enlarged; the nucleus might have the same size as the cytoplasm of the cell (nuclear-cytoplasmic ratio may approach 1:1, instead of the normal 1:4 or 1:6 ratio). Giant cells \u2013 considerably larger than their neighbors \u2013 may form and possess either one enormous nucleus or several nuclei (syncytia). Anaplastic nuclei are variable and bizarre in size and shape.\ninvasion or infiltration (medical literature uses these terms as synonymous equivalents. However, for clarity, the articles that follow adhere to a convention that they mean slightly different things; this convention is not followed outside these articles):\nInvasion or invasiveness is the spatial expansion of the tumor through uncontrolled mitosis, in the sense that the neoplasm invades the space occupied by adjacent tissue, thereby pushing the other tissue aside and eventually compressing the tissue. Often these tumors are associated with clearly outlined tumors in imaging.\nInfiltration is the behavior of the tumor either to grow (microscopic) tentacles that push into the surrounding tissue (often making the outline of the tumor undefined or diffuse) or to have tumor cells \"seeded\" into the tissue beyond the circumference of the tumorous mass; this does not mean that an infiltrative tumor does not take up space or does not compress the surrounding tissue as it grows, but an infiltrating neoplasm makes it difficult to say where the tumor ends and the healthy tissue starts.\nmetastasis (spread to other locations in the body via lymph or blood).\nOf the above malignant characteristics, some elements do not apply to primary neoplasms of the brain:\n\nPrimary brain tumors rarely metastasize to other organs; some forms of primary brain tumors can metastasize but will not spread outside the intracranial cavity or the central spinal canal. Due to the BBB, cancerous cells of a primary neoplasm cannot enter the bloodstream and get carried to another location in the body. (Occasional isolated case reports suggest spread of certain brain tumors outside the central nervous system, e.g. bone metastasis of glioblastoma.)\nPrimary brain tumors generally are invasive (i.e. they will expand spatially and intrude into the space occupied by other brain tissue and compress those brain tissues); however, some of the more malignant primary brain tumors will infiltrate the surrounding tissue.\n\nBy genetics\nIn 2016, the WHO restructured their classifications of some categories of gliomas to include distinct genetic mutations that have been useful in differentiating tumor types, prognoses, and treatment responses. Genetic mutations are typically detected via immunohistochemistry, a technique that visualizes the presence or absence of a targeted protein via staining.\n\nMutations in IDH1 and IDH2 genes are commonly found in low-grade gliomas\nLoss of both IDH genes combined with loss of chromosome arms 1p and 19q indicates the tumor is an oligodendroglioma\nLoss of TP53 and ATRX characterizes astrocytomas\nGenes EGFR, TERT, and PTEN, are commonly altered in gliomas and are useful in differentiating tumor grade and biology\n\nSpecific types\nAnaplastic astrocytoma, Anaplastic oligodendroglioma, Astrocytoma, Central neurocytoma, Choroid plexus carcinoma, Choroid plexus papilloma, Choroid plexus tumor, Colloid cyst, Dysembryoplastic neuroepithelial tumour, Ependymal tumor, Fibrillary astrocytoma, Giant-cell glioblastoma, Glioblastoma, Gliomatosis cerebri, Gliosarcoma, Hemangiopericytoma, Medulloblastoma, Medulloepithelioma, Meningeal carcinomatosis, Neuroblastoma, Neurocytoma, Oligoastrocytoma, Oligodendroglioma, Optic nerve sheath meningioma, Pediatric ependymoma, Pilocytic astrocytoma, Pinealoblastoma, Pineocytoma, Pleomorphic anaplastic neuroblastoma, Pleomorphic xanthoastrocytoma, Primary central nervous system lymphoma, Sphenoid wing meningioma, Subependymal giant cell astrocytoma, Subependymoma, Trilateral retinoblastoma.\n\nTreatment\nA medical team generally assesses the treatment options and presents them to the person affected and their family. Various types of treatment are available depending on tumor type and location, and may be combined to produce the best chances of survival:\n\nSurgery: complete or partial resection of the tumor with the objective of removing as many tumor cells as possible.\nRadiotherapy: the most commonly used treatment for brain tumors; the tumor is irradiated with beta, x rays or gamma rays.\nChemotherapy: a treatment option for cancer, however, it is not always used to treat brain tumors as the blood\u2013brain barrier can prevent some drugs from reaching the cancerous cells.\nA variety of experimental therapies are available through clinical trials.\nSurvival rates in primary brain tumors depend on the type of tumor, age, functional status of the patient, the extent of surgical removal and other factors specific to each case.\nStandard care for anaplastic oligodendrogliomas and anaplastic oligoastrocytomas is surgery followed by radiotherapy. One study found a survival benefit for the addition of chemotherapy to radiotherapy after surgery, compared with radiotherapy alone.\n\nSurgery\nThe primary and most desired course of action described in medical literature is surgical removal (resection) via craniotomy. Minimally invasive techniques are becoming the dominant trend in neurosurgical oncology. The main objective of surgery is to remove as many tumor cells as possible, with complete removal being the best outcome and cytoreduction (\"debulking\") of the tumor otherwise. A Gross Total Resection (GTR) occurs when all visible signs of the tumor are removed, and subsequent scans show no apparent tumor. In some cases access to the tumor is impossible and impedes or prohibits surgery.\nMany meningiomas, with the exception of some tumors located at the skull base, can be successfully removed surgically.\nMost pituitary adenomas can be removed surgically, often using a minimally invasive approach through the nasal cavity and skull base (trans-nasal, trans-sphenoidal approach). Large pituitary adenomas require a craniotomy (opening of the skull) for their removal. Radiotherapy, including stereotactic approaches, is reserved for inoperable cases.\nSeveral current research studies aim to improve the surgical removal of brain tumors by labeling tumor cells with 5-aminolevulinic acid that causes them to fluoresce. Postoperative radiotherapy and chemotherapy are integral parts of the therapeutic standard for malignant tumors.\nMultiple metastatic tumors are generally treated with radiotherapy and chemotherapy rather than surgery and the prognosis in such cases is determined by the primary tumor, and is generally poor.\n\nRadiation therapy\nThe goal of radiation therapy is to kill tumor cells while leaving normal brain tissue unharmed. In standard external beam radiation therapy, multiple treatments of standard-dose \"fractions\" of radiation are applied to the brain. This process is repeated for a total of 10 to 30 treatments, depending on the type of tumor. This additional treatment provides some patients with improved outcomes and longer survival rates.\nRadiosurgery is a treatment method that uses computerized calculations to focus radiation at the site of the tumor while minimizing the radiation dose to the surrounding brain. Radiosurgery may be an adjunct to other treatments, or it may represent the primary treatment technique for some tumors. Forms used include stereotactic radiosurgery, such as Gamma knife, Cyberknife or Novalis Tx radiosurgery.\nRadiotherapy is the most common treatment for secondary brain tumors. The amount of radiotherapy depends on the size of the area of the brain affected by cancer. Conventional external beam \"whole-brain radiotherapy treatment\" (WBRT) or \"whole-brain irradiation\" may be suggested if there is a risk that other secondary tumors will develop in the future. Stereotactic radiotherapy is usually recommended in cases involving fewer than three small secondary brain tumors. Radiotherapy may be used following, or in some cases in place of, resection of the tumor. Forms of radiotherapy used for brain cancer include external beam radiation therapy, the most common, and brachytherapy and proton therapy, the last especially used for children.\nPeople who receive stereotactic radiosurgery (SRS) and whole-brain radiation therapy (WBRT) for the treatment of metastatic brain tumors have more than twice the risk of developing learning and memory problems than those treated with SRS alone. Results of a 2021 systematic review found that when using SRS as the initial treatment, survival or death related to brain metastasis was not greater than alone versus SRS with WBRT.\nPostoperative conventional daily radiotherapy improves survival for adults with good functional well-being and high grade glioma compared to no postoperative radiotherapy. Hypofractionated radiation therapy has similar efficacy for survival as compared to conventional radiotherapy, particularly for individuals aged 60 and older with glioblastoma.\n\nChemotherapy\nPatients undergoing chemotherapy are administered drugs designed to kill tumor cells. Although chemotherapy may improve overall survival in patients with the most malignant primary brain tumors, it does so in only about 20 percent of patients. Chemotherapy is often used in young children instead of radiation, as radiation may have negative effects on the developing brain. The decision to prescribe this treatment is based on a patient's overall health, type of tumor, and extent of cancer. The toxicity and many side effects of the drugs, and the uncertain outcome of chemotherapy in brain tumors puts this treatment further down the line of treatment options with surgery and radiation therapy preferred.\nUCLA Neuro-Oncology publishes real-time survival data for patients with a diagnosis of glioblastoma. They are the only institution in the United States that displays how brain tumor patients are performing on current therapies. They also show a listing of chemotherapy agents used to treat high-grade glioma tumors.\nGenetic mutations have significant effects on the effectiveness of chemotherapy. Gliomas with IDH1 or IDH2 mutations respond better to chemotherapy than those without the mutation. Loss of chromosome arms 1p and 19q also indicate better response to chemoradiation.\n\nOther\nA shunt may be used to relieve symptoms caused by intracranial pressure, by reducing the build-up of fluid (hydrocephalus) caused by the blockage of the free flow of cerebrospinal fluid.\n\nPrognosis\nThe prognosis of brain cancer depends on the type of cancer diagnosed. Medulloblastoma has a good prognosis with chemotherapy, radiotherapy, and surgical resection while glioblastoma has a median survival of only 15 months even with aggressive chemoradiotherapy and surgery. Brainstem gliomas have the poorest prognosis of any form of brain cancer, with most patients dying within one year, even with therapy that typically consists of radiation to the tumor along with corticosteroids. However, one type, focal brainstem gliomas in children, seems open to exceptional prognosis and long-term survival has frequently been reported.\nPrognosis is also affected by presentation of genetic mutations. Certain mutations provide better prognosis than others. IDH1 and IDH2 mutations in gliomas, as well as deletion of chromosome arms 1p and 19q, generally indicate better prognosis. TP53, ATRX, EGFR, PTEN, and TERT mutations are also useful in determining prognosis.\n\nGlioblastoma\nGlioblastoma is the most aggressive (grade 4) and most common form of a malignant primary brain tumor. Even when aggressive multimodality therapy consisting of radiotherapy, chemotherapy, and surgical excision is used, median survival is only 15\u201318 months. Standard therapy for glioblastoma consists of maximal surgical resection of the tumor, followed by radiotherapy between two and four weeks after the surgical procedure to remove the cancer, then by chemotherapy, such as temozolomide. Most patients with glioblastoma take a corticosteroid, typically dexamethasone, during their illness to relieve symptoms. Experimental treatments include targeted therapy, gamma knife radiosurgery, boron neutron capture therapy, gene therapy, and chemowafer implants.\n\nOligodendrogliomas\nOligodendrogliomas are incurable but slowly progressive malignant brain tumors. They can be treated with surgical resection, chemotherapy, radiotherapy or a combination. For some suspected low-grade (grade II) tumors, only a course of watchful waiting and symptomatic therapy is opted for. These tumors show co-deletions of the p and q arms of chromosome 1 and chromosome 19 respectively (1p19q co-deletion) and have been found to be especially chemosensitive with one report claiming them to be one of the most chemosensitive tumors. A median survival of up to 16.7 years has been reported for grade II oligodendrogliomas.\n\nAcoustic neuroma\nAcoustic neuromas are non-cancerous tumors. They can be treated with surgery, radiation therapy, or observation. Early intervention with surgery or radiation is recommended to prevent progressive hearing loss.\n\nEpidemiology\nFigures for incidences of cancers of the brain show a significant difference between more- and less-developed countries (the less-developed countries have lower incidences of tumors of the brain). This could be explained by undiagnosed tumor-related deaths (patients in extremely poor situations do not get diagnosed, simply because they do not have access to the modern diagnostic facilities required to diagnose a brain tumor) and by deaths caused by other poverty-related causes that preempt a patient's life before tumors develop or tumors become life-threatening. Nevertheless, statistics suggest that certain forms of primary brain tumors are more common among certain populations.\nThe incidence of low-grade astrocytoma has not been shown to vary significantly with nationality. However, studies examining the incidence of malignant central nervous system (CNS) tumors have shown some variation with national origin. Since some high-grade lesions arise from low-grade tumors, these trends are worth mentioning. Specifically, the incidence of CNS tumors in the United States, Israel, and the Nordic countries is relatively high, while Japan and Asian countries have a lower incidence. These differences probably reflect some biological differences as well as differences in pathologic diagnosis and reporting.\nWorldwide data on incidence of cancer can be found at the WHO (World Health Organization) and is handled by the IARC (International Agency for Research on Cancer) located in France.\n\nUnited States\nIn the United States in 2015, approximately 166,039 people were living with brain or other central nervous system tumors. Over 2018, it was projected that there would be 23,880 new cases of brain tumors and 16,830 deaths in 2018 as a result, accounting for 1.4 percent of all cancers and 2.8 percent of all cancer deaths. Median age of diagnosis was 58 years old, while median age of death was 65. Diagnosis was slightly more common in males, at approximately 7.5 cases per 100 000 people, while females saw 2 fewer at 5.4. Deaths as a result of brain cancer were 5.3 per 100 000 for males, and 3.6 per 100 000 for females, making brain cancer the 10th leading cause of cancer death in the United States. Overall lifetime risk of developing brain cancer is approximated at 0.6 percent for men and women.\n\nUK\nBrain, other CNS or intracranial tumors are the ninth most common cancer in the UK (around 10,600 people were diagnosed in 2013), and it is the eighth most common cause of cancer death (around 5,200 people died in 2012). White British patients with brain tumour are 30% more likely to die within a year of diagnosis than patients from other ethnicities.  The reason for this is unknown.\n\nChildren\nIn the United States more than 28,000 people under 20 are estimated to have a brain tumor. About 3,720 new cases of brain tumors are expected to be diagnosed in those under 15 in 2019. Higher rates were reported in 1985\u20131994 than in 1975\u20131983. There is some debate as to the reasons; one theory is that the trend is the result of improved diagnosis and reporting, since the jump occurred at the same time that MRIs became available widely, and there was no coincident jump in mortality. Central nervous system tumors make up 20\u201325 percent of cancers in children.\nThe average survival rate for all primary brain cancers in children is 74%. Brain cancers are the most common cancer in children under 19, are result in more death in this group than leukemia. Younger people do less well.\nThe most common brain tumor types in children (0\u201314) are: pilocytic astrocytoma, malignant glioma, medulloblastoma, neuronal and mixed neuronal-glial tumors, and ependymoma.\nIn children under 2, about 70% of brain tumors are medulloblastomas, ependymomas, and low-grade gliomas. Less commonly, and seen usually in infants, are teratomas and atypical teratoid rhabdoid tumors. Germ cell tumors, including teratomas, make up just 3% of pediatric primary brain tumors, but the worldwide incidence varies significantly.\nIn the UK, 429 children aged 14 and under are diagnosed with a brain tumour on average each year, and 563 children and young people under the age of 19 are diagnosed.\n\nResearch\nImmunotherapy\nCancer immunotherapy is being actively studied. For malignant gliomas no therapy has been shown to improve life expectancy as of 2015.\n\nVesicular stomatitis virus\nIn 2000, researchers used the vesicular stomatitis virus (VSV) to infect and kill cancer cells without affecting healthy cells.\n\nRetroviral replicating vectors\nLed by Prof. Nori Kasahara, researchers from USC, who are now at UCLA, reported in 2001 the first successful example of applying the use of retroviral replicating vectors towards transducing cell lines derived from solid tumors. Building on this initial work, the researchers applied the technology to in vivo models of cancer and in 2005 reported a long-term survival benefit in an experimental brain tumor animal model.  Subsequently, in preparation for human clinical trials, this technology was further developed by Tocagen (a pharmaceutical company primarily focused on brain cancer treatments) as a combination treatment (Toca 511 & Toca FC). This has been under investigation since 2010 in a Phase I\/II clinical trial for the potential treatment of recurrent high-grade glioma including glioblastoma and anaplastic astrocytoma. No results have yet been published.\n\nNon-invasive detection\nEfforts to detect and monitor development and treatment response of brain tumors by liquid biopsy from blood, cerebrospinal fluid or urine, are in the early stages of development.\n\nSee also\nBrain\nTumor\nNervous system neoplasm\nList of brain tumor cases\n\nReferences\nExternal links\n\nBrain and CNS cancers at Curlie\nBrain tumour information from Cancer Research UK\nNeuro-Oncology: Cancer Management Guidelines\nMedPix Teaching File MR Scans of Primary Brain Lymphoma, etc.","24":"The broken escalator phenomenon is the sensation of losing balance, confusion or dizziness reported by some people when stepping onto an escalator which is not working. It is said that there is a brief, odd sensation of imbalance, despite full awareness that the escalator is not going to move.\nIt has been shown that this effect causes people to step inappropriately fast onto a moving platform that is no longer moving, even when this is obvious to the participant.\nEven though subjects are fully aware that the platform or escalator is not moving now, parts of their brains still act on previous experience gained when it was moving, and so misjudge how to step onto it. Thus, this effect demonstrates the separateness of the declarative and procedural functions of the brain.\nThe broken escalator phenomenon is the result of a locomotor after-effect which replicates the posture adopted when walking onto a moving platform to stabilise oneself. This after-effect was studied by  Adolfo Bronstein and Raymond Reynolds in an experiment published in 2003, then explored further through a series of additional experiments by Bronstein and associates.\nThe phenomenon was originally discussed by Brian Simpson (1992) who named it the \"escalator effect\" and regarded it as the perceptual consequence of a failed expectation.  He thought it had something in common with the Duncker Effect. He also considered the related sensation experienced on alighting from a stationary escalator.\n\nInitial experiment\nBronstein and Reynolds' initial experiment attempted to reproduce the conditions of the broken escalator phenomenon by asking subjects to walk onto a stationary sled (BEFORE trials), then walk onto it while it was moving (MOVING trials), and finally once again while it was stationary (AFTER trials). Subjects all experienced an after-effect when walking onto the stationary sled in the AFTER trials, characterised by a forward sway of the torso, increased walking speed and increased muscular leg activity (measured with EMG) compared to the BEFORE condition. All subjects expressed surprise at their behaviour and compared the experience to that of walking onto an out-of-order escalator.\n\nMotor adaptation\nThe after-effect was found to be a direct consequence of motor adaptation. When facing an external threat to our balance, our central nervous system will trigger neural processes in order to stabilise our posture. In this case, when walking onto a moving platform \u2013 such as a moving escalator \u2013 people will adopt methods to prevent a backwards fall. In the MOVING trials of the experiment, these methods consist of a forward sway of the torso, increased gait velocity and increased leg EMG activity. Thus, when we step onto a moving escalator, we alter our posture and gait in order to stabilise ourselves against this external threat to our balance.\nThe after-effect consists of an inappropriate expression of this method. In the AFTER trials, instead of walking onto the stationary sled the same way they did for the BEFORE trials, subjects adopted a similar method to the MOVING trials. This resulted in them walking inappropriately fast and excessively swaying their torso when stepping onto the sled, leading to a sensation of dizziness. Similarly, when walking onto the non moving escalator, people adopt the same method that they use for a moving escalator, therefore almost failing to keep their balance.\n\nDissociation between the cognitive and motor systems\nThe broken escalator phenomenon is the result of the dissociation between the declarative and procedural functions of the central nervous system. The central nervous system enables us to adapt to the movement of the escalator, however this locomotor adaptation is inappropriately expressed when walking onto a broken escalator. Aware that the escalator will not move, we still modify our gait and posture as if to adapt to movement. This shows a separation between our declarative (or cognitive) system and our procedural (or motor) system \u2013 between what we know and what we do.\nIn the brain, declarative memory processes memories we are consciously aware of, whereas procedural memory processes our movements. The fact that we walk inappropriately fast onto an escalator we know to be broken is evidence of motor adaptation without declarative memory. The motor system operates without cognitive control, leading to the unconscious generation of the after-effect. Subjects in an additional experiment were indeed unable to suppress the after-effect even when consciously and voluntarily attempting to do so.\n\nPre-emptive postural response\nBronstein et al. remarked in 2013 that signs of the after-effect could be measured before foot-sled contact, suggesting that the after-effect is what they labelled a 'pre-emptive postural response'.\nThe after-effect is pre-emptive in that it anticipates a threat to balance rather than being triggered by one. Postural control is usually generated by an external threat, for example a slippery surface will lead to a more cautionary gait, but in this case the postural adaptation is an aversive, 'just in case' strategy. When stepping onto the broken escalator, the person will anticipate its movement, just to make sure they would not fall if the escalator were to move. The person undertakes a 'worst-case scenario' which seems the most strategic option: preparing for the platform to move even though it won't is better than the opposite.\nIndeed, as well as the distinction between the motor and cognitive systems, another factor leading to the after-effect has been argued to be fear-related mechanisms. The fear of falling if somehow the escalator moved could be enough to adopt a precautionary behaviour despite knowledge that it is broken. Fear-related mechanisms are indeed known to be impervious to cognitive control.\nHowever, another experiment demonstrated that the aftereffect was not as intense when subjects walked onto the stationary sled with the opposite leg of the one they'd stepped with in the MOVING trials. If the after-effect was generated by a fear that the platform would move, it would be triggered whenever and however the subject stepped onto it. This therefore suggests that the after-effect is mainly generated by procedural memory, which is most intense when the conditions of the adaptation phase are perfectly replicated.\n\n'Braking' the after-effect\nThe after-effect is an internally generated postural threat which must be dealt with. Without a 'braking' system to reduce its impact on our balance, we would fall when stepping onto a stationary escalator.\nExternal threats to our balance are dealt with by the vestibular system. However, a 2008 experiment showed that the intensity of the after-effect in subjects lacking a vestibular function (labyrinthine defective subjects) was not superior to normal subjects, suggesting that the vestibular system is not responsible for 'braking' the aftereffect. Instead, in the first AFTER trial (when the after-effect is the strongest) an increase in leg EMG activity is observed for all subjects. This increase is generated unconsciously and before foot-sled contact. The 'braking' of the after-effect is therefore an anticipatory process, rather than being triggered by an external threat: the central nervous system anticipates that the after-effect will occur and that it will threaten our balance, and generates mechanisms to deal with the threat. When stepping onto a stationary escalator, anticipatory motor mechanisms prevent us from falling by attenuating the after-effect.\n\nSee also\nBalance disorder \u2013 Physiological disturbance of perception\nIdeomotor phenomenon \u2013 Concept in hypnosis and psychological research\nIllusions of self-motion \u2013 Misperception of one's location or movement\nMotion sickness \u2013 Nausea caused by motion or perceived motion\nProprioception \u2013 Sense of self-movement, force, and body position\nSense of balance, also known as Equilibrioception \u2013 Physiological sense regarding posture\nSpatial disorientation \u2013 Inability of a person to correctly determine their body position in space\nVertigo \u2013 Type of dizziness where a person has the sensation of moving or surrounding objects moving\n\n\n== References ==","25":"A brain tumor occurs when abnormal cells form within the brain. There are two main types of tumors: malignant (cancerous) tumors and benign (non-cancerous) tumors. These can be further classified as primary tumors, which start within the brain, and secondary tumors, which most commonly have spread from tumors located outside the brain, known as brain metastasis tumors. All types of brain tumors may produce symptoms that vary depending on the size of the tumor and the part of the brain that is involved. Where symptoms exist, they may include headaches, seizures, problems with vision, vomiting and mental changes. Other symptoms may include difficulty walking, speaking, with sensations, or unconsciousness.\nThe cause of most brain tumors is unknown, though up to 4% of brain cancers may be caused by CT scan radiation. Uncommon risk factors include exposure to vinyl chloride, Epstein\u2013Barr virus, ionizing radiation, and inherited syndromes such as neurofibromatosis, tuberous sclerosis, and von Hippel-Lindau Disease. Studies on mobile phone exposure have not shown a clear risk. The most common types of primary tumors in adults are meningiomas (usually benign) and astrocytomas such as glioblastomas. In children, the most common type is a malignant medulloblastoma. Diagnosis is usually by medical examination along with computed tomography (CT) or magnetic resonance imaging (MRI). The result is then often confirmed by a biopsy. Based on the findings, the tumors are divided into different grades of severity.\nTreatment may include some combination of surgery, radiation therapy and chemotherapy. If seizures occur, anticonvulsant medication may be needed. Dexamethasone and furosemide are medications that may be used to decrease swelling around the tumor. Some tumors grow gradually, requiring only monitoring and possibly needing no further intervention. Treatments that use a person's immune system are being studied. Outcomes for malignant tumors vary considerably depending on the type of tumor and how far it has spread at diagnosis. Although benign tumors only grow in one area, they may still be life-threatening depending on their size and location. Malignant glioblastomas usually have very poor outcomes, while benign meningiomas usually have good outcomes. The average five-year survival rate for all (malignant) brain cancers in the United States is 33%.\nSecondary, or metastatic, brain tumors are about four times as common as primary brain tumors, with about half of metastases coming from lung cancer. Primary brain tumors occur in around 250,000 people a year globally, and make up less than 2% of cancers. In children younger than 15, brain tumors are second only to acute lymphoblastic leukemia as the most common form of cancer. In NSW Australia in 2005, the average lifetime economic cost of a case of brain cancer was AU$1.9 million, the greatest of any type of cancer.\n\nSigns and symptoms\nThe signs and symptoms of brain tumors are broad. People may experience symptoms regardless of whether the tumor is benign (not cancerous) or cancerous. Primary and secondary brain tumors present with similar symptoms, depending on the location, size, and rate of growth of the tumor. For example, larger tumors in the frontal lobe can cause changes in the ability to think. However, a smaller tumor in an area such as Wernicke's area (small area responsible for language comprehension) can result in a greater loss of function.\n\nHeadaches\nHeadaches as a result of raised intracranial pressure can be an early symptom of brain cancer. However, isolated headache without other symptoms is rare, and other symptoms including visual abnormalities may occur before headaches become common. Certain warning signs for headache exist which make the headache more likely to be associated with brain cancer. These are defined as \"abnormal neurological examination, headache worsened by Valsalva maneuver, headache causing awakening from sleep, new headache in the older population, progressively worsening headache, atypical headache features, or patients who do not fulfill the strict definition of migraine\". Other associated signs are headaches that are worse in the morning or that subside after vomiting.\n\nLocation-specific symptoms\nThe brain is divided into lobes and each lobe or area has its own function. A tumor in any of these lobes may affect the area's performance. The symptoms experienced are often linked to the location of the tumor, but each person may experience something different.\n\nFrontal lobe: Tumors may contribute to poor reasoning, inappropriate social behavior, personality changes, poor planning, lower inhibition, and decreased production of speech (Broca's area).\nTemporal lobe: Tumors in this lobe may contribute to poor memory, loss of hearing, and difficulty in language comprehension (Wernicke's area is located in this lobe).\nParietal lobe: Tumors here may result in poor interpretation of languages, difficulty with speaking, writing, drawing, naming, and recognizing, and poor spatial and visual perception.\nOccipital lobe: Damage to this lobe may result in poor vision or loss of vision.\nCerebellum: Tumors in this area may cause poor balance, muscle movement, and posture.\nBrain stem: Tumors on the brainstem can cause seizures, endocrine problems, respiratory changes, visual changes, headaches and partial paralysis.\n\nBehavior changes\nA person's personality may be altered due to the tumor-damaging lobes of the brain. Since the frontal, temporal, and parietal lobes control inhibition, emotions, mood, judgement, reasoning, and behavior, a tumor in those regions can cause inappropriate social behavior, temper tantrums, laughing at things which merit no laughter, and even psychological symptoms such as depression and anxiety. More research is needed into the effectiveness and safety of medication for depression in people with brain tumors.\nPersonality changes can have damaging effects such as unemployment, unstable relationships, and a lack of control.\n\nCause\nThe best-known cause of brain cancers is ionizing radiation. Approximately 4% of brain cancers in the general population are caused by CT-scan radiation. For brain cancers that follow a CT scan at lags of 2 years or more, it has been estimated that 40% are attributable to CT-scan radiation. The relationship between ionizing radiation and brain cancers can be best explained by radiation carcinogenesis, and by traditional models of oncogenesis. The stochastic effects of ionizing radiation demonstrate a dose-response relationship to the probability of occurrence, but no dose-response relationship to severity of disease. The majority of radiation-induced brain cancers are caused by ionizing radiation from medical sources such as CT scans.\nMutations and deletions of tumor suppressor genes, such as P53, are thought to be the cause of some forms of brain tumor. Inherited conditions, such as Von Hippel\u2013Lindau disease, tuberous sclerosis, multiple endocrine neoplasia, and neurofibromatosis type 2 carry a high risk for the development of brain tumors. People with celiac disease have a slightly increased risk of developing brain tumors. Smoking may increase the risk, but evidence of this remains unclear.\nAlthough studies have not shown any link between cell-phone or mobile-phone radiation and the occurrence of brain tumors, the World Health Organization has classified mobile-phone radiation on the IARC scale into Group 2B \u2013 possibly carcinogenic.\nThe claim that cell-phone usage may cause brain cancer is likely based on epidemiological studies which observed a slight increase in glioma risk among heavy users of wireless phones. When those studies were conducted, GSM (2G) phones were in use. Modern, third-generation (3G) phones emit, on average, about 1% of the energy emitted by those GSM (2G) phones, and therefore the finding of an association between cell-phone usage and increased risk of brain cancer is not based upon current phone usage.\n\nPathophysiology\nMeninges\nHuman brains are surrounded by a system of connective tissue membranes called meninges that separate the brain from the skull. This three-layered covering is composed of (from the outside in) the dura mater, arachnoid mater, and pia mater. The arachnoid and pia are physically connected and thus often considered as a single layer, the leptomeninges. Between the arachnoid mater and the pia mater is the subarachnoid space which contains cerebrospinal fluid (CSF). This fluid circulates in the narrow spaces between cells and through the cavities in the brain called ventricles, to support and protect the brain tissue. Blood vessels enter the central nervous system through the perivascular space above the pia mater. The cells in the blood vessel walls are joined tightly, forming the blood\u2013brain barrier which protects the brain from toxins that might enter through the blood.\nTumors of the meninges are meningiomas and are often benign. Though not technically a tumor of brain tissue, they are often considered brain tumors since they protrude into the space where the brain is, causing symptoms. Since they are usually slow-growing tumors, meningiomas can be quite large by the time symptoms appear.\n\nBrain matter\nThe brains of humans and other vertebrates are composed of very soft tissue and have a gelatin-like texture. Living brain tissue has a pink tint in color on the outside (gray matter), and nearly complete white on the inside (white matter), with subtle variations in color. The three largest divisions of the brain are:\n\nCerebral cortex\nBrainstem\nCerebellum\nThese areas are composed of two broad classes of cells: neurons and glia. These two types are equally numerous in the brain as a whole, although glial cells outnumber neurons roughly 4 to 1 in the cerebral cortex. Glia come in several types, which perform a number of critical functions, including structural support, metabolic support, insulation, and guidance of development. Primary tumors of the glial cells are called gliomas and often are malignant by the time they are diagnosed.\nThe thalamus and hypothalamus are major divisions of the diencephalon, with the pituitary gland and pineal gland attached at the bottom; tumors of the pituitary and pineal gland are often benign.\nThe brainstem lies between the large cerebral cortex and the spinal cord. It is divided into the midbrain, pons, and medulla oblongata.\n\nSpinal cord\nThe spinal cord is considered a part of the central nervous system. It is made up of the same cells as the brain: neurons and glial cells.\n\nDiagnosis\nAlthough there is no specific or singular symptom or sign, the presence of a combination of symptoms and the lack of corresponding indications of other causes can be an indicator for investigation towards the possibility of a brain tumor. Brain tumors have similar characteristics and obstacles when it comes to diagnosis and therapy with tumors located elsewhere in the body. However, they create specific issues that follow closely to the properties of the organ they are in.\nThe diagnosis will often start by taking a medical history noting medical antecedents, and current symptoms. Clinical and laboratory investigations will serve to exclude infections as the cause of the symptoms. Examinations in this stage may include the eyes, otolaryngological (or ENT) and electrophysiological exams. The use of electroencephalography (EEG) often plays a role in the diagnosis of brain tumors.\nBrain tumors, when compared to tumors in other areas of the body, pose a challenge for diagnosis. Commonly, radioactive tracers are uptaken in large volumes in tumors due to the high activity of tumor cells, allowing for radioactive imaging of the tumor. However, most of the brain is separated from the blood by the blood\u2013brain barrier (BBB), a membrane that exerts a strict control over what substances are allowed to pass into the brain. Therefore, many tracers that may reach tumors in other areas of the body easily would be unable to reach brain tumors until there was a disruption of the BBB by the tumor. Disruption of the BBB is well imaged via MRI or CT scan, and is therefore regarded as the main diagnostic indicator for malignant gliomas, meningiomas, and brain metastases.\nSwelling or obstruction of the passage of cerebrospinal fluid (CSF) from the brain may cause (early) signs of increased intracranial pressure which translates clinically into headaches, vomiting, or an altered state of consciousness, and in children changes to the diameter of the skull and bulging of the fontanelles. More complex symptoms such as endocrine dysfunctions should alarm doctors not to exclude brain tumors.\nA bilateral temporal visual field defect (due to compression of the optic chiasm) or dilation of the pupil, and the occurrence of either slowly evolving or the sudden onset of focal neurologic symptoms, such as cognitive and behavioral impairment (including impaired judgment, memory loss, lack of recognition, spatial orientation disorders), personality or emotional changes, hemiparesis, hypoesthesia, aphasia, ataxia, visual field impairment, impaired sense of smell, impaired hearing, facial paralysis, double vision, or more severe symptoms such as tremors, paralysis on one side of the body hemiplegia, or (epileptic) seizures in a patient with a negative history for epilepsy, should raise the possibility of a brain tumor.\n\nImaging\nMedical imaging plays a central role in the diagnosis of brain tumors. Early imaging methods \u2013 invasive and sometimes dangerous \u2013 such as pneumoencephalography and cerebral angiography have been abandoned in favor of non-invasive, high-resolution techniques, especially magnetic resonance imaging (MRI) and computed tomography (CT) scans, though MRI is typically the reference standard used. Neoplasms will often show as differently colored masses (also referred to as processes) in CT or MRI results.\n\nBenign brain tumors often show up as hypodense (darker than brain tissue) mass lesions on CT scans. On MRI, they appear either hypodense or isointense (same intensity as brain tissue) on T1-weighted scans, or hyperintense (brighter than brain tissue) on T2-weighted MRI, although the appearance is variable.\nContrast agent uptake, sometimes in characteristic patterns, can be demonstrated on either CT or MRI scans in most malignant primary and metastatic brain tumors.\nPressure areas where the brain tissue has been compressed by a tumor also appear hyperintense on T2-weighted scans and might indicate the presence of a diffuse neoplasm due to an unclear outline. Swelling around the tumor known as peritumoral edema can also show a similar result. This is because these tumors disrupt the normal functioning of the BBB and lead to an increase in its permeability.\nMore recently, advancements have been made to increase the utility of MRI in providing physiological data that can help to inform diagnosis and prognosis. MRI itself is sufficient in identifying the brain tumor's location and morphology, but other types of MRI may be used on top of that, such as MRA, MRS, pMRI, fMRI, and DWI. These imaging techniques help doctors and surgeons to diagnose the type of tumor, plan for surgery, and to assess treatment and radiation\/chemotherapy. Treatment with radiation and chemotherapy can lead to treatment induced changes in the brain, visible on conventional imaging and which can be difficult to differentiate from tumor recurrence.\n\nDifferent Types of MRI Scans\nMagnetic Resonance Angiography (MRA) \u2013  looks at the blood vessels in the brain. In the diagnosis of brain tumor, MRAs are typically carried out before surgery to help surgeons get a better understanding of the tumor vasculature. For example, a study was done where surgeons were able to separate benign brain tumors from malignant ones by analyzing the shapes of the blood vessels that were extracted from MRA. Although not required, some MRA may inject contrast agent, gadolinium, into the patient to get an enhanced image\nMagnetic Resonance Spectroscopy (MRS) \u2013  measures the metabolic changes or chemical changes inside the tumor. The most common MRS is proton spectroscopy with its frequency measured in parts per million (ppm). Gliomas or malignant brain tumors have different spectra from normal brain tissue in that they have greater choline levels and lower N-acetyl aspartate (NAA) signals. Using MRS in brain tumor diagnosis can help doctors identify the type of tumor and its aggressiveness. For example, benign brain tumors or meningioma have increased alanine levels. It can also help to distinguish brain tumors from scar tissues or dead tissues caused by previous radiation treatment, which does not have increased choline levels that brain tumors have, and from tumor-mimicking lesions such as abscesses or infarcts.\nPerfusion Magnetic Resonance Imaging (pMRI) \u2013  assess the blood volume and blood flow of different parts of the brain and brain tumors. pMRI requires the injection of contrast agent, usually gadopentetate dimeglumine (Gd-DTPA) into the veins in order to enhance the contrast. pMRI provides a cerebral blood volume map that shows the tumor vascularity and angiogenesis. Brain tumors would require a larger blood supply and thus, would show a high cerebral blood volume on the pMRI map. The vascular morphology and degree of angiogenesis from pMRI help to determine the grade and malignancy of brain tumors. For brain tumor diagnosis, pMRI is useful in determining the best site to perform biopsy and to help reduce sampling error. pMRI is also valuable for after treatment to determine if the abnormal area is a remaining tumor or a scar tissue. For patients that are undergoing anti-angiogenesis cancer therapy, pMRI can give the doctors a better sense of efficacy of the treatment by monitoring tumor cerebral blood volume.\nFunctional MRI (fMRI) \u2013  measures blood flow changes in active parts of the brain while the patient is performing tasks and provides specific locations of the brain that are responsible for certain functions. Before performing a brain tumor surgery on patients, neurosurgeons would use fMRI to avoid damage to structures of the brain that correspond with important brain functions while resecting the tumor at the same time. Preoperative fMRI is important because it is often difficult to distinguish the anatomy near the tumor as it distorts its surrounding regions. Neurosurgeons would use fMRI to plan whether to perform a resection where tumor is surgically removed as much as possible, a biopsy where they take a surgical sampling amount to provide a diagnosis, or to not undergo surgery at all. For example, a neurosurgeon may be opposed to resecting a tumor near the motor cortex as that would affect the patient's movements. Without preoperative fMRI, the neurosurgeon would have to perform an awake-craniotomy where the patient would have to interact during open surgery to see if tumor removal would affect important brain functions.\nDiffusion Weighted Imaging (DWI) \u2013  a form of MRI that measures random Brownian motion of water molecules along a magnetic field gradient. For brain tumor diagnosis, measurement of apparent diffusion coefficient (ADC) in brain tumors allow doctors to categorize tumor type. Most brain tumors have higher ADC than normal brain tissues and doctors can match the observed ADC of the patient's brain tumor with a list of accepted ADC to identify tumor type. DWI is also useful for treatment and therapy purposes where changes in diffusion can be analyzed in response to drug, radiation, or gene therapy. Successful response results in apoptosis and increase in diffusion while failed treatment results in unchanged diffusion values.\n\nOther Types of Imaging Techniques\nComputed Tomography (CT) Scan \u2013  uses x-rays to take pictures from different angles and computer processing to combine the pictures into a 3D image. A CT scan usually serves as an alternative to MRI in cases where the patient cannot have an MRI due to claustrophobia or pacemaker. Compared to MRI, a CT scan shows a more detailed image of the bone structures near the tumor and can be used to measure the tumor's size. Like an MRI, a contrast dye may also be injected into the veins or ingested by mouth before a CT scan to better outline any tumors that may be present. CT scans use contrast materials that are iodine-based and barium sulfate compounds. The downside of using CT scans as opposed to MRI is that some brain tumors do not show up well on CT scans because some intra-axial masses are faint and resemble normal brain tissue. In some scenarios, brain tumors in CT scans may be mistaken for infarction, infection, and demyelination. To suspect that an intra-axial mass is a brain tumor instead of other possibilities, there must be unexplained calcifications in the brain, preservation of the cortex, and disproportionate mass effect.\nCT Angiography (CTA) \u2013  provides information about the blood vessels in the brain using X-rays. A contrast agent is always required to be injected into the patient in the CT scanner. CTA serves as an alternative to MRA.\nPositron Emission Tomography (PET) Scan \u2013  uses radioactive substances, with the most common one being a sugar known as FDG, while more specific tracers for glioma are emerging. This injected substance is taken up by cells that are actively dividing. Tumor cells are more active in dividing so they would absorb more of the radioactive substance. After injection, a scanner would be used to create an image of the radioactive areas in the brain. PET scans are used more often for high-grade tumors than for low-grade tumors. It is useful after treatment to help doctors determine if the abnormal area on an MRI image is a remaining tumor or a scar tissue. Scar tissues will not show up on PET scans while tumors would.\nHowever, these techniques cannot alone diagnose high- versus low-grade gliomas, and thus the definitive diagnosis of brain tumor should only be confirmed by histological examination of tumor tissue samples obtained either by means of brain biopsy or open surgery. The histological examination is essential for determining the appropriate treatment and the correct prognosis. This examination, performed by a pathologist, typically has three stages: interoperative examination of fresh tissue, preliminary microscopic examination of prepared tissues, and follow-up examination of prepared tissues after immunohistochemical staining or genetic analysis.\n\nPathology\nTumors have characteristics that allow the determination of malignancy and how they will evolve, and determining these characteristics will allow the medical team to determine the management plan.\nAnaplasia or dedifferentiation: loss of differentiation of cells and of their orientation to one another and blood vessels, a characteristic of anaplastic tumor tissue. Anaplastic cells have lost total control of their normal functions and many have deteriorated cell structures. Anaplastic cells often have abnormally high nuclear-to-cytoplasmic ratios, and many are multinucleated. Additionally, the nucleus of anaplastic cells is usually unnaturally shaped or oversized. Cells can become anaplastic in two ways: neoplastic tumor cells can dedifferentiate to become anaplasias (the dedifferentiation causes the cells to lose all of their normal structure\/function), or cancer stem cells can increase their capacity to multiply (i.e., uncontrollable growth due to failure of differentiation).\nAtypia: an indication of abnormality of a cell (which may be indicative of malignancy). Significance of the abnormality is highly dependent on context.\nNeoplasia: the (uncontrolled) division of cells. As such, neoplasia is not problematic but its consequences are: the uncontrolled division of cells means that the mass of a neoplasm increases in size, and in a confined space such as the intracranial cavity this quickly becomes problematic because the mass invades the space of the brain pushing it aside, leading to compression of the brain tissue and increased intracranial pressure and destruction of brain parenchyma. Increased intracranial pressure (ICP) may be attributable to the direct mass effect of the tumor, increased blood volume, or increased cerebrospinal fluid (CSF) volume, which may, in turn, have secondary symptoms.\nNecrosis: the (premature) death of cells, caused by external factors such as infection, toxin or trauma. Necrotic cells send the wrong chemical signals which prevent phagocytes from disposing of the dead cells, leading to a buildup of dead tissue, cell debris and toxins at or near the site of the necrotic cells\nArterial and venous hypoxia, or the deprivation of adequate oxygen supply to certain areas of the brain, occurs when a tumor makes use of nearby blood vessels for its supply of blood and the neoplasm enters into competition for nutrients with the surrounding brain tissue. More generally a neoplasm may cause release of metabolic end products (e.g., free radicals, altered electrolytes, neurotransmitters), and release and recruitment of cellular mediators (e.g., cytokines) that disrupt normal parenchymal function.\n\nClassification\nTumors can be benign or malignant, can occur in different parts of the brain, and may be classified as primary or secondary. A primary tumor is one that has started in the brain, as opposed to a metastatic tumor, which is one that has spread to the brain from another area of the body. The incidence of metastatic tumors is approximately four times greater than primary tumors. Tumors may or may not be symptomatic: some tumors are discovered because the patient has symptoms, others show up incidentally on an imaging scan, or at an autopsy.\nGrading of the tumors of the central nervous system commonly occurs on a 4-point scale (I-IV) created by the World Health Organization in 1993. Grade I tumors are the least severe and commonly associated with long-term survival, with severity and prognosis worsening as the grade increases. Low-grade tumors are often benign, while higher grades are aggressively malignant and\/or metastatic. Other grading scales do exist, many based upon the same criteria as the WHO scale and graded from I-IV.\n\nPrimary\nThe most common primary brain tumors are:\n\nGliomas (50.4%)\nMeningiomas (20.8%)\nPituitary adenomas (15%)\nNerve sheath tumors (10%)\nThese common tumors can also be organized according to tissue of origin as shown below:\n\nSecondary\nSecondary tumors of the brain are metastatic and have invaded the brain from cancers originating in other organs. This means that a cancerous neoplasm has developed in another organ elsewhere in the body and that cancer cells have leaked from that primary tumor and then entered the lymphatic system and blood vessels.  They then circulate through the bloodstream, and are deposited in the brain. There, these cells continue growing and dividing, becoming another invasive neoplasm of primary cancer's tissue. Secondary tumors of the brain are very common in the terminal phases of patients with an incurable metastasized cancer; the most common types of cancers that bring about secondary tumors of the brain are lung cancer, breast cancer, malignant melanoma, kidney cancer, and colon cancer (in decreasing order of frequency).\nSecondary brain tumors are more common than primary ones; in the United States, there are about 170,000 new cases every year. Secondary brain tumors are the most common cause of tumors in the intracranial cavity. The skull bone structure can also be subject to a neoplasm that by its very nature reduces the volume of the intracranial cavity, and can damage the brain.\n\nBy behavior\nBrain tumors or intracranial neoplasms can be cancerous (malignant) or non-cancerous (benign). However, the definitions of malignant or benign neoplasms differ from those commonly used in other types of cancerous or non-cancerous neoplasms in the body.\nIn cancers elsewhere in the body, three malignant properties differentiate benign tumors from malignant forms of cancer: benign tumors are self-limited and do not invade or metastasize. Characteristics of malignant tumors include:\n\nuncontrolled mitosis (growth by division beyond the normal limits)\nanaplasia: the cells in the neoplasm have an obviously different form (in size and shape). Anaplastic cells display marked pleomorphism. The cell nuclei are characteristically extremely hyperchromatic (darkly stained) and enlarged; the nucleus might have the same size as the cytoplasm of the cell (nuclear-cytoplasmic ratio may approach 1:1, instead of the normal 1:4 or 1:6 ratio). Giant cells \u2013 considerably larger than their neighbors \u2013 may form and possess either one enormous nucleus or several nuclei (syncytia). Anaplastic nuclei are variable and bizarre in size and shape.\ninvasion or infiltration (medical literature uses these terms as synonymous equivalents. However, for clarity, the articles that follow adhere to a convention that they mean slightly different things; this convention is not followed outside these articles):\nInvasion or invasiveness is the spatial expansion of the tumor through uncontrolled mitosis, in the sense that the neoplasm invades the space occupied by adjacent tissue, thereby pushing the other tissue aside and eventually compressing the tissue. Often these tumors are associated with clearly outlined tumors in imaging.\nInfiltration is the behavior of the tumor either to grow (microscopic) tentacles that push into the surrounding tissue (often making the outline of the tumor undefined or diffuse) or to have tumor cells \"seeded\" into the tissue beyond the circumference of the tumorous mass; this does not mean that an infiltrative tumor does not take up space or does not compress the surrounding tissue as it grows, but an infiltrating neoplasm makes it difficult to say where the tumor ends and the healthy tissue starts.\nmetastasis (spread to other locations in the body via lymph or blood).\nOf the above malignant characteristics, some elements do not apply to primary neoplasms of the brain:\n\nPrimary brain tumors rarely metastasize to other organs; some forms of primary brain tumors can metastasize but will not spread outside the intracranial cavity or the central spinal canal. Due to the BBB, cancerous cells of a primary neoplasm cannot enter the bloodstream and get carried to another location in the body. (Occasional isolated case reports suggest spread of certain brain tumors outside the central nervous system, e.g. bone metastasis of glioblastoma.)\nPrimary brain tumors generally are invasive (i.e. they will expand spatially and intrude into the space occupied by other brain tissue and compress those brain tissues); however, some of the more malignant primary brain tumors will infiltrate the surrounding tissue.\n\nBy genetics\nIn 2016, the WHO restructured their classifications of some categories of gliomas to include distinct genetic mutations that have been useful in differentiating tumor types, prognoses, and treatment responses. Genetic mutations are typically detected via immunohistochemistry, a technique that visualizes the presence or absence of a targeted protein via staining.\n\nMutations in IDH1 and IDH2 genes are commonly found in low-grade gliomas\nLoss of both IDH genes combined with loss of chromosome arms 1p and 19q indicates the tumor is an oligodendroglioma\nLoss of TP53 and ATRX characterizes astrocytomas\nGenes EGFR, TERT, and PTEN, are commonly altered in gliomas and are useful in differentiating tumor grade and biology\n\nSpecific types\nAnaplastic astrocytoma, Anaplastic oligodendroglioma, Astrocytoma, Central neurocytoma, Choroid plexus carcinoma, Choroid plexus papilloma, Choroid plexus tumor, Colloid cyst, Dysembryoplastic neuroepithelial tumour, Ependymal tumor, Fibrillary astrocytoma, Giant-cell glioblastoma, Glioblastoma, Gliomatosis cerebri, Gliosarcoma, Hemangiopericytoma, Medulloblastoma, Medulloepithelioma, Meningeal carcinomatosis, Neuroblastoma, Neurocytoma, Oligoastrocytoma, Oligodendroglioma, Optic nerve sheath meningioma, Pediatric ependymoma, Pilocytic astrocytoma, Pinealoblastoma, Pineocytoma, Pleomorphic anaplastic neuroblastoma, Pleomorphic xanthoastrocytoma, Primary central nervous system lymphoma, Sphenoid wing meningioma, Subependymal giant cell astrocytoma, Subependymoma, Trilateral retinoblastoma.\n\nTreatment\nA medical team generally assesses the treatment options and presents them to the person affected and their family. Various types of treatment are available depending on tumor type and location, and may be combined to produce the best chances of survival:\n\nSurgery: complete or partial resection of the tumor with the objective of removing as many tumor cells as possible.\nRadiotherapy: the most commonly used treatment for brain tumors; the tumor is irradiated with beta, x rays or gamma rays.\nChemotherapy: a treatment option for cancer, however, it is not always used to treat brain tumors as the blood\u2013brain barrier can prevent some drugs from reaching the cancerous cells.\nA variety of experimental therapies are available through clinical trials.\nSurvival rates in primary brain tumors depend on the type of tumor, age, functional status of the patient, the extent of surgical removal and other factors specific to each case.\nStandard care for anaplastic oligodendrogliomas and anaplastic oligoastrocytomas is surgery followed by radiotherapy. One study found a survival benefit for the addition of chemotherapy to radiotherapy after surgery, compared with radiotherapy alone.\n\nSurgery\nThe primary and most desired course of action described in medical literature is surgical removal (resection) via craniotomy. Minimally invasive techniques are becoming the dominant trend in neurosurgical oncology. The main objective of surgery is to remove as many tumor cells as possible, with complete removal being the best outcome and cytoreduction (\"debulking\") of the tumor otherwise. A Gross Total Resection (GTR) occurs when all visible signs of the tumor are removed, and subsequent scans show no apparent tumor. In some cases access to the tumor is impossible and impedes or prohibits surgery.\nMany meningiomas, with the exception of some tumors located at the skull base, can be successfully removed surgically.\nMost pituitary adenomas can be removed surgically, often using a minimally invasive approach through the nasal cavity and skull base (trans-nasal, trans-sphenoidal approach). Large pituitary adenomas require a craniotomy (opening of the skull) for their removal. Radiotherapy, including stereotactic approaches, is reserved for inoperable cases.\nSeveral current research studies aim to improve the surgical removal of brain tumors by labeling tumor cells with 5-aminolevulinic acid that causes them to fluoresce. Postoperative radiotherapy and chemotherapy are integral parts of the therapeutic standard for malignant tumors.\nMultiple metastatic tumors are generally treated with radiotherapy and chemotherapy rather than surgery and the prognosis in such cases is determined by the primary tumor, and is generally poor.\n\nRadiation therapy\nThe goal of radiation therapy is to kill tumor cells while leaving normal brain tissue unharmed. In standard external beam radiation therapy, multiple treatments of standard-dose \"fractions\" of radiation are applied to the brain. This process is repeated for a total of 10 to 30 treatments, depending on the type of tumor. This additional treatment provides some patients with improved outcomes and longer survival rates.\nRadiosurgery is a treatment method that uses computerized calculations to focus radiation at the site of the tumor while minimizing the radiation dose to the surrounding brain. Radiosurgery may be an adjunct to other treatments, or it may represent the primary treatment technique for some tumors. Forms used include stereotactic radiosurgery, such as Gamma knife, Cyberknife or Novalis Tx radiosurgery.\nRadiotherapy is the most common treatment for secondary brain tumors. The amount of radiotherapy depends on the size of the area of the brain affected by cancer. Conventional external beam \"whole-brain radiotherapy treatment\" (WBRT) or \"whole-brain irradiation\" may be suggested if there is a risk that other secondary tumors will develop in the future. Stereotactic radiotherapy is usually recommended in cases involving fewer than three small secondary brain tumors. Radiotherapy may be used following, or in some cases in place of, resection of the tumor. Forms of radiotherapy used for brain cancer include external beam radiation therapy, the most common, and brachytherapy and proton therapy, the last especially used for children.\nPeople who receive stereotactic radiosurgery (SRS) and whole-brain radiation therapy (WBRT) for the treatment of metastatic brain tumors have more than twice the risk of developing learning and memory problems than those treated with SRS alone. Results of a 2021 systematic review found that when using SRS as the initial treatment, survival or death related to brain metastasis was not greater than alone versus SRS with WBRT.\nPostoperative conventional daily radiotherapy improves survival for adults with good functional well-being and high grade glioma compared to no postoperative radiotherapy. Hypofractionated radiation therapy has similar efficacy for survival as compared to conventional radiotherapy, particularly for individuals aged 60 and older with glioblastoma.\n\nChemotherapy\nPatients undergoing chemotherapy are administered drugs designed to kill tumor cells. Although chemotherapy may improve overall survival in patients with the most malignant primary brain tumors, it does so in only about 20 percent of patients. Chemotherapy is often used in young children instead of radiation, as radiation may have negative effects on the developing brain. The decision to prescribe this treatment is based on a patient's overall health, type of tumor, and extent of cancer. The toxicity and many side effects of the drugs, and the uncertain outcome of chemotherapy in brain tumors puts this treatment further down the line of treatment options with surgery and radiation therapy preferred.\nUCLA Neuro-Oncology publishes real-time survival data for patients with a diagnosis of glioblastoma. They are the only institution in the United States that displays how brain tumor patients are performing on current therapies. They also show a listing of chemotherapy agents used to treat high-grade glioma tumors.\nGenetic mutations have significant effects on the effectiveness of chemotherapy. Gliomas with IDH1 or IDH2 mutations respond better to chemotherapy than those without the mutation. Loss of chromosome arms 1p and 19q also indicate better response to chemoradiation.\n\nOther\nA shunt may be used to relieve symptoms caused by intracranial pressure, by reducing the build-up of fluid (hydrocephalus) caused by the blockage of the free flow of cerebrospinal fluid.\n\nPrognosis\nThe prognosis of brain cancer depends on the type of cancer diagnosed. Medulloblastoma has a good prognosis with chemotherapy, radiotherapy, and surgical resection while glioblastoma has a median survival of only 15 months even with aggressive chemoradiotherapy and surgery. Brainstem gliomas have the poorest prognosis of any form of brain cancer, with most patients dying within one year, even with therapy that typically consists of radiation to the tumor along with corticosteroids. However, one type, focal brainstem gliomas in children, seems open to exceptional prognosis and long-term survival has frequently been reported.\nPrognosis is also affected by presentation of genetic mutations. Certain mutations provide better prognosis than others. IDH1 and IDH2 mutations in gliomas, as well as deletion of chromosome arms 1p and 19q, generally indicate better prognosis. TP53, ATRX, EGFR, PTEN, and TERT mutations are also useful in determining prognosis.\n\nGlioblastoma\nGlioblastoma is the most aggressive (grade 4) and most common form of a malignant primary brain tumor. Even when aggressive multimodality therapy consisting of radiotherapy, chemotherapy, and surgical excision is used, median survival is only 15\u201318 months. Standard therapy for glioblastoma consists of maximal surgical resection of the tumor, followed by radiotherapy between two and four weeks after the surgical procedure to remove the cancer, then by chemotherapy, such as temozolomide. Most patients with glioblastoma take a corticosteroid, typically dexamethasone, during their illness to relieve symptoms. Experimental treatments include targeted therapy, gamma knife radiosurgery, boron neutron capture therapy, gene therapy, and chemowafer implants.\n\nOligodendrogliomas\nOligodendrogliomas are incurable but slowly progressive malignant brain tumors. They can be treated with surgical resection, chemotherapy, radiotherapy or a combination. For some suspected low-grade (grade II) tumors, only a course of watchful waiting and symptomatic therapy is opted for. These tumors show co-deletions of the p and q arms of chromosome 1 and chromosome 19 respectively (1p19q co-deletion) and have been found to be especially chemosensitive with one report claiming them to be one of the most chemosensitive tumors. A median survival of up to 16.7 years has been reported for grade II oligodendrogliomas.\n\nAcoustic neuroma\nAcoustic neuromas are non-cancerous tumors. They can be treated with surgery, radiation therapy, or observation. Early intervention with surgery or radiation is recommended to prevent progressive hearing loss.\n\nEpidemiology\nFigures for incidences of cancers of the brain show a significant difference between more- and less-developed countries (the less-developed countries have lower incidences of tumors of the brain). This could be explained by undiagnosed tumor-related deaths (patients in extremely poor situations do not get diagnosed, simply because they do not have access to the modern diagnostic facilities required to diagnose a brain tumor) and by deaths caused by other poverty-related causes that preempt a patient's life before tumors develop or tumors become life-threatening. Nevertheless, statistics suggest that certain forms of primary brain tumors are more common among certain populations.\nThe incidence of low-grade astrocytoma has not been shown to vary significantly with nationality. However, studies examining the incidence of malignant central nervous system (CNS) tumors have shown some variation with national origin. Since some high-grade lesions arise from low-grade tumors, these trends are worth mentioning. Specifically, the incidence of CNS tumors in the United States, Israel, and the Nordic countries is relatively high, while Japan and Asian countries have a lower incidence. These differences probably reflect some biological differences as well as differences in pathologic diagnosis and reporting.\nWorldwide data on incidence of cancer can be found at the WHO (World Health Organization) and is handled by the IARC (International Agency for Research on Cancer) located in France.\n\nUnited States\nIn the United States in 2015, approximately 166,039 people were living with brain or other central nervous system tumors. Over 2018, it was projected that there would be 23,880 new cases of brain tumors and 16,830 deaths in 2018 as a result, accounting for 1.4 percent of all cancers and 2.8 percent of all cancer deaths. Median age of diagnosis was 58 years old, while median age of death was 65. Diagnosis was slightly more common in males, at approximately 7.5 cases per 100 000 people, while females saw 2 fewer at 5.4. Deaths as a result of brain cancer were 5.3 per 100 000 for males, and 3.6 per 100 000 for females, making brain cancer the 10th leading cause of cancer death in the United States. Overall lifetime risk of developing brain cancer is approximated at 0.6 percent for men and women.\n\nUK\nBrain, other CNS or intracranial tumors are the ninth most common cancer in the UK (around 10,600 people were diagnosed in 2013), and it is the eighth most common cause of cancer death (around 5,200 people died in 2012). White British patients with brain tumour are 30% more likely to die within a year of diagnosis than patients from other ethnicities.  The reason for this is unknown.\n\nChildren\nIn the United States more than 28,000 people under 20 are estimated to have a brain tumor. About 3,720 new cases of brain tumors are expected to be diagnosed in those under 15 in 2019. Higher rates were reported in 1985\u20131994 than in 1975\u20131983. There is some debate as to the reasons; one theory is that the trend is the result of improved diagnosis and reporting, since the jump occurred at the same time that MRIs became available widely, and there was no coincident jump in mortality. Central nervous system tumors make up 20\u201325 percent of cancers in children.\nThe average survival rate for all primary brain cancers in children is 74%. Brain cancers are the most common cancer in children under 19, are result in more death in this group than leukemia. Younger people do less well.\nThe most common brain tumor types in children (0\u201314) are: pilocytic astrocytoma, malignant glioma, medulloblastoma, neuronal and mixed neuronal-glial tumors, and ependymoma.\nIn children under 2, about 70% of brain tumors are medulloblastomas, ependymomas, and low-grade gliomas. Less commonly, and seen usually in infants, are teratomas and atypical teratoid rhabdoid tumors. Germ cell tumors, including teratomas, make up just 3% of pediatric primary brain tumors, but the worldwide incidence varies significantly.\nIn the UK, 429 children aged 14 and under are diagnosed with a brain tumour on average each year, and 563 children and young people under the age of 19 are diagnosed.\n\nResearch\nImmunotherapy\nCancer immunotherapy is being actively studied. For malignant gliomas no therapy has been shown to improve life expectancy as of 2015.\n\nVesicular stomatitis virus\nIn 2000, researchers used the vesicular stomatitis virus (VSV) to infect and kill cancer cells without affecting healthy cells.\n\nRetroviral replicating vectors\nLed by Prof. Nori Kasahara, researchers from USC, who are now at UCLA, reported in 2001 the first successful example of applying the use of retroviral replicating vectors towards transducing cell lines derived from solid tumors. Building on this initial work, the researchers applied the technology to in vivo models of cancer and in 2005 reported a long-term survival benefit in an experimental brain tumor animal model.  Subsequently, in preparation for human clinical trials, this technology was further developed by Tocagen (a pharmaceutical company primarily focused on brain cancer treatments) as a combination treatment (Toca 511 & Toca FC). This has been under investigation since 2010 in a Phase I\/II clinical trial for the potential treatment of recurrent high-grade glioma including glioblastoma and anaplastic astrocytoma. No results have yet been published.\n\nNon-invasive detection\nEfforts to detect and monitor development and treatment response of brain tumors by liquid biopsy from blood, cerebrospinal fluid or urine, are in the early stages of development.\n\nSee also\nBrain\nTumor\nNervous system neoplasm\nList of brain tumor cases\n\nReferences\nExternal links\n\nBrain and CNS cancers at Curlie\nBrain tumour information from Cancer Research UK\nNeuro-Oncology: Cancer Management Guidelines\nMedPix Teaching File MR Scans of Primary Brain Lymphoma, etc.","26":"The brainstem (or brain stem) is the posterior stalk-like part of the brain that connects the cerebrum with the spinal cord. In the human brain the brainstem is composed of the midbrain, the pons, and the medulla oblongata. The midbrain is continuous with the thalamus of the diencephalon through the tentorial notch, and sometimes the diencephalon is included in the brainstem.\nThe brainstem is very small, making up around only 2.6 percent of the brain's total weight. It has the critical roles of regulating heart and respiratory function, helping to control heart rate and breathing rate. It also provides the main motor and sensory nerve supply to the face and neck via the cranial nerves. Ten pairs of cranial nerves come from the brainstem. Other roles include the regulation of the central nervous system and the body's sleep cycle. It is also of prime importance in the conveyance of motor and sensory pathways from the rest of the brain to the body, and from the body back to the brain. These pathways include the corticospinal tract (motor function), the dorsal column-medial lemniscus pathway (fine touch, vibration sensation, and proprioception), and the spinothalamic tract (pain, temperature, itch, and crude touch).\n\nStructure\nThe parts of the brainstem are the midbrain, the pons, and the medulla oblongata; the diencephalon is sometimes considered part of the brainstem.\nThe brainstem extends from just above the tentorial notch superiorly to the first cervical vertebra below the foramen magnum inferiorly.\n\nMidbrain\nThe midbrain is further subdivided into three parts: tectum, tegmentum, and the ventral tegmental area.  The tectum forms the ceiling. The tectum comprises the paired structure of the superior and inferior colliculi and is the dorsal covering of the cerebral aqueduct. The inferior colliculus is the principal midbrain nucleus of the auditory pathway and receives input from several peripheral brainstem nuclei, as well as inputs from the auditory cortex. Its inferior brachium (arm-like process) reaches to the medial geniculate nucleus of the diencephalon. The superior colliculus is positioned above the inferior colliculus, and marks the rostral midbrain. It is involved in the special sense of vision and sends its superior brachium to the lateral geniculate body of the diencephalon.\nThe tegmentum which forms the floor of the midbrain, is ventral to the cerebral aqueduct.  Several nuclei, tracts, and the reticular formation are contained here.\nThe ventral tegmental area (VTA) is composed of paired cerebral peduncles. These transmit axons of upper motor neurons.\n\nMidbrain nuclei\nThe midbrain consists of:\n\nPeriaqueductal gray:  The area of gray matter around the cerebral aqueduct contains various neurons involved in the pain desensitization pathway.  Neurons synapse here. When stimulated by a signal, the synaptic connections activate neurons in the nucleus raphe magnus; the pathway then projects down into the posterior grey column of the spinal cord which prevents pain sensation transmission.\nOculomotor nerve nucleus:  This is the third cranial nerve nucleus.\nTrochlear nerve nucleus:  This is the fourth cranial nerve.\nRed nucleus: This is a motor nucleus that sends a descending tract to the lower motor neurons.\nSubstantia nigra pars compacta: This is a concentration of neurons in the ventral portion of the midbrain that uses dopamine as its neurotransmitter and is involved in both motor function and emotion. Its dysfunction is implicated in Parkinson's disease.\nReticular formation: This is a large area in the midbrain that is involved in various important functions of the midbrain. In particular, it contains lower motor neurons, is involved in the pain desensitization pathway, is involved in the arousal and consciousness systems, and contains the locus coeruleus, which is involved in intensive alertness modulation and in autonomic reflexes.\nCentral tegmental tract: Directly anterior to the floor of the fourth ventricle, this is a pathway by which many tracts project up to the cortex and down to the spinal cord.\nVentral tegmental area: A dopaminergic nucleus, known as group A10 cells is located close to the midline on the floor of the midbrain.\nRostromedial tegmental nucleus: A GABAergic nucleus located adjacent to the ventral tegmental area.\n\nPons\nThe pons lies between the midbrain and the medulla oblongata. It is separated from the midbrain by the superior pontine sulcus, and from the medulla by the inferior pontine sulcus. It contains tracts that carry signals from the cerebrum to the medulla and to the cerebellum and also tracts that carry sensory signals to the thalamus. The pons is connected to the cerebellum by the cerebellar peduncles. The pons houses the respiratory pneumotaxic center and apneustic center that make up the pontine respiratory group in the respiratory center. The pons co-ordinates activities of the cerebellar hemispheres.\nThe pons and medulla oblongata are parts of the hindbrain that form much of the brainstem.\n\nMedulla oblongata\nThe medulla oblongata, often just referred to as the medulla, is the lower half of the brainstem continuous with the spinal cord. Its upper part is continuous with the pons.:\u200a1121\u200a. The medulla contains the cardiac, dorsal and ventral respiratory groups, and vasomotor centres, dealing with heart rate, breathing and blood pressure. Another important medullary structure is the area postrema whose functions include the control of vomiting.\n\nAppearance\nFrom the front\n\nIn the medial part of the medulla is the anterior median fissure.  Moving laterally on each side are the medullary pyramids.  The pyramids contain the fibers of the corticospinal tract (also called the pyramidal tract), or the upper motor neuronal axons as they head inferiorly to synapse on lower motor neuronal cell bodies within the anterior grey column of the spinal cord.\nThe anterolateral sulcus is lateral to the pyramids.  Emerging from the anterolateral sulci are the CN XII (hypoglossal nerve) rootlets.  Lateral to these rootlets and the anterolateral sulci are the olives.  The olives are swellings in the medulla containing underlying inferior nucleary nuclei (containing various nuclei and afferent fibers).  Lateral (and dorsal) to the olives are the rootlets for CN IX (glossopharyngeal), CN X (vagus) and CN XI (accessory nerve).  The pyramids end at the pontine medulla junction, noted most obviously by the large basal pons.  From this junction, CN VI (abducens nerve), CN VII (facial nerve) and CN VIII (vestibulocochlear nerve) emerge.  At the level of the midpons, CN V (the trigeminal nerve) emerges.  Cranial nerve III (the oculomotor nerve) emerges ventrally from the midbrain, while the CN IV (the trochlear nerve) emerges out from the dorsal aspect of the midbrain.\nBetween the two pyramids can be seen a decussation of fibers which marks the transition from the medulla to the spinal cord. The medulla is above the decussation and the spinal cord below.\n\nFrom behind\n\nThe most medial part of the medulla is the posterior median sulcus.  Moving laterally on each side is the gracile fasciculus, and lateral to that is the cuneate fasciculus.  Superior to each of these, and directly inferior to the obex, are the gracile and cuneate tubercles, respectively.  Underlying these are their respective nuclei.  The obex marks the end of the fourth ventricle and the beginning of the central canal.  The posterior intermediate sulcus separates the gracile fasciculus from the cuneate fasciculus.  Lateral to the cuneate fasciculus is the lateral funiculus.\nSuperior to the obex is the floor of the fourth ventricle.  In the floor of the fourth ventricle, various nuclei can be visualized by the small bumps that they make in the overlying tissue.  In the midline and directly superior to the obex is the vagal trigone and superior to that it the hypoglossal trigone.  Underlying each of these are motor nuclei for the respective cranial nerves.  Superior to these trigones are fibers running laterally in both directions.  These fibers are known collectively as the striae medullares.  Continuing in a rostral direction, the large bumps are called the facial colliculi.  Each facial colliculus, contrary to their names, do not contain the facial nerve nuclei.  Instead, they have facial nerve axons traversing superficial to underlying abducens (CN VI) nuclei.  Lateral to all these bumps previously discussed is an indented line, or sulcus that runs rostrally, and is known as the sulcus limitans.  This separates the medial motor neurons from the lateral sensory neurons.  Lateral to the sulcus limitans is the area of the vestibular system, which is involved in special sensation.  Moving rostrally, the inferior, middle, and superior cerebellar peduncles are found connecting the midbrain to the cerebellum.  Directly rostral to the superior cerebellar peduncle, there is the superior medullary velum and then the two trochlear nerves.  This marks the end of the pons as the inferior colliculus is directly rostral and marks the caudal midbrain. Middle cerebellar peduncle is located inferior and lateral to the superior cerebellar peduncle, connecting pons to the cerebellum. Likewise, inferior cerebellar peduncle is found connecting the medulla oblongata to the cerebellum.\n\nBlood supply\nThe main supply of blood to the brainstem is provided by the basilar arteries and the vertebral arteries.:\u200a740\n\nDevelopment\nThe human brainstem emerges from two of the three primary brain vesicles formed of the neural tube.  The mesencephalon is the second of the three primary vesicles, and does not further differentiate into a secondary brain vesicle.  This will become the midbrain.  The third primary vesicle, the rhombencephalon (hindbrain) will further differentiate into two secondary vesicles, the metencephalon and the myelencephalon.  The metencephalon will become the cerebellum and the pons.  The more caudal myelencephalon will become the medulla.\n\nFunction\nThe brainstem plays important functions in breathing, heart rate, arousal \/ consciousness, sleep \/ wake functions and attention \/ concentration.  \nThere are three main functions of the brainstem:\n\nThe brainstem plays a role in conduction. That is, all information relayed from the body to the cerebrum and cerebellum and vice versa must traverse the brainstem. The ascending pathways coming from the body to the brain are the sensory pathways and include the spinothalamic tract for pain and temperature sensation and the dorsal column-medial lemniscus pathway (DCML) including the gracile fasciculus and the cuneate fasciculus for touch, proprioception, and pressure sensation. The facial sensations have similar pathways and will travel in the spinothalamic tract and the DCML.  Descending tracts are the axons of upper motor neurons destined to synapse on lower motor neurons in the ventral horn and posterior horn.  In addition, there are upper motor neurons that originate in the brainstem's vestibular, red, tectal, and reticular nuclei, which also descend and synapse in the spinal cord.\nThe cranial nerves III-XII emerge from the brainstem. These cranial nerves supply the face, head, and viscera. (The first two pairs of cranial nerves arise from the cerebrum).\nThe brainstem has integrative functions being involved in cardiovascular system control, respiratory control, pain sensitivity control, alertness, awareness, and consciousness.  Thus, brainstem damage is a very serious and often life-threatening problem.\n\nCranial nerves\nTen of the twelve pairs of cranial nerves either target or are sourced from the brainstem nuclei.:\u200a725\u200a The nuclei of the oculomotor nerve (III) and trochlear nerve (IV) are located in the midbrain. The nuclei of the trigeminal nerve (V), abducens nerve (VI), facial nerve (VII) and vestibulocochlear nerve (VIII) are located in the pons. The nuclei of the glossopharyngeal nerve (IX), vagus nerve (X), accessory nerve (XI) and hypoglossal nerve (XII) are located in the medulla. The fibers of these cranial nerves exit the brainstem from these nuclei.\n\nClinical significance\nDiseases of the brainstem can result in abnormalities in the function of cranial nerves that may lead to visual disturbances, pupil abnormalities, changes in sensation, muscle weakness, hearing problems, vertigo, swallowing and speech difficulty, voice change, and co-ordination problems. Localizing neurological lesions in the brainstem may be very precise, although it relies on a clear understanding on the functions of brainstem anatomical structures and how to test them.\nBrainstem stroke syndrome can cause a range of impairments including locked-in syndrome.\nDuret haemorrhages are areas of bleeding in the midbrain and upper pons due to a downward traumatic displacement of the brainstem.:\u200a842\u200a\nCysts known as syrinxes can affect the brainstem, in a condition, called syringobulbia. These fluid-filled cavities can be congenital, acquired or the result of a tumor.\nCriteria for claiming brainstem death in the UK have developed in order to make the decision of when to stop ventilation of somebody who could not otherwise sustain life. These determining factors are that the patient is irreversibly unconscious and incapable of breathing unaided. All other possible causes must be ruled out that might otherwise indicate a temporary condition. The state of irreversible brain damage has to be unequivocal. There are brainstem reflexes that are checked for by two senior doctors so that imaging technology is unnecessary. The absence of the cough and gag reflexes, of the corneal reflex and the vestibulo\u2013ocular reflex need to be established; the pupils of the eyes must be fixed and dilated; there must be an absence of motor response to stimulation and an absence of breathing marked by concentrations of carbon dioxide in the arterial blood. All of these tests must be repeated after a certain time before death can be declared.\n\nAdditional images\nSee also\nTriune brain \u2013 reptilian brain\n\nReferences\nExternal links\n\nComparative Neuroscience at Wikiversity","27":"A computed tomography scan (CT scan; formerly called computed axial tomography scan or CAT scan) is a medical imaging technique used to obtain detailed internal images of the body. The personnel that perform CT scans are called radiographers or radiology technologists.\nCT scanners use a rotating X-ray tube and a row of detectors placed in a gantry to measure X-ray attenuations by different tissues inside the body. The multiple X-ray measurements taken from different angles are then processed on a computer using tomographic reconstruction algorithms to produce tomographic (cross-sectional) images (virtual \"slices\") of a body. CT scans can be used in patients with metallic implants or pacemakers, for whom magnetic resonance imaging (MRI) is contraindicated.\nSince its development in the 1970s, CT scanning has proven to be a versatile imaging technique. While CT is most prominently used in medical diagnosis, it can also be used to form images of non-living objects. The 1979 Nobel Prize in Physiology or Medicine was awarded jointly to South African-American physicist Allan MacLeod Cormack and British electrical engineer Godfrey Hounsfield \"for the development of computer-assisted tomography\".\n\nTypes\nOn the basis of image acquisition and procedures, various type of scanners are available in the market.\n\nSequential CT\nSequential CT, also known as step-and-shoot CT, is a type of scanning method in which the CT table moves stepwise. The table increments to a particular location and then stops which is followed by the X-ray tube rotation and acquisition of a slice. The table then increments again, and another slice is taken. The table movement stops while taking slices. This results in an increased time of scanning.\n\nSpiral CT\nSpinning tube, commonly called spiral CT, or helical CT, is an imaging technique in which an entire X-ray tube is spun around the central axis of the area being scanned. These are the dominant type of scanners on the market because they have been manufactured longer and offer a lower cost of production and purchase. The main limitation of this type of CT is the bulk and inertia of the equipment (X-ray tube assembly and detector array on the opposite side of the circle) which limits the speed at which the equipment can spin. Some designs use two X-ray sources and detector arrays offset by an angle, as a technique to improve temporal resolution.\n\nElectron beam tomography\nElectron beam tomography (EBT) is a specific form of CT in which a large enough X-ray tube is constructed so that only the path of the electrons, travelling between the cathode and anode of the X-ray tube, are spun using deflection coils. This type had a major advantage since sweep speeds can be much faster, allowing for less blurry imaging of moving structures, such as the heart and arteries. Fewer scanners of this design have been produced when compared with spinning tube types, mainly due to the higher cost associated with building a much larger X-ray tube and detector array and limited anatomical coverage.\n\nDual Energy CT\nDual Energy CT, also known as Spectral CT, is an advancement of Computed Tomography in which two energies are used to create two sets of data. A Dual Energy CT may employ Dual source, Single source with dual detector layer, Single source with energy switching methods to get two different sets of data.  \n\nDual source CT is an advanced scanner with a two X-ray tube detector system, unlike conventional single tube systems. These two detector systems are mounted on a single gantry at 90\u00b0 in the same plane. Dual Source CT scanners allow fast scanning with higher temporal resolution by acquiring a full CT slice in only half a rotation. Fast imaging reduces motion blurring at high heart rates and potentially allowing for shorter breath-hold time. This is particularly useful for ill patients having difficulty holding their breath or unable to take heart-rate lowering medication.\nSingle Source with Energy switching is another mode of Dual energy CT in which a single tube is operated at two different energies by switching the energies frequently.\n\nCT perfusion imaging\nCT perfusion imaging is a specific form of CT to assess flow through blood vessels whilst injecting a contrast agent. Blood flow, blood transit time, and organ blood volume, can all be calculated with reasonable sensitivity and specificity. This type of CT may be used on the heart, although sensitivity and specificity for detecting abnormalities are still lower than for other forms of CT. This may also be used on the brain, where CT perfusion imaging can often detect poor brain perfusion well before it is detected using a conventional spiral CT scan. This is better for stroke diagnosis than other CT types.\n\nPET CT\nPositron emission tomography\u2013computed tomography is a hybrid CT modality which combines, in a single gantry, a positron emission tomography (PET) scanner and an X-ray computed tomography (CT) scanner, to acquire sequential images from both devices in the same session, which are combined into a single superposed (co-registered) image. Thus, functional imaging obtained by PET, which depicts the spatial distribution of metabolic or biochemical activity in the body can be more precisely aligned or correlated with anatomic imaging obtained by CT scanning.\nPET-CT gives both anatomical and functional details of an organ under examination and is helpful in detecting different type of cancers.\n\nMedical use\nSince its introduction in the 1970s, CT has become an important tool in medical imaging to supplement conventional X-ray imaging and medical ultrasonography. It has more recently been used for preventive medicine or screening for disease, for example, CT colonography for people with a high risk of colon cancer, or full-motion heart scans for people with a high risk of heart disease. Several institutions offer full-body scans for the general population although this practice goes against the advice and official position of many professional organizations in the field primarily due to the radiation dose applied.\nThe use of CT scans has increased dramatically over the last two decades in many countries. An estimated 72 million scans were performed in the United States in 2007 and more than 80 million in 2015.\n\nHead\nCT scanning of the head is typically used to detect infarction (stroke), tumors, calcifications, haemorrhage, and bone trauma. Of the above, hypodense (dark) structures can indicate edema and infarction, hyperdense (bright) structures indicate calcifications and haemorrhage and bone trauma can be seen as disjunction in bone windows. Tumors can be detected by the swelling and anatomical distortion they cause, or by surrounding edema. CT scanning of the head is also used in CT-guided stereotactic surgery and radiosurgery for treatment of intracranial tumors, arteriovenous malformations, and other surgically treatable conditions using a device known as the N-localizer.\n\nNeck\nContrast CT is generally the initial study of choice for neck masses in adults. CT of the thyroid plays an important role in the evaluation of thyroid cancer. CT scan often incidentally finds thyroid abnormalities, and so is often the preferred investigation modality for thyroid abnormalities.\n\nLungs\nA CT scan can be used for detecting both acute and chronic changes in the lung parenchyma, the tissue of the lungs. It is particularly relevant here because normal two-dimensional X-rays do not show such defects. A variety of techniques are used, depending on the suspected abnormality. For evaluation of chronic interstitial processes such as emphysema, and fibrosis, thin sections with high spatial frequency reconstructions are used; often scans are performed both on inspiration and expiration. This special technique is called high resolution CT that produces a sampling of the lung, and not continuous images.\n\nBronchial wall thickening can be seen on lung CTs and generally (but not always) implies inflammation of the bronchi.\nAn incidentally found nodule in the absence of symptoms (sometimes referred to as an incidentaloma) may raise concerns that it might represent a tumor, either benign or malignant. Perhaps persuaded by fear, patients and doctors sometimes agree to an intensive schedule of CT scans, sometimes up to every three months and beyond the recommended guidelines, in an attempt to do surveillance on the nodules. However, established guidelines advise that patients without a prior history of cancer and whose solid nodules have not grown over a two-year period are unlikely to have any malignant cancer. For this reason, and because no research provides supporting evidence that intensive surveillance gives better outcomes, and because of risks associated with having CT scans, patients should not receive CT screening in excess of those recommended by established guidelines.\n\nAngiography\nComputed tomography angiography (CTA) is a type of contrast CT to visualize the arteries and veins throughout the body. This ranges from arteries serving the brain to those bringing blood to the lungs, kidneys, arms and legs. An example of this type of exam is CT pulmonary angiogram (CTPA) used to diagnose pulmonary embolism (PE). It employs computed tomography and an iodine-based contrast agent to obtain an image of the pulmonary arteries. CT scans can reduce the risk of angiography by providing clinicians with more information about the positioning and number of clots prior to the procedure.\n\nCardiac\nA CT scan of the heart is performed to gain knowledge about cardiac or coronary anatomy. Traditionally, cardiac CT scans are used to detect, diagnose, or follow up coronary artery disease. More recently CT has played a key role in the fast-evolving field of transcatheter structural heart interventions, more specifically in the transcatheter repair and replacement of heart valves.\nThe main forms of cardiac CT scanning are:\n\nCoronary CT angiography (CCTA): the use of CT to assess the coronary arteries of the heart. The subject receives an intravenous injection of radiocontrast, and then the heart is scanned using a high-speed CT scanner, allowing radiologists to assess the extent of occlusion in the coronary arteries, usually to diagnose coronary artery disease.\nCoronary CT calcium scan: also used for the assessment of severity of coronary artery disease. Specifically, it looks for calcium deposits in the coronary arteries that can narrow arteries and increase the risk of a heart attack. A typical coronary CT calcium scan is done without the use of radiocontrast, but it can possibly be done from contrast-enhanced images as well.\nTo better visualize the anatomy, post-processing of the images is common. Most common are multiplanar reconstructions (MPR) and volume rendering. For more complex anatomies and procedures, such as heart valve interventions, a true 3D reconstruction or a 3D print is created based on these CT images to gain a deeper understanding.\n\nAbdomen and pelvis\nCT is an accurate technique for diagnosis of abdominal diseases like Crohn's disease, GIT bleeding, and diagnosis and staging of cancer, as well as follow-up after cancer treatment to assess response. It is commonly used to investigate acute abdominal pain.\nNon-enhanced computed tomography is today the gold standard for diagnosing urinary stones. The size, volume and density of stones can be estimated to help clinicians guide further treatment; size is especially important in predicting spontaneous passage of a stone.\n\nAxial skeleton and extremities\nFor the axial skeleton and extremities, CT is often used to image complex fractures, especially ones around joints, because of its ability to reconstruct the area of interest in multiple planes. Fractures, ligamentous injuries, and dislocations can easily be recognized with a 0.2 mm resolution. With modern dual-energy CT scanners, new areas of use have been established, such as aiding in the diagnosis of gout.\n\nBiomechanical use\nCT is used in biomechanics to quickly reveal the geometry, anatomy, density and elastic moduli of biological tissues.\n\nOther uses\nIndustrial use\nIndustrial CT scanning (industrial computed tomography) is a process which uses X-ray equipment to produce 3D representations of components both externally and internally. Industrial CT scanning has been used in many areas of industry for internal inspection of components. Some of the key uses for CT scanning have been flaw detection, failure analysis, metrology, assembly analysis, image-based finite element methods and reverse engineering applications. CT scanning is also employed in the imaging and conservation of museum artifacts.\n\nAviation security\nCT scanning has also found an application in transport security (predominantly airport security) where it is currently used in a materials analysis context for explosives detection CTX (explosive-detection device) and is also under consideration for automated baggage\/parcel security scanning using computer vision based object recognition algorithms that target the detection of specific threat items based on 3D appearance (e.g. guns, knives, liquid containers). Its usage in airport security pioneered at Shannon Airport in March 2022 has ended the ban on liquids over 100 ml there, a move that Heathrow Airport plans for a full roll-out on 1 December 2022 and the TSA spent $781.2 million on an order for over 1,000 scanners, ready to go live in the summer.\n\nGeological use\nX-ray CT is used in geological studies to quickly reveal materials inside a drill core. Dense minerals such as pyrite and barite appear brighter and less dense components such as clay appear dull in CT images.\n\nCultural heritage use\nX-ray CT and micro-CT can also be used for the conservation and preservation of objects of cultural heritage. For many fragile objects, direct research and observation can be damaging and can degrade the object over time. Using CT scans, conservators and researchers are able to determine the material composition of the objects they are exploring, such as the position of ink along the layers of a scroll, without any additional harm. These scans have been optimal for research focused on the workings of the Antikythera mechanism or the text hidden inside the charred outer layers of the En-Gedi Scroll. However, they are not optimal for every object subject to these kinds of research questions, as there are certain artifacts like the Herculaneum papyri in which the material composition has very little variation along the inside of the object. After scanning these objects, computational methods can be employed to examine the insides of these objects, as was the case with the virtual unwrapping of the En-Gedi scroll and the Herculaneum papyri. Micro-CT has also proved useful for analyzing more recent artifacts such as still-sealed historic correspondence that employed the technique of letterlocking (complex folding and cuts) that provided a \"tamper-evident locking mechanism\". Further examples of use cases in archaeology is imaging the contents of sarcophagi or ceramics.\nRecently, CWI in Amsterdam has collaborated with Rijksmuseum to investigate art object inside details in the framework called IntACT.\n\nMicro organism research\nVaried types of fungus can degrade wood to different degrees, one Belgium research group has been used X-ray CT 3 dimension with sub-micron resolution unveiled fungi can penetrate micropores of 0.6 \u03bcm under certain conditions.\n\nTimber sawmill\nSawmills use industrial CT scanners to detect round defects, for instance knots, to improve total value of timber productions. Most sawmills are planning to incorporate this robust detection tool to improve productivity in the long run, however initial investment cost is high.\n\nInterpretation of results\nPresentation\nThe result of a CT scan is a volume of voxels, which may be presented to a human observer by various methods, which broadly fit into the following categories:\n\nSlices (of varying thickness). Thin slice is generally regarded as planes representing a thickness of less than 3 mm. Thick slice is generally regarded as planes representing a thickness between 3 mm and 5 mm.\nProjection, including maximum intensity projection and average intensity projection\nVolume rendering (VR)\nTechnically, all volume renderings become projections when viewed on a 2-dimensional display, making the distinction between projections and volume renderings a bit vague. The epitomes of volume rendering models feature a mix of for example coloring and shading in order to create realistic and observable representations.\nTwo-dimensional CT images are conventionally rendered so that the view is as though looking up at it from the patient's feet. Hence, the left side of the image is to the patient's right and vice versa, while anterior in the image also is the patient's anterior and vice versa. This left-right interchange corresponds to the view that physicians generally have in reality when positioned in front of patients.\n\nGrayscale\nPixels in an image obtained by CT scanning are displayed in terms of relative radiodensity. The pixel itself is displayed according to the mean attenuation of the tissue(s) that it corresponds to on a scale from +3,071 (most attenuating) to \u22121,024 (least attenuating) on the Hounsfield scale. A pixel is a two dimensional unit based on the matrix size and the field of view. When the CT slice thickness is also factored in, the unit is known as a voxel, which is a three-dimensional unit. Water has an attenuation of 0 Hounsfield units (HU), while air is \u22121,000 HU, cancellous bone is typically +400 HU, and cranial bone can reach 2,000 HU. The attenuation of metallic implants depends on the atomic number of the element used: Titanium usually has an amount of +1000 HU, iron steel can completely block the X-ray and is, therefore, responsible for well-known line-artifacts in computed tomograms. Artifacts are caused by abrupt transitions between low- and high-density materials, which results in data values that exceed the dynamic range of the processing electronics.\n\nWindowing\nCT data sets have a very high dynamic range which must be reduced for display or printing. This is typically done via a process of \"windowing\", which maps a range (the \"window\") of pixel values to a grayscale ramp. For example, CT images of the brain are commonly viewed with a window extending from 0 HU to 80 HU. Pixel values of 0 and lower, are displayed as black; values of 80 and higher are displayed as white; values within the window are displayed as a gray intensity proportional to position within the window. The window used for display must be matched to the X-ray density of the object of interest, in order to optimize the visible detail. Window width and window level parameters are used to control the windowing of a scan.\n\nMultiplanar reconstruction and projections\nMultiplanar reconstruction (MPR) is the process of converting data from one anatomical plane (usually transverse) to other planes. It can be used for thin slices as well as projections. Multiplanar reconstruction is possible as present CT scanners provide almost isotropic resolution.\nMPR is used almost in every scan. The spine is frequently examined with it. An image of the spine in axial plane can only show one vertebral bone at a time and cannot show its relation with other vertebral bones. By reformatting the data in other planes, visualization of the relative position can be achieved in sagittal and coronal plane.\nNew software allows the reconstruction of data in non-orthogonal (oblique) planes, which help in the visualization of organs which are not in orthogonal planes. It is better suited for visualization of the anatomical structure of the bronchi as they do not lie orthogonal to the direction of the scan.\nCurved-plane reconstruction (or curved planar reformation = CPR) is performed mainly for the evaluation of vessels. This type of reconstruction helps to straighten the bends in a vessel, thereby helping to visualize a whole vessel in a single image or in multiple images. After a vessel has been \"straightened\", measurements such as cross-sectional area and length can be made. This is helpful in preoperative assessment of a surgical procedure.\nFor 2D projections used in radiation therapy for quality assurance and planning of external beam radiotherapy, including digitally reconstructed radiographs, see Beam's eye view.\n\nVolume rendering\nA threshold value of radiodensity is set by the operator (e.g., a level that corresponds to bone). With the help of edge detection image processing algorithms a 3D model can be constructed from the initial data and displayed on screen. Various thresholds can be used to get multiple models, each anatomical component such as muscle, bone and cartilage can be differentiated on the basis of different colours given to them. However, this mode of operation cannot show interior structures.\nSurface rendering is limited technique as it displays only the surfaces that meet a particular threshold density, and which are towards the viewer. However, In volume rendering, transparency, colours and shading are used which makes it easy to present a volume in a single image. For example, Pelvic bones could be displayed as semi-transparent, so that, even viewing at an oblique angle one part of the image does not hide another.\n\nImage quality\nDose versus image quality\nAn important issue within radiology today is how to reduce the radiation dose during CT examinations without compromising the image quality. In general, higher radiation doses result in higher-resolution images, while lower doses lead to increased image noise and unsharp images. However, increased dosage raises the adverse side effects, including the risk of radiation-induced cancer \u2013 a four-phase abdominal CT gives the same radiation dose as 300 chest X-rays. Several methods that can reduce the exposure to ionizing radiation during a CT scan exist.\n\nNew software technology can significantly reduce the required radiation dose. New iterative tomographic reconstruction algorithms (e.g., iterative Sparse Asymptotic Minimum Variance) could offer super-resolution without requiring higher radiation dose.\nIndividualize the examination and adjust the radiation dose to the body type and body organ examined. Different body types and organs require different amounts of radiation.\nHigher resolution is not always suitable, such as detection of small pulmonary masses.\n\nArtifacts\nAlthough images produced by CT are generally faithful representations of the scanned volume, the technique is susceptible to a number of artifacts, such as the following:Chapters 3 and 5\n\nStreak artifact\nStreaks are often seen around materials that block most X-rays, such as metal or bone. Numerous factors contribute to these streaks: under sampling, photon starvation, motion, beam hardening, and Compton scatter. This type of artifact commonly occurs in the posterior fossa of the brain, or if there are metal implants. The streaks can be reduced using newer reconstruction techniques. Approaches such as metal artifact reduction (MAR) can also reduce this artifact. MAR techniques include spectral imaging, where CT images are taken with photons of different energy levels, and then synthesized into monochromatic images with special software such as GSI (Gemstone Spectral Imaging).\nPartial volume effect\nThis appears as \"blurring\" of edges. It is due to the scanner being unable to differentiate between a small amount of high-density material (e.g., bone) and a larger amount of lower density (e.g., cartilage). The reconstruction assumes that the X-ray attenuation within each voxel is homogeneous; this may not be the case at sharp edges. This is most commonly seen in the z-direction (craniocaudal direction), due to the conventional use of highly anisotropic voxels, which have a much lower out-of-plane resolution, than in-plane resolution. This can be partially overcome by scanning using thinner slices, or an isotropic acquisition on a modern scanner.\nRing artifact\nProbably the most common mechanical artifact, the image of one or many \"rings\" appears within an image. They are usually caused by the variations in the response from individual elements in a two dimensional X-ray detector due to defect or miscalibration. Ring artifacts can largely be reduced by intensity normalization, also referred to as flat field correction. Remaining rings can be suppressed by a transformation to polar space, where they become linear stripes. A comparative evaluation of ring artefact reduction on X-ray tomography images showed that the method of Sijbers and Postnov can effectively suppress ring artefacts.\nNoise\nThis appears as grain on the image and is caused by a low signal to noise ratio. This occurs more commonly when a thin slice thickness is used. It can also occur when the power supplied to the X-ray tube is insufficient to penetrate the anatomy.\nWindmill\nStreaking appearances can occur when the detectors intersect the reconstruction plane. This can be reduced with filters or a reduction in pitch.\nBeam hardening\nThis can give a \"cupped appearance\" when grayscale is visualized as height. It occurs because conventional sources, like X-ray tubes emit a polychromatic spectrum. Photons of higher photon energy levels are typically attenuated less. Because of this, the mean energy of the spectrum increases when passing the object, often described as getting \"harder\". This leads to an effect increasingly underestimating material thickness, if not corrected. Many algorithms exist to correct for this artifact. They can be divided into mono- and multi-material methods.\n\nAdvantages\nCT scanning has several advantages over traditional two-dimensional medical radiography. First, CT eliminates the superimposition of images of structures outside the area of interest. Second, CT scans have greater image resolution, enabling examination of finer details. CT can distinguish between tissues that differ in radiographic density by 1% or less. Third, CT scanning enables multiplanar reformatted imaging: scan data can be visualized in the transverse (or axial), coronal, or sagittal plane, depending on the diagnostic task.\nThe improved resolution of CT has permitted the development of new investigations. For example, CT angiography avoids the invasive insertion of a catheter. CT scanning can perform a virtual colonoscopy with greater accuracy and less discomfort for the patient than a traditional colonoscopy. Virtual colonography is far more accurate than a barium enema for detection of tumors and uses a lower radiation dose.\nCT is a moderate-to-high radiation diagnostic technique. The radiation dose for a particular examination depends on multiple factors: volume scanned, patient build, number and type of scan protocol, and desired resolution and image quality. Two helical CT scanning parameters, tube current and pitch, can be adjusted easily and have a profound effect on radiation. CT scanning is more accurate than two-dimensional radiographs in evaluating anterior interbody fusion, although they may still over-read the extent of fusion.\n\nAdverse effects\nCancer\nThe radiation used in CT scans can damage body cells, including DNA molecules, which can lead to radiation-induced cancer. The radiation doses received from CT scans is variable. Compared to the lowest dose X-ray techniques, CT scans can have 100 to 1,000 times higher dose than conventional X-rays. However, a lumbar spine X-ray has a similar dose as a head CT. Articles in the media often exaggerate the relative dose of CT by comparing the lowest-dose X-ray techniques (chest X-ray) with the highest-dose CT techniques. In general, a routine abdominal CT has a radiation dose similar to three years of average background radiation.\nLarge scale population-based studies have consistently demonstrated that low dose radiation from CT scans has impacts on cancer incidence in a variety of cancers. For example, in a large population-based cohort it was found that up to 4% of brain cancers were caused by CT scan radiation. Some experts project that in the future, between three and five percent of all cancers would result from medical imaging. An Australian study of 10.9 million people reported that the increased incidence of cancer after CT scan exposure in this cohort was mostly due to irradiation. In this group, one in every 1,800 CT scans was followed by an excess cancer. If the lifetime risk of developing cancer is 40% then the absolute risk rises to 40.05% after a CT. The risks of CT scan radiation are especially important in patients undergoing recurrent CT scans within a short time span of one to five years.\nSome experts note that CT scans are known to be \"overused,\" and \"there is distressingly little evidence of better health outcomes associated with the current high rate of scans.\" On the other hand, a recent paper analyzing the data of patients who received high cumulative doses showed a high degree of appropriate use. This creates an important issue of cancer risk to these patients. Moreover, a highly significant finding that was previously unreported is that some patients received >100 mSv dose from CT scans in a single day, which counteracts existing criticisms some investigators may have on the effects of protracted versus acute exposure.\nThere are contrarian views and the debate is ongoing. Some studies have shown that publications indicating an increased risk of cancer from typical doses of body CT scans are plagued with serious methodological limitations and several highly improbable results, concluding that no evidence indicates such low doses cause any long-term harm.\nOne study estimated that as many as 0.4% of cancers in the United States resulted from CT scans, and that this may have increased to as much as 1.5 to 2% based on the rate of CT use in 2007. Others dispute this estimate, as there is no consensus that the low levels of radiation used in CT scans cause damage. Lower radiation doses are used in many cases, such as in the investigation of renal colic.\nA person's age plays a significant role in the subsequent risk of cancer. Estimated lifetime cancer mortality risks from an abdominal CT of a one-year-old is 0.1%, or 1:1000 scans. The risk for someone who is 40 years old is half that of someone who is 20 years old with substantially less risk in the elderly. The International Commission on Radiological Protection estimates that the risk to a fetus being exposed to 10 mGy (a unit of radiation exposure) increases the rate of cancer before 20 years of age from 0.03% to 0.04% (for reference a CT pulmonary angiogram exposes a fetus to 4 mGy). A 2012 review did not find an association between medical radiation and cancer risk in children noting however the existence of limitations in the evidences over which the review is based. CT scans can be performed with different settings for lower exposure in children with most manufacturers of CT scans as of 2007 having this function built in. Furthermore, certain conditions can require children to be exposed to multiple CT scans. \nCurrent recommendations are to inform patients of the risks of CT scanning. However, employees of imaging centers tend not to communicate such risks unless patients ask.\n\nContrast reactions\nIn the United States half of CT scans are contrast CTs using intravenously injected radiocontrast agents. The most common reactions from these agents are mild, including nausea, vomiting, and an itching rash. Severe life-threatening reactions may rarely occur. Overall reactions occur in 1 to 3% with nonionic contrast and 4 to 12% of people with ionic contrast. Skin rashes may appear within a week to 3% of people.\nThe old radiocontrast agents caused anaphylaxis in 1% of cases while the newer, low-osmolar agents cause reactions in 0.01\u20130.04% of cases. Death occurs in about 2 to 30 people per 1,000,000 administrations, with newer agents being safer.\nThere is a higher risk of mortality in those who are female, elderly or in poor health, usually secondary to either anaphylaxis or acute kidney injury.\nThe contrast agent may induce contrast-induced nephropathy. This occurs in 2 to 7% of people who receive these agents, with greater risk in those who have preexisting kidney failure, preexisting diabetes, or reduced intravascular volume. People with mild kidney impairment are usually advised to ensure full hydration for several hours before and after the injection. For moderate kidney failure, the use of iodinated contrast should be avoided; this may mean using an alternative technique instead of CT. Those with severe kidney failure requiring dialysis require less strict precautions, as their kidneys have so little function remaining that any further damage would not be noticeable and the dialysis will remove the contrast agent; it is normally recommended, however, to arrange dialysis as soon as possible following contrast administration to minimize any adverse effects of the contrast.\nIn addition to the use of intravenous contrast, orally administered contrast agents are frequently used when examining the abdomen. These are frequently the same as the intravenous contrast agents, merely diluted to approximately 10% of the concentration. However, oral alternatives to iodinated contrast exist, such as very dilute (0.5\u20131% w\/v) barium sulfate suspensions. Dilute barium sulfate has the advantage that it does not cause allergic-type reactions or kidney failure, but cannot be used in patients with suspected bowel perforation or suspected bowel injury, as leakage of barium sulfate from damaged bowel can cause fatal peritonitis.\nSide effects from contrast agents, administered intravenously in some CT scans, might impair kidney performance in patients with kidney disease, although this risk is now believed to be lower than previously thought.\n\nScan dose\nThe table reports average radiation exposures; however, there can be a wide variation in radiation doses between similar scan types, where the highest dose could be as much as 22 times higher than the lowest dose. A typical plain film X-ray involves radiation dose of 0.01 to 0.15 mGy, while a typical CT can involve 10\u201320 mGy for specific organs, and can go up to 80 mGy for certain specialized CT scans.\nFor purposes of comparison, the world average dose rate from naturally occurring sources of background radiation is 2.4 mSv per year, equal for practical purposes in this application to 2.4 mGy per year. While there is some variation, most people (99%) received less than 7 mSv per year as background radiation. Medical imaging as of 2007 accounted for half of the radiation exposure of those in the United States with CT scans making up two thirds of this amount. In the United Kingdom it accounts for 15% of radiation exposure. The average radiation dose from medical sources is \u22480.6 mSv per person globally as of 2007. Those in the nuclear industry in the United States are limited to doses of 50 mSv a year and 100 mSv every 5 years.\nLead is the main material used by radiography personnel for shielding against scattered X-rays.\n\nRadiation dose units\nThe radiation dose reported in the gray or mGy unit is proportional to the amount of energy that the irradiated body part is expected to absorb, and the physical effect (such as DNA double strand breaks) on the cells' chemical bonds by X-ray radiation is proportional to that energy.\nThe sievert unit is used in the report of the effective dose. The sievert unit, in the context of CT scans, does not correspond to the actual radiation dose that the scanned body part absorbs but to another radiation dose of another scenario, the whole body absorbing the other radiation dose and the other radiation dose being of a magnitude, estimated to have the same probability to induce cancer as the CT scan. Thus, as is shown in the table above, the actual radiation that is absorbed by a scanned body part is often much larger than the effective dose suggests. A specific measure, termed the computed tomography dose index (CTDI), is commonly used as an estimate of the radiation absorbed dose for tissue within the scan region, and is automatically computed by medical CT scanners.\nThe equivalent dose is the effective dose of a case, in which the whole body would actually absorb the same radiation dose, and the sievert unit is used in its report. In the case of non-uniform radiation, or radiation given to only part of the body, which is common for CT examinations, using the local equivalent dose alone would overstate the biological risks to the entire organism.\n\nEffects of radiation\nMost adverse health effects of radiation exposure may be grouped in two general categories:\n\ndeterministic effects (harmful tissue reactions) due in large part to the killing\/malfunction of cells following high doses;\nstochastic effects, i.e., cancer and heritable effects involving either cancer development in exposed individuals owing to mutation of somatic cells or heritable disease in their offspring owing to mutation of reproductive (germ) cells.\nThe added lifetime risk of developing cancer by a single abdominal CT of 8 mSv is estimated to be 0.05%, or 1 one in 2,000.\nBecause of increased susceptibility of fetuses to radiation exposure, the radiation dosage of a CT scan is an important consideration in the choice of medical imaging in pregnancy.\n\nExcess doses\nIn October, 2009, the US Food and Drug Administration (FDA) initiated an investigation of brain perfusion CT (PCT) scans, based on radiation burns caused by incorrect settings at one particular facility for this particular type of CT scan. Over 200 patients were exposed to radiation at approximately eight times the expected dose for an 18-month period; over 40% of them lost patches of hair. This event prompted a call for increased CT quality assurance programs. It was noted that \"while unnecessary radiation exposure should be avoided, a medically needed CT scan obtained with appropriate acquisition parameter has benefits that outweigh the radiation risks.\" Similar problems have been reported at other centers. These incidents are believed to be due to human error.\n\nProcedure\nCT scan procedure varies according to the type of the study and the organ being imaged. The patient is made to lie on the CT table and the centering of the table is done according to the body part. The IV line is established in case of contrast-enhanced CT. After selecting proper and rate of contrast from the pressure injector, the scout is taken to localize and plan the scan. Once the plan is selected, the contrast is given. The raw data is processed according to the study and proper windowing is done to make scans easy to diagnose.\n\nPreparation\nPatient preparation may vary according to the type of scan. The general patient preparation includes.\n\nSigning the informed consent.\nRemoval of metallic objects and jewelry from the region of interest.\nChanging to the hospital gown according to hospital protocol.\nChecking of kidney function, especially creatinine and urea levels (in case of CECT).\n\nMechanism\nComputed tomography operates by using an X-ray generator that rotates around the object; X-ray detectors are positioned on the opposite side of the circle from the X-ray source. As the X-rays pass through the patient, they are attenuated differently by various tissues according to the tissue density. A visual representation of the raw data obtained is called a sinogram, yet it is not sufficient for interpretation. Once the scan data has been acquired, the data must be processed using a form of tomographic reconstruction, which produces a series of cross-sectional images. These cross-sectional images are made up of small units of pixels or voxels.\nPixels in an image obtained by CT scanning are displayed in terms of relative radiodensity. The pixel itself is displayed according to the mean attenuation of the tissue(s) that it corresponds to on a scale from +3,071 (most attenuating) to \u22121,024 (least attenuating) on the Hounsfield scale. A pixel is a two dimensional unit based on the matrix size and the field of view. When the CT slice thickness is also factored in, the unit is known as a voxel, which is a three-dimensional unit.\nWater has an attenuation of 0 Hounsfield units (HU), while air is \u22121,000 HU, cancellous bone is typically +400 HU, and cranial bone can reach 2,000 HU or more (os temporale) and can cause artifacts. The attenuation of metallic implants depends on the atomic number of the element used: Titanium usually has an amount of +1000 HU, iron steel can completely extinguish the X-ray and is, therefore, responsible for well-known line-artifacts in computed tomograms. Artifacts are caused by abrupt transitions between low- and high-density materials, which results in data values that exceed the dynamic range of the processing electronics. Two-dimensional CT images are conventionally rendered so that the view is as though looking up at it from the patient's feet. Hence, the left side of the image is to the patient's right and vice versa, while anterior in the image also is the patient's anterior and vice versa. This left-right interchange corresponds to the view that physicians generally have in reality when positioned in front of patients.\nInitially, the images generated in CT scans were in the transverse (axial) anatomical plane, perpendicular to the long axis of the body. Modern scanners allow the scan data to be reformatted as images in other planes. Digital geometry processing can generate a three-dimensional image of an object inside the body from a series of two-dimensional radiographic images taken by rotation around a fixed axis. These cross-sectional images are widely used for medical diagnosis and therapy.\n\nContrast\nContrast media used for X-ray CT, as well as for plain film X-ray, are called radiocontrasts. Radiocontrasts for CT are, in general, iodine-based. This is useful to highlight structures such as blood vessels that otherwise would be difficult to delineate from their surroundings. Using contrast material can also help to obtain functional information about tissues. Often, images are taken both with and without radiocontrast.\n\nHistory\nThe history of X-ray computed tomography goes back to at least 1917 with the mathematical theory of the Radon transform. In October 1963, William H. Oldendorf received a U.S. patent for a \"radiant energy apparatus for investigating selected areas of interior objects obscured by dense material\". The first commercially viable CT scanner was invented by Godfrey Hounsfield in 1972.\nIt is often claimed that revenues from the sales of The Beatles' records in the 1960s helped fund the development of the first CT scanner at EMI. The first production X-ray CT machines were in fact called EMI scanners.\n\nEtymology\nThe word tomography is derived from the Greek tome 'slice' and graphein 'to write'. Computed tomography was originally known as the \"EMI scan\" as it was developed in the early 1970s at a research branch of EMI, a company best known today for its music and recording business. It was later known as computed axial tomography (CAT or CT scan) and body section r\u00f6ntgenography.\nThe term CAT scan is no longer in technical use because current CT scans enable for multiplanar reconstructions. This makes CT scan the most appropriate term, which is used by radiologists in common vernacular as well as in textbooks and scientific papers.\nIn Medical Subject Headings (MeSH), computed axial tomography was used from 1977 to 1979, but the current indexing explicitly includes X-ray in the title.\nThe term sinogram was introduced by Paul Edholm and Bertil Jacobson in 1975.\n\nSociety and culture\nCampaigns\nIn response to increased concern by the public and the ongoing progress of best practices, the Alliance for Radiation Safety in Pediatric Imaging was formed within the Society for Pediatric Radiology. In concert with the American Society of Radiologic Technologists, the American College of Radiology and the American Association of Physicists in Medicine, the Society for Pediatric Radiology developed and launched the Image Gently Campaign which is designed to maintain high-quality imaging studies while using the lowest doses and best radiation safety practices available on pediatric patients. This initiative has been endorsed and applied by a growing list of various professional medical organizations around the world and has received support and assistance from companies that manufacture equipment used in Radiology.\nFollowing upon the success of the Image Gently campaign, the American College of Radiology, the Radiological Society of North America, the American Association of Physicists in Medicine and the American Society of Radiologic Technologists have launched a similar campaign to address this issue in the adult population called Image Wisely.\nThe World Health Organization and International Atomic Energy Agency (IAEA) of the United Nations have also been working in this area and have ongoing projects designed to broaden best practices and lower patient radiation dose.\n\nPrevalence\nUse of CT has increased dramatically over the last two decades. An estimated 72 million scans were performed in the United States in 2007, accounting for close to half of the total per-capita dose rate from radiologic and nuclear medicine procedures. Of the CT scans, six to eleven percent are done in children, an increase of seven to eightfold from 1980. Similar increases have been seen in Europe and Asia. In Calgary, Canada, 12.1% of people who present to the emergency with an urgent complaint received a CT scan, most commonly either of the head or of the abdomen. The percentage who received CT, however, varied markedly by the emergency physician who saw them from 1.8% to 25%. In the emergency department in the United States, CT or MRI imaging is done in 15% of people who present with injuries as of 2007 (up from 6% in 1998).\nThe increased use of CT scans has been the greatest in two fields: screening of adults (screening CT of the lung in smokers, virtual colonoscopy, CT cardiac screening, and whole-body CT in asymptomatic patients) and CT imaging of children. Shortening of the scanning time to around 1 second, eliminating the strict need for the subject to remain still or be sedated, is one of the main reasons for the large increase in the pediatric population (especially for the diagnosis of appendicitis). As of 2007, in the United States a proportion of CT scans are performed unnecessarily. Some estimates place this number at 30%. There are a number of reasons for this including: legal concerns, financial incentives, and desire by the public. For example, some healthy people avidly pay to receive full-body CT scans as screening. In that case, it is not at all clear that the benefits outweigh the risks and costs. Deciding whether and how to treat incidentalomas is complex, radiation exposure is not negligible, and the money for the scans involves opportunity cost.\n\nManufacturers\nMajor manufacturers of CT scanning devices and equipment are:\n\n GE HealthCare\n Siemens Healthineers\n Canon Medical Systems Corporation (formerly Toshiba Medical Systems)\n Koninklijke Philips N.V.\n Fujifilm Healthcare (formerly Hitachi Medical Systems)\n Neusoft Medical Systems\n United Imaging\n\nResearch\nPhoton-counting computed tomography is a CT technique currently under development. Typical CT scanners use energy integrating detectors; photons are measured as a voltage on a capacitor which is proportional to the X-rays detected. However, this technique is susceptible to noise and other factors which can affect the linearity of the voltage to X-ray intensity relationship. Photon counting detectors (PCDs) are still affected by noise but it does not change the measured counts of photons. PCDs have several potential advantages, including improving signal (and contrast) to noise ratios, reducing doses, improving spatial resolution, and through use of several energies, distinguishing multiple contrast agents. PCDs have only recently become feasible in CT scanners due to improvements in detector technologies that can cope with the volume and rate of data required. As of February 2016, photon counting CT is in use at three sites. Some early research has found the dose reduction potential of photon counting CT for breast imaging to be very promising. In view of recent findings of high cumulative doses to patients from recurrent CT scans, there has been a push for scanning technologies and techniques that reduce ionising radiation doses to patients to sub-milliSievert (sub-mSv in the literature) levels during the CT scan process, a goal that has been lingering.\n\nSee also\nReferences\nExternal links\n\nDevelopment of CT imaging\nCT Artefacts\u2014PPT by David Platten\nFiller A (2009-06-30). \"The History, Development and Impact of Computed Imaging in Neurological Diagnosis and Neurosurgery: CT, MRI, and DTI\". Nature Precedings: 1. doi:10.1038\/npre.2009.3267.4. ISSN 1756-0357.\nBoone JM, McCollough CH (2021). \"Computed tomography turns 50\". Physics Today. 74 (9): 34\u201340. Bibcode:2021PhT....74i..34B. doi:10.1063\/PT.3.4834. ISSN 0031-9228. S2CID 239718717.","28":"Calcium carbonate is a chemical compound with the chemical formula CaCO3. It is a common substance found in rocks as the minerals calcite and aragonite, most notably in chalk and limestone, eggshells, gastropod shells, shellfish skeletons and pearls. Materials containing much calcium carbonate or resembling it are described as calcareous. Calcium carbonate is the active ingredient in agricultural lime and is produced when calcium ions in hard water react with carbonate ions to form limescale. It has medical use as a calcium supplement or as an antacid, but excessive consumption can be hazardous and cause hypercalcemia and digestive issues.\n\nChemistry\nCalcium carbonate shares the typical properties of other carbonates. Notably it\n\nreacts with acids, releasing carbonic acid which quickly disintegrates into carbon dioxide and water:\nCaCO3(s) + 2 H+(aq) \u2192 Ca2+(aq) + CO2(g) + H2O(l)\nreleases carbon dioxide upon heating, called a thermal decomposition reaction, or calcination (to above 840 \u00b0C in the case of CaCO3), to form calcium oxide, CaO, commonly called quicklime, with reaction enthalpy 178 kJ\/mol:\nCaCO3(s) \u2192 CaO(s) + CO2(g)\nreacts with gaseous hydrogen to form methane and water vapor plus solid calcium oxide or calcium hydroxide depending on temperature and product gas composition. Various metals including palladium and nickel are catalysts for the reaction.\nCalcium carbonate reacts with water that is saturated with carbon dioxide to form the soluble calcium bicarbonate.\n\nCaCO3(s) + CO2(g) + H2O(l) \u2192 Ca(HCO3)2(aq)\nThis reaction is important in the erosion of carbonate rock, forming caverns, and leads to hard water in many regions.\nAn unusual form of calcium carbonate is the hexahydrate ikaite, CaCO3\u00b76H2O. Ikaite is stable only below 8 \u00b0C.\n\nPreparation\nThe vast majority of calcium carbonate used in industry is extracted by mining or quarrying. Pure calcium carbonate (such as for food or pharmaceutical use), can be produced from a pure quarried source (usually marble).\nAlternatively, calcium carbonate is prepared from calcium oxide. Water is added to give calcium hydroxide then carbon dioxide is passed through this solution to precipitate the desired calcium carbonate, referred to in the industry as precipitated calcium carbonate (PCC) This process is called carbonatation:\n\nCaO + H2O \u2192 Ca(OH)2\nCa(OH)2 + CO2 \u2192 CaCO3 + H2O\nIn a laboratory, calcium carbonate can easily be crystallized from calcium chloride (CaCl2), by placing an aqueous solution of CaCl2 in a desiccator alongside ammonium carbonate [NH4]2CO3. In the desiccator, ammonium carbonate is exposed to air and decomposes into ammonia, carbon dioxide, and water. The carbon dioxide then diffuses into the aqueous solution of calcium chloride, reacts with the calcium ions and the water, and forms calcium carbonate.\n\nStructure\nThe thermodynamically stable form of CaCO3 under normal conditions is hexagonal \u03b2-CaCO3 (the mineral calcite). Other forms can be prepared, the denser (2.83 g\/cm3) orthorhombic \u03bb-CaCO3 (the mineral aragonite) and hexagonal \u03bc-CaCO3, occurring as the mineral vaterite. The aragonite form can be prepared by precipitation at temperatures above 85 \u00b0C; the vaterite form can be prepared by precipitation at 60 \u00b0C. Calcite contains calcium atoms coordinated by six oxygen atoms; in aragonite they are coordinated by nine oxygen atoms. The vaterite structure is not fully understood. Magnesium carbonate (MgCO3) has the calcite structure, whereas strontium carbonate (SrCO3) and barium carbonate (BaCO3) adopt the aragonite structure, reflecting their larger ionic radii.\n\nPolymorphs\nCalcium carbonate crystallizes in three anhydrous polymorphs, of which calcite is the thermodynamically most stable at room temperature, aragonite is only slightly less so, and vaterite is the least stable.\n\nCrystal structure\nThe calcite crystal structure is trigonal, with space group R3c (No. 167 in the International Tables for Crystallography), and Pearson symbol hR10. Aragonite is orthorhombic, with space group Pmcn (No 62), and Pearson Symbol oP20. Vaterite is composed of at least two different coexisting crystallographic structures. The major structure exhibits hexagonal symmetry in space group P63\/mmc, the minor structure is still unknown.\n\nCrystallization\nAll three polymorphs crystallize simultaneously from aqueous solutions under ambient conditions. In additive-free aqueous solutions, calcite forms easily as the major product, while aragonite appears only as a minor product.\nAt high saturation, vaterite is typically the first phase precipitated, which is followed by a transformation of the vaterite to calcite. This behavior seems to follow Ostwald's rule, in which the least stable polymorph crystallizes first, followed by the crystallization of different polymorphs via a sequence of increasingly stable phases. However, aragonite, whose stability lies between those of vaterite and calcite, seems to be the exception to this rule, as aragonite does not form as a precursor to calcite under ambient conditions.\n\nAragonite occurs in majority when the reaction conditions inhibit the formation of calcite and\/or promote the nucleation of aragonite. For example, the formation of aragonite is promoted by the presence of magnesium ions, or by using proteins and peptides derived from biological calcium carbonate. Some polyamines such as cadaverine and Poly(ethylene imine) have been shown to facilitate the formation of aragonite over calcite.\n\nSelection by organisms\nOrganisms, such as molluscs and arthropods, have shown the ability to grow all three crystal polymorphs of calcium carbonate, mainly as protection (shells) and muscle attachments. Moreover, they exhibit a remarkable capability of phase selection over calcite and aragonite, and some organisms can switch between the two polymorphs. The ability of phase selection is usually attributed to the use of specific macromolecules or combinations of macromolecules by such organisms.\n\nOccurrence\nGeological sources\nCalcite, aragonite and vaterite are pure calcium carbonate minerals. Industrially important source rocks which are predominantly calcium carbonate include limestone, chalk, marble and travertine.\n\nBiological sources\nEggshells, snail shells and most seashells are predominantly calcium carbonate and can be used as industrial sources of that chemical. Oyster shells have enjoyed recent recognition as a source of dietary calcium, but are also a practical industrial source. Dark green vegetables such as broccoli and kale contain dietarily significant amounts of calcium carbonate, but they are not practical as an industrial source.\nAnnelids in the family Lumbricidae, earthworms, possess a regionalization of the digestive track called calciferous glands, Kalkdr\u00fcsen, or glandes de Morren, that processes calcium and CO2 into calcium carbonate, which is later excreted into the dirt. The function of these glands is unknown but is believed to serve as a CO2 regulation mechanism within the animals' tissues. This process is ecologically significant, stabilizing the pH of acid soils.\n\nExtraterrestrial\nBeyond Earth, strong evidence suggests the presence of calcium carbonate on Mars. Signs of calcium carbonate have been detected at more than one location (notably at Gusev and Huygens craters). This provides some evidence for the past presence of liquid water.\n\nGeology\nCarbonate is found frequently in geologic settings and constitutes an enormous carbon reservoir. Calcium carbonate occurs as aragonite, calcite and dolomite as significant constituents of the calcium cycle. The carbonate minerals form the rock types: limestone, chalk, marble, travertine, tufa, and others.\n\nIn warm, clear tropical waters corals are more abundant than towards the poles where the waters are cold. Calcium carbonate contributors, including plankton (such as coccoliths and planktic foraminifera), coralline algae, sponges, brachiopods, echinoderms, bryozoa and mollusks, are typically found in shallow water environments where sunlight and filterable food are more abundant. Cold-water carbonates do exist at higher latitudes but have a very slow growth rate. The calcification processes are changed by ocean acidification.\nWhere the oceanic crust is subducted under a continental plate sediments will be carried down to warmer zones in the asthenosphere and lithosphere. Under these conditions calcium carbonate decomposes to produce carbon dioxide which, along with other gases, give rise to explosive volcanic eruptions.\n\nCarbonate compensation depth\nThe carbonate compensation depth (CCD) is the point in the ocean where the rate of precipitation of calcium carbonate is balanced by the rate of dissolution due to the conditions present. Deep in the ocean, the temperature drops and pressure increases. Increasing pressure also increases the solubility of calcium carbonate. Calcium carbonate is unusual in that its solubility increases with decreasing temperature. The carbonate compensation depth ranges from 4,000 to 6,000 meters below sea level in modern oceans, and the various polymorphs (calcite, aragonite) have different compensation depths based on their stability.\n\nRole in taphonomy\nCalcium carbonate can preserve fossils through permineralization. Most of the vertebrate fossils of the Two Medicine Formation\u2014a geologic formation known for its duck-billed dinosaur eggs\u2014are preserved by CaCO3 permineralization. This type of preservation conserves high levels of detail, even down to the microscopic level. However, it also leaves specimens vulnerable to weathering when exposed to the surface.\nTrilobite populations were once thought to have composed the majority of aquatic life during the Cambrian, due to the fact that their calcium carbonate-rich shells were more easily preserved than those of other species, which had purely chitinous shells.\n\nUses\nConstruction\nThe main use of calcium carbonate is in the construction industry, either as a building material, or limestone aggregate for road building, as an ingredient of cement, or as the starting material for the preparation of builders' lime by burning in a kiln. However, because of weathering mainly caused by acid rain, calcium carbonate (in limestone form) is no longer used for building purposes on its own, but only as a raw primary substance for building materials.\nCalcium carbonate is also used in the purification of iron from iron ore in a blast furnace. The carbonate is calcined in situ to give calcium oxide, which forms a slag with various impurities present, and separates from the purified iron.\nIn the oil industry, calcium carbonate is added to drilling fluids as a formation-bridging and filtercake-sealing agent; it is also a weighting material which increases the density of drilling fluids to control the downhole pressure. Calcium carbonate is added to swimming pools, as a pH corrector for maintaining alkalinity and offsetting the acidic properties of the disinfectant agent.\nIt is also used as a raw material in the refining of sugar from sugar beet; it is calcined in a kiln with anthracite to produce calcium oxide and carbon dioxide. This burnt lime is then slaked in fresh water to produce a calcium hydroxide suspension for the precipitation of impurities in raw juice during carbonatation.\nCalcium carbonate in the form of chalk has traditionally been a major component of blackboard chalk. However, modern manufactured chalk is mostly gypsum, hydrated calcium sulfate CaSO4\u00b72H2O. Calcium carbonate is a main source for growing biorock. Precipitated calcium carbonate (PCC), pre-dispersed in slurry form, is a common filler material for latex gloves with the aim of achieving maximum saving in material and production costs.\nFine ground calcium carbonate (GCC) is an essential ingredient in the microporous film used in diapers and some building films, as the pores are nucleated around the calcium carbonate particles during the manufacture of the film by biaxial stretching. GCC and PCC are used as a filler in paper because they are cheaper than wood fiber. Printing and writing paper can contain 10\u201320% calcium carbonate. In North America, calcium carbonate has begun to replace kaolin in the production of glossy paper. Europe has been practicing this as alkaline papermaking or acid-free papermaking for some decades. PCC used for paper filling and paper coatings is precipitated and prepared in a variety of shapes and sizes having characteristic narrow particle size distributions and equivalent spherical diameters of 0.4 to 3 micrometers.\nCalcium carbonate is widely used as an extender in paints, in particular matte emulsion paint where typically 30% by weight of the paint is either chalk or marble. It is also a popular filler in plastics. Some typical examples include around 15\u201320% loading of chalk in unplasticized polyvinyl chloride (uPVC) drainpipes, 5\u201315% loading of stearate-coated chalk or marble in uPVC window profile. PVC cables can use calcium carbonate at loadings of up to 70 phr (parts per hundred parts of resin) to improve mechanical properties (tensile strength and elongation) and electrical properties (volume resistivity). Polypropylene compounds are often filled with calcium carbonate to increase rigidity, a requirement that becomes important at high usage temperatures. Here the percentage is often 20\u201340%. It also routinely used as a filler in thermosetting resins (sheet and bulk molding compounds) and has also been mixed with ABS, and other ingredients, to form some types of compression molded \"clay\" poker chips. Precipitated calcium carbonate, made by dropping calcium oxide into water, is used by itself or with additives as a white paint, known as whitewashing.\nCalcium carbonate is added to a wide range of trade and do it yourself adhesives, sealants, and decorating fillers. Ceramic tile adhesives typically contain 70% to 80% limestone. Decorating crack fillers contain similar levels of marble or dolomite. It is also mixed with putty in setting stained glass windows, and as a resist to prevent glass from sticking to kiln shelves when firing glazes and paints at high temperature.\nIn ceramic glaze applications, calcium carbonate is known as whiting, and is a common ingredient for many glazes in its white powdered form. When a glaze containing this material is fired in a kiln, the whiting acts as a flux material in the glaze. Ground calcium carbonate is an abrasive (both as scouring powder and as an ingredient of household scouring creams), in particular in its calcite form, which has the relatively low hardness level of 3 on the Mohs scale, and will therefore not scratch glass and most other ceramics, enamel, bronze, iron, and steel, and have a moderate effect on softer metals like aluminium and copper. A paste made from calcium carbonate and deionized water can be used to clean tarnish on silver.\n\nHealth and diet\nCalcium carbonate is widely used medicinally as an inexpensive dietary calcium supplement for gastric antacid (such as Tums and Eno). It may be used as a phosphate binder for the treatment of hyperphosphatemia (primarily in patients with chronic kidney failure). It is used in the pharmaceutical industry as an inert filler for tablets and other pharmaceuticals.\nCalcium carbonate is used in the production of calcium oxide as well as toothpaste and has seen a resurgence as a food preservative and color retainer, when used in or with products such as organic apples.\nCalcium carbonate is used therapeutically as phosphate binder in patients on maintenance haemodialysis. It is the most common form of phosphate binder prescribed, particularly in non-dialysis chronic kidney disease. Calcium carbonate is the most commonly used phosphate binder, but clinicians are increasingly prescribing the more expensive, non-calcium-based phosphate binders, particularly sevelamer.\nExcess calcium from supplements, fortified food, and high-calcium diets can cause milk-alkali syndrome, which has serious toxicity and can be fatal. In 1915, Bertram Sippy introduced the \"Sippy regimen\" of hourly ingestion of milk and cream, and the gradual addition of eggs and cooked cereal, for 10 days, combined with alkaline powders, which provided symptomatic relief for peptic ulcer disease. Over the next several decades, the Sippy regimen resulted in kidney failure, alkalosis, and hypercalcaemia, mostly in men with peptic ulcer disease. These adverse effects were reversed when the regimen stopped, but it was fatal in some patients with protracted vomiting. Milk-alkali syndrome declined in men after effective treatments for peptic ulcer disease arose. Since the 1990s it has been most frequently reported in women taking calcium supplements above the recommended range of 1.2 to 1.5 grams daily, for prevention and treatment of osteoporosis, and is exacerbated by dehydration. Calcium has been added to over-the-counter products, which contributes to inadvertent excessive intake. Excessive calcium intake can lead to hypercalcemia, complications of which include vomiting, abdominal pain and altered mental status.\nAs a food additive it is designated E170, and it has an INS number of 170. Used as an acidity regulator, anticaking agent, stabilizer or color it is approved for usage in the EU, US and Australia and New Zealand. It is \"added by law to all UK milled bread flour except wholemeal\". It is used in some soy milk and almond milk products as a source of dietary calcium; at least one study suggests that calcium carbonate might be as bioavailable as the calcium in cow's milk. Calcium carbonate is also used as a firming agent in many canned and bottled vegetable products.\nSeveral calcium supplement formulations have been documented to contain the chemical element lead, posing a public health concern. Lead is commonly found in natural sources of calcium.\n\nAgriculture and aquaculture\nAgricultural lime, powdered chalk or limestone, is used as a cheap method of neutralising acidic soil, making it suitable for planting, also used in aquaculture industry for pH regulation of pond soil before initiating culture. There is interest in understanding whether or not it can affect pesticide adsorption and desorption in calcareous soil.\n\nHousehold cleaning\nCalcium carbonate is a key ingredient in many household cleaning powders like Comet and is used as a scrubbing agent.\n\nPollution mitigation\nIn 1989, a researcher, Ken Simmons, introduced CaCO3 into the Whetstone Brook in Massachusetts. His hope was that the calcium carbonate would counter the acid in the stream from acid rain and save the trout that had ceased to spawn. Although his experiment was a success, it did increase the amount of aluminium ions in the area of the brook that was not treated with the limestone. This shows that CaCO3 can be added to neutralize the effects of acid rain in river ecosystems. Currently calcium carbonate is used to neutralize acidic conditions in both soil and water. Since the 1970s, such liming has been practiced on a large scale in Sweden to mitigate acidification and several thousand lakes and streams are limed repeatedly.\nCalcium carbonate is also used in flue-gas desulfurization applications eliminating harmful SO2 and NO2 emissions from coal and other fossil fuels burnt in large fossil fuel power stations.\n\nPlastics\nCalcium carbonate is commonly used in the plastic industry as a filler. When it is incorporated in a plastic material, it can improve the hardness, stiffness, dimensional stability and processability of the material.\n\nCalcination equilibrium\nCalcination of limestone using charcoal fires to produce quicklime has been practiced since antiquity by cultures all over the world. The temperature at which limestone yields calcium oxide is usually given as 825 \u00b0C, but stating an absolute threshold is misleading. Calcium carbonate exists in equilibrium with calcium oxide and carbon dioxide at any temperature. At each temperature there is a partial pressure of carbon dioxide that is in equilibrium with calcium carbonate. At room temperature the equilibrium overwhelmingly favors calcium carbonate, because the equilibrium CO2 pressure is only a tiny fraction of the partial CO2 pressure in air, which is about 0.035 kPa.\nAt temperatures above 550 \u00b0C the equilibrium CO2 pressure begins to exceed the CO2 pressure in air. So above 550 \u00b0C, calcium carbonate begins to outgas CO2 into air. However, in a charcoal fired kiln, the concentration of CO2 will be much higher than it is in air. Indeed, if all the oxygen in the kiln is consumed in the fire, then the partial pressure of CO2 in the kiln can be as high as 20 kPa.\n\nThe table shows that this partial pressure is not achieved until the temperature is nearly 800 \u00b0C. For the outgassing of CO2 from calcium carbonate to happen at an economically useful rate, the equilibrium pressure must significantly exceed the ambient pressure of CO2. And for it to happen rapidly, the equilibrium pressure must exceed total atmospheric pressure of 101 kPa, which happens at 898 \u00b0C.\n\nSolubility\nWith varying CO2 pressure\nCalcium carbonate is poorly soluble in pure water (47 mg\/L at normal atmospheric CO2 partial pressure as shown below).\nThe equilibrium of its solution is given by the equation (with dissolved calcium carbonate on the right):\n\nwhere the solubility product for [Ca2+][CO2\u22123] is given as anywhere from Ksp = 3.7\u00d710\u22129 to Ksp = 8.7\u00d710\u22129 at 25 \u00b0C, depending upon the data source. What the equation means is that the product of molar concentration of calcium ions (moles of dissolved Ca2+ per liter of solution) with the molar concentration of dissolved CO2\u22123 cannot exceed the value of Ksp. This seemingly simple solubility equation, however, must be taken along with the more complicated equilibrium of carbon dioxide with water (see carbonic acid). Some of the CO2\u22123 combines with H+ in the solution according to\n\nHCO\u22123 is known as the bicarbonate ion. Calcium bicarbonate is many times more soluble in water than calcium carbonate\u2014indeed it exists only in solution.\nSome of the HCO\u22123 combines with H+ in solution according to\n\nSome of the H2CO3 breaks up into water and dissolved carbon dioxide according to\n\nAnd dissolved carbon dioxide is in equilibrium with atmospheric carbon dioxide according to\n\nFor ambient air, PCO2 is around 3.5\u00d710\u22124 atm (or equivalently 35 Pa). The last equation above fixes the concentration of dissolved CO2 as a function of PCO2, independent of the concentration of dissolved CaCO3. At atmospheric partial pressure of CO2, dissolved CO2 concentration is 1.2\u00d710\u22125 moles per liter. The equation before that fixes the concentration of H2CO3 as a function of CO2 concentration. For [CO2] = 1.2\u00d710\u22125, it results in [H2CO3] = 2.0\u00d710\u22128 moles per liter. When [H2CO3] is known, the remaining three equations together with\n\n(which is true for all aqueous solutions), and the constraint that the solution must be electrically neutral, i.e., the overall charge of dissolved positive ions [Ca2+] + 2 [H+] must be cancelled out by the overall charge of dissolved negative ions [HCO\u22123] + [CO2\u22123] + [OH\u2212], make it possible to solve simultaneously for the remaining five unknown concentrations (the previously mentioned form of the neutrality is valid only if calcium carbonate has been put in contact with pure water or with a neutral pH solution; in the case where the initial water solvent pH is not neutral, the balance is not neutral).\nThe adjacent table shows the result for [Ca2+] and [H+] (in the form of pH) as a function of ambient partial pressure of CO2 (Ksp = 4.47\u00d710\u22129 has been taken for the calculation).\n\nAt atmospheric levels of ambient CO2 the table indicates that the solution will be slightly alkaline with a maximum CaCO3 solubility of 47 mg\/L.\nAs ambient CO2 partial pressure is reduced below atmospheric levels, the solution becomes more and more alkaline. At extremely low PCO2, dissolved CO2, bicarbonate ion, and carbonate ion largely evaporate from the solution, leaving a highly alkaline solution of calcium hydroxide, which is more soluble than CaCO3. For PCO2 = 10\u221212 atm, the [Ca2+][OH\u2212]2 product is still below the solubility product of Ca(OH)2 (8\u00d710\u22126). For still lower CO2 pressure, Ca(OH)2 precipitation will occur before CaCO3 precipitation.\nAs ambient CO2 partial pressure increases to levels above atmospheric, pH drops, and much of the carbonate ion is converted to bicarbonate ion, which results in higher solubility of Ca2+.\nThe effect of the latter is especially evident in day-to-day life of people who have hard water. Water in aquifers underground can be exposed to levels of CO2 much higher than atmospheric. As such water percolates through calcium carbonate rock, the CaCO3 dissolves according to one of the trends above. When that same water then emerges from the tap, in time it comes into equilibrium with CO2 levels in the air by outgassing its excess CO2. The calcium carbonate becomes less soluble as a result, and the excess precipitates as lime scale. This same process is responsible for the formation of stalactites and stalagmites in limestone caves.\nTwo hydrated phases of calcium carbonate, monohydrocalcite CaCO3\u00b7H2O and ikaite CaCO3\u00b76H2O, may precipitate from water at ambient conditions and persist as metastable phases.\n\nWith varying pH, temperature and salinity: CaCO3 scaling in swimming pools\nIn contrast to the open equilibrium scenario above, many swimming pools are managed by addition of sodium bicarbonate (NaHCO3) to the concentration of about 2 mmol\/L as a buffer, then control of pH through use of HCl, NaHSO4, Na2CO3, NaOH or chlorine formulations that are acidic or basic. In this situation, dissolved inorganic carbon (total inorganic carbon) is far from equilibrium with atmospheric CO2. Progress towards equilibrium through outgassing of CO2 is slowed by\n\nIn this situation, the dissociation constants for the much faster reactions\n\nH2CO3 \u21cc H+ + HCO\u22123 \u21cc 2 H+ + CO2\u22123\nallow the prediction of concentrations of each dissolved inorganic carbon species in solution, from the added concentration of HCO\u22123 (which constitutes more than 90% of Bjerrum plot species from pH 7 to pH 8 at 25 \u00b0C in fresh water). Addition of HCO\u22123 will increase CO2\u22123 concentration at any pH. Rearranging the equations given above, we can see that [Ca2+] = \u2060Ksp\/[CO2\u22123]\u2060, and [CO2\u22123] = \u2060Ka2 [HCO\u22123]\/[H+]\u2060. Therefore, when HCO\u22123 concentration is known, the maximum concentration of Ca2+ ions before scaling through CaCO3 precipitation can be predicted from the formula:\n\n[Ca2+]max = \u2060Ksp\/Ka2\u2060 \u00d7 \u2060[H+]\/[HCO\u22123]\u2060\nThe solubility product for CaCO3 (Ksp) and the dissociation constants for the dissolved inorganic carbon species (including Ka2) are all substantially affected by temperature and salinity, with the overall effect that [Ca2+]max increases from freshwater to saltwater, and decreases with rising temperature, pH, or added bicarbonate level, as illustrated in the accompanying graphs.\nThe trends are illustrative for pool management, but whether scaling occurs also depends on other factors including interactions with Mg2+, [B(OH)4]\u2212 and other ions in the pool, as well as supersaturation effects. Scaling is commonly observed in electrolytic chlorine generators, where there is a high pH near the cathode surface and scale deposition further increases temperature. This is one reason that some pool operators prefer borate over bicarbonate as the primary pH buffer, and avoid the use of pool chemicals containing calcium.\n\nSolubility in a strong or weak acid solution\nSolutions of strong (HCl), moderately strong (sulfamic) or weak (acetic, citric, sorbic, lactic, phosphoric) acids are commercially available. They are commonly used as descaling agents to remove limescale deposits. The maximum amount of CaCO3 that can be \"dissolved\" by one liter of an acid solution can be calculated using the above equilibrium equations.\n\nIn the case of a strong monoacid with decreasing acid concentration [A] = [A\u2212], we obtain (with CaCO3 molar mass = 100 g\/mol):\n\nwhere the initial state is the acid solution with no Ca2+ (not taking into account possible CO2 dissolution) and the final state is the solution with saturated Ca2+. For strong acid concentrations, all species have a negligible concentration in the final state with respect to Ca2+ and A\u2212 so that the neutrality equation reduces approximately to 2[Ca2+] = [A\u2212] yielding [Ca2+] \u2248 0.5 [A\u2212]. When the concentration decreases, [HCO\u22123] becomes non-negligible so that the preceding expression is no longer valid. For vanishing acid concentrations, one can recover the final pH and the solubility of CaCO3 in pure water.\nIn the case of a weak monoacid (here we take acetic acid with pKa = 4.76) with decreasing total acid concentration [A] = [A\u2212] + [AH], we obtain:\n\nFor the same total acid concentration, the initial pH of the weak acid is less acid than the one of the strong acid; however, the maximum amount of CaCO3 which can be dissolved is approximately the same. This is because in the final state, the pH is larger than the pKa, so that the weak acid is almost completely dissociated, yielding in the end as many H+ ions as the strong acid to \"dissolve\" the calcium carbonate.\nThe calculation in the case of phosphoric acid (which is the most widely used for domestic applications) is more complicated since the concentrations of the four dissociation states corresponding to this acid must be calculated together with [HCO\u22123], [CO2\u22123], [Ca2+], [H+] and [OH\u2212]. The system may be reduced to a seventh degree equation for [H+] the numerical solution of which gives\n\nwhere [A] = [H3PO4] + [H2PO\u22124] + [HPO2\u22124] + [PO3\u22124] is the total acid concentration. Thus phosphoric acid is more efficient than a monoacid since at the final almost neutral pH, the second dissociated state concentration [HPO2\u22124] is not negligible (see phosphoric acid).\n\nSee also\nReferences\nExternal links\nInternational Chemical Safety Card 1193\nCID 516889 from PubChem\nATC codes: A02AC01 (WHO) and A12AA04 (WHO)\nThe British Calcium Carbonate Association \u2013 What is calcium carbonate Archived 24 May 2008 at the Wayback Machine\nCDC \u2013 NIOSH Pocket Guide to Chemical Hazards \u2013 Calcium Carbonate","29":"In medicine, the caloric reflex test (sometimes termed 'vestibular caloric stimulation') is a test of the vestibulo-ocular reflex that involves irrigating cold or warm water or air into the external auditory canal. This method was developed by Robert B\u00e1r\u00e1ny, who won a Nobel prize in 1914 for this discovery.\n\nUtility\nThe test is commonly used by physicians, audiologists and other trained professionals to validate a diagnosis of asymmetric function in the peripheral vestibular system. Calorics are usually a subtest of the electronystagmography (ENG) battery of tests. It is one of several tests which can be used to test for brain stem death.\nOne novel use of this test has been to provide temporary pain relief from phantom limb pains in amputees and paraplegics. It can also induce a temporary remission of anosognosia, the visual and personal aspects of hemispatial neglect, hemianesthesia, and other consequences of right hemispheric damage.\n\nTechnique and results\nIce cold or warm water or air is introduced into the external auditory canal, usually using a syringe. The temperature difference between the body and the injected water creates a convective current in the endolymph of the nearby lateral semicircular canal. Hot and cold water produce currents in opposite directions and therefore a horizontal nystagmus in opposite directions.\nIn patients with an intact brainstem: \n\nIf the water is warm (44 \u00b0C or above) endolymph in the ipsilateral horizontal canal rises, causing an increased rate of firing in the vestibular afferent nerve. This situation mimics a head turn to the ipsilateral side. Both eyes will turn toward the contralateral ear, with horizontal nystagmus (quick horizontal eye movements) to the ipsilateral ear.\nIf the water is cold, relative to body temperature (30 \u00b0C or below), the endolymph falls within the semicircular canal, decreasing the rate of vestibular afferent firing. This situation mimics a head turn to the contralateral side. The eyes then turn toward the ipsilateral ear, with horizontal nystagmus to the contralateral ear.\nAbsent reactive eye movement suggests vestibular weakness of the horizontal semicircular canal of the side being stimulated.\nIn comatose patients with cerebral damage, the fast phase of nystagmus will be absent as this is controlled by the cerebrum. As a result, using cold water irrigation will result in deviation of the eyes toward the ear being irrigated. If both phases are absent, this suggests the patient's brainstem reflexes are also damaged and carries a very poor prognosis.\n\nSee also\nBalance disorder\nInner ear\n\n\n== References ==","30":"Carbon monoxide poisoning typically occurs from breathing in carbon monoxide (CO) at excessive levels. Symptoms are often described as \"flu-like\" and commonly include headache, dizziness, weakness, vomiting, chest pain, and confusion. Large exposures can result in loss of consciousness, arrhythmias, seizures, or death. The classically described \"cherry red skin\" rarely occurs. Long-term complications may include chronic fatigue, trouble with memory, and movement problems.\nCO is a colorless and odorless gas which is initially non-irritating. It is produced during incomplete burning of organic matter. This can occur from motor vehicles, heaters, or cooking equipment that run on carbon-based fuels. Carbon monoxide primarily causes adverse effects by combining with hemoglobin to form carboxyhemoglobin (symbol COHb or HbCO) preventing the blood from carrying oxygen and expelling carbon dioxide as carbaminohemoglobin. Additionally, many other hemoproteins such as myoglobin, Cytochrome P450, and mitochondrial cytochrome oxidase are affected, along with other metallic and non-metallic cellular targets.\nDiagnosis is typically based on a HbCO level of more than 3% among nonsmokers and more than 10% among smokers. The biological threshold for carboxyhemoglobin tolerance is typically accepted to be 15% COHb, meaning toxicity is consistently observed at levels in excess of this concentration. The FDA has previously set a threshold of 14% COHb in certain clinical trials evaluating the therapeutic potential of carbon monoxide. In general, 30% COHb is considered severe carbon monoxide poisoning. The highest reported non-fatal carboxyhemoglobin level was 73% COHb.\nEfforts to prevent poisoning include carbon monoxide detectors, proper venting of gas appliances, keeping chimneys clean, and keeping exhaust systems of vehicles in good repair. Treatment of poisoning generally consists of giving 100% oxygen along with supportive care. This procedure is often carried out until symptoms are absent and the HbCO level is less than 3%\/10%.\nCarbon monoxide poisoning is relatively common, resulting in more than 20,000 emergency room visits a year in the United States. It is the most common type of fatal poisoning in many countries. In the United States, non-fire related cases result in more than 400 deaths a year. Poisonings occur more often in the winter, particularly from the use of portable generators during power outages. The toxic effects of CO have been known since ancient history. The discovery that hemoglobin is affected by CO emerged with an investigation by James Watt and Thomas Beddoes into the therapeutic potential of hydrocarbonate in 1793, and later confirmed by Claude Bernard between 1846 and 1857.\n\nBackground\nCarbon monoxide is not toxic to all forms of life, and the toxicity is a classical dose-dependent example of hormesis. Small amounts of carbon monoxide are naturally produced through many enzymatic and non-enzymatic reactions across phylogenetic kingdoms where it can serve as an important neurotransmitter (subcategorized as a gasotransmitter) and a potential therapeutic agent. In the case of prokaryotes, some bacteria produce, consume and respond to carbon monoxide whereas certain other microbes are susceptible to its toxicity. Currently, there are no known adverse effects on photosynthesizing plants.\nThe harmful effects of carbon monoxide are generally considered to be due to tightly binding with the prosthetic heme moiety of hemoproteins that results in interference with cellular operations, for example: carbon monoxide binds with hemoglobin to form carboxyhemoglobin which affects gas exchange and cellular respiration. Inhaling excessive concentrations of the gas can lead to hypoxic injury, nervous system damage, and even death.\nAs pioneered by Esther Killick, different species and different people across diverse demographics may have different carbon monoxide tolerance levels. The carbon monoxide tolerance level for any person is altered by several factors, including genetics (hemoglobin mutations), behavior such as activity level, rate of ventilation, a pre-existing cerebral or cardiovascular disease, cardiac output, anemia, sickle cell disease and other hematological disorders, geography and barometric pressure, and metabolic rate.\n\nPhysiology\nCarbon monoxide is produced naturally by many physiologically relevant enzymatic and non-enzymatic reactions best exemplified by heme oxygenase catalyzing the biotransformation of heme (an iron protoporphyrin) into biliverdin and eventually bilirubin. Aside from physiological signaling, most carbon monoxide is stored as carboxyhemoglobin at non-toxic levels below 3% HbCO.\n\nTherapeutics\nSmall amounts of CO are beneficial and enzymes exist that produce it at times of oxidative stress. A variety of drugs are being developed to introduce small amounts of CO, these drugs are commonly called carbon monoxide-releasing molecules. Historically, the therapeutic potential of factitious airs, notably carbon monoxide as hydrocarbonate, was investigated by Thomas Beddoes, James Watt, Tiberius Cavallo, James Lind, Humphry Davy, and others in many labs such as the Pneumatic Institution.\n\nSigns and symptoms\nOn average, exposures at 100 ppm or greater is dangerous to human health. The WHO recommended levels of indoor CO exposure in 24 hours is 4 mg\/m3. Acute exposure should not exceed 10 mg\/m3 in 8 hours, 35 mg\/m3 in one hour and 100 mg\/m3 in 15 minutes.\n\nAcute poisoning\nThe main manifestations of carbon monoxide poisoning develop in the organ systems most dependent on oxygen use, the central nervous system and the heart. The initial symptoms of acute carbon monoxide poisoning include headache, nausea, malaise, and fatigue. These symptoms are often mistaken for a virus such as influenza or other illnesses such as food poisoning or gastroenteritis. Headache is the most common symptom of acute carbon monoxide poisoning; it is often described as dull, frontal, and continuous. Increasing exposure produces cardiac abnormalities including fast heart rate, low blood pressure, and cardiac arrhythmia; central nervous system symptoms include delirium, hallucinations, dizziness, unsteady gait, confusion, seizures, central nervous system depression, unconsciousness, respiratory arrest, and death. Less common symptoms of acute carbon monoxide poisoning include myocardial ischemia, atrial fibrillation, pneumonia, pulmonary edema, high blood sugar, lactic acidosis, muscle necrosis, acute kidney failure, skin lesions, and visual and auditory problems. Carbon monoxide exposure may lead to a significantly shorter life span due to heart damage.\nOne of the major concerns following acute carbon monoxide poisoning is the severe delayed neurological manifestations that may occur. Problems may include difficulty with higher intellectual functions, short-term memory loss, dementia, amnesia, psychosis, irritability, a strange gait, speech disturbances, Parkinson's disease-like syndromes, cortical blindness, and a depressed mood. Depression may occur in those who did not have pre-existing depression. These delayed neurological sequelae may occur in up to 50% of poisoned people after 2 to 40 days. It is difficult to predict who will develop delayed sequelae; however, advanced age, loss of consciousness while poisoned, and initial neurological abnormalities may increase the chance of developing delayed symptoms.\n\nChronic poisoning\nChronic exposure to relatively low levels of carbon monoxide may cause persistent headaches, lightheadedness, depression, confusion, memory loss, nausea, hearing disorders and vomiting. It is unknown whether low-level chronic exposure may cause permanent neurological damage. Typically, upon removal from exposure to carbon monoxide, symptoms usually resolve themselves, unless there has been an episode of severe acute poisoning. However, one case noted permanent memory loss and learning problems after a three-year exposure to relatively low levels of carbon monoxide from a faulty furnace.\nChronic exposure may worsen cardiovascular symptoms in some people. Chronic carbon monoxide exposure might increase the risk of developing atherosclerosis. Long-term exposures to carbon monoxide present the greatest risk to persons with coronary heart disease and in females who are pregnant.\nIn experimental animals, carbon monoxide appears to worsen noise-induced hearing loss at noise exposure conditions that would have limited effects on hearing otherwise. In humans, hearing loss has been reported following carbon monoxide poisoning. Unlike the findings in animal studies, noise exposure was not a necessary factor for the auditory problems to occur.\n\nFatal poisoning\nOne classic sign of carbon monoxide poisoning is more often seen in the dead rather than the living \u2013 people have been described as looking red-cheeked and healthy. However, since this \"cherry-red\" appearance is more common in the dead, it is not considered a useful diagnostic sign in clinical medicine. In autopsy examinations, the appearance of carbon monoxide poisoning is notable because unembalmed dead persons are normally bluish and pale, whereas dead carbon-monoxide poisoned people may appear unusually lifelike in coloration. The colorant effect of carbon monoxide in such postmortem circumstances is thus analogous to its use as a red colorant in the commercial meat-packing industry.\n\nEpidemiology\nThe true number of cases of carbon monoxide poisoning is unknown, since many non-lethal exposures go undetected. From the available data, carbon monoxide poisoning is the most common cause of injury and death due to poisoning worldwide. Poisoning is typically more common during the winter months. This is due to increased domestic use of gas furnaces, gas or kerosene space heaters, and kitchen stoves during the winter months, which if faulty and\/or used without adequate ventilation, may produce excessive carbon monoxide. Carbon monoxide detection and poisoning also increases during power outages, when electric heating and cooking appliances become inoperative and residents may temporarily resort to fuel-burning space heaters, stoves, and grills (some of which are safe only for outdoor use but nonetheless are errantly burned indoors).\nIt has been estimated that more than 40,000 people per year seek medical attention for carbon monoxide poisoning in the United States. 95% of carbon monoxide poisoning deaths in Australia are due to gas space heaters. In many industrialized countries, carbon monoxide is the cause of more than 50% of fatal poisonings. In the United States, approximately 200 people die each year from carbon monoxide poisoning associated with home fuel-burning heating equipment. Carbon monoxide poisoning contributes to the approximately 5,613 smoke inhalation deaths each year in the United States. The CDC reports, \"Each year, more than 500 Americans die from unintentional carbon monoxide poisoning, and more than 2,000 commit suicide by intentionally poisoning themselves.\" For the 10-year period from 1979 to 1988, 56,133 deaths from carbon monoxide poisoning occurred in the United States, with 25,889 of those being suicides, leaving 30,244 unintentional deaths. A report from New Zealand showed that 206 people died from carbon monoxide poisoning in the years of 2001 and 2002. In total carbon monoxide poisoning was responsible for 43.9% of deaths by poisoning in that country. In South Korea, 1,950 people had been poisoned by carbon monoxide with 254 deaths from 2001 through 2003. A report from Jerusalem showed 3.53 per 100,000 people were poisoned annually from 2001 through 2006. In Hubei, China, 218 deaths from poisoning were reported over a 10-year period with 16.5% being from carbon monoxide exposure.\n\nCauses\nCarbon monoxide is a product of combustion of organic matter under conditions of restricted oxygen supply, which prevents complete oxidation to carbon dioxide (CO2). Sources of carbon monoxide include cigarette smoke, house fires, faulty furnaces, heaters, wood-burning stoves, internal combustion vehicle exhaust, electrical generators, propane-fueled equipment such as portable stoves, and gasoline-powered tools such as leaf blowers, lawn mowers, high-pressure washers, concrete cutting saws, power trowels, and welders. Exposure typically occurs when equipment is used in buildings or semi-enclosed spaces.\nRiding in the back of pickup trucks has led to poisoning in children. Idling automobiles with the exhaust pipe blocked by snow has led to the poisoning of car occupants. Any perforation between the exhaust manifold and shroud can result in exhaust gases reaching the cabin. Generators and propulsion engines on boats, especially houseboats, has resulted in fatal carbon monoxide exposures.\nPoisoning may also occur following the use of a self-contained underwater breathing apparatus (SCUBA) due to faulty diving air compressors.\nIn caves carbon monoxide can build up in enclosed chambers due to the presence of decomposing organic matter. In coal mines incomplete combustion may occur during explosions resulting in the production of afterdamp. The gas is up to 3% CO and may be fatal after just a single breath. Following an explosion in a colliery, adjacent interconnected mines may become dangerous due to the afterdamp leaking from mine to mine. Such an incident followed the Trimdon Grange explosion which killed men in the Kelloe mine.\nAnother source of poisoning is exposure to the organic solvent dichloromethane, also known as methylene chloride, found in some paint strippers, as the metabolism of dichloromethane produces carbon monoxide. In November 2019, an EPA ban on dichloromethane in paint strippers for consumer use took effect in the United States.\n\nPrevention\nDetectors\nPrevention remains a vital public health issue, requiring public education on the safe operation of appliances, heaters, fireplaces, and internal-combustion engines, as well as increased emphasis on the installation of carbon monoxide detectors. Carbon monoxide is tasteless, odourless, and colourless, and therefore can not be detected by visual cues or smell.\nThe United States Consumer Product Safety Commission has stated, \"carbon monoxide detectors are as important to home safety as smoke detectors are,\" and recommends each home have at least one carbon monoxide detector, and preferably one on each level of the building. These devices, which are relatively inexpensive and widely available, are either battery- or AC-powered, with or without battery backup. In buildings, carbon monoxide detectors are usually installed around heaters and other equipment. If a relatively high level of carbon monoxide is detected, the device sounds an alarm, giving people the chance to evacuate and ventilate the building. Unlike smoke detectors, carbon monoxide detectors do not need to be placed near ceiling level.\nThe use of carbon monoxide detectors has been standardized in many areas. In the US, NFPA 720\u20132009, the carbon monoxide detector guidelines published by the National Fire Protection Association, mandates the placement of carbon monoxide detectors\/alarms on every level of the residence, including the basement, in addition to outside sleeping areas. In new homes, AC-powered detectors must have battery backup and be interconnected to ensure early warning of occupants at all levels. NFPA 720-2009 is the first national carbon monoxide standard to address devices in non-residential buildings. These guidelines, which now pertain to schools, healthcare centers, nursing homes, and other non-residential buildings, include three main points:\n\n1. A secondary power supply (battery backup) must operate all carbon monoxide notification appliances for at least 12 hours,\n2. Detectors must be on the ceiling in the same room as permanently installed fuel-burning appliances, and\n3. Detectors must be located on every habitable level and in every HVAC zone of the building.\nGas organizations will often recommend getting gas appliances serviced at least once a year.\n\nLegal requirements\nThe NFPA standard is not necessarily enforced by law. As of April 2006, the US state of Massachusetts requires detectors to be present in all residences with potential CO sources, regardless of building age and whether they are owner-occupied or rented. This is enforced by municipal inspectors and was inspired by the death of 7-year-old Nicole Garofalo in 2005 due to snow blocking a home heating vent. Other jurisdictions may have no requirement or only mandate detectors for new construction or at time of sale.\n\nWorld Health Organization recommendations\nThe following guideline values (ppm values rounded) and periods of time-weighted average exposures have been determined in such a way that the carboxyhemoglobin (COHb) level of 2.5% is not exceeded, even when a normal subject engages in light or moderate exercise:\n\n100 mg\/m3 (87 ppm) for 15 min\n60 mg\/m3 (52 ppm) for 30 min\n30 mg\/m3 (26 ppm) for 1 h\n10 mg\/m3 (9 ppm) for 8 h\n7 mg\/m3 (6 ppm) for 24 h (for indoor air quality, so as not to exceed 2% COHb for chronic exposure)\n\nDiagnosis\nAs many symptoms of carbon monoxide poisoning also occur with many other types of poisonings and infections (such as the flu), the diagnosis is often difficult. A history of potential carbon monoxide exposure, such as being exposed to a residential fire, may suggest poisoning, but the diagnosis is confirmed by measuring the levels of carbon monoxide in the blood. This can be determined by measuring the amount of carboxyhemoglobin compared to the amount of hemoglobin in the blood.\nThe ratio of carboxyhemoglobin to hemoglobin molecules in an average person may be up to 5%, although cigarette smokers who smoke two packs per day may have levels up to 9%. In symptomatic poisoned people they are often in the 10\u201330% range, while persons who die may have postmortem blood levels of 30\u201390%.\nAs people may continue to experience significant symptoms of CO poisoning long after their blood carboxyhemoglobin concentration has returned to normal, presenting to examination with a normal carboxyhemoglobin level (which may happen in late states of poisoning) does not rule out poisoning.\n\nMeasuring\nCarbon monoxide may be quantitated in blood using spectrophotometric methods or chromatographic techniques in order to confirm a diagnosis of poisoning in a person or to assist in the forensic investigation of a case of fatal exposure.\nA CO-oximeter can be used to determine carboxyhemoglobin levels. Pulse CO-oximeters estimate carboxyhemoglobin with a non-invasive finger clip similar to a pulse oximeter. These devices function by passing various wavelengths of light through the fingertip and measuring the light absorption of the different types of hemoglobin in the capillaries. The use of a regular pulse oximeter is not effective in the diagnosis of carbon monoxide poisoning as these devices may be unable to distinguish carboxyhemoglobin from oxyhemoglobin.\nBreath CO monitoring offers an alternative to pulse CO-oximetry. Carboxyhemoglobin levels have been shown to have a strong correlation with breath CO concentration. However, many of these devices require the user to inhale deeply and hold their breath to allow the CO in the blood to escape into the lung before the measurement can be made. As this is not possible in people who are unresponsive, these devices may not appropriate for use in on-scene emergency care detection of CO poisoning.\n\nDifferential diagnosis\nThere are many conditions to be considered in the differential diagnosis of carbon monoxide poisoning. The earliest symptoms, especially from low level exposures, are often non-specific and readily confused with other illnesses, typically flu-like viral syndromes, depression, chronic fatigue syndrome, chest pain, and migraine or other headaches. Carbon monoxide has been called a \"great mimicker\" due to the presentation of poisoning being diverse and nonspecific. Other conditions included in the differential diagnosis include acute respiratory distress syndrome, altitude sickness, lactic acidosis, diabetic ketoacidosis, meningitis, methemoglobinemia, or opioid or toxic alcohol poisoning.\n\nTreatment\nInitial treatment for carbon monoxide poisoning is to immediately remove the person from the exposure without endangering further people. Those who are unconscious may require CPR on site. Administering oxygen via non-rebreather mask shortens the half-life of carbon monoxide from 320 minutes, when breathing normal air, to only 80 minutes. Oxygen hastens the dissociation of carbon monoxide from carboxyhemoglobin, thus turning it back into hemoglobin. Due to the possible severe effects in the baby, pregnant women are treated with oxygen for longer periods of time than non-pregnant people.\n\nHyperbaric oxygen\nHyperbaric oxygen is also used in the treatment of carbon monoxide poisoning, as it may hasten dissociation of CO from carboxyhemoglobin and cytochrome oxidase to a greater extent than normal oxygen. Hyperbaric oxygen at three times atmospheric pressure reduces the half life of carbon monoxide to 23 minutes, compared to 80 minutes for oxygen at regular atmospheric pressure. It may also enhance oxygen transport to the tissues by plasma, partially bypassing the normal transfer through hemoglobin. However, it is controversial whether hyperbaric oxygen actually offers any extra benefits over normal high flow oxygen, in terms of increased survival or improved long-term outcomes. There have been randomized controlled trials in which the two treatment options have been compared; of the six performed, four found hyperbaric oxygen improved outcome and two found no benefit for hyperbaric oxygen. Some of these trials have been criticized for apparent flaws in their implementation. A review of all the literature concluded that the role of hyperbaric oxygen is unclear and the available evidence neither confirms nor denies a medically meaningful benefit. The authors suggested a large, well designed, externally audited, multicentre trial to compare normal oxygen with hyperbaric oxygen. While hyperbaric oxygen therapy is used for severe poisonings, the benefit over standard oxygen delivery is unclear.\n\nOther\nFurther treatment for other complications such as seizure, hypotension, cardiac abnormalities, pulmonary edema, and acidosis may be required. Hypotension requires treatment with intravenous fluids; vasopressors may be required to treat myocardial depression. Cardiac dysrhythmias are treated with standard advanced cardiac life support protocols. If severe, metabolic acidosis is treated with sodium bicarbonate. Treatment with sodium bicarbonate is controversial as acidosis may increase tissue oxygen availability. Treatment of acidosis may only need to consist of oxygen therapy. The delayed development of neuropsychiatric impairment is one of the most serious complications of carbon monoxide poisoning. Brain damage is confirmed following MRI or CAT scans. Extensive follow up and supportive treatment is often required for delayed neurological damage. Outcomes are often difficult to predict following poisoning, especially people who have symptoms of cardiac arrest, coma, metabolic acidosis, or have high carboxyhemoglobin levels. One study reported that approximately 30% of people with severe carbon monoxide poisoning will have a fatal outcome. It has been reported that electroconvulsive therapy (ECT) may increase the likelihood of delayed neuropsychiatric sequelae (DNS) after carbon monoxide (CO) poisoning. A device that also provides some carbon dioxide to stimulate faster breathing (sold under the brand name ClearMate) may also be used.\n\nPathophysiology\nThe precise mechanisms by which the effects of carbon monoxide are induced upon bodily systems are complex and not yet fully understood. Known mechanisms include carbon monoxide binding to hemoglobin, myoglobin and mitochondrial cytochrome c oxidase and restricting oxygen supply, and carbon monoxide causing brain lipid peroxidation.\n\nHemoglobin\nCarbon monoxide has a higher diffusion coefficient compared to oxygen, and the main enzyme in the human body that produces carbon monoxide is heme oxygenase, which is located in nearly all cells and platelets. Most endogenously produced CO is stored bound to hemoglobin as carboxyhemoglobin. The simplistic understanding for the mechanism of carbon monoxide toxicity is based on excess carboxyhemoglobin decreasing the oxygen-delivery capacity of the blood to tissues throughout the body. In humans, the affinity between hemoglobin and carbon monoxide is approximately 240 times stronger than the affinity between hemoglobin and oxygen. However, certain mutations such as the Hb-Kirklareli mutation has a relative 80,000 times greater affinity for carbon monoxide than oxygen resulting in systemic carboxyhemoglobin reaching a sustained level of 16% COHb.\nHemoglobin is a tetramer with four prosthetic heme groups to serve as oxygen binding sites. The average red blood cell contains 250 million hemoglobin molecules, therefore 1 billion heme sites capable of binding gas. The binding of carbon monoxide at any one of these sites increases the oxygen affinity of the remaining three sites, which causes the hemoglobin molecule to retain oxygen that would otherwise be delivered to the tissue; therefore carbon monoxide binding at any site may be as dangerous as carbon monoxide binding to all sites. Delivery of oxygen is largely driven by the Bohr effect and Haldane effect. To provide a simplified synopsis of the molecular mechanism of systemic gas exchange in layman's terms, upon inhalation of air it was widely thought oxygen binding to any of the heme sites triggers a conformational change in the globin\/protein unit of hemoglobin which then enables the binding of additional oxygen to each of the other vacant heme sites. Upon arrival to the cell\/tissues, oxygen release into the tissue is driven by \"acidification\" of the local pH (meaning a relatively higher concentration of 'acidic' protons\/hydrogen ions) caused by an increase in the biotransformation of carbon dioxide waste into carbonic acid via carbonic anhydrase. In other words, oxygenated arterial blood arrives at cells in the \"hemoglobin R-state\" which has deprotonated\/unionized amino acid residues (regarding nitrogen\/amines) due to the less-acidic arterial pH environment (arterial blood averages pH 7.407 whereas venous blood is slightly more acidic at pH 7.371). The \"T-state\" of hemoglobin is deoxygenated in venous blood partially due to protonation\/ionization caused by the acidic environment hence causing a conformation unsuited for oxygen-binding (in other words, oxygen is 'ejected' upon arrival to the cell because acid \"attacks\" the amines of hemoglobin causing ionization\/protonation of the amine residues resulting in a conformation change unsuited for retaining oxygen). Furthermore, the mechanism for formation of carbaminohemoglobin generates additional 'acidic' hydrogen ions that may further stabilize the protonated\/ionized deoxygenated hemoglobin. Upon return of venous blood into the lung and subsequent exhalation of carbon dioxide, the blood is \"de-acidified\" (see also: hyperventilation) allowing for the deprotonation\/unionization of hemoglobin to then re-enable oxygen-binding as part of the transition to arterial blood (note this process is complex due to involvement of chemoreceptors and other physiological functionalities). Carbon monoxide is not 'ejected' due to acid, therefore carbon monoxide poisoning disturbs this physiological process hence the venous blood of poisoning patients is bright red akin to arterial blood since the carbonyl\/carbon monoxide is retained. Hemoglobin is dark in deoxygenated venous blood, but it has a bright red color when carrying blood in oxygenated arterial blood and when converted into carboxyhemoglobin in both arterial and venous blood, so poisoned cadavers and even commercial meats treated with carbon monoxide acquire an unnatural lively reddish hue.\nAt toxic concentrations, carbon monoxide as carboxyhemoglobin significantly interferes with respiration and gas exchange by simultaneously inhibiting acquisition and delivery of oxygen to cells and preventing formation of carbaminohemoglobin which accounts for approximately 30% of carbon dioxide exportation. Therefore, a patient with carbon monoxide poisoning may experience severe hypoxia and acidosis (potentially both respiratory acidosis and metabolic acidosis) in addition to the toxicities of excess carbon monoxide inhibiting numerous hemoproteins, metallic and non-metallic targets which affect cellular machinery.\n\nMyoglobin\nCarbon monoxide also binds to the hemeprotein myoglobin. It has a high affinity for myoglobin, about 60 times greater than that of oxygen. Carbon monoxide bound to myoglobin may impair its ability to utilize oxygen. This causes reduced cardiac output and hypotension, which may result in brain ischemia. A delayed return of symptoms have been reported. This results following a recurrence of increased carboxyhemoglobin levels; this effect may be due to a late release of carbon monoxide from myoglobin, which subsequently binds to hemoglobin.\n\nCytochrome oxidase\nAnother mechanism involves effects on the mitochondrial respiratory enzyme chain that is responsible for effective tissue utilization of oxygen. Carbon monoxide binds to cytochrome oxidase with less affinity than oxygen, so it is possible that it requires significant intracellular hypoxia before binding. This binding interferes with aerobic metabolism and efficient adenosine triphosphate synthesis. Cells respond by switching to anaerobic metabolism, causing anoxia, lactic acidosis, and eventual cell death. The rate of dissociation between carbon monoxide and cytochrome oxidase is slow, causing a relatively prolonged impairment of oxidative metabolism.\n\nCentral nervous system effects\nThe mechanism that is thought to have a significant influence on delayed effects involves formed blood cells and chemical mediators, which cause brain lipid peroxidation (degradation of unsaturated fatty acids). Carbon monoxide causes endothelial cell and platelet release of nitric oxide, and the formation of oxygen free radicals including peroxynitrite. In the brain this causes further mitochondrial dysfunction, capillary leakage, leukocyte sequestration, and apoptosis. The result of these effects is lipid peroxidation, which causes delayed reversible demyelination of white matter in the central nervous system known as Grinker myelinopathy, which can lead to edema and necrosis within the brain. This brain damage occurs mainly during the recovery period. This may result in cognitive defects, especially affecting memory and learning, and movement disorders. These disorders are typically related to damage to the cerebral white matter and basal ganglia. Hallmark pathological changes following poisoning are bilateral necrosis of the white matter, globus pallidus, cerebellum, hippocampus and the cerebral cortex.\n\nPregnancy\nCarbon monoxide poisoning in pregnant women may cause severe adverse fetal effects. Poisoning causes fetal tissue hypoxia by decreasing the release of maternal oxygen to the fetus. Carbon monoxide also crosses the placenta and combines with fetal hemoglobin, causing more direct fetal tissue hypoxia. Additionally, fetal hemoglobin has a 10 to 15% higher affinity for carbon monoxide than adult hemoglobin, causing more severe poisoning in the fetus than in the adult. Elimination of carbon monoxide is slower in the fetus, leading to an accumulation of the toxic chemical. The level of fetal morbidity and mortality in acute carbon monoxide poisoning is significant, so despite mild maternal poisoning or following maternal recovery, severe fetal poisoning or death may still occur.\n\nHistory\nHumans have maintained a complex relationship with carbon monoxide since first learning to control fire circa 800,000 BC. Primitive cavemen probably discovered the toxicity of carbon monoxide upon introducing fire into their dwellings. The early development of metallurgy and smelting technologies emerging circa 6,000 BC through the Bronze Age likewise plagued humankind with carbon monoxide exposure. Apart from the toxicity of carbon monoxide, indigenous Native Americans may have experienced the neuroactive properties of carbon monoxide through shamanistic fireside rituals.\nEarly civilizations developed mythological tales to explain the origin of fire, such as Vulcan, Pkharmat, and Prometheus from Greek mythology who shared fire with humans. Aristotle (384\u2013322 BC) first recorded that burning coals produced toxic fumes. Greek physician Galen (129\u2013199 AD) speculated that there was a change in the composition of the air that caused harm when inhaled, and symptoms of CO poisoning appeared in Cassius Iatrosophista's Quaestiones Medicae et Problemata Naturalia circa 130 AD. Julian the Apostate, Caelius Aurelianus, and several others similarly documented early knowledge of the toxicity symptoms of carbon monoxide poisoning as caused by coal fumes in the ancient era.\nDocumented cases by Livy and Cicero allude to carbon monoxide being used as a method of suicide in ancient Rome. Emperor Lucius Verus used smoke to execute prisoners. Many deaths have been linked to carbon monoxide poisoning including Emperor Jovian, Empress Fausta, and Seneca. The most high-profile death by carbon monoxide poisoning may possibly have been Cleopatra or Edgar Allan Poe.\nIn the fifteenth century, coal miners believed sudden death was caused by evil spirits; carbon monoxide poisoning has been linked to supernatural and paranormal experiences, witchcraft, etc. throughout the following centuries including in the modern present day exemplified by Carrie Poppy's investigations.\nGeorg Ernst Stahl mentioned carbonarii halitus in 1697 in reference to toxic vapors thought to be carbon monoxide. Friedrich Hoffmann conducted the first modern scientific investigation into carbon monoxide poisoning from coal in 1716, notably rejecting villagers attributing death to demonic superstition. Herman Boerhaave conducted the first scientific experiments on the effect of carbon monoxide (coal fumes) on animals in the 1730s. Joseph Priestley is credited with first synthesizing carbon monoxide in 1772 which he had called heavy inflammable air, and Carl Wilhelm Scheele isolated carbon monoxide from coal in 1773 suggesting it to be the toxic entity.\nThe dose-dependent risk of carbon monoxide poisoning as hydrocarbonate was investigated in the late 1790s by Thomas Beddoes, James Watt, Tiberius Cavallo, James Lind, Humphry Davy, and many others in the context of inhalation of factitious airs, much of which occurred at the Pneumatic Institution.\nWilliam Cruickshank discovered carbon monoxide as a molecule containing one carbon and one oxygen atom in 1800, thereby initiating the modern era of research exclusively focused on carbon monoxide. The mechanism for toxicity was first suggested by James Watt in 1793, followed by Adrien Chenot in 1854 and finally demonstrated by Claude Bernard after 1846 as published in 1857 and also independently published by Felix Hoppe-Seyler in the same year.\nThe first controlled clinical trial studying the toxicity of carbon monoxide occurred in 1973.\n\nHistorical detection\nCarbon monoxide poisoning has plagued coal miners for many centuries. In the context of mining, carbon monoxide is widely known as whitedamp. John Scott Haldane identified carbon monoxide as the lethal constituent of afterdamp, the gas created by combustion, after examining many bodies of miners killed in pit explosions. By 1911, Haldane introduced the use of small animals for miners to detect dangerous levels of carbon monoxide underground, either white mice or canaries which have little tolerance for carbon monoxide thereby offering an early warning, i.e. canary in a coal mine. The canary in British pits was replaced in 1986 by the electronic gas detector.\nThe first qualitative analytical method to detect carboxyhemoglobin emerged in 1858 with a colorimetric method developed by Felix Hoppe-Seyler, and the first quantitative analysis method emerged in 1880 with Josef von Fodor.\n\nHistorical treatment\nThe use of oxygen emerged with anecdotal reports such as Humphry Davy having been treated with oxygen in 1799 upon inhaling three quarts of hydrocarbonate (water gas). Samuel Witter developed an oxygen inhalation protocol in response to carbon monoxide poisoning in 1814. Similarly, an oxygen inhalation protocol was recommend for malaria (literally translated to \"bad air\") in 1830 based on malaria symptoms aligning with carbon monoxide poisoning. Other oxygen protocols emerged in the late 1800s. The use of hyperbaric oxygen in rats following poisoning was studied by Haldane in 1895 while its use in humans began in the 1960s.\n\nIncidents\nThe worst accidental mass poisoning from carbon monoxide was the Balvano train disaster which occurred on 3 March 1944 in Italy, when a freight train with many illegal passengers stalled in a tunnel, leading to the death of over 500 people.\nOver 50 people are suspected to have died from smoke inhalation as a result of the Branch Davidian Massacre during the Waco siege in 1993.\n\nWeaponization\nIn ancient history, Hannibal executed Roman prisoners with coal fumes during the Second Punic War.\nThe extermination of stray dogs by a carbon monoxide gas chamber was described in 1874. In 1884, an article appeared in Scientific American describing the use of a carbon monoxide gas chamber for slaughterhouse operations as well as euthanizing a variety of animals.\nAs part of the Holocaust during World War II, the Nazis used gas vans at Chelmno extermination camp and elsewhere to murder an estimated 700,000 or more people by carbon monoxide poisoning. This method was also used in the gas chambers of several death camps such as Treblinka, Sobibor, and Belzec. Gassing with carbon monoxide started in Action T4. The gas was supplied by IG Farben in pressurized cylinders and fed by tubes into the gas chambers built at various mental hospitals, such as Hartheim Euthanasia Centre. Exhaust fumes from tank engines, for example, were used to supply the gas to the chambers.\n\nReferences\nExternal links\nCenters for Disease Control and Prevention (CDC) \u2013 Carbon Monoxide \u2013 NIOSH Workplace Safety and Health Topic\nInternational Programme on Chemical Safety (1999). Carbon Monoxide, Environmental Health Criteria 213, Geneva: WHO","31":"The central nervous system (CNS) is the part of the nervous system consisting primarily of the brain and spinal cord. The CNS is so named because the brain integrates the received information and coordinates and influences the activity of all parts of the bodies of bilaterally symmetric and triploblastic animals\u2014that is, all multicellular animals except sponges and diploblasts. It is a structure composed of nervous tissue positioned along the rostral (nose end) to caudal (tail end) axis of the body and may have an enlarged section at the rostral end which is a brain. Only arthropods, cephalopods and vertebrates have a true brain, though precursor structures exist in onychophorans, gastropods and lancelets.\nThe rest of this article exclusively discusses the vertebrate central nervous system, which is radically distinct from all other animals.\n\nOverview\nIn vertebrates, the brain and spinal cord are both enclosed in the meninges. The meninges provide a barrier to chemicals dissolved in the blood, protecting the brain from most neurotoxins commonly found in food. Within the meninges the brain and spinal cord are bathed in cerebral spinal fluid which replaces the body fluid found outside the cells of all bilateral animals.\nIn vertebrates, the CNS is contained within the dorsal body cavity, while the brain is housed in the cranial cavity within the skull. The spinal cord is housed in the spinal canal within the vertebrae. Within the CNS, the interneuronal space is filled with a large amount of supporting non-nervous cells called neuroglia or glia from the Greek for \"glue\".\nIn vertebrates, the CNS also includes the retina and the optic nerve (cranial nerve II), as well as the olfactory nerves and olfactory epithelium. As parts of the CNS, they connect directly to brain neurons without intermediate ganglia. The olfactory epithelium is the only central nervous tissue outside the meninges in direct contact with the environment, which opens up a pathway for therapeutic agents which cannot otherwise cross the meninges barrier.\n\nStructure\nThe CNS consists of two major structures: the brain and spinal cord. The brain is encased in the skull, and protected by the cranium. The spinal cord is continuous with the brain and lies caudally to the brain. It is protected by the vertebrae. The spinal cord reaches from the base of the skull, and continues through or starting below the foramen magnum, and terminates roughly level with the first or second lumbar vertebra, occupying the upper sections of the vertebral canal.\n\nWhite and gray matter\nMicroscopically, there are differences between the neurons and tissue of the CNS and the peripheral nervous system (PNS). The CNS is composed of white and gray matter. This can also be seen macroscopically on brain tissue. The white matter consists of axons and oligodendrocytes, while the gray matter consists of neurons and unmyelinated fibers. Both tissues include a number of glial cells (although the white matter contains more), which are often referred to as supporting cells of the CNS. Different forms of glial cells have different functions, some acting almost as scaffolding for neuroblasts to climb during neurogenesis such as bergmann glia, while others such as microglia are a specialized form of macrophage, involved in the immune system of the brain as well as the clearance of various metabolites from the brain tissue. Astrocytes may be involved with both clearance of metabolites as well as transport of fuel and various beneficial substances to neurons from the capillaries of the brain. Upon CNS injury astrocytes will proliferate, causing gliosis, a form of neuronal scar tissue, lacking in functional neurons.\nThe brain (cerebrum as well as midbrain and hindbrain) consists of a cortex, composed of neuron-bodies constituting gray matter, while internally there is more white matter that form tracts and commissures. Apart from cortical gray matter there is also subcortical gray matter making up a large number of different nuclei.\n\nSpinal cord\nFrom and to the spinal cord are projections of the peripheral nervous system in the form of spinal nerves (sometimes segmental nerves). The nerves connect the spinal cord to skin, joints, muscles etc. and allow for the transmission of efferent motor as well as afferent sensory signals and stimuli. This allows for voluntary and involuntary motions of muscles, as well as the perception of senses.\nAll in all 31 spinal nerves project from the brain stem, some forming plexa as they branch out, such as the brachial plexa, sacral plexa etc. Each spinal nerve will carry both sensory and motor signals, but the nerves synapse at different regions of the spinal cord, either from the periphery to sensory relay neurons that relay the information to the CNS or from the CNS to motor neurons, which relay the information out.\nThe spinal cord relays information up to the brain through spinal tracts through the final common pathway to the thalamus and ultimately to the cortex.\n\nCranial nerves\nApart from the spinal cord, there are also peripheral nerves of the PNS that synapse through intermediaries or ganglia directly on the CNS. These 12 nerves exist in the head and neck region and are called cranial nerves. Cranial nerves bring information to the CNS to and from the face, as well as to certain muscles (such as the trapezius muscle, which is innervated by accessory nerves as well as certain cervical spinal nerves).\nTwo pairs of cranial nerves; the olfactory nerves and the optic nerves are often considered structures of the CNS. This is because they do not synapse first on peripheral ganglia, but directly on CNS neurons. The olfactory epithelium is significant in that it consists of CNS tissue expressed in direct contact to the environment, allowing for administration of certain pharmaceuticals and drugs.\n\nBrain\nAt the anterior end of the spinal cord lies the brain. The brain makes up the largest portion of the CNS. It is often the main structure referred to when speaking of the nervous system in general. The brain is the major functional unit of the CNS. While the spinal cord has certain processing ability such as that of spinal locomotion and can process reflexes, the brain is the major processing unit of the nervous system.\n\nBrainstem\nThe brainstem consists of the medulla, the pons and the midbrain. The medulla can be referred to as an extension of the spinal cord, which both have similar organization and functional properties. The tracts passing from the spinal cord to the brain pass through here.\nRegulatory functions of the medulla nuclei include control of blood pressure and breathing. Other nuclei are involved in balance, taste, hearing, and control of muscles of the face and neck.\nThe next structure rostral to the medulla is the pons, which lies on the ventral anterior side of the brainstem. Nuclei in the pons include pontine nuclei which work with the cerebellum and transmit information between the cerebellum and the cerebral cortex.\nIn the dorsal posterior pons lie nuclei that are involved in the functions of breathing, sleep, and taste.\nThe midbrain, or mesencephalon, is situated above and rostral to the pons. It includes nuclei linking distinct parts of the motor system, including the cerebellum, the basal ganglia and both cerebral hemispheres, among others. Additionally, parts of the visual and auditory systems are located in the midbrain, including control of automatic eye movements.\nThe brainstem at large provides entry and exit to the brain for a number of pathways for motor and autonomic control of the face and neck through cranial nerves, Autonomic control of the organs is mediated by the tenth cranial nerve. A large portion of the brainstem is involved in such autonomic control of the body. Such functions may engage the heart, blood vessels, and pupils, among others.\nThe brainstem also holds the reticular formation, a group of nuclei involved in both arousal and alertness.\n\nCerebellum\nThe cerebellum lies behind the pons. The cerebellum is composed of several dividing fissures and lobes. Its function includes the control of posture and the coordination of movements of parts of the body, including the eyes and head, as well as the limbs. Further, it is involved in motion that has been learned and perfected through practice, and it will adapt to new learned movements.\nDespite its previous classification as a motor structure, the cerebellum also displays connections to areas of the cerebral cortex involved in language and cognition. These connections have been shown by the use of medical imaging techniques, such as functional MRI and Positron emission tomography.\nThe body of the cerebellum holds more neurons than any other structure of the brain, including that of the larger cerebrum, but is also more extensively understood than other structures of the brain, as it includes fewer types of different neurons. It handles and processes sensory stimuli, motor information, as well as balance information from the vestibular organ.\n\nDiencephalon\nThe two structures of the diencephalon worth noting are the thalamus and the hypothalamus. The thalamus acts as a linkage between incoming pathways from the peripheral nervous system as well as the optical nerve (though it does not receive input from the olfactory nerve) to the cerebral hemispheres. Previously it was considered only a \"relay station\", but it is engaged in the sorting of information that will reach cerebral hemispheres (neocortex).\nApart from its function of sorting information from the periphery, the thalamus also connects the cerebellum and basal ganglia with the cerebrum. In common with the aforementioned reticular system the thalamus is involved in wakefulness and consciousness, such as though the SCN.\nThe hypothalamus engages in functions of a number of primitive emotions or feelings such as hunger, thirst and maternal bonding. This is regulated partly through control of secretion of hormones from the pituitary gland. Additionally the hypothalamus plays a role in motivation and many other behaviors of the individual.\n\nCerebrum\nThe cerebrum of cerebral hemispheres make up the largest visual portion of the human brain. Various structures combine to form the cerebral hemispheres, among others: the cortex, basal ganglia, amygdala and hippocampus. The hemispheres together control a large portion of the functions of the human brain such as emotion, memory, perception and motor functions. Apart from this the cerebral hemispheres stand for the cognitive capabilities of the brain.\nConnecting each of the hemispheres is the corpus callosum as well as several additional commissures.\nOne of the most important parts of the cerebral hemispheres is the cortex, made up of gray matter covering the surface of the brain. Functionally, the cerebral cortex is involved in planning and carrying out of everyday tasks.\nThe hippocampus is involved in storage of memories, the amygdala plays a role in perception and communication of emotion, while the basal ganglia play a major role in the coordination of voluntary movement.\n\nDifference from the peripheral nervous system\nThis differentiates the CNS from the PNS, which consists of neurons, axons, and Schwann cells. Oligodendrocytes and Schwann cells have similar functions in the CNS and PNS, respectively. Both act to add myelin sheaths to the axons, which acts as a form of insulation allowing for better and faster proliferation of electrical signals along the nerves. Axons in the CNS are often very short, barely a few millimeters, and do not need the same degree of isolation as peripheral nerves. Some peripheral nerves can be over 1 meter in length, such as the nerves to the big toe. To ensure signals move at sufficient speed, myelination is needed.\nThe way in which the Schwann cells and oligodendrocytes myelinate nerves differ. A Schwann cell usually myelinates a single axon, completely surrounding it. Sometimes, they may myelinate many axons, especially when in areas of short axons. Oligodendrocytes usually myelinate several axons. They do this by sending out thin projections of their cell membrane, which envelop and enclose the axon.\n\nDevelopment\nDuring early development of the vertebrate embryo, a longitudinal groove on the neural plate gradually deepens and the ridges on either side of the groove (the neural folds) become elevated, and ultimately meet, transforming the groove into a closed tube called the neural tube. The formation of the neural tube is called neurulation.  At this stage, the walls of the neural tube contain proliferating neural stem cells in a region called the ventricular zone.  The neural stem cells, principally radial glial cells, multiply and generate neurons through the process of neurogenesis, forming the rudiment of the CNS.\nThe neural tube gives rise to both brain and spinal cord.  The anterior (or 'rostral') portion of the neural tube initially differentiates into three brain vesicles (pockets): the prosencephalon at the front, the mesencephalon, and, between the mesencephalon and the spinal cord, the rhombencephalon. (By six weeks in the human embryo) the prosencephalon then divides further into the telencephalon and diencephalon; and the rhombencephalon divides into the metencephalon and myelencephalon.  The spinal cord is derived from the posterior or 'caudal' portion of the neural tube.\nAs a vertebrate grows, these vesicles differentiate further still. The telencephalon differentiates into, among other things, the striatum, the hippocampus and the neocortex, and its cavity becomes the first and second ventricles. Diencephalon elaborations include the subthalamus, hypothalamus, thalamus and epithalamus, and its cavity forms the third ventricle. The tectum, pretectum, cerebral peduncle and other structures develop out of the mesencephalon, and its cavity grows into the mesencephalic duct (cerebral aqueduct). The metencephalon becomes, among other things, the pons and the cerebellum, the myelencephalon forms the medulla oblongata, and their cavities develop into the fourth ventricle.\n\nEvolution\nPlanaria\nPlanarians, members of the phylum Platyhelminthes (flatworms), have the simplest, clearly defined delineation of a nervous system into a CNS and a PNS.\nTheir primitive brains, consisting of two fused anterior ganglia, and longitudinal nerve cords form the CNS. Like vertebrates, have a distinct CNS and PNS. The nerves projecting laterally from the CNS form their PNS.\nA molecular study found that more than 95% of the 116 genes involved in the nervous system of planarians, which includes genes related to the CNS, also exist in humans.\n\nArthropoda\nIn arthropods, the ventral nerve cord, the subesophageal ganglia and the supraesophageal ganglia are usually seen as making up the CNS. Arthropoda, unlike vertebrates, have inhibitory motor neurons due to their small size.\n\nChordata\nThe CNS of chordates differs from that of other animals in being placed dorsally in the body, above the gut and notochord\/spine. The basic pattern of the CNS is highly conserved throughout the different species of vertebrates and during evolution. The major trend that can be observed is towards a progressive telencephalisation: the telencephalon of reptiles is only an appendix to the large olfactory bulb, while in mammals it makes up most of the volume of the CNS. In the human brain, the telencephalon covers most of the diencephalon and the entire mesencephalon. Indeed, the allometric study of brain size among different species shows a striking continuity from rats to whales, and allows us to complete the knowledge about the evolution of the CNS obtained through cranial endocasts.\n\nMammals\nMammals \u2013 which appear in the fossil record after the first fishes, amphibians, and reptiles \u2013 are the only vertebrates to possess the evolutionarily recent, outermost part of the cerebral cortex (main part of the telencephalon excluding olfactory bulb) known as the neocortex. This part of the brain is, in mammals, involved in higher thinking and further processing of all senses in the sensory cortices (processing for smell was previously only done by its bulb while those for non-smell senses were only done by the tectum). The neocortex of monotremes (the duck-billed platypus and several species of spiny anteaters) and of marsupials (such as kangaroos, koalas, opossums, wombats, and Tasmanian devils) lack the convolutions \u2013 gyri and sulci \u2013 found in the neocortex of most placental mammals (eutherians).\nWithin placental mammals, the size and complexity of the neocortex increased over time. The area of the neocortex of mice is only about 1\/100 that of monkeys, and that of monkeys is only about 1\/10 that of humans. In addition, rats lack convolutions in their neocortex (possibly also because rats are small mammals), whereas cats have a moderate degree of convolutions, and humans have quite extensive convolutions. Extreme convolution of the neocortex is found in dolphins, possibly related to their complex echolocation.\n\nClinical significance\nDiseases\nThere are many CNS diseases and conditions, including infections such as encephalitis and poliomyelitis, early-onset neurological disorders including ADHD and autism, seizure disorders such as epilepsy, headache disorders such as migraine, late-onset neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and essential tremor, autoimmune and inflammatory diseases such as multiple sclerosis and acute disseminated encephalomyelitis, genetic disorders such as Krabbe's disease and Huntington's disease, as well as amyotrophic lateral sclerosis and adrenoleukodystrophy. Lastly, cancers of the central nervous system can cause severe illness and, when malignant, can have very high mortality rates. Symptoms depend on the size, growth rate, location and malignancy of tumors and can include alterations in motor control, hearing loss, headaches and changes in cognitive ability and autonomic functioning.\nSpecialty professional organizations recommend that neurological imaging of the brain be done only to answer a specific clinical question and not as routine screening.\n\nReferences\nExternal links\n\nOverview of the Central Nervous System at the Wayback Machine (archived 2012-02-18)\nHigh-Resolution Cytoarchitectural Primate Brain Atlases\nExplaining the human nervous system.\nThe Department of Neuroscience at Wikiversity\nCentral nervous system histology","32":"The cerebellopontine angle (CPA) (Latin: angulus cerebellopontinus) is located between the cerebellum and the pons. The cerebellopontine angle is the site of the cerebellopontine angle cistern.\nThe cerebellopontine angle is also the site of a set of neurological disorders known as the cerebellopontine angle syndrome.\n\nStructure\nThe cerebellopontine angle is formed by the cerebellopontine fissure. This fissure is made when the cerebellum folds over to the pons, creating a sharply defined angle between them. The angle formed in turn creates a subarachnoid cistern, the cerebellopontine angle cistern. The pia mater follows the outline of the fissure and the arachnoid mater continues across the divide so that the subarachnoid space is dilated at this area, forming the cerebellopontine angle cistern.\nThe anterior inferior cerebellar artery (AICA) is the principal vessel of the cerebellopontine angle. It also contains two cranial nerves \u2013 the vestibulocochlear nerve and the facial nerve; the cerebellar flocculus and the lateral recess of the fourth ventricle.\n\nClinical significance\nTumours can arise in the cerebellopontine angle. Four out of five of these tumours are vestibular schwannomas (commonly known as acoustic neuromas).\nOthers found include:\n\nArachnoid cyst\nFacial nerve tumour\nLipoma\nMeningioma\nSchwannoma of other cranial nerves (e.g. CN V >VII>IX, X, XI)\nMetastasis\nIntracranial epidermoid cyst\n\nReferences\nExternal links\nhttps:\/\/web.archive.org\/web\/20110720103743\/http:\/\/www.path.utah.edu\/casepath\/PM%20Cases\/PMCase7\/PMCase7Part4.htm","33":"In tetrapods, cervical vertebrae (sg.: vertebra) are the vertebrae of the neck, immediately below the skull. Truncal vertebrae (divided into thoracic and lumbar vertebrae in mammals) lie caudal (toward the tail) of cervical vertebrae. In sauropsid species, the cervical vertebrae bear cervical ribs. In lizards and saurischian dinosaurs, the cervical ribs are large; in birds, they are small and completely fused to the vertebrae. The vertebral transverse processes of mammals are homologous to the cervical ribs of other amniotes. Most mammals have seven cervical vertebrae, with the only three known exceptions being the manatee with six, the two-toed sloth with five or six, and the three-toed sloth with nine. \nIn humans, cervical vertebrae are the smallest of the true vertebrae and can be readily distinguished from those of the thoracic or lumbar regions by the presence of a foramen (hole) in each transverse process, through which the vertebral artery, vertebral veins, and inferior cervical ganglion pass. The remainder of this article focuses upon human anatomy.\n\nStructure\nBy convention, the cervical vertebrae are numbered, with the first one (C1) closest to the skull and higher numbered vertebrae (C2\u2013C7) proceeding away from the skull and down the spine.\nThe general characteristics of the third through sixth cervical vertebrae are described here. The first, second, and seventh vertebrae are extraordinary, and are detailed later.\n\nThe bodies of these four vertebrae are small, and broader from side to side than from front to back.\nThe anterior and posterior surfaces are flattened and of equal depth; the former is placed on a lower level than the latter, and its inferior border is prolonged downward, so as to overlap the upper and forepart of the vertebra below.\nThe upper surface is concave transversely, and presents a projecting lip on either side.\nThe lower surface is concave from front to back, convex from side to side, and presents laterally shallow concavities that receive the corresponding projecting lips of the underlying vertebra.\nThe pedicles are directed laterally and backward, and attach to the body midway between its upper and lower borders, so that the superior vertebral notch is as deep as the inferior, but it is, at the same time, narrower.\nThe laminae are narrow and thinner above than below; the vertebral foramen is large and of a triangular form.\nThe spinous process is short and bifid, the two divisions being often of unequal size. Because the spinous processes are so short, certain superficial muscles (the trapezius and splenius capitis) attach to the nuchal ligament rather than directly to the vertebrae; the nuchal ligament itself attaching to the spinous processes of C2\u2013C7 and to the posterior tubercle of the atlas.\nThe superior and inferior articular processes of cervical vertebrae have fused on either or both sides to form articular pillars, columns of bone that project laterally from the junction of the pedicle and lamina.\nThe articular facets are flat and of an oval form:\nthe superior face backward, upward, and slightly medially.\nthe inferior face forward, downward, and slightly laterally.\nThe transverse processes are each pierced by the foramen transversarium, which, in the upper six vertebrae, gives passage to the vertebral artery and vein, as well as a plexus of sympathetic nerves. Each process consists of an anterior and a posterior part. These two parts are joined, outside the foramen, by a bar of bone that exhibits a deep sulcus on its upper surface for the passage of the corresponding spinal nerve.\nThe anterior portion is the homologue of the rib in the thoracic region, and is therefore named the costal process or costal element. It arises from the side of the body, is directed laterally in front of the foramen, and ends in a tubercle, the anterior tubercle.\nThe posterior part, the true transverse process, springs from the vertebral arch behind the foramen and is directed forward and laterally; it ends in a flattened vertical tubercle, the posterior tubercle.\nThe anterior tubercle of the sixth cervical vertebra is known as the carotid tubercle or Chassaignac tubercle (for \u00c9douard Chassaignac). This separates the carotid artery from the vertebral artery and the carotid artery can be massaged against this tubercle to relieve the symptoms of supraventricular tachycardia. The carotid tubercle is also used as a landmark for anaesthesia of the brachial plexus and cervical plexus.\nThe cervical spinal nerves emerge from above the cervical vertebrae. For example, the cervical spinal nerve 3 (C3) passes above C3.\n\nAtlas and axis\nThe atlas (C1) and axis (C2) are the two topmost vertebrae.\nThe atlas (C1) is the topmost vertebra, and along with the axis forms the joint connecting the skull and spine. It lacks a vertebral body, spinous process, and discs either superior or inferior to it. It is ring-like and consists of an anterior arch, posterior arch, and two lateral masses.\nThe axis (C2) forms the pivot on which the atlas rotates. The most distinctive characteristic of this bone is the strong odontoid process (dens) that rises perpendicularly from the upper surface of the body and articulates with C1. The body is deeper in front than behind, and prolonged downward anteriorly so as to overlap the upper and front part of the third vertebra.\n\nVertebra prominens\nThe vertebra prominens, or C7, has a distinctive long and prominent spinous process, which is palpable from the skin surface. Sometimes, the seventh cervical vertebra is associated with an abnormal extra rib, known as a cervical rib, which develops from the anterior root of the transverse process. These ribs are usually small, but may occasionally compress blood vessels (such as the subclavian artery or subclavian vein) or nerves in the brachial plexus, causing pain, numbness, tingling, and weakness in the upper limb, a condition known as thoracic outlet syndrome. Very rarely, this rib occurs in a pair.\nThe long spinous process of C7 is thick and nearly horizontal in direction. It is not bifurcated, and ends in a tubercle that the ligamentum nuchae attaches to. This process is not always the most prominent of the spinous processes, being found only about 70% of the time, C6 or T1 can sometimes be the most prominent.\nThe transverse processes are of considerable size; their posterior roots are large and prominent, while the anterior are small and faintly marked. The upper surface of each usually has a shallow sulcus for the eighth spinal nerve, and its extremity seldom presents more than a trace of bifurcation.\nThe transverse foramen may be as large as that in the other cervical vertebrae, but it is generally smaller on one or both sides; occasionally, it is double, and sometimes it is absent.\nOn the left side, it occasionally gives passage to the vertebral artery; more frequently, the vertebral vein traverses it on both sides, but the usual arrangement is for both artery and vein to pass in front of the transverse process, not through the foramen.\n\nFunction\nThe movement of nodding the head takes place predominantly through flexion and extension at the atlanto-occipital joint between the atlas and the occipital bone. However, the cervical spine is comparatively mobile, and some component of this movement is due to flexion and extension of the vertebral column itself. This movement between the atlas and occipital bone is often referred to as the \"yes joint\", owing to its nature of being able to move the head in an up-and-down fashion.\nThe movement of shaking or rotating the head left and right happens almost entirely at the joint between the atlas and the axis, the atlanto-axial joint. A small amount of rotation of the vertebral column itself contributes to the movement. This movement between the atlas and axis is often referred to as the \"no joint\", owing to its nature of being able to rotate the head in a side-to-side fashion.\n\nClinical significance\nCervical degenerative changes arise from conditions such as spondylosis, stenosis of intervertebral discs, and the formation of osteophytes. The changes are seen on radiographs, which are used in a grading system from 0\u20134 ranging from no changes (0) to early with minimal development of osteophytes (1) to mild with definite osteophytes (2) to moderate with additional disc space stenosis or narrowing (3) to the stage of many large osteophytes, severe narrowing of the disc space, and more severe vertebral end plate sclerosis (4).\nInjuries to the cervical spine are common at the level of the second cervical vertebrae, but neurological injury is uncommon. C4 and C5 are the areas that see the highest amount of cervical spine trauma.\nIf it does occur, however, it may cause death or profound disability, including paralysis of the arms, legs, and diaphragm, which leads to respiratory failure.\nCommon patterns of injury include the odontoid fracture and the hangman's fracture, both of which are often treated with immobilization in a cervical collar or halo brace.\nA common practice is to immobilize a patient's cervical spine to prevent further damage during transport to hospital. This practice has come under review recently as incidence rates of unstable spinal trauma can be as low as 2% in immobilized patients. In clearing the cervical spine, Canadian studies have developed the Canadian C-Spine Rule (CCR) for physicians to decide who should receive radiological imaging.\n\nLandmarks\nThe vertebral column is often used as a marker of human anatomy. This includes:\n\nAt C1, base of the nose and the hard palate\nAt C2, the teeth of a closed mouth\nAt C3, the mandible and hyoid bone\nAt C4, the common carotid artery bifurcates.\nFrom C4\u20135, the thyroid cartilage\nFrom C6\u20137, the cricoid cartilage\nAt C6, the oesophagus becomes continuous with the laryngopharynx and also where the larynx becomes continuous with the trachea. It is also the level where the carotid pulse can be palpated against the transverse process of the C6 vertebrae.\n\nAdditional images\nSee also\nVertebral column\nCervical fracture\n\nReferences\nThis article incorporates text in the public domain from page 97 of the 20th edition of Gray's Anatomy (1918)\n\nExternal links\n\nDiagram at kenyon.edu\nCervical Spine Anatomy\nMnemonic for Landmarks\nCervical vertebra quiz\nCervical vertebrae - BlueLink Anatomy - University of Michigan Medical School","34":"Spondylosis is the degeneration of the vertebral column from any cause. In the more narrow sense it refers to spinal osteoarthritis, the age-related degeneration of the spinal column, which is the most common cause of spondylosis. The degenerative process in osteoarthritis chiefly affects the vertebral bodies, the neural foramina and the facet joints (facet syndrome). If severe, it may cause pressure on the spinal cord or nerve roots with subsequent sensory or motor disturbances, such as pain, paresthesia, imbalance, and muscle weakness in the limbs.\nWhen the space between two adjacent vertebrae narrows, compression of a nerve root emerging from the spinal cord may result in radiculopathy (sensory and motor disturbances, such as severe pain in the neck, shoulder, arm, back, or leg, accompanied by muscle weakness).  Less commonly, direct pressure on the spinal cord (typically in the cervical spine) may result in myelopathy, characterized by global weakness, gait dysfunction, loss of balance, and loss of bowel or bladder control. The patient may experience shocks (paresthesia) in hands and legs because of nerve compression and lack of blood flow. If vertebrae of the neck are involved it is labelled cervical spondylosis. Lower back spondylosis is labeled lumbar spondylosis. The term is from Ancient Greek \u03c3\u03c0\u03cc\u03bd\u03b4\u03c5\u03bb\u03bf\u03c2 sp\u00f3ndylos, \"a vertebra\", in plural \"vertebrae \u2013 the backbone\".\n\nSigns and symptoms\nCervical spondylosis\nIn cervical spondylosis, a patient may be presented with dull neck pain with neck stiffness in the initial stages of the disease. As the disease progresses, symptoms related to radiculopathy (due to compression of exiting spinal nerve by narrowed intervertebral foramen) or myelopathy (due to compression on the spinal cord) can occur. Reduced range of motion of the neck is the most frequent objective finding on physical examination.\nIn cervical radiculopathy, there would be numbness, tingling, or burning pain at the skin area supplied by the spinal nerve, shooting pain along the course of the spinal nerve, or weakness or absent tendon reflex of the muscle supplied by the nerve. This symptom can be provoked by neck extension. Therefore, Spurling's test, which take advantage of this phenomenon, is performed by extending and laterally flexing the patient's head and placing downward pressure on it to narrow the intervertebral foramen. Neck or shoulder pain on the ipsilateral side (i.e., the side to which the head is flexed) indicates a positive result for this test. A positive test result is not necessarily a positive result for spondylosis and as such additional testing is required.\nIn cervical myelopathy, almost always involves both the upper and lower limbs. A person may experience difficult gait or limb stiffness in the early stages of the disease. Iliopsoas muscle is the first group of muscles that is affected. Lower limb weaknesses without any upper limb involvement should raise the suspicion of thoracic cord compression. Finger escape sign is performed to detect the weakness of the fingers. A person's forearm is pronated and the fingers are extended. If the person has myelopathy, there will be slow abduction and flexion of the fingers on the ulnar side. The degree of loss of sensation may be different on both upper limbs. Lhermitte sign is performed by asking a person to gently extend the neck. Those with cervical myelopathy will produce a feeling of electrical shock down the spine or arms. Muscle spasticity, hyperreflexia or even clonus are characteristic of myelopathy. Other abnormal reflexes such as Hoffmann's reflex, upward response of plantar reflex (Babinski response), and Wartenberg's sign (abnormal abduction of little finger) can also be elicited.\n\nLumbar spondylosis\nSince the spinal cord ends at L1 or L2 vertebral levels, the job of nerve transmission is continued by spinal nerves for the remaining part of the vertebral canal. Degenerative process of spondylosis such as disc bulging, osteophyte formation, and hypertrophy of the superior articular process all contributes to the narrowing of the spinal canal and intervertebral foramen, leading to compression of these spinal nerves that results in radiculopathy-related symptoms. \nNarrowing of the lumbar spinal canal causes a clinical condition known as neurogenic claudication, characterised by symptoms such as lower back pain, leg pain, leg numbness, and leg weakness that worsens with standing and walking and improves with sitting and lying down.\n\nComplications\nA rare but severe complication of this disease is vertebrobasilar insufficiency. This is a result of the vertebral artery becoming occluded as it passes up in the transverse foramen. The spinal joints become stiff in cervical spondylosis. Thus the chondrocytes which maintain the disc become deprived of nutrition and die. Secondary osteophytes may cause stenosis for spinal nerves, manifesting as radiculopathy.\n\nCauses\nCongenital cervical spine stenosis commonly occurs due to short pedicles (that form the vertebral arch). When the spinal canal diameter divided by vertebral body diameter is less than 0.82, or the anteroposterior diameter of the spinal canal is less than 1.3cm or the distance between the pedicles is less than 2.3cm during imaging, cervical spine stenosis is diagnosed symptoms correlate well with the spinal canal narrowings. This is because some patients may not have any symptoms at all even when there is severe cervical spine spondylosis.\nSpondylosis is caused from years of constant abnormal pressure, caused by joint subluxation, stress induced by sports, acute and\/or repetitive trauma, or poor posture, being placed on the vertebrae and the discs between them.  The abnormal stress causes the body to form new bone in order to compensate for the new weight distribution.  This abnormal weight bearing from bone displacement will cause spondylosis to occur.  Poor postures and loss of the normal spinal curves can lead to spondylosis as well.  Spondylosis can affect a person at any age; however, older people are more susceptible.\nDegeneration of the intervertebral disc, facet joints, and its capsules, and ligamentum flavum all can also cause spinal canal narrowing.\n\nDiagnosis\nThose with neck pain only without any positive neurological findings usually do not require an x-ray of the cervical spine. For those with chronic neck pain, a cervical spine x-ray may be indicated. There are various ways of doing cervical spine X-rays such as anteroposterior (AP) view, lateral view, Swimmer's view, and oblique view. Cervical X-rays may show osteophytes, decreased intervertebral disc height, narrowing of the spinal canal, and abnormal alignment (kyphosis of the cervical spine). Flexion and extension view of the cervical spine is helpful to look for spondylolisthesis (slippage of one vertebra over another).\nMRI and CT scans are helpful for diagnosis but generally are not definitive and must be considered together with physical examinations and history.\nCT scan is helpful to see small bony elements of the spine such as facet joint and to determine whether there is osteoporosis of the spine. However, interverterbral foramen and ligaments are not well visualised on the CT. Therefore, contrast is injected into the spinal canal via lumbar puncture and then imaged using CT scan (known as CT myelography). CT myelography is useful when the person is contraindicated to MRI scan due to presence of pacemaker or infusion pump in the body.\nMRI is the investigation of choice to investigate radiculopathy and myelopathy. MRI can show intervertebral foramen, spinal canal, ligaments, degree of disc degeneration or herniation, alignment of the spine, and changes on the spinal cord accurately.\n\nTreatment\nMany of the treatments for cervical spondylosis have not been subjected to rigorous, controlled trials. Surgery is advocated for cervical radiculopathy in patients who have intractable pain, progressive symptoms, or weakness that fails to improve with conservative therapy. Surgical indications for cervical spondylosis with myelopathy (CSM) remain somewhat controversial, but \"most clinicians recommend operative therapy over conservative therapy for moderate-to-severe myelopathy\".\nPhysical therapy may be effective for restoring range of motion, flexibility and core strengthening. Decompressive therapies (i.e., manual mobilization, mechanical traction) may also help alleviate pain. However, physical therapy and osteopathy cannot \"cure\" the degeneration, and some people view that strong compliance with postural modification is necessary to realize maximum benefit from decompression, adjustments and flexibility rehabilitation.\n\nSurgery\nCurrent surgical procedures used to treat spondylosis aim to alleviate the signs and symptoms of the disease by decreasing pressure in the spinal canal (decompression surgery) and\/or by controlling spine movement (fusion surgery) but the evidence is limited in support of some aspects of these procedures. \nIn cervical myelopathy, if the spine still retains its neutral or lordotic alignment, with one or two involved spinal segments, anterior approaches such as anterior cervical discectomy (removal of the intervertebral disc) and fusion (joining two or more vertebrae together), anterior cervical corpectomy (removal of vertebral body) and fusion, and cervical arthroplasty (joint surgery) can be used to relieve the spinal cord from compression. The anterior approach is also preferred when the source of compression arises from the anterior part of the cervical canal. If the cervical spine is in a fixed kyphotic position and with one to two involved spinal segments, posterior approaches such as laminoplasty (removal of lamina with a bone graft or metal plate as replacement) or laminectomy (removal of lamina without any replacement) with or without fusion can be used for decompression. The posterior approach is also used when the source of compression arises from the posterior part of the spinal canal. The posterior approach also avoids some technical challenges associated with the anterior approach, such as obesity, short neck, barrel chest, or previous anterior neck surgery. If three or more spinal segments are involved, both anterior and posterior approaches are used.\nDecompression surgery: The vertebral column can be operated on from both an anterior and posterior approach. The approach varies depending on the site and cause of root compression. Commonly, osteophytes and portions of intervertebral disc are removed.\nFusion surgery: Performed when there is evidence of spinal instability or mal-alignment. Use of instrumentation (such as pedicle screws) in fusion surgeries varies across studies.\n\nSee also\nLaminectomy\n\nReferences\nFurther reading\n\n\n== External links ==","35":"The cerebellum (pl.: cerebella or cerebellums; Latin for \"little brain\") is a major feature of the hindbrain of all vertebrates. Although usually smaller than the cerebrum, in some animals such as the mormyrid fishes it may be as large as it or even larger. In humans, the cerebellum plays an important role in motor control and cognitive functions such as attention and language as well as emotional control such as regulating fear and pleasure responses, but its movement-related functions are the most solidly established. The human cerebellum does not initiate movement, but contributes to coordination, precision, and accurate timing: it receives input from sensory systems of the spinal cord and from other parts of the brain, and integrates these inputs to fine-tune motor activity. Cerebellar damage produces disorders in fine movement, equilibrium, posture, and motor learning in humans.\nAnatomically, the human cerebellum has the appearance of a separate structure attached to the bottom of the brain, tucked underneath the cerebral hemispheres. Its cortical surface is covered with finely spaced parallel grooves, in striking contrast to the broad irregular convolutions of the cerebral cortex. These parallel grooves conceal the fact that the cerebellar cortex is actually a continuous thin layer of tissue tightly folded in the style of an accordion. Within this thin layer are several types of neurons with a highly regular arrangement, the most important being Purkinje cells and granule cells. This complex neural organization gives rise to a massive signal-processing capability, but almost all of the output from the cerebellar cortex passes through a set of small deep nuclei lying in the white matter interior of the cerebellum.\nIn addition to its direct role in motor control, the cerebellum is necessary for several types of motor learning, most notably learning to adjust to changes in sensorimotor relationships. Several theoretical models have been developed to explain sensorimotor calibration in terms of synaptic plasticity within the cerebellum. These models derive from those formulated by David Marr and James Albus, based on the observation that each cerebellar Purkinje cell receives two dramatically different types of input: one comprises thousands of weak inputs from the parallel fibers of the granule cells; the other is an extremely strong input from a single climbing fiber. The basic concept of the Marr\u2013Albus theory is that the climbing fiber serves as a \"teaching signal\", which induces a long-lasting change in the strength of parallel fiber inputs. Observations of long-term depression in parallel fiber inputs have provided some support for theories of this type, but their validity remains controversial.\n\nStructure\nAt the level of gross anatomy, the cerebellum consists of a tightly folded layer of cortex, with white matter underneath and a fluid-filled ventricle at the base. Four deep cerebellar nuclei are embedded in the white matter. Each part of the cortex consists of the same small set of neuronal elements, laid out in a highly stereotyped geometry. At an intermediate level, the cerebellum and its auxiliary structures can be separated into several hundred or thousand independently functioning modules called \"microzones\" or \"microcompartments\".\n\nGross anatomy\nThe cerebellum is located in the posterior cranial fossa. The fourth ventricle, pons and medulla are in front of the cerebellum. It is separated from the overlying cerebrum by a layer of leathery dura mater, the cerebellar tentorium; all of its connections with other parts of the brain travel through the pons. Anatomists classify the cerebellum as part of the metencephalon, which also includes the pons; the metencephalon is the upper part of the rhombencephalon or \"hindbrain\". Like the cerebral cortex, the cerebellum is divided into two cerebellar hemispheres; it also contains a narrow midline zone (the vermis). A set of large folds is, by convention, used to divide the overall structure into 10 smaller \"lobules\". Because of its large number of tiny granule cells, the cerebellum contains more neurons than the total from the rest of the brain, but takes up only 10% of the total brain volume. The number of neurons in the cerebellum is related to the number of neurons in the neocortex. There are about 3.6 times as many neurons in the cerebellum as in the neocortex, a ratio that is conserved across many different mammalian species.\nThe unusual surface appearance of the cerebellum conceals the fact that most of its volume is made up of a very tightly folded layer of gray matter: the cerebellar cortex. Each ridge or gyrus in this layer is called a folium. High\u2011resolution MRI finds the adult human cerebellar cortex has an area of 730 square cm, packed within a volume of dimensions 6 cm \u00d7 5 cm \u00d7 10 cm. Underneath the gray matter of the cortex lies white matter, made up largely of myelinated nerve fibers running to and from the cortex. Embedded within the white matter\u2014which is sometimes called the arbor vitae (tree of life) because of its branched, tree-like appearance in cross-section\u2014are four deep cerebellar nuclei, composed of gray matter.\nConnecting the cerebellum to different parts of the nervous system are three paired cerebellar peduncles. These are the superior cerebellar peduncle, the middle cerebellar peduncle and the inferior cerebellar peduncle, named by their position relative to the vermis. The superior cerebellar peduncle is mainly an output to the cerebral cortex, carrying efferent fibers via thalamic nuclei to upper motor neurons in the cerebral cortex. The fibers arise from the deep cerebellar nuclei. The middle cerebellar peduncle is connected to the pons and receives all of its input from the pons mainly from the pontine nuclei. The input to the pons is from the cerebral cortex and is relayed from the pontine nuclei via transverse pontine fibers to the cerebellum. The middle peduncle is the largest of the three and its afferent fibers are grouped into three separate fascicles taking their inputs to different parts of the cerebellum. The inferior cerebellar peduncle receives input from afferent fibers from the vestibular nuclei, spinal cord and the tegmentum. Output from the inferior peduncle is via efferent fibers to the vestibular nuclei and the reticular formation. The whole of the cerebellum receives modulatory input from the inferior olivary nucleus via the inferior cerebellar peduncle.\n\nSubdivisions\nBased on the surface appearance, three lobes can be distinguished within the cerebellum: the anterior lobe (above the primary fissure), the posterior lobe (below the primary fissure), and the flocculonodular lobe (below the posterior fissure). These lobes divide the cerebellum from rostral to caudal (in humans, top to bottom). In terms of function, however, there is a more important distinction along the medial-to-lateral dimension. Leaving out the flocculonodular lobe, which has distinct connections and functions, the cerebellum can be parsed functionally into a medial sector called the spinocerebellum and a larger lateral sector called the cerebrocerebellum. A narrow strip of protruding tissue along the midline is called the cerebellar vermis. (Vermis is Latin for \"worm\".)\nThe smallest region, the flocculonodular lobe, is often called the vestibulocerebellum. It is the oldest part in evolutionary terms (archicerebellum) and participates mainly in balance and spatial orientation; its primary connections are with the vestibular nuclei, although it also receives visual and other sensory input. Damage to this region causes disturbances of balance and gait.\nThe medial zone of the anterior and posterior lobes constitutes the spinocerebellum, also known as paleocerebellum. This sector of the cerebellum functions mainly to fine-tune body and limb movements. It receives proprioceptive input from the dorsal columns of the spinal cord (including the spinocerebellar tract) and from the cranial trigeminal nerve, as well as from visual and auditory systems. It sends fibers to deep cerebellar nuclei that, in turn, project to both the cerebral cortex and the brain stem, thus providing modulation of descending motor systems.\nThe lateral zone, which in humans is by far the largest part, constitutes the cerebrocerebellum, also known as neocerebellum. It receives input exclusively from the cerebral cortex (especially the parietal lobe) via the pontine nuclei (forming cortico-ponto-cerebellar pathways), and sends output mainly to the ventrolateral thalamus (in turn connected to motor areas of the premotor cortex and primary motor area of the cerebral cortex) and to the red nucleus. There is disagreement about the best way to describe the functions of the lateral cerebellum: It is thought to be involved in planning movement that is about to occur, in evaluating sensory information for action, and in a number of purely cognitive functions, such as determining the verb which best fits with a certain noun (as in \"sit\" for \"chair\").\n\nMicroanatomy\nTwo types of neuron play dominant roles in the cerebellar circuit: Purkinje cells and granule cells. Three types of axons also play dominant roles: mossy fibers and climbing fibers (which enter the cerebellum from outside), and parallel fibers (which are the axons of granule cells). There are two main pathways through the cerebellar circuit, originating from mossy fibers and climbing fibers, both eventually terminating in the deep cerebellar nuclei.\nMossy fibers project directly to the deep nuclei, but also give rise to the following pathway: mossy fibers \u2192 granule cells \u2192 parallel fibers \u2192 Purkinje cells \u2192 deep nuclei. Climbing fibers project to Purkinje cells and also send collaterals directly to the deep nuclei. The mossy fiber and climbing fiber inputs each carry fiber-specific information; the cerebellum also receives dopaminergic, serotonergic, noradrenergic, and cholinergic inputs that presumably perform global modulation.\nThe cerebellar cortex is divided into three layers. At the bottom lies the thick granular layer, densely packed with granule cells, along with interneurons, mainly Golgi cells but also including Lugaro cells and unipolar brush cells. In the middle lies the Purkinje layer, a narrow zone that contains the cell bodies of Purkinje cells and Bergmann glial cells. At the top lies the molecular layer, which contains the flattened dendritic trees of Purkinje cells, along with the huge array of parallel fibers penetrating the Purkinje cell dendritic trees at right angles. This outermost layer of the cerebellar cortex also contains two types of inhibitory interneuron: stellate cells and basket cells. Both stellate and basket cells form GABAergic synapses onto Purkinje cell dendrites.\n\nLayers of the cerebellar cortex\nMolecular layer\nThe top, outermost layer of the cerebellar cortex is the molecular layer. This layer contains the flattened dendritic trees of Purkinje cells, and the huge array of parallel fibers, from the granular layer, that penetrate the Purkinje cell dendritic trees at right angles. The molecular layer also contains two types of inhibitory interneuron: stellate cells and basket cells. Both stellate and basket cells form GABAergic synapses onto Purkinje cell dendrites.\n\nPurkinje layer\nPurkinje cells are among the most distinctive neurons in the brain, and one of the earliest types to be recognized\u2014they were first described by the Czech anatomist Jan Evangelista Purkyn\u011b in 1837. They are distinguished by the shape of their dendritic tree: the dendrites branch very profusely, but are severely flattened in a plane perpendicular to the cerebellar folds. Thus, the dendrites of a Purkinje cell form a dense planar net, through which parallel fibers pass at right angles. The dendrites are covered with dendritic spines, each of which receives synaptic input from a parallel fiber. Purkinje cells receive more synaptic inputs than any other type of cell in the brain\u2014estimates of the number of spines on a single human Purkinje cell run as high as 200,000. The large, spherical cell bodies of Purkinje cells are packed into a narrow layer (one cell thick) of the cerebellar cortex, called the Purkinje layer. After emitting collaterals that affect nearby parts of the cortex, their axons travel into the deep cerebellar nuclei, where they make on the order of 1,000 contacts each with several types of nuclear cells, all within a small domain. Purkinje cells use GABA as their neurotransmitter, and therefore exert inhibitory effects on their targets.\nPurkinje cells form the heart of the cerebellar circuit, and their large size and distinctive activity patterns have made it relatively easy to study their response patterns in behaving animals using extracellular recording techniques. Purkinje cells normally emit action potentials at a high rate even in the absence of the synaptic input. In awake, behaving animals, mean rates averaging around 40 Hz are typical. The spike trains show a mixture of what are called simple and complex spikes. A simple spike is a single action potential followed by a refractory period of about 10 ms; a complex spike is a stereotyped sequence of action potentials with very short inter-spike intervals and declining amplitudes. Physiological studies have shown that complex spikes (which occur at baseline rates around 1 Hz and never at rates much higher than 10 Hz) are reliably associated with climbing fiber activation, while simple spikes are produced by a combination of baseline activity and parallel fiber input. Complex spikes are often followed by a pause of several hundred milliseconds during which simple spike activity is suppressed.\nA specific, recognizable feature of Purkinje neurons is the expression of calbindin. Calbindin staining of rat brain after unilateral chronic sciatic nerve injury suggests that Purkinje neurons may be newly generated in the adult brain, initiating the organization of new cerebellar lobules.\n\nGranular layer\nCerebellar granule cells, in contrast to Purkinje cells, are among the smallest neurons in the brain. They are also the most numerous neurons in the brain: In humans, estimates of their total number average around 50 billion, which means that about 3\/4 of the brain's neurons are cerebellar granule cells. Their cell bodies are packed into a thick layer at the bottom of the cerebellar cortex. A granule cell emits only four to five dendrites, each of which ends in an enlargement called a dendritic claw. These enlargements are sites of excitatory input from mossy fibers and inhibitory input from Golgi cells.\nThe thin, unmyelinated axons of granule cells rise vertically to the upper (molecular) layer of the cortex, where they split in two, with each branch traveling horizontally to form a parallel fiber; the splitting of the vertical branch into two horizontal branches gives rise to a distinctive \"T\" shape. A human parallel fiber runs for an average of 3 mm in each direction from the split, for a total length of about 6 mm (about 1\/10 of the total width of the cortical layer). As they run along, the parallel fibers pass through the dendritic trees of Purkinje cells, contacting one of every 3\u20135 that they pass, making a total of 80\u2013100 synaptic connections with Purkinje cell dendritic spines. Granule cells use glutamate as their neurotransmitter, and therefore exert excitatory effects on their targets.\n\nGranule cells receive all of their input from mossy fibers, but outnumber them by 200 to 1 (in humans). Thus, the information in the granule cell population activity state is the same as the information in the mossy fibers, but recoded in a much more expansive way. Because granule cells are so small and so densely packed, it is difficult to record their spike activity in behaving animals, so there is little data to use as a basis for theorizing. The most popular concept of their function was proposed in 1969 by David Marr, who suggested that they could encode combinations of mossy fiber inputs. The idea is that with each granule cell receiving input from only 4\u20135 mossy fibers, a granule cell would not respond if only a single one of its inputs were active, but would respond if more than one were active. This combinatorial coding scheme would potentially allow the cerebellum to make much finer distinctions between input patterns than the mossy fibers alone would permit.\n\nMossy fibers\nMossy fibers enter the granular layer from their points of origin, many arising from the pontine nuclei, others from the spinal cord, vestibular nuclei etc. In the human cerebellum, the total number of mossy fibers has been estimated at 200 million. These fibers form excitatory synapses with the granule cells and the cells of the deep cerebellar nuclei. Within the granular layer, a mossy fiber generates a series of enlargements called rosettes. The contacts between mossy fibers and granule cell dendrites take place within structures called glomeruli. Each glomerulus has a mossy fiber rosette at its center, and up to 20 granule cell dendritic claws contacting it. Terminals from Golgi cells infiltrate the structure and make inhibitory synapses onto the granule cell dendrites. The entire assemblage is surrounded by a sheath of glial cells. Each mossy fiber sends collateral branches to several cerebellar folia, generating a total of 20\u201330 rosettes; thus a single mossy fiber makes contact with an estimated 400\u2013600 granule cells.\n\nClimbing fibers\nPurkinje cells also receive input from the inferior olivary nucleus on the contralateral side of the brainstem via climbing fibers. Although the inferior olive lies in the medulla oblongata and receives input from the spinal cord, brainstem and cerebral cortex, its output goes entirely to the cerebellum. A climbing fiber gives off collaterals to the deep cerebellar nuclei before entering the cerebellar cortex, where it splits into about 10 terminal branches, each of which gives input to a single Purkinje cell. In striking contrast to the 100,000-plus inputs from parallel fibers, each Purkinje cell receives input from exactly one climbing fiber; but this single fiber \"climbs\" the dendrites of the Purkinje cell, winding around them and making a total of up to 300 synapses as it goes. The net input is so strong that a single action potential from a climbing fiber is capable of producing an extended complex spike in the Purkinje cell: a burst of several spikes in a row, with diminishing amplitude, followed by a pause during which activity is suppressed. The climbing fiber synapses cover the cell body and proximal dendrites; this zone is devoid of parallel fiber inputs.\nClimbing fibers fire at low rates, but a single climbing fiber action potential induces a burst of several action potentials in a target Purkinje cell (a complex spike). The contrast between parallel fiber and climbing fiber inputs to Purkinje cells (over 100,000 of one type versus exactly one of the other type) is perhaps the most provocative feature of cerebellar anatomy, and has motivated much of the theorizing. In fact, the function of climbing fibers is the most controversial topic concerning the cerebellum. There are two schools of thought, one following Marr and Albus in holding that climbing fiber input serves primarily as a teaching signal, the other holding that its function is to shape cerebellar output directly. Both views have been defended in great length in numerous publications. In the words of one review, \"In trying to synthesize the various hypotheses on the function of the climbing fibers, one has the sense of looking at a drawing by Escher. Each point of view seems to account for a certain collection of findings, but when one attempts to put the different views together, a coherent picture of what the climbing fibers are doing does not appear. For the majority of researchers, the climbing fibers signal errors in motor performance, either in the usual manner of discharge frequency modulation or as a single announcement of an 'unexpected event'. For other investigators, the message lies in the degree of ensemble synchrony and rhythmicity among a population of climbing fibers.\"\n\nDeep nuclei\nThe deep nuclei of the cerebellum are clusters of gray matter lying within the white matter at the core of the cerebellum. They are, with the minor exception of the nearby vestibular nuclei, the sole sources of output from the cerebellum. These nuclei receive collateral projections from mossy fibers and climbing fibers as well as inhibitory input from the Purkinje cells of the cerebellar cortex. The four nuclei (dentate, globose, emboliform, and fastigial) each communicate with different parts of the brain and cerebellar cortex. (The globose and the emboliform nuclei are also referred to as combined in the interposed nucleus). The fastigial and interposed nuclei belong to the spinocerebellum. The dentate nucleus, which in mammals is much larger than the others, is formed as a thin, convoluted layer of gray matter, and communicates exclusively with the lateral parts of the cerebellar cortex. The flocculus of the flocculonodular lobe is the only part of the cerebellar cortex that does not project to the deep nuclei\u2014its output goes to the vestibular nuclei instead.\nThe majority of neurons in the deep nuclei have large cell bodies and spherical dendritic trees with a radius of about 400 \u03bcm, and use glutamate as their neurotransmitter. These cells project to a variety of targets outside the cerebellum. Intermixed with them are a lesser number of small cells, which use GABA as a neurotransmitter and project exclusively to the inferior olivary nucleus, the source of climbing fibers. Thus, the nucleo-olivary projection provides an inhibitory feedback to match the excitatory projection of climbing fibers to the nuclei. There is evidence that each small cluster of nuclear cells projects to the same cluster of olivary cells that send climbing fibers to it; there is strong and matching topography in both directions.\nWhen a Purkinje cell axon enters one of the deep nuclei, it branches to make contact with both large and small nuclear cells, but the total number of cells contacted is only about 35 (in cats). Conversely, a single deep nuclear cell receives input from approximately 860 Purkinje cells (again in cats).\n\nCompartments\nFrom the viewpoint of gross anatomy, the cerebellar cortex appears to be a homogeneous sheet of tissue, and, from the viewpoint of microanatomy, all parts of this sheet appear to have the same internal structure. There are, however, a number of respects in which the structure of the cerebellum is compartmentalized. There are large compartments that are generally known as zones; these can be divided into smaller compartments known as microzones.\nThe first indications of compartmental structure came from studies of the receptive fields of cells in various parts of the cerebellar cortex. Each body part maps to specific points in the cerebellum, but there are numerous repetitions of the basic map, forming an arrangement that has been called \"fractured somatotopy\". A clearer indication of compartmentalization is obtained by immunostaining the cerebellum for certain types of protein. The best-known of these markers are called \"zebrins\", because staining for them gives rise to a complex pattern reminiscent of the stripes on a zebra. The stripes generated by zebrins and other compartmentalization markers are oriented perpendicular to the cerebellar folds\u2014that is, they are narrow in the mediolateral direction, but much more extended in the longitudinal direction. Different markers generate different sets of stripes, the widths and lengths vary as a function of location, but they all have the same general shape.\nOscarsson in the late 1970s proposed that these cortical zones can be partitioned into smaller units called microzones. A microzone is defined as a group of Purkinje cells all having the same somatotopic receptive field. Microzones were found to contain on the order of 1000 Purkinje cells each, arranged in a long, narrow strip, oriented perpendicular to the cortical folds. Thus, as the adjoining diagram illustrates, Purkinje cell dendrites are flattened in the same direction as the microzones extend, while parallel fibers cross them at right angles.\nIt is not only receptive fields that define the microzone structure: The climbing fiber input from the inferior olivary nucleus is equally important. The branches of a climbing fiber (usually numbering about 10) usually activate Purkinje cells belonging to the same microzone. Moreover, olivary neurons that send climbing fibers to the same microzone tend to be coupled by gap junctions, which synchronize their activity, causing Purkinje cells within a microzone to show correlated complex spike activity on a millisecond time scale. Also, the Purkinje cells belonging to a microzone all send their axons to the same small cluster of output cells within the deep cerebellar nuclei. Finally, the axons of basket cells are much longer in the longitudinal direction than in the mediolateral direction, causing them to be confined largely to a single microzone. The consequence of all this structure is that cellular interactions within a microzone are much stronger than interactions between different microzones.\nIn 2005, Richard Apps and Martin Garwicz summarized evidence that microzones themselves form part of a larger entity they call a multizonal microcomplex. Such a microcomplex includes several spatially separated cortical microzones, all of which project to the same group of deep cerebellar neurons, plus a group of coupled olivary neurons that project to all of the included microzones as well as to the deep nuclear area.\n\nBlood supply\nThe cerebellum is provided with blood from three paired major arteries: the superior cerebellar artery (SCA), the anterior inferior cerebellar artery (AICA), and the posterior inferior cerebellar artery (PICA). The SCA supplies the upper region of the cerebellum. It divides at the upper surface and branches into the pia mater where the branches anastomose with those of the anterior and posterior inferior cerebellar arteries. The AICA supplies the front part of the undersurface of the cerebellum. The PICA arrives at the undersurface, where it divides into a medial branch and a lateral branch. The medial branch continues backward to the cerebellar notch between the two hemispheres of the cerebellum; while the lateral branch supplies the under surface of the cerebellum, as far as its lateral border, where it anastomoses with the AICA and the SCA.\n\nFunction\nThe strongest clues to the function of the cerebellum have come from examining the consequences of damage to it. Animals and humans with cerebellar dysfunction show, above all, problems with motor control, on the same side of the body as the damaged part of the cerebellum. They continue to be able to generate motor activity but lose precision, producing erratic, uncoordinated, or incorrectly timed movements. A standard test of cerebellar function is to reach with the tip of the finger for a target at arm's length: A healthy person will move the fingertip in a rapid straight trajectory, whereas a person with cerebellar damage will reach slowly and erratically, with many mid-course corrections. Deficits in non-motor functions are more difficult to detect. Thus, the general conclusion reached decades ago is that the basic function of the cerebellum is to calibrate the detailed form of a movement, not to initiate movements or to decide which movements to execute.\nPrior to the 1990s the function of the cerebellum was almost universally believed to be purely motor-related, but newer findings have brought that view into question. Functional imaging studies have shown cerebellar activation in relation to language, attention, and mental imagery; correlation studies have shown interactions between the cerebellum and non-motor areas of the cerebral cortex; and a variety of non-motor symptoms have been recognized in people with damage that appears to be confined to the cerebellum. In particular, the cerebellar cognitive affective syndrome or Schmahmann's syndrome has been described in adults and children. Estimates based on functional mapping of the cerebellum using functional MRI suggest that more than half of the cerebellar cortex is interconnected with association zones of the cerebral cortex.\nKenji Doya has argued that the cerebellum's function is best understood not in terms of the behaviors it affects, but the neural computations it performs; the cerebellum consists of a large number of more or less independent modules, all with the same geometrically regular internal structure, and therefore all, it is presumed, performing the same computation. If the input and output connections of a module are with motor areas (as many are), then the module will be involved in motor behavior; but, if the connections are with areas involved in non-motor cognition, the module will show other types of behavioral correlates. Thus the cerebellum has been implicated in the regulation of many differing functional traits such as affection, emotion including emotional body language perception and behavior. The cerebellum, Doya proposes, is best understood as predictive action selection based on \"internal models\" of the environment or a device for supervised learning, in contrast to the basal ganglia, which perform reinforcement learning, and the cerebral cortex, which performs unsupervised learning. Three decades of brain research have led to the proposal that the cerebellum generates optimized mental models and interacts closely with the cerebral cortex, where updated internal models are experienced as creative intuition (\"a ha\") in working memory.\n\nPrinciples\nThe comparative simplicity and regularity of the cerebellar anatomy led to an early hope that it might imply a similar simplicity of computational function, as expressed in one of the first books on cerebellar electrophysiology, The Cerebellum as a Neuronal Machine by John C. Eccles, Masao Ito, and J\u00e1nos Szent\u00e1gothai. Although a full understanding of cerebellar function has remained elusive, at least four principles have been identified as important: (1) feedforward processing, (2) divergence and convergence, (3) modularity, and (4) plasticity.\n\nFeedforward processing: The cerebellum differs from most other parts of the brain (especially the cerebral cortex) in that the signal processing is almost entirely feedforward\u2014that is, signals move unidirectionally through the system from input to output, with very little recurrent internal transmission. The small amount of recurrence that does exist consists of mutual inhibition; there are no mutually excitatory circuits. This feedforward mode of operation means that the cerebellum, in contrast to the cerebral cortex, cannot generate self-sustaining patterns of neural activity. Signals enter the circuit, are processed by each stage in sequential order, and then leave. As Eccles, Ito, and Szent\u00e1gothai wrote, \"This elimination in the design of all possibility of reverberatory chains of neuronal excitation is undoubtedly a great advantage in the performance of the cerebellum as a computer, because what the rest of the nervous system requires from the cerebellum is presumably not some output expressing the operation of complex reverberatory circuits in the cerebellum but rather a quick and clear response to the input of any particular set of information.\"\nDivergence and convergence: In the human cerebellum, information from 200 million mossy fiber inputs is expanded to 40 billion granule cells, whose parallel fiber outputs then converge onto 15 million Purkinje cells. Because of the way that they are lined up longitudinally, the 1000 or so Purkinje cells belonging to a microzone may receive input from as many as 100 million parallel fibers, and focus their own output down to a group of less than 50 deep nuclear cells. Thus, the cerebellar network receives a modest number of inputs, processes them very extensively through its rigorously structured internal network, and sends out the results via a very limited number of output cells.\nModularity: The cerebellar system is functionally divided into more or less independent modules, which probably number in the hundreds to thousands. All modules have a similar internal structure, but different inputs and outputs. A module (a multizonal microcompartment in the terminology of Apps and Garwicz) consists of a small cluster of neurons in the inferior olivary nucleus, a set of long narrow strips of Purkinje cells in the cerebellar cortex (microzones), and a small cluster of neurons in one of the deep cerebellar nuclei. Different modules share input from mossy fibers and parallel fibers, but in other respects they appear to function independently\u2014the output of one module does not appear to significantly influence the activity of other modules.\nPlasticity: The synapses between parallel fibers and Purkinje cells, and the synapses between mossy fibers and deep nuclear cells, are both susceptible to modification of their strength. In a single cerebellar module, input from as many as a billion parallel fibers converges onto a group of less than 50 deep nuclear cells, and the influence of each parallel fiber on those nuclear cells is adjustable. This arrangement gives tremendous flexibility for fine-tuning the relationship between the cerebellar inputs and outputs.\n\nLearning\nThere is considerable evidence that the cerebellum plays an essential role in some types of motor learning. The tasks where the cerebellum most clearly comes into play are those in which it is necessary to make fine adjustments to the way an action is performed. There has, however, been much dispute about whether learning takes place within the cerebellum itself, or whether it merely serves to provide signals that promote learning in other brain structures. Most theories that assign learning to the circuitry of the cerebellum are derived from the ideas of David Marr and James Albus, who postulated that climbing fibers provide a teaching signal that induces synaptic modification in parallel fiber\u2013Purkinje cell synapses. Marr assumed that climbing fiber input would cause synchronously activated parallel fiber inputs to be strengthened. Most subsequent cerebellar-learning models, however, have followed Albus in assuming that climbing fiber activity would be an error signal, and would cause synchronously activated parallel fiber inputs to be weakened. Some of these later models, such as the Adaptive Filter model of Fujita made attempts to understand cerebellar function in terms of optimal control theory.\nThe idea that climbing fiber activity functions as an error signal has been examined in many experimental studies, with some supporting it but others casting doubt. In a pioneering study by Gilbert and Thach from 1977, Purkinje cells from monkeys learning a reaching task showed increased complex spike activity\u2014which is known to reliably indicate activity of the cell's climbing fiber input\u2014during periods when performance was poor. Several studies of motor learning in cats observed complex spike activity when there was a mismatch between an intended movement and the movement that was actually executed. Studies of the vestibulo\u2013ocular reflex (which stabilizes the visual image on the retina when the head turns) found that climbing fiber activity indicated \"retinal slip\", although not in a very straightforward way.\nOne of the most extensively studied cerebellar learning tasks is the eyeblink conditioning paradigm, in which a neutral conditioned stimulus (CS) such as a tone or a light is repeatedly paired with an unconditioned stimulus (US), such as an air puff, that elicits a blink response. After such repeated presentations of the CS and US, the CS will eventually elicit a blink before the US, a conditioned response or CR. Experiments showed that lesions localized either to a specific part of the interposed nucleus (one of the deep cerebellar nuclei) or to a few specific points in the cerebellar cortex would abolish learning of a conditionally timed blink response. If cerebellar outputs are pharmacologically inactivated while leaving the inputs and intracellular circuits intact, learning takes place even while the animal fails to show any response, whereas, if intracerebellar circuits are disrupted, no learning takes place\u2014these facts taken together make a strong case that the learning, indeed, occurs inside the cerebellum.\n\nTheories and computational models\nThe large base of knowledge about the anatomical structure and behavioral functions of the cerebellum have made it a fertile ground for theorizing\u2014there are perhaps more theories of the function of the cerebellum than of any other part of the brain. The most basic distinction among them is between \"learning theories\" and \"performance theories\"\u2014that is, theories that make use of synaptic plasticity within the cerebellum to account for its role in learning, versus theories that account for aspects of ongoing behavior on the basis of cerebellar signal processing. Several theories of both types have been formulated as mathematical models and simulated using computers.\nPerhaps the earliest \"performance\" theory was the \"delay line\" hypothesis of Valentino Braitenberg. The original theory put forth by Braitenberg and Roger Atwood in 1958 proposed that slow propagation of signals along parallel fibers imposes predictable delays that allow the cerebellum to detect time relationships within a certain window. Experimental data did not support the original form of the theory, but Braitenberg continued to argue for modified versions. The hypothesis that the cerebellum functions essentially as a timing system has also been advocated by Richard Ivry. Another influential \"performance\" theory is the Tensor network theory of Pellionisz and Llin\u00e1s, which provided an advanced mathematical formulation of the idea that the fundamental computation performed by the cerebellum is to transform sensory into motor coordinates.\nTheories in the \"learning\" category almost all derive from publications by Marr and Albus. Marr's 1969 paper proposed that the cerebellum is a device for learning to associate elemental movements encoded by climbing fibers with mossy fiber inputs that encode the sensory context. Albus proposed in 1971 that a cerebellar Purkinje cell functions as a perceptron, a neurally inspired abstract learning device. The most basic difference between the Marr and Albus theories is that Marr assumed that climbing fiber activity would cause parallel fiber synapses to be strengthened, whereas Albus proposed that they would be weakened. Albus also formulated his version as a software algorithm he called a CMAC (Cerebellar Model Articulation Controller), which has been tested in a number of applications.\n\nClinical significance\nDamage to the cerebellum often causes motor-related symptoms, the details of which depend on the part of the cerebellum involved and how it is damaged. Damage to the flocculonodular lobe may show up as a loss of equilibrium and in particular an altered, irregular walking gait, with a wide stance caused by difficulty in balancing. Damage to the lateral zone typically causes problems in skilled voluntary and planned movements which can cause errors in the force, direction, speed and amplitude of movements. Other manifestations include hypotonia (decreased muscle tone), dysarthria (problems with speech articulation), dysmetria (problems judging distances or ranges of movement), dysdiadochokinesia (inability to perform rapid alternating movements such as walking), impaired check reflex or rebound phenomenon, and intention tremor (involuntary movement caused by alternating contractions of opposing muscle groups). Damage to the midline portion may disrupt whole-body movements, whereas damage localized more laterally is more likely to disrupt fine movements of the hands or limbs. Damage to the upper part of the cerebellum tends to cause gait impairments and other problems with leg coordination; damage to the lower part is more likely to cause uncoordinated or poorly aimed movements of the arms and hands, as well as difficulties in speed. This complex of motor symptoms is called ataxia.\nTo identify cerebellar problems, neurological examination includes assessment of gait (a broad-based gait being indicative of ataxia), finger-pointing tests and assessment of posture. If cerebellar dysfunction is indicated, a magnetic resonance imaging scan can be used to obtain a detailed picture of any structural alterations that may exist.\nThe list of medical problems that can produce cerebellar damage is long, including stroke, hemorrhage, swelling of the brain (cerebral edema), tumors, alcoholism, physical trauma such as gunshot wounds or explosives, and chronic degenerative conditions such as olivopontocerebellar atrophy. Some forms of migraine headache may also produce temporary dysfunction of the cerebellum, of variable severity. Infection can result in cerebellar damage in such conditions as the prion diseases and Miller Fisher syndrome, a variant of Guillain\u2013Barr\u00e9 syndrome.\n\nAging\nThe human cerebellum changes with age. These changes may differ from those of other parts of the brain.\nThe cerebellum is the youngest brain region (and body part) in centenarians according to an epigenetic biomarker of tissue age known as epigenetic clock: it is about 15 years younger than expected in a centenarian. Further, gene expression patterns in the human cerebellum show less age-related alteration than that in the cerebral cortex.\nSome studies have reported reductions in numbers of cells or volume of tissue, but the amount of data relating to this question is not very large.\n\nDevelopmental and degenerative disorders\nCongenital malformation, hereditary disorders, and acquired conditions can affect cerebellar structure and, consequently, cerebellar function. Unless the causative condition is reversible, the only possible treatment is to help people live with their problems. Visualization of the fetal cerebellum by ultrasound scan at 18 to 20 weeks of pregnancy can be used to screen for fetal neural tube defects with a sensitivity rate of up to 99%.\nIn normal development, endogenous sonic hedgehog signaling stimulates rapid proliferation of cerebellar granule neuron progenitors (CGNPs) in the external granule layer (EGL). Cerebellar development occurs during late embryogenesis and the early postnatal period, with CGNP proliferation in the EGL peaking during early development (postnatal day 7 in the mouse). As CGNPs terminally differentiate into cerebellar granule cells (also called cerebellar granule neurons, CGNs), they migrate to the internal granule layer (IGL), forming the mature cerebellum (by post-natal day 20 in the mouse). Mutations that abnormally activate Sonic hedgehog signaling predispose to cancer of the cerebellum (medulloblastoma) in humans with Gorlin Syndrome and in genetically engineered mouse models.\nCongenital malformation or underdevelopment (hypoplasia) of the cerebellar vermis is a characteristic of both Dandy\u2013Walker syndrome and Joubert syndrome. In very rare cases, the entire cerebellum may be absent. The inherited neurological disorders Machado\u2013Joseph disease, ataxia telangiectasia, and Friedreich's ataxia cause progressive neurodegeneration linked to cerebellar loss. Congenital brain malformations outside the cerebellum can, in turn, cause herniation of cerebellar tissue, as seen in some forms of Arnold\u2013Chiari malformation.\nOther conditions that are closely linked to cerebellar degeneration include the idiopathic progressive neurological disorders multiple system atrophy and Ramsay Hunt syndrome type I, and the autoimmune disorder paraneoplastic cerebellar degeneration, in which tumors elsewhere in the body elicit an autoimmune response that causes neuronal loss in the cerebellum. Cerebellar atrophy can result from an acute deficiency of vitamin B1 (thiamine) as seen in beriberi and in Wernicke\u2013Korsakoff syndrome, or vitamin E deficiency.\nCerebellar atrophy has been observed in many other neurological disorders including Huntington's disease, multiple sclerosis, essential tremor, progressive myoclonus epilepsy, and Niemann\u2013Pick disease. Cerebellar atrophy can also occur as a result of exposure to toxins including heavy metals or pharmaceutical or recreational drugs.\n\nPain\nThere is a general consensus that the cerebellum is involved in pain processing. The cerebellum receives pain input from both descending cortico-cerebellar pathways and ascending spino-cerebellar pathways, through the pontine nuclei and inferior olives. Some of this information is transferred to the motor system inducing a conscious motor avoidance of pain, graded according to pain intensity.\nThese direct pain inputs, as well as indirect inputs, are thought to induce long-term pain avoidance behavior that results in chronic posture changes and consequently, in functional and anatomical remodeling of vestibular and proprioceptive nuclei. As a result, chronic neuropathic pain can induce macroscopic anatomical remodeling of the hindbrain, including the cerebellum. The magnitude of this remodeling and the induction of neuron progenitor markers suggest the contribution of adult neurogenesis to these changes.\n\nComparative anatomy and evolution\nThe circuits in the cerebellum are similar across all classes of vertebrates, including fish, reptiles, birds, and mammals. There is also an analogous brain structure in cephalopods with well-developed brains, such as octopuses. This has been taken as evidence that the cerebellum performs functions important to all animal species with a brain.\nThere is considerable variation in the size and shape of the cerebellum in different vertebrate species. In amphibians, it is little developed, and in lampreys, and hagfish, the cerebellum is barely distinguishable from the brain-stem. Although the spinocerebellum is present in these groups, the primary structures are small, paired-nuclei corresponding to the vestibulocerebellum. The cerebellum is a bit larger in reptiles, considerably larger in birds, and larger still in mammals. The large paired and convoluted lobes found in humans are typical of mammals, but the cerebellum is, in general, a single median lobe in other groups, and is either smooth or only slightly grooved. In mammals, the neocerebellum is the major part of the cerebellum by mass, but, in other vertebrates, it is typically the spinocerebellum.\nThe cerebellum of cartilaginous and bony fishes is extraordinarily large and complex. In at least one important respect, it differs in internal structure from the mammalian cerebellum: The fish cerebellum does not contain discrete deep cerebellar nuclei. Instead, the primary targets of Purkinje cells are a distinct type of cell distributed across the cerebellar cortex, a type not seen in mammals. In mormyrid fish (a family of weakly electrosensitive freshwater fish), the cerebellum is considerably larger than the rest of the brain. The largest part of it is a special structure called the valvula, which has an unusually regular architecture and receives much of its input from the electrosensory system.\nThe hallmark of the mammalian cerebellum is an expansion of the lateral lobes, whose main interactions are with the neocortex. As monkeys evolved into great apes, the expansion of the lateral lobes continued, in tandem with the expansion of the frontal lobes of the neocortex. In ancestral hominids, and in Homo sapiens until the middle Pleistocene period, the cerebellum continued to expand, but the frontal lobes expanded more rapidly. The most recent period of human evolution, however, may actually have been associated with an increase in the relative size of the cerebellum, as the neocortex reduced its size somewhat while the cerebellum expanded. The size of the human cerebellum, compared to the rest of the brain, has been increasing in size while the cerebrum decreased in size  With both the development and implementation of motor tasks, visual-spatial skills and learning taking place in the cerebellum, the growth of the cerebellum is thought to have some form of correlation to greater human cognitive abilities. The lateral hemispheres of the cerebellum are now 2.7 times greater in both humans and apes than they are in monkeys. These changes in the cerebellum size cannot be explained by greater muscle mass. They show that either the development of the cerebellum is tightly linked to that of the rest of the brain or that neural activities taking place in the cerebellum were important during Hominidae evolution. Due to the cerebellum's role in cognitive functions, the increase in its size may have played a role in cognitive expansion.\n\nCerebellum-like structures\nMost vertebrate species have a cerebellum and one or more cerebellum-like structures, brain areas that resemble the cerebellum in terms of cytoarchitecture and neurochemistry. The only cerebellum-like structure found in mammals is the dorsal cochlear nucleus (DCN), one of the two primary sensory nuclei that receive input directly from the auditory nerve. The DCN is a layered structure, with the bottom layer containing granule cells similar to those of the cerebellum, giving rise to parallel fibers that rise to the superficial layer and travel across it horizontally. The superficial layer contains a set of GABAergic neurons called cartwheel cells that resemble Purkinje cells anatomically and chemically\u2014they receive parallel fiber input, but do not have any inputs that resemble climbing fibers. The output neurons of the DCN are pyramidal cells. They are glutamatergic, but also resemble Purkinje cells in some respects\u2014they have spiny, flattened superficial dendritic trees that receive parallel fiber input, but they also have basal dendrites that receive input from auditory nerve fibers, which travel across the DCN in a direction at right angles to the parallel fibers. The DCN is most highly developed in rodents and other small animals, and is considerably reduced in primates. Its function is not well understood; the most popular speculations relate it to spatial hearing in one way or another.\nMost species of fish and amphibians possess a lateral line system that senses pressure waves in water. One of the brain areas that receives primary input from the lateral line organ, the medial octavolateral nucleus, has a cerebellum-like structure, with granule cells and parallel fibers. In electrosensitive fish, the input from the electrosensory system goes to the dorsal octavolateral nucleus, which also has a cerebellum-like structure. In ray-finned fishes (by far the largest group), the optic tectum has a layer\u2014the marginal layer\u2014that is cerebellum-like.\nAll of these cerebellum-like structures appear to be primarily sensory-related rather than motor-related. All of them have granule cells that give rise to parallel fibers that connect to Purkinje-like neurons with modifiable synapses, but none have climbing fibers comparable to those of the cerebellum\u2014instead they receive direct input from peripheral sensory organs. None has a demonstrated function, but the most influential speculation is that they serve to transform sensory inputs in some sophisticated way, perhaps to compensate for changes in body posture. In fact, James M. Bower and others have argued, partly on the basis of these structures and partly on the basis of cerebellar studies, that the cerebellum itself is fundamentally a sensory structure, and that it contributes to motor control by moving the body in a way that controls the resulting sensory signals. Despite Bower's viewpoint, there is also strong evidence that the cerebellum directly influences motor output in mammals.\n\nHistory\nDescriptions\nEven the earliest anatomists were able to recognize the cerebellum by its distinctive appearance. Aristotle and Herophilus (quoted in Galen) called it the \u03c0\u03b1\u03c1\u03b5\u03b3\u03ba\u03b5\u03c6\u03b1\u03bb\u03af\u03c2 (parenkephalis), as opposed to the \u1f10\u03b3\u03ba\u03ad\u03c6\u03b1\u03bb\u03bf\u03c2 (enkephalos) or brain proper. Galen's extensive description is the earliest that survives. He speculated that the cerebellum was the source of motor nerves.\nFurther significant developments did not come until the Renaissance. Vesalius discussed the cerebellum briefly, and the anatomy was described more thoroughly by Thomas Willis in 1664. More anatomical work was done during the 18th century, but it was not until early in the 19th century that the first insights into the function of the cerebellum were obtained. Luigi Rolando in 1809 established the key finding that damage to the cerebellum results in motor disturbances. Jean Pierre Flourens in the first half of the 19th century carried out detailed experimental work, which revealed that animals with cerebellar damage can still move, but with a loss of coordination (strange movements, awkward gait, and muscular weakness), and that recovery after the lesion can be nearly complete unless the lesion is very extensive. By the beginning of the 20th century, it was widely accepted that the primary function of the cerebellum relates to motor control; the first half of the 20th century produced several detailed descriptions of the clinical symptoms associated with cerebellar disease in humans.\n\nEtymology\nThe name cerebellum is a diminutive of cerebrum (brain); it can be translated literally as little brain. The Latin name is a direct translation of the Ancient Greek \u03c0\u03b1\u03c1\u03b5\u03b3\u03ba\u03b5\u03c6\u03b1\u03bb\u03af\u03c2 (parenkephalis), which was used in the works of Aristotle, the first known writer to describe the structure. No other name is used in the English-language literature, but historically a variety of Greek or Latin-derived names have been used, including cerebrum parvum, encephalion, encranion, cerebrum posterius, and parencephalis.\n\nSee also\nVestibulo\u2013ocular reflex\nEyeblink conditioning\n\nReferences\nThis article was submitted to WikiJournal of Medicine for external academic peer review in 2016 (reviewer reports). The updated content was reintegrated into the Wikipedia page under a CC-BY-SA-3.0 license (2016). The version of record as reviewed is: \nMarion Wright; William Skaggs; Finn \u00c5rup Nielsen; et al. (30 June 2016). \"The Cerebellum\" (PDF). WikiJournal of Medicine. 3 (1). doi:10.15347\/WJM\/2016.001. ISSN 2002-4436. Wikidata Q44001486.\n\nExternal links\n\nLlinas R, Negrello MN (2015). \"Cerebellum\". Scholarpedia. 10 (1): 4606. Bibcode:2015SchpJ..10.4606L. doi:10.4249\/scholarpedia.4606.\nCerebellum\u2013Cell Centered Database\nHandbook of the Cerebellum and Cerebellar Disorders \u2013 Manto, M., Gruol, D.L., Schmahmann, J., Koibuchi, N., Rossi, F. (Eds.) \u2013 Springer \u2013 New York\nStained brain slice images which include the \"cerebellum\" at the BrainMaps project\nA Man's Incomplete Brain Reveals Cerebellum's Role In Thought And Emotion\nWoman living without cerebellum\nEssentials of Cerebellum and Cerebellar Disorders. Gruol, D.L., Koibuchi, N., Manto, M., Molinari, M., Schmahmann, J.D., Shen, Y. (Eds.). Springer, New York, 2016\nCerebellum histology images\nThe Cerebellum \u2013 Journal (Springer Nature)\nCerebellum and Ataxias \u2013 Journal (BioMed Central)","36":"Chemical synapses are biological junctions through which neurons' signals can be sent to each other and to non-neuronal cells such as those in muscles or glands. Chemical synapses allow neurons to form circuits within the central nervous system. They are crucial to the biological computations that underlie perception and thought. They allow the nervous system to connect to and control other systems of the body.\nAt a chemical synapse, one neuron releases neurotransmitter molecules into a small space (the synaptic cleft) that is adjacent to another neuron. The neurotransmitters are contained within small sacs called synaptic vesicles, and are released into the synaptic cleft by exocytosis. These molecules then bind to neurotransmitter receptors on the postsynaptic cell. Finally, the neurotransmitters are cleared from the synapse through one of several potential mechanisms including enzymatic degradation or re-uptake by specific transporters either on the presynaptic cell or on some other neuroglia to terminate the action of the neurotransmitter.\nThe adult human brain is estimated to contain from 1014 to 5 \u00d7 1014 (100\u2013500 trillion) synapses. Every cubic millimeter of cerebral cortex contains roughly a billion (short scale, i.e. 109) of them. The number of synapses in the  human cerebral cortex has separately been estimated at 0.15 quadrillion (150 trillion)\nThe word \"synapse\" was introduced by Sir Charles Scott Sherrington in 1897. Chemical synapses are not the only type of biological synapse: electrical and immunological synapses also exist. Without a qualifier, however, \"synapse\" commonly refers to chemical synapses.\n\nStructure\nSynapses are functional connections between neurons, or between neurons and other types of cells.  A typical neuron gives rise to several thousand synapses, although there are some types that make far fewer.  Most synapses connect axons to dendrites, but there are also other types of connections, including axon-to-cell-body, axon-to-axon, and dendrite-to-dendrite. Synapses are generally too small to be recognizable using a light microscope except as points where the membranes of two cells appear to touch, but their cellular elements can be visualized clearly using an electron microscope.\nChemical synapses pass information directionally from a presynaptic cell to a postsynaptic cell and are therefore asymmetric in structure and function. The presynaptic axon terminal, or synaptic bouton, is a specialized area within the axon of the presynaptic cell that contains neurotransmitters enclosed in small membrane-bound spheres called synaptic vesicles (as well as a number of other supporting structures and organelles, such as mitochondria and endoplasmic reticulum). Synaptic vesicles are docked at the presynaptic plasma membrane at regions called active zones.\nImmediately opposite is a region of the postsynaptic cell containing neurotransmitter receptors; for synapses between two neurons the postsynaptic region may be found on the dendrites or cell body. Immediately behind the postsynaptic membrane is an elaborate complex of interlinked proteins called the postsynaptic density (PSD).\nProteins in the PSD are involved in anchoring and trafficking neurotransmitter receptors and modulating the activity of these receptors. The receptors and PSDs are often found in specialized protrusions from the main dendritic shaft called dendritic spines.\nSynapses may be described as symmetric or asymmetric. When examined under an electron microscope, asymmetric synapses are characterized by rounded vesicles in the presynaptic cell, and a prominent postsynaptic density. Asymmetric synapses are typically excitatory. Symmetric synapses in contrast have flattened or elongated vesicles, and do not contain a prominent postsynaptic density. Symmetric synapses are typically inhibitory.\nThe synaptic cleft\u2014also called synaptic gap\u2014is a gap between the pre- and postsynaptic cells that is about 20 nm (0.02 \u03bc) wide. The small volume of the cleft allows neurotransmitter concentration to be raised and lowered rapidly.\nAn autapse is a chemical (or electrical) synapse formed when the axon of one neuron synapses with its own dendrites.\n\nSignaling in chemical synapses\nOverview\nHere is a summary of the sequence of events that take place in synaptic transmission from a presynaptic neuron to a postsynaptic cell.  Each step is explained in more detail below.  Note that with the exception of the final step, the entire process may run only a few hundred microseconds, in the fastest synapses.\n\nThe process begins with a wave of electrochemical excitation called an action potential traveling along the membrane of the presynaptic cell, until it reaches the synapse.\nThe electrical depolarization of the membrane at the synapse causes channels to open that are permeable to calcium ions.\nCalcium ions flow through the presynaptic membrane, rapidly increasing the calcium concentration in the interior.\nThe high calcium concentration activates a set of calcium-sensitive proteins attached to vesicles that contain a neurotransmitter chemical.\nThese proteins change shape, causing the membranes of some \"docked\" vesicles to fuse with the membrane of the presynaptic cell, thereby opening the vesicles and dumping their neurotransmitter contents into the synaptic cleft, the narrow space between the membranes of the pre- and postsynaptic cells.\nThe neurotransmitter diffuses within the cleft.  Some of it escapes, but some of it binds to chemical receptor molecules located on the membrane of the postsynaptic cell.\nThe binding of neurotransmitter causes the receptor molecule to be activated in some way.  Several types of activation are possible, as described in more detail below.  In any case, this is the key step by which the synaptic process affects the behavior of the postsynaptic cell.\nDue to thermal vibration, the motion of atoms, vibrating about their equilibrium positions in a crystalline solid, neurotransmitter molecules eventually break loose from the receptors and drift away.\nThe neurotransmitter is either reabsorbed by the presynaptic cell, and then repackaged for future release, or else it is broken down metabolically.\n\nNeurotransmitter release\nThe release of a neurotransmitter is triggered by the arrival of a nerve impulse (or action potential) and occurs through an unusually rapid process of cellular secretion (exocytosis). Within the presynaptic nerve terminal, vesicles containing neurotransmitter are localized near the synaptic membrane. The arriving action potential produces an influx of calcium ions through voltage-dependent, calcium-selective ion channels at the down stroke of the action potential (tail current). Calcium ions then bind to synaptotagmin proteins found within the membranes of the synaptic vesicles, allowing the vesicles to fuse with the presynaptic membrane. The fusion of a vesicle is a stochastic process, leading to frequent failure of synaptic transmission at the very small synapses that are typical for the central nervous system. Large chemical synapses (e.g. the neuromuscular junction), on the other hand, have a synaptic release probability, in effect, of 1. Vesicle fusion is driven by the action of a set of proteins in the presynaptic terminal known as SNAREs. As a whole, the protein complex or structure that mediates the docking and fusion of presynaptic vesicles is called the active zone. The membrane added by the fusion process is later retrieved by endocytosis and recycled for the formation of fresh neurotransmitter-filled vesicles.\nAn exception to the general trend of neurotransmitter release by vesicular fusion is found in the type II receptor cells of  mammalian taste buds. Here the neurotransmitter ATP is released directly from the cytoplasm into the synaptic cleft via voltage gated channels.\n\nReceptor binding\nReceptors on the opposite side of the synaptic gap bind neurotransmitter molecules. Receptors can respond in either of two general ways. First, the receptors may directly open ligand-gated ion channels in the postsynaptic cell membrane, causing ions to enter or exit the cell and changing the local transmembrane potential. The resulting change in voltage is called a postsynaptic potential. In general, the result is excitatory in the case of depolarizing currents, and inhibitory in the case of hyperpolarizing currents. Whether a synapse is excitatory or inhibitory depends on what type(s) of ion channel conduct the postsynaptic current(s), which in turn is a function of the type of receptors and neurotransmitter employed at the synapse. The second way a receptor can affect membrane potential is by modulating the production of chemical messengers inside the postsynaptic neuron. These second messengers can then amplify the inhibitory or excitatory response to neurotransmitters.\n\nTermination\nAfter a neurotransmitter molecule binds to a receptor molecule, it must be removed to allow for the postsynaptic membrane to continue to relay subsequent EPSPs and\/or IPSPs. This removal can happen through one or more processes:\n\nThe neurotransmitter may diffuse away due to thermally-induced oscillations of both it and the receptor, making it available to be broken down metabolically outside the neuron or to be reabsorbed.\nEnzymes within the subsynaptic membrane may inactivate\/metabolize the neurotransmitter.\nReuptake pumps may actively pump the neurotransmitter back into the presynaptic axon terminal for reprocessing and re-release following a later action potential.\n\nSynaptic strength\nThe strength of a synapse has been defined by Bernard Katz as the product of (presynaptic) release probability pr, quantal size q (the postsynaptic response to the release of a single neurotransmitter vesicle, a 'quantum'), and n, the number of release sites. \"Unitary connection\" usually refers to an unknown number of individual synapses connecting a presynaptic neuron to a postsynaptic neuron.  \nThe amplitude of postsynaptic potentials (PSPs) can be as low as 0.4 mV to as high as 20 mV.  The amplitude of a PSP can be modulated by neuromodulators or can change as a result of previous activity.  Changes in the synaptic strength can be short-term, lasting seconds to minutes, or long-term (long-term potentiation, or LTP), lasting hours.  Learning and memory are believed to result from long-term changes in synaptic strength, via a mechanism known as synaptic plasticity.\n\nReceptor desensitization\nDesensitization of the postsynaptic receptors is a decrease in response to the same neurotransmitter stimulus. It means that the strength of a synapse may in effect diminish as a train of action potentials arrive in rapid succession \u2013 a phenomenon that gives rise to the so-called frequency dependence of synapses. The nervous system exploits this property for computational purposes, and can tune its synapses through such means as phosphorylation of the proteins involved.\n\nSynaptic plasticity\nSynaptic transmission can be changed by previous activity. These changes are called synaptic plasticity and may result in either a decrease in the efficacy of the synapse, called depression,  or an increase in efficacy, called potentiation. These changes can either be long-term or short-term. Forms of short-term plasticity include synaptic fatigue or depression and synaptic augmentation. Forms of long-term plasticity include long-term depression and long-term potentiation. Synaptic plasticity can be either homosynaptic (occurring at a single synapse) or heterosynaptic (occurring at multiple synapses).\n\nHomosynaptic plasticity\nHomosynaptic plasticity (or also homotropic modulation) is a change in the synaptic strength that results from the history of activity at a particular synapse.  This can result from changes in presynaptic calcium as well as feedback onto presynaptic receptors, i.e. a form of autocrine signaling. Homosynaptic plasticity can affect the number and replenishment rate of vesicles or it can affect the relationship between calcium and vesicle release. Homosynaptic plasticity can also be postsynaptic in nature. It can result in either an increase or decrease in synaptic strength.\nOne example is neurons of the sympathetic nervous system (SNS), which release noradrenaline, which, besides affecting postsynaptic receptors, also affects presynaptic \u03b12-adrenergic receptors, inhibiting further release of noradrenaline. This effect is utilized with clonidine to perform inhibitory effects on the SNS.\n\nHeterosynaptic plasticity\nHeterosynaptic plasticity (or also heterotropic modulation) is a change in synaptic strength that results from the activity of other neurons. Again, the plasticity can alter the number of vesicles or their replenishment rate or the relationship between calcium and vesicle release.  Additionally, it could directly affect calcium influx.  Heterosynaptic plasticity can also be postsynaptic in nature, affecting receptor sensitivity.\nOne example is again neurons of the sympathetic nervous system, which release noradrenaline, which, in addition, generates an inhibitory effect on presynaptic terminals of neurons of the parasympathetic nervous system.\n\nIntegration of synaptic inputs\nIn general, if an excitatory synapse is strong enough, an action potential in the presynaptic neuron will trigger an action potential in the postsynaptic cell. In many cases the excitatory postsynaptic potential (EPSP) will not reach the threshold for eliciting an action potential. When action potentials from multiple presynaptic neurons fire simultaneously, or if a single presynaptic neuron fires at a high enough frequency, the EPSPs can overlap and summate. If enough EPSPs overlap, the summated EPSP can reach the threshold for initiating an action potential. This process is known as summation, and can serve as a high pass filter for neurons.\nOn the other hand, a presynaptic neuron releasing an inhibitory neurotransmitter, such as GABA, can cause an inhibitory postsynaptic potential (IPSP) in the postsynaptic neuron, bringing the membrane potential farther away from the threshold, decreasing its excitability and making it more difficult for the neuron to initiate an action potential. If an IPSP overlaps with an EPSP, the IPSP can in many cases prevent the neuron from firing an action potential. In this way, the output of a neuron may depend on the input of many different neurons, each of which may have a different degree of influence, depending on the strength and type of synapse with that neuron. John Carew Eccles performed some of the important early experiments on synaptic integration, for which he received the Nobel Prize for Physiology or Medicine in 1963.\n\nVolume transmission\nWhen a neurotransmitter is released at a synapse, it reaches its highest concentration inside the narrow space of the synaptic cleft, but some of it is certain to diffuse away before being reabsorbed or broken down.  If it diffuses away, it has the potential to activate receptors that are located either at other synapses or on the membrane away from any synapse.  The extrasynaptic activity of a neurotransmitter is known as volume transmission.  It is well established that such effects occur to some degree, but their functional importance has long been a matter of controversy.\nRecent work indicates that volume transmission may be the predominant mode of interaction for some special types of neurons.  In the mammalian cerebral cortex, a class of neurons called neurogliaform cells can inhibit other nearby cortical neurons by releasing the neurotransmitter GABA into the extracellular space. Along the same vein, GABA released from neurogliaform cells into the extracellular space also acts on surrounding astrocytes, assigning a role for volume transmission in the control of ionic and neurotransmitter homeostasis. Approximately 78% of neurogliaform cell boutons do not form classical synapses. This may be the first definitive example of neurons communicating chemically where classical synapses are not present.\n\nRelationship to electrical synapses\nAn electrical synapse is an electrically conductive link between two abutting neurons that is formed at a narrow gap between the pre- and postsynaptic cells, known as a gap junction. At gap junctions, cells approach within about 3.5 nm of each other, rather than the 20 to 40 nm distance that separates cells at chemical synapses. As opposed to chemical synapses, the postsynaptic potential in electrical synapses is not caused by the opening of ion channels by chemical transmitters, but rather by direct electrical coupling between both neurons. Electrical synapses are faster than chemical synapses. Electrical synapses are found throughout the nervous system, including in the retina, the reticular nucleus of the thalamus, the neocortex, and in the hippocampus. While chemical synapses are found between both excitatory and inhibitory neurons, electrical synapses are most commonly found between smaller local inhibitory neurons.  Electrical synapses can exist between two axons, two dendrites, or between an axon and a dendrite.  In some fish and amphibians, electrical synapses can be found within the same terminal of a chemical synapse, as in Mauthner cells.\n\nEffects of drugs\nOne of the most important features of chemical synapses is that they are the site of action for the majority of psychoactive drugs.  Synapses are affected by drugs, such as curare, strychnine, cocaine, morphine, alcohol, LSD, and countless others. These drugs have different effects on synaptic function, and often are restricted to synapses that use a specific neurotransmitter. For example, curare is a poison that stops acetylcholine from depolarizing the postsynaptic membrane, causing paralysis. Strychnine blocks the inhibitory effects of the neurotransmitter glycine, which causes the body to pick up and react to weaker and previously ignored stimuli, resulting in uncontrollable muscle spasms. Morphine acts on synapses that use endorphin neurotransmitters, and alcohol increases the inhibitory effects of the neurotransmitter GABA. LSD interferes with synapses that use the neurotransmitter serotonin. Cocaine blocks reuptake of dopamine and therefore increases its effects.\n\nHistory and etymology\nDuring the 1950s, Bernard Katz and Paul Fatt observed spontaneous miniature synaptic currents at the frog neuromuscular junction. Based on these observations, they developed the 'quantal hypothesis' that is the basis for our current understanding of neurotransmitter release as exocytosis and for which Katz received the Nobel Prize in Physiology or Medicine in 1970. In the late 1960s, Ricardo Miledi and Katz advanced the hypothesis that depolarization-induced influx of calcium ions triggers exocytosis.\nSir Charles Scott Sherringtonin coined the word 'synapse' and the history of the word was given by Sherrington in a letter he wrote to John Fulton:\n\n'I felt the need of some name to call the junction between nerve-cell and nerve-cell... I suggested using \"syndesm\"... He [ Sir Michael Foster ] consulted his Trinity friend Verrall, the Euripidean scholar, about it, and Verrall suggested \"synapse\" (from the Greek \"clasp\").'\u2013Charles Scott Sherrington\n\nSee also\nAcclimatisation (neurons)\nNeuroscience\nNeurexin\nRibbon synapse\n\nNotes\nReferences\nCarlson, Neil R. (2007). Physiology of Behavior (9th ed.). Boston, MA: Pearson Education. ISBN 978-0-205-59389-7.\nKandel, Eric R.; Schwartz, James H.; Jessell, Thomas M. (2000). Principles of Neural Science (4th ed.). New York: McGraw-Hill. ISBN 978-0-8385-7701-1.\nLlin\u00e1s R, Sugimori M, Simon SM (April 1982). \"Transmission by presynaptic spike-like depolarization in the squid giant synapse\". Proc. Natl. Acad. Sci. U.S.A. 79 (7): 2415\u20139. Bibcode:1982PNAS...79.2415L. doi:10.1073\/pnas.79.7.2415. PMC 346205. PMID 6954549.\nLlin\u00e1s R, Steinberg IZ, Walton K (1981). \"Relationship between presynaptic calcium current and postsynaptic potential in squid giant synapse\". Biophysical Journal. 33 (3): 323\u2013352. Bibcode:1981BpJ....33..323L. doi:10.1016\/S0006-3495(81)84899-0. PMC 1327434. PMID 6261850.\nBear, Mark F.; Connors, Barry W.; Paradiso, Michael A. (2001). Neuroscience: Exploring the Brain. Hagerstown, MD: Lippincott Williams & Wilkins. ISBN 978-0-7817-3944-3.\nHormuzdi, SG; Filippov, MA; Mitropoulou, G; Monyer, H; Bruzzone, R (March 2004). \"Electrical synapses: a dynamic signaling system that shapes the activity of neuronal networks\". Biochim Biophys Acta. 1662 (1\u20132): 113\u2013137. doi:10.1016\/j.bbamem.2003.10.023. PMID 15033583.\nKarp, Gerald (2005). Cell and Molecular Biology: concepts and experiments (4th ed.). Hoboken, NJ: John Wiley & Sons. ISBN 978-0-471-46580-5.\nNicholls, J.G.; Martin, A.R.; Wallace, B.G.; Fuchs, P.A. (2001). From Neuron to Brain (4th ed.). Sunderland, MA: Sinauer Associates. ISBN 978-0-87893-439-3.\n\nExternal links\n\nSynapse Review for Kids\nSynapse \u2013 Cell Centered Database\nAtlas of Ultrastructure Neurocytology An electron microscope picture gallery assembled by Kristen Harris' lab of synapses and other neuronal structures.","37":"In neurology, the Chiari malformation ( kee-AR-ee; CM) is a structural defect in the cerebellum, characterized by a downward displacement of one or both cerebellar tonsils through the foramen magnum (the opening at the base of the skull).\nCMs can cause headaches, difficulty swallowing, vomiting, dizziness, neck pain, unsteady gait, poor hand coordination, numbness and tingling of the hands and feet, and speech problems. Less often, people may experience ringing or buzzing in the ears, weakness, slow heart rhythm, or fast heart rhythm, curvature of the spine (scoliosis) related to spinal cord impairment, abnormal breathing, such as central sleep apnea, characterized by periods of breathing cessation during sleep, and, in severe cases, paralysis. CM can sometimes lead to non-communicating hydrocephalus as a result of obstruction of cerebrospinal fluid (CSF) outflow. The CSF outflow is caused by phase difference in outflow and influx of blood in the vasculature of the brain.\nThe malformation is named after the Austrian pathologist Hans Chiari. A type II CM is also known as an Arnold\u2013Chiari malformation in honor of Chiari and German pathologist Julius Arnold.\n\nSigns and symptoms\nFindings are due to brainstem and lower cranial nerve dysfunction. Onset of symptoms are less likely to be present during adulthood in most patients. Younger children generally have a substantially different presentation of clinical symptoms from older children. Younger children are more likely to have a more rapid neurological degeneration with profound brainstem dysfunction over several days.\nHeadache is the most common symptom in those with Chiari malformation type 1 (in which only the cerebellar tonsils descend below the foramen magnum). This headache is usually occipital or sub-occipital in location (but may also present in other cranial areas), is usually dull or throbbing in character and is characteristically associated with Valsalva maneuvers (such as bearing down, coughing, sneezing, bending over or forcefully exhaling against a closed airway).\nSymptoms that may be due to Chiari malformations include:\n\nNeurogenic dysphagia: Difficulty swallowing.\nCyanosis: Bluish discoloration of skin while feeding.\nWeak crying\nFacial weakness\nAspiration\nHeadaches aggravated by Valsalva maneuvers\nTinnitus (ringing in the ears)\nLhermitte's sign (electrical sensation that runs down the back and into the limbs)\nVertigo (dizziness)\nNausea\nSchmahmann's syndrome\nNystagmus (irregular eye movements; typically, so-called \"downbeat nystagmus\")\nFacial pain\nMuscle weakness\nImpaired gag reflex\nDysphagia (difficulty swallowing)\nRestless leg syndrome\nSleep apnea\nSleep disorders\nImpaired coordination\nSevere cases may develop all the symptoms and signs of a bulbar palsy\nParalysis due to pressure at the cervico-medullary junction may progress in a so-called \"clockwise\" fashion, affecting the right arm, then the right leg, then the left leg, and finally the left arm; or the opposite way around.\nPapilledema on fundoscopic exam due to increased intracranial pressure\nPupillary dilation\nDysautonomia: tachycardia (rapid heart), syncope (fainting), polydipsia (extreme thirst), chronic fatigue\nApnea: Sudden pause of breathing, usually during sleep.\nOpisthotonos: Spasm of the head which causes head to arch backwards. More common in infants than adults.\nStridor\nThe blockage of cerebrospinal fluid (CSF) flow may also cause a syrinx to form, eventually leading to syringomyelia. Central cord symptoms such as hand weakness, dissociated sensory loss, and, in severe cases, paralysis may occur.\n\nSyringomyelia\nSyringomyelia is most often chronic progressive degenerative disorder characterized by a fluid-filled cyst located in the spinal cord. However, there can be also cases where the syrinx in terms of size and extent of symptoms actually stays stable throughout a lifetime. Syringomyelia symptoms include pain, weakness, numbness, and stiffness in the back, shoulders, arms or legs. Other symptoms include headaches, the inability to feel changes in the temperature, sweating, sexual dysfunction, and loss of bowel and bladder control. It is usually seen in the cervical region but can extend into the medulla oblongata and pons or it can reach downward into the thoracic or lumbar segments. Syringomyelia is often associated with type I Chiari malformation and is commonly seen between the C-4 and C-6 levels. The exact development of syringomyelia is unknown but many theories suggest that the herniated tonsils in type I Chiari malformations cause a \"plug\" to form, which does not allow an outlet of CSF from the brain to the spinal canal. Syringomyelia is present in 25% of patients with type I Chiari malformations.\n\nPathophysiology\nThe most common pathophysiological mechanism by which Chiari type I malformations occurs is due to a congenitally small posterior fossa. Other pathophysiological mechanisms involve increased intracranial pressure above the foramen magnum which causes a downward pressure against the cerebellum, thus causing the cerebellar tonsils to displace below the foramen magnum. Such causes include hydrocephalus (an accumulation of cerebrospinal fluid [CSF] around the brain), space occupying lesions in the brain such as tumors, subdural hematomas or other subdural fluid collections, arachnoid cysts, craniosynostosis (early closure of the cranial sutures)(especially of the lambdoid suture), hyperostosis (an excessive growth of bone) (such as craniometaphyseal dysplasia, osteopetrosis). Another pathophysiological mechanism by which Chiari malformations form is by negative pressure or a pulling force from below the foramen magnum which pulls against the brain, causing the cerebellar tonsils to herniate past the foramen magnum. Causes of this negative or pulling pressure include a tethered cord or an excessively tense Filum terminale, a cerebrospinal fluid leak creating a negative pressure around the spinal cord as the fluid surrounding the cord leaks out, or a CSF-venous fistula, in which the CSF leaks into a nearby vein.\nTraumatic brain injury may cause delayed acquired Chiari malformation, but the pathophysiology of this is unknown. Additionally, ectopia may be present but asymptomatic until a whiplash injury causes it to become symptomatic. Other conditions linked to Chiari malformations include X-linked vitamin D-resistant rickets, and neurofibromatosis type I.\n\nDiagnosis\nDiagnosis is made through a combination of patient history, neurological examination, and medical imaging. Magnetic resonance imaging (MRI) is considered the preferred imaging modality for Chiari malformation. The MRI visualizes neural tissue such as the cerebellar tonsils and spinal cord as well as bone and other soft tissues. CT and CT myelography are other options and were used prior to the advent of MRI, unfortunately the resolution of CT based modalities do not characterize syringomyelia and other neural abnormalities as well.\nBy convention, the cerebellar tonsil position is measured relative to the basion-opisthion line, using sagittal T1 MRI images or sagittal CT images. The selected cutoff distance for abnormal tonsil position is somewhat arbitrary, as not every person will be symptomatic at a certain amount of tonsil displacement, and the probability of symptoms and syrinx increases with greater displacement; however, greater than 5 mm is the most frequently cited cutoff number, though some consider 3\u20135 mm to be \"borderline\"; pathological signs and syrinx may occur beyond that distance. One study showed little difference in cerebellar tonsil position between standard recumbent MRI and upright MRI for patients without a history of whiplash injury. Neuroradiological investigation is used to first rule out any intracranial condition that could be responsible for tonsillar herniation. Neuroradiological diagnostics evaluate the severity of crowding of the neural structures within the posterior cranial fossa and their pressure against the foramen magnum. Chiari 1.5 is a term used when both brainstem and tonsillar herniation through the foramen magnum are present.\nThe diagnosis of a Chiari II malformation can be made prenatally, through ultrasound.\n\nClassification\nIn the late 19th century, Austrian pathologist Hans Chiari described seemingly related anomalies of the hindbrain, the so-called Chiari malformations I, II and III. Later, other investigators added a fourth (Chiari IV) malformation. The scale of severity is rated I \u2013 IV, with IV being the most severe. Types III and IV are very rare. Since Dr. Chiari's original descriptions Chiari 0, 1.5, 3.5, and 5 have been described in the medical literature.\n\nTypes of Chiari malformation\nOther conditions sometimes causally associated with Chiari malformation include hydrocephalus, syringomyelia, spinal curvature, tethered spinal cord syndrome, and connective tissue disorders such as Ehlers\u2013Danlos syndrome and Marfan syndrome.\nChiari malformation is the most frequently used term for this set of conditions. The use of the term \"Arnold\u2013Chiari malformation\" has fallen somewhat out of favor over time, although it is used to refer to the type II malformation. Current sources use \"Chiari malformation\" to describe its four specific types, reserving the term \"Arnold\u2013Chiari\" for type II only. Some sources still use \"Arnold\u2013Chiari\" for all four types.\nChiari malformation or Arnold\u2013Chiari malformation should not be confused with Budd\u2013Chiari syndrome, a hepatic condition also named for Hans Chiari.\nIn Pseudo-Chiari Malformation, leaking of CSF may cause displacement of the cerebellar tonsils and similar symptoms sufficient to be mistaken for a Chiari I malformation.\n\nTreatment\nWhile there is no current cure, the treatments for Chiari malformation are surgery and management of symptoms. Treatment is directed on the occurrence of clinical symptoms rather than the radiological findings. The presence of a syrinx is known to give specific signs and symptoms that vary from dysesthetic sensations to algothermal dissociation to spasticity and paresis. These are important indications that decompressive surgery is needed for patients with Chiari Malformation Type II. Type II patients have severe brainstem damage and rapidly diminishing neurological response.\nDecompressive surgery involves removing the lamina of the first and sometimes the second or third cervical vertebrae and part of the occipital bone of the skull to relieve pressure. The flow of spinal fluid may be augmented by a shunt. The surgery may involve the opening of the dura mater to allow decompression of the brain. A dural graft may be applied to cover the expanded posterior fossa. In those with type I Chiari malformations (especially those with a syrinx), a bone resection with duraplasty (compared to a bone resection without duraplasty) is associated with greater symptom relief and has a higher rate of symptomatic remission, and a lower need for re-operation. However, a bone resection with duraplasty was also associated with a higher rate of surgical complications. Re-operation may be needed in up to 6.8% of patients, and possible causes of re-operation include incomplete decompression and dural scarring. Other complications that are possible in surgical repair of type I Chiari malformations include an aseptic meningitis due to irritation from the dural grafts which is seen in 32% of cases. Rates of aseptic meningitis are lower with dural allografts or autografts as compared to bovine or synthetic grafts. Another complication includes a CSF leak, which may occur in up to 21% of people post-operatively.\nA small number of neurological surgeons believe that detethering the spinal cord as an alternate approach relieves the compression of the brain against the skull opening (foramen magnum), obviating the need for decompression surgery and associated trauma. However, this approach is significantly less documented in the medical literature, with reports on only a handful of patients. The alternative spinal surgery is also not without risk.\nComplications of decompression surgery can arise. They include bleeding, damage to structures in the brain and spinal canal, meningitis, CSF fistulas, occipito-cervical instability, and pseudomeningocele. Rare post-operative complications include hydrocephalus and brainstem compression by retroflexion of odontoid. Also, an extended CVD created by a wide opening and big duroplasty can cause a cerebellar \"slump\". This complication needs to be corrected by cranioplasty.\nIn certain cases, irreducible compression of the brainstem occurs from in front (anteriorly or ventral) resulting in a smaller posterior fossa and associated Chiari malformation. In these cases, an anterior decompression is required. The most commonly used approach is to operate through the mouth (transoral) to remove the bone compressing the brainstem, typically the odontoid. This results in decompressing the brainstem and therefore gives more room for the cerebellum, thus decompressing the Chiari malformation. Arnold Menzes, MD, is the neurosurgeon who pioneered this approach in the 1970s at the University of Iowa. Between 1984 and 2008 (the MR imaging era), 298 patients with irreducible ventral compression of the brainstem and Chiari type I malformation underwent a transoral approach for ventral cervicomedullary decompression at the University of Iowa. The results have been excellent resulting in improved brainstem function and resolution of the Chiari malformation in the majority of patients.\n\nEpidemiology\nThe incidence of congenital Chiari I malformation was previously believed to be in the range of one per 1000 births, but is likely much higher. Women are three times more likely than men to have a congenital Chiari malformation. Type II malformations are more prevalent in people of Celtic descent. A study using upright MRI found cerebellar tonsillar ectopia in 23% of adults with headache from motor-vehicle-accident head trauma. Upright MRI was more than twice as sensitive as standard MRI, likely because gravity affects cerebellar position.\nCases of congenital Chiari malformation may be explained by evolutionary and genetic factors. Typically, an infant's brain weighs around 400g at birth and triples to 1100-1400g by age 11. At the same time the cranium triples in volume from 500 cm3 to 1500 cm3 to accommodate the growing brain. During human evolution, the skull underwent numerous changes to accommodate the growing brain. The evolutionary changes included increased size and shape of the skull, decreased basal angle and basicranial length. These modifications resulted in significant reduction of the size of the posterior fossa in modern humans. In normal adults, the posterior fossa comprises 27% of the total intracranial space, while in adults with Chiari Type I, it is only 21%. H. neanderthalensis had platycephalic (flattened) skulls. Some cases of Chiari are associated with platybasia (flattening of the skull base).\n\nHistory\nThe history of Chiari malformation:\n\n1883: Cleland was the first to describe Chiari II or Arnold\u2013Chiari malformation on his report of a child with spina bifida, hydrocephalus, and anatomical alterations of the cerebellum and brainstem.\n1891: Hans Chiari, a Viennese pathologist, described the case of a 17-year-old female with elongation of the tonsils into cone shaped projections which accompany the medulla and are crammed into the spinal canal.\n1907: Schwalbe and Gredig, pupils of German pathologist Julius Arnold, described four cases of meningomyelocele and alterations in the brainstem and cerebellum, and gave the name \"Arnold\u2013Chiari\" to these malformations.\n1932: Van Houweninge Graftdijk was the first to report the surgical treatment of Chiari malformations. All patients died from surgery or postoperative complications.\n1935: Russell and Donald suggested that decompression of the spinal cord at the foramen magnum might facilitate the CSF circulation.\n1940: Gustafson and Oldberg diagnosed Chiari malformation with syringomyelia.\n1974: Bloch et al. described the tonsils position to be classified between 7 mm and 8 mm below cerebellum.\n1985: Aboulezz used MRI for discovery of extension\n\nSociety and culture\nThe condition was brought to the mainstream on the series CSI: Crime Scene Investigation in the tenth-season episode \"Internal Combustion\" on February 4, 2010.\nChiari malformation was briefly mentioned on the medical drama House M.D. in the fifth-season episode \"House Divided\", It was the focus of the sixth-season episode \"The Choice\". It is also the focus of Private Practice Season 4 episode 4, where a pregnant woman is diagnosed with it. It was the cause of death on the reality television series Dr. G: Medical Examiner in the sixth-season episode \"Bruised and Battered\". It was also mentioned in the medical drama A Gifted Man, in the first-season episode \"In Case of Separation Anxiety\".\nIt is also featured in the 3rd and 4th episode of the 7th season of the series Rizzoli & Isles where Dr. Maura Isles is diagnosed with the condition.\n\nNotable people\nRosanne Cash \u2013 U.S. singer-songwriter; daughter of Johnny Cash\nJulia Clukey \u2013 U.S. luge competitor for Team USA in 2010 Vancouver Winter Olympics\nJoanna David \u2013 British television and stage actress\nJ. B. Holmes \u2013 U.S. professional golfer\nMarissa Irwin \u2013 U.S. fashion model with Chiari secondary to Ehlers\u2013Danlos syndrome\nBobby Jones \u2013 U.S. World Golf Hall of Fame golfer and founder of the Augusta National Golf Club\nAllysa Seely \u2013 U.S. Gold Medalist at the 2016 Summer Paralympics in the paratriathlon\nLeah Shapiro \u2013 U.S. drummer for the band Black Rebel Motorcycle Club\nMichelle Stilwell \u2013 Canadian wheelchair racer and politician\nRachid Taha \u2013 Algerian singer\nSabre Norris \u2013 Australian skateboarder and surfer\n\nSee also\nHypermobility spectrum disorder\n\nNotes\n\n\n== References ==","38":"Cholesteatoma is a destructive and expanding growth consisting of keratinizing squamous epithelium in the middle ear and\/or mastoid process. Cholesteatomas are not cancerous as the name may suggest, but can cause significant problems because of their erosive and expansile properties. This can result in the destruction of the bones of the middle ear (ossicles), as well as growth through the base of the skull into the brain. They often become infected and can result in chronically draining ears. Treatment almost always consists of surgical removal.\n\nSigns and symptoms\nOther more common conditions (e.g. otitis externa) may also present with these symptoms, but cholesteatoma is much more serious and should not be overlooked. If a patient presents to a doctor with ear discharge and hearing loss, the doctor should consider cholesteatoma until the disease is definitely excluded. Other less common symptoms (all less than 15%) of cholesteatoma may include pain, balance disruption, tinnitus, earache, headaches and bleeding from the ear. There can also be facial nerve weakness. Balance symptoms in the presence of a cholesteatoma raise the possibility that the cholesteatoma is eroding the balance organs in the inner ear.\nDoctors' initial inspections may only reveal an ear canal full of discharge. Until the doctor has cleaned the ear and inspected the entire tympanic membrane, cholesteatoma cannot be diagnosed. Once the debris is cleared, cholesteatoma can give rise to a number of appearances. If there is significant inflammation, the tympanic membrane may be partially obscured by an aural polyp. If there is less inflammation, the cholesteatoma may present the appearance of 'semolina' discharging from a defect in the tympanic membrane. The posterior and superior parts of the tympanic membrane are most commonly affected. If the cholesteatoma has been dry, the cholesteatoma may present the appearance of 'wax over the attic'. The attic is just above the eardrum.\nIf untreated, a cholesteatoma can eat or cause erosion of the three small bones located in the middle ear (the malleus, incus and stapes, collectively called ossicles). This can result in nerve deterioration, imbalance, vertigo, and deafness early in the disease. It can also affect and erode, through the enzymes it produces, the thin bone structure that isolates the top of the ear from the brain, as well as lay the covering of the brain open to infection with serious complications (rarely even death due to brain abscess and sepsis).\nBoth the acquired as well as the congenital types of the disease can affect the facial nerve that extends from the brain to the face and passes through the inner and middle ear and leaves at the anterior tip of the mastoid bone, and then rises to the front of the ear and extends into the upper and lower face.\n\nCause\nCholesteatomas occur in two basic classifications: Acquired cholesteatomas, which are more common, are usually caused by pathological alteration of the ear drum leading to accumulation of keratin within the middle ear. Congenital cholesteatomas are usually middle ear epidermal cysts that are identified deep within an intact ear drum often in the superior anterior portion.\nCholesteatomas do not contain cholesterol or fat and should not be confused with cholesterol granulomas.\n\nCongenital cholesteatoma\nKeratin-filled cysts that grow medial to the tympanic membrane are considered to be congenital if they fulfill the following criteria (Levenson's criteria):\n\nmass medial to the tympanic membrane\nnormal tympanic membrane\nno previous history of ear discharge, perforation or ear surgery\nCongenital cholesteatomas occur at three important sites: the middle ear, the petrous apex, and the cerebropontinio angle. They are most often found deep to the anterior aspect of the ear drum, and a vestigial structure, the epidermoid formation, from which congenital cholesteatoma may originate, has been identified in this area.\nNot all middle ear epidermal cysts are congenital, as they can be acquired either by metaplasia of the middle ear mucosa or by traumatic implantation of ear canal or tympanic membrane skin. In addition, cholesteatoma inadvertently left by a surgeon usually regrows as an epidermal cyst. Some authors have also suggested hereditary factors.\n\nAcquired cholesteatoma\nMore commonly, keratin accumulates in a pouch of tympanic membrane which extends into the middle ear space. This abnormal folding or 'retraction' of the tympanic membrane arises in one of the following ways:\n\nJackler's theory: Mucosal coupling with traction generated by interaction of migrating opposing surfaces leading to formation of cholesteatoma.\nWittmaack's theory: Invagination of tympanic membrane from the attic or part of pars tensa in the form of retraction pockets lead to the formation of cholesteatoma.\nRuedi's theory: The basal cells of germinal layer of skin proliferate under the influence of infection and lay down keratinising squamous epithelium.\nHabermann's theory: The epithelium from the meatus or outer drum surface grows into the middle ear through a pre-existing perforation and form cholesteatoma.\nCholesteatoma may also arise as a result of metaplasia of the middle ear mucosa  or implantation following trauma.\n\nDiagnosis\nCholesteatoma is diagnosed by a medical doctor by physical examination of the ear. A CT scan may help to rule out other, often more serious causes for the patient's clinical presentation. Non-ionizing radiation imaging techniques (MRI) may be suitable to replace a CT scan, if determined necessary by a physician.\n\nTreatment\nCholesteatoma is a persistent disease. Once the diagnosis of cholesteatoma is made in a patient who can tolerate a general anesthetic, the standard treatment is to surgically remove the growth.\nThe challenge of cholesteatoma surgery is to permanently remove the cholesteatoma whilst retaining or reconstructing the normal functions of the structures housed within the temporal bone.\nThe general objective of cholesteatoma surgery has two parts. It is both directed against the underlying pathology and directed towards maintaining the normal functions of the temporal bone. These aims are conflicting and this makes cholesteatoma surgery extremely challenging.\nSometimes, the situation results in a clash of surgical aims. The need to fully remove a progressive disease like cholesteatoma is the surgeon's first priority. Preservation of hearing is secondary to this primary aim. If the disease can be removed easily so that there is no increased risk of residual disease, then the ossicles may be preserved. If the disease is difficult to remove, so that there is an increased risk of residual disease, then removal of involved ossicles in order to fully clear cholesteatoma has generally been regarded as necessary and reasonable.\nIn other words, the aims of cholesteatoma treatment form a hierarchy. The paramount objective is the complete removal of cholesteatoma. The remaining objectives, such as hearing preservation, are subordinate to the need for complete removal of cholesteatoma. This hierarchy of aims has led to the development of a wide range of strategies for the treatment of cholesteatoma.\n\nSurgery\nThe variation in technique in cholesteatoma surgery results from each surgeon's judgment whether to retain or remove certain structures housed within the temporal bone in order to facilitate the removal of cholesteatoma. This typically involves some form of mastoidectomy which may or may not involve removing the posterior ear canal wall and the ossicles.\nRemoval of the canal wall facilitates the complete clearance of cholesteatoma from the temporal bone in three ways: \n\nIt removes a large surface onto which cholesteatoma may be adherent;\nIt removes a barrier behind which the cholesteatoma may be hidden;\nIt removes an impediment to the introduction of instruments used for the removal of cholesteatoma.\nThus removal of the canal wall provides one of the most effective strategies for achieving the primary aim of cholesteatoma surgery, the complete removal of cholesteatoma. However, there is a trade-off, since the functional impact of canal wall removal is also important.\nThe removal of the ear canal wall results in: \n\na space, the \"mastoid cavity\", which is less likely than the original ear canal to resist infection;\nexposure of the ossicles, which may allow the subsequent formation of a new cholesteatoma deep to the ossicles. To prevent this, these ossicles must be removed, which may diminish the patient's hearing.\nThe formation of a mastoid cavity by removal of the canal wall is the simplest and most effective procedure for facilitating the removal of cholesteatoma, but may bestow the most lasting infirmity due to loss of ear function upon the patient treated in this way.\nThe following strategies are employed to mitigate the effects of canal wall removal:\n\nCareful design and construction of the mastoid cavity. This is essential for the health and integrity of the protective sheet of migrating, keratising epithelium which lines the distorted ear canal. This requires the surgeon to saucerise the cavity. A high facial ridge and an inappropriately small cartilaginous meatus are obstructions to epithelial migration and are particularly high risk factors for failure of the self-cleaning mechanism of the external ear.\nPartial obliteration of the mastoid cavity. This can be performed using a wide range of materials. Many of these resorb in time, which means that the long-term results of such surgery are poorer than the short-term results.\nReconstruction of the ear canal wall. Canal wall reconstruction has been performed using ear canal skin alone, fascia, cartilage and titanium as well as by replacing the original intact wall. If the reconstruction is poorly performed, it may result in a high rate of recurrent cholesteatoma.\nPreservation of the ear canal wall. If poorly performed, it may result in a high rate of both residual and recurrent cholesteatoma.\nReconstruction of the chain of hearing bones using a passive middle ear implant.\nClearly, preservation and restoration of ear function at the same time as total removal of cholesteatoma requires a high level of surgical expertise.\n\nEndoscopic surgery\nTraditionally, ear surgery has been performed using the surgical microscope. The direct line of view dictated by that approach necessitates using the mastoid as the access port to the middle ear. It has long been recognized that failure in cholesteatoma surgery occurs in some of the out of view spaces of the tympanic cavity like the sinus tympani and facial recess that are out of view using the traditional microscopic technique. More recently, the endoscope has been increasingly utilized in the surgical management of cholesteatoma in one of two ways:\n\nUsing the endoscope as an ancillary instrument to the microscope when trying to visualize areas that are hidden from the microscope such as the sinus tympani. This work was pioneered by Professor Thomassin.\nUsing the endoscope as the main surgical instrument through the ear canal, or what's called: endoscopic ear surgery, pioneered by Professor Tarabichi.\nThere are multiple advantages for the use of the endoscope in cholesteatoma surgery:\n\nThe endoscope's wide angle of view and the ability to \"see around the corner\".\nThe endoscope allows minimally invasive access through the natural ear canal, rather than through the usual 5 cm incision behind the ear utilized in traditional microscopic surgery.\nThe 30-degree endoscope allows access to the bony area of the Eustachian tube to address any obstructive pathologies.\n\nPrognosis\nIt is important that the patient attend periodic follow-up checks, because even after careful microscopic surgical removal, cholesteatomas may recur. Such recurrence may arise many years, or even decades, after treatment.\nA 'residual cholesteatoma' may develop if the initial surgery failed to completely remove the original; residual cholesteatomas typically become evident within the first few years after the initial surgery.\nA 'recurrent cholesteatoma' is a new cholesteatoma that develops when the underlying causes of the initial cholesteatoma are still present. Such causes can include, for example, poor Eustachian tube function, which results in retraction of the ear drum, and failure of the normal outward migration of skin.\nIn a retrospective study of 345 patients with middle ear cholesteatoma operated on by the same surgeon, the overall 5-year recurrence rate was 11.8%. In a different study with a mean follow-up period of 7.3 years, the recurrence rate was 12.3%, with the recurrence rate being higher in children than in adults. The use of the endoscope as an ancillary instrument has been shown to reduce the incidence of residual cholesteatoma. Although more studies are needed, so far, new techniques addressing underlying Eustachian tube dysfunction such as transtympanic dilatation of the Eustachian tube has not been shown to change outcomes of chronic ear surgery.\n\nInfections\nRecent findings indicate that the keratinizing squamous epithelium of the middle ear could be subjected to human papillomavirus infection. Indeed, DNA belonging to oncogenic HPV16 has been detected in cholesteatoma tissues, thereby underlining that keratinizing squamous epithelia could potentially be a target tissue for HPV infection.\n\nEpidemiology\nIn one study, the number of new cases of cholesteatoma in Iowa was estimated in 1975\u201376 to be just under one new case per 10,000 citizens per year. Cholesteatoma affects all age groups, from infants through to the elderly. The peak incidence occurs in the second decade.\n\nSee also\nChronic suppurative otitis media\nOtic polyp\nEndoscopic ear surgery\n\nReferences\nExternal links\n\nInformation on Cholesteatomas Archived 2014-07-28 at the Wayback Machine\nLaser Cholesteatoma Surgery","39":"The term chronic subjective dizziness (CSD) is used to describe a commonly encountered type of dizziness that is not easily categorized into one of several other types, and for which the physical examination is typically normal.  Patients with CSD frequently initially suffer a sudden injury of some sort to their vestibular system, the neurologic network that preserves sense of balance. Even after this initial injury has healed, people with CSD usually describe a vague sense of unsteadiness worsened by triggers in their environment such as high places, standing on moving objects, or standing in motion-rich environments like busy streets or crowds. There is a clear indication that anxiety and other mental illnesses play a role in the dizziness symptoms that occur with CSD. However, the condition is categorized as chronic functional vestibular disorder, not  as a structural or psychiatric condition.\nProposals include renaming it persistent postural-perceptual dizziness (PPPD) which better captures the multiple aspects of the condition under its title. It is under that title the World Health Organization has included PPPD in its draft list of diagnoses to be included to the next edition of the International Classification of Diseases (ICD-11) in 2017.\n\nSigns and symptoms\nSymptoms can include:\n\nA constant sense of unsteadiness, rocking or swaying, dizziness or lightheadedness\nDisequilibrium on most days for at least 3 months\nSpatial orientation problems\nOff-kilter sensation\nExtreme sensitivity to movement and\/or complex visual stimuli such as grocery stores or driving in certain weather conditions\nWorsening dizziness with experience of complex visual environments such as walking through a grocery store\nHeavy-headedness; a feeling of floating, wooziness\nSymptoms of CSD can be worsened by any self-precipitated motion, usually from the head, or the witnessing of motion from another subject. These are usually less noticeable when the person is lying still.\n\nDiagnosis\nDiagnosis can be difficult as there is not a specific test for PPPD but rather a series of elimination tests for other vestibular causes.  If elimination test are all normal and the symptoms match a PPPD diagnosis is possible.\n\nTreatment\nEffective treatments include vestibular rehabilitation therapy, medications such as SSRIs and psychotherapy, including the most effectively represented cognitive behavioral therapy.\nPromising results were also found with transcranial direct-current stimulation combined with vestibular rehabilitation with significant improvement in symptoms of patients over a sham group in an exploratory study.\nMore recently, a study showed non-invasive vagus nerve stimulation to offer significant effect in PPPD patients regarding the quality of life, postural balance control, attack severity and depression level, with no reported serious side effects. The findings are argued to imply vagus nerve stimulation to be a safe and promising treatment option in patients with treatment-refractory PPPD and suggesting the need for further research.\n\nHistory\nPerhaps the first account of CSD was the German neurologist Karl Westphal's portrayal in the late 1800s of people who suffered dizziness, anxiety and spatial disorientation when shopping in town squares.  This phenomenon was called \"agoraphobia\", meaning a fear of the marketplace.  The term is now used to describe a psychological fear, but Westphal's original description included many symptoms of dizziness and imbalance not included in the modern psychiatric definition. Unlike people who feel anxious in crowds because they feel something bad will happen, people with CSD may dislike crowds because all the movement leads to a sensation of dizziness.\n\n\n== References ==","40":"Computed tomography angiography (also called CT angiography or CTA) is a computed tomography technique used for angiography\u2014the visualization of arteries and veins\u2014throughout the human body. Using contrast injected into the blood vessels, images are created to look for blockages, aneurysms (dilations of walls), dissections (tearing of walls), and stenosis (narrowing of vessel). CTA can be used to visualize the vessels of the heart, the aorta and other large blood vessels, the lungs, the kidneys, the head and neck, and the arms and legs. CTA can also be used to localise arterial or venous bleed of the gastrointestinal system.\n\nMedical uses\nCTA can be used to examine blood vessels in many key areas of the body including the brain, kidneys, pelvis, and the lungs.\n\nCoronary CT angiography\nCoronary CT angiography (CCTA) is the use of CT angiography to assess the arteries of the heart. The patient receives an intravenous injection of contrast and then the heart is scanned using a high speed CT scanner. With the advances in CT technology, patients are typically able to be scanned without needing medicines by simply holding their breath during the scan. CTA is used to assess heart or vessel irregularities, location of stents and whether they are still open, and occasionally to check for atherosclerotic disease. This method displays the anatomical detail of blood vessels more precisely than magnetic resonance imaging (MRI) or ultrasound. Today, many patients can undergo CTA in place of a conventional catheter angiogram, a minor procedure during which a catheter is passed through the blood vessels all the way to the heart. However, CCTA has not fully replaced this procedure. CCTA is able to detect narrowing of blood vessels in time for corrective therapy to be done. CCTA is a useful way of screening for arterial disease because it is safer, much less time-consuming than catheter angiography, and is also a cost-effective procedure.\n\nAorta and great arteries\nCTA can be used in the chest and abdomen to identify aneurysms in the aorta or other major blood vessels. These areas of weakened blood vessel walls that bulge out can life-threatening if they rupture. CTA is the test of choice when assessing aneurysm before and after endovascular stenting due to the ability to detect calcium within the wall. Another positive of CTA in abdominal aortic aneurysm assessment is that it allows for better estimation of blood vessel dilation and can better detect blood clots compared to standard angiography.\nCTA is used also to identify arterial dissection, including aortic dissection in the aorta or its major branches. Arterial dissection is when the layers of the artery wall peel away from each other; this causes pain and can be life-threatening. CTA is a quick and non-invasive method of identifying dissections and can show the extent of the disease and if there is leakage.\n\nPulmonary arteries\nCT pulmonary angiogram (CTPA) is used to examine the pulmonary arteries in the lungs, most commonly to rule out pulmonary embolism (PE), a serious but treatable condition. It has become the technique of choice for detection of pulmonary embolism due to its wide availability, short exam time, ability to see other diseases that may present like pulmonary embolisms, and a high degree of confidence in the validity of the test. In this test, a PE will appear as a dark spot inside the blood vessel or a sudden stop of the bright contrast material.\nCT angiography should not be used to evaluate for pulmonary embolism when other tests indicate that there is a low probability of a person having this condition. A D-dimer assay might be a preferred alternative to test for pulmonary embolism, and that test and a low clinical prediction score on the Wells test or Geneva score can exclude pulmonary embolism as a possibility.\n\nRenal arteries\nVisualization of blood flow in the renal arteries (those supplying the kidneys) in patients with high blood pressure and those suspected of having kidney disorders can be performed using CTA. Stenosis (narrowing) of a renal artery is a cause of hypertension (high blood pressure) in some patients and can be corrected. A special computerized method of viewing the images makes renal CT angiography a very accurate examination. CTA is also used in the  assessment of native and transplant renal arteries. While CTA is great for imaging of the kidneys, it lacks the ability to perform procedures at the same time. Thus traditional catheter angiography is used in cases of acute renal hemorrhage or acute arterial obstruction.\n\nBrain and neck vessels\nCTA can be used assess acute stroke patients by identifying clots in the arteries of the brain. It can also be used to identify small aneurysms or arteriovenous malformation inside the brain that can be life-threatening. While CTA can produce high quality images of the carotid arteries for grading the level of stenosis (narrowing of the vessel), calcium deposits (calcified plaques) in the area where the vessels split can lead to interference with accurate stenosis grading. Because of this, magnetic resonance angiography is used more often for this purpose. Other applications of CTA are identifying moyamoya disease, dissections of intracranial arteries, detection of carotid-cavernous fistula, planning for intracranial-extracranial bypass surgery, and involvement of brain tumours such as meningioma with surrounding intracranial vessels.\n\nPeripheral arteries\nCTA can be used in the legs to detect atherosclerotic disease that has narrowed the arteries. It can also be used to image vessels in suspected blockages, trauma cases, or patients with surgical complications.\n\nTechnique\nCT angiography is a contrast CT where images are taken with a certain delay after injection of radiocontrast material. The contrast material is radiodense causing it to light up brightly within the blood vessels of interest. In order for the CT scanner to be able to scan the correct area where the contrast is, the scanner uses either automatic detectors which start scanning when enough contrast is present, or small test boluses. With the small test bolus, a small amount of contrast is injected in order to detect the speed that the contrast will move through the blood vessels. After determining this speed, the full bolus is injected and the scan is begun at the timing determined by the test bolus. After the scan is completed the images are post-processed to better visualize the vessels and can even be created in the 3D images.\n\nRisks\nHarms of overuse of CT angiography include radiation exposure and the possibility of finding then seeking treatment for a clinically insignificant pulmonary embolism which ought not be treated.\n\nAdverse reactions\nA reaction may occur whenever iodine contrast is injected. These reactions range in severity and it is difficult to predict if they will occur. With the current practice of using low-osmolar contrast these adverse reactions only occur in ~0.1% of cases. The severity of the reaction can be broken down into three groups:\n\nMild - no treatment required: nausea, vomiting, and\/or hives.\nModerate - requires treatment: severe hives, lightheadedness or brief loss of consciousness, mild bronchospasm, and\/or increased heart rate.\nSevere - requires immediate treatment: severe bronchospasm, throat swelling, seizure, severe low blood pressure, and\/or cardiac arrest.\nA patient with a history of allergy to contrast may be advised to take medications such as corticosteroids or histamine (H1) blockers before CTA to lessen the risk of allergic reaction or to undergo a different exam that does not call for contrast material injection. Patients should also be well hydrated in order to minimize possible adverse effects of contrast.\nContrary to popular belief there is no correlation between seafood allergies and reactions to iodine contrast, as shown by many recent studies.\n\nKidneys\nHistorically it has been thought that contrast material can lead to contrast-induced nephropathy (also called CIN) in any patient. However, recent studies have shown that the risk of kidney injury caused by contrast agent in patients with no history of kidney problems occurs extremely infrequently.\nThe use of CTA in people with kidney failure, kidney disease or long-standing severe diabetes should be weighed carefully as the use of IV iodine contrast material may further harm kidney function. The decision not to use contrast agents must be weighed against the possibility of misdiagnoses if contrast is not used.\n\nRadiation\nCompared with other imaging modalities, CTA is associated with a significant dose of ionizing radiation. Varying significantly with patient age, sex, and exam protocol, radiation risk models predict coronary CTA to increase lifetime cancer risk.\nCT angiography should not be performed in patients who are pregnant as the contrast and radiation may lead to harm to the fetus. The extent of harm to the fetus has not been fully determined.\n\nHistory\nBy 1994 CT angiography began to replace conventional angiography in diagnosing and characterizing most cardiovascular abnormalities. Prior to this, conventional angiography had been in use for 70 years.\n\nSee also\nAngiography\nMagnetic resonance angiography\n\n\n== References ==","41":"The common cold or the cold is a viral infectious disease of the upper respiratory tract that primarily affects the respiratory mucosa of the nose, throat, sinuses, and larynx. Signs and symptoms may appear in as little as two days after exposure to the virus. These may include coughing, sore throat, runny nose, sneezing, headache, and fever. People usually recover in seven to ten days, but some symptoms may last up to three weeks. Occasionally, those with other health problems may develop pneumonia.\nWell over 200 virus strains are implicated in causing the common cold, with rhinoviruses, coronaviruses, adenoviruses and enteroviruses being the most common. They spread through the air or indirectly through contact with objects in the environment, followed by transfer to the mouth or nose. Risk factors include going to child care facilities, not sleeping well, and psychological stress. The symptoms are mostly due to the body's immune response to the infection rather than to tissue destruction by the viruses themselves. The symptoms of influenza are similar to those of a cold, although usually more severe and less likely to include a runny nose.\nThere is no vaccine for the common cold. The primary methods of prevention are hand washing; not touching the eyes, nose or mouth with unwashed hands; and staying away from sick people. Some evidence supports the use of face masks. There is also no cure, but the symptoms can be treated. Zinc may reduce the duration and severity of symptoms if started shortly after the onset of symptoms. Nonsteroidal anti-inflammatory drugs (NSAIDs) such as ibuprofen may help with pain. Antibiotics, however, should not be used, as all colds are caused by viruses, and there is no good evidence that cough medicines are effective.\nThe common cold is the most frequent infectious disease in humans. Under normal circumstances, the average adult gets two to three colds a year, while the average child may get six to eight. Infections occur more commonly during the winter. These infections have existed throughout human history.\n\nSigns and symptoms\nThe typical symptoms of a cold include cough, runny nose, sneezing, nasal congestion, and a sore throat, sometimes accompanied by muscle ache, fatigue, headache, and loss of appetite. A sore throat is present in about 40% of cases, a cough in about 50%, and muscle aches in about 50%. In adults, a fever is generally not present but it is common in infants and young children. The cough is usually mild compared to that accompanying influenza. While a cough and a fever indicate a higher likelihood of influenza in adults, a great deal of similarity exists between these two conditions. A number of the viruses that cause the common cold may also result in asymptomatic infections.\nThe color of the mucus or nasal secretion may vary from clear to yellow to green and does not indicate the class of agent causing the infection.\n\nProgression\nA cold usually begins with fatigue, a feeling of being chilled, sneezing, and a headache, followed in a couple of days by a runny nose and cough. Symptoms may begin within sixteen hours of exposure and typically peak two to four days after onset. They usually resolve in seven to ten days, but some can last for up to three weeks. The average duration of cough is eighteen days and in some cases people develop a post-viral cough which can linger after the infection is gone. In children, the cough lasts for more than ten days in 35\u201340% of cases and continues for more than 25 days in 10%.\n\nCauses\nViruses\nThe common cold is an infection of the upper respiratory tract which can be caused by many different viruses. The most commonly implicated is a rhinovirus (30\u201380%), a type of picornavirus with 99 known serotypes. Other commonly implicated viruses include adenoviruses, enteroviruses, parainfluenza and RSV. Frequently more than one virus is present. In total, more than 200 viral types are associated with colds. The viral cause of some common colds (20\u201330%) is unknown.\n\nTransmission\nThe common cold virus is typically transmitted via airborne droplets, direct contact with infected nasal secretions, or fomites (contaminated objects).  Which of these routes is of primary importance has not been determined. As with all respiratory pathogens once presumed to transmit via respiratory droplets, it is highly likely to be carried by the aerosols generated during routine breathing, talking, and singing.  The viruses may survive for prolonged periods in the environment (over 18 hours for rhinoviruses) and can be picked up by people's hands and subsequently carried to their eyes or noses where infection occurs. Transmission from animals is considered highly unlikely; an outbreak documented at a British scientific base on Adelaide Island after seventeen weeks of isolation was thought to have been caused by transmission from a contaminated object or an asymptomatic human carrier, rather than from the husky dogs which were also present at the base.\nTransmission is common in daycare and schools due to the proximity of many children with little immunity and poor hygiene. These infections are then brought home to other members of the family. There is no evidence that recirculated air during commercial flight is a method of transmission. People sitting close to each other appear to be at greater risk of infection.\nRhinovirus-caused colds are most infectious during the first three days of symptoms; they are much less infectious afterwards.\n\nOther\nHerd immunity, generated from previous exposure to cold viruses, plays an important role in limiting viral spread, as seen with younger populations that have greater rates of respiratory infections. Poor immune function is a risk factor for disease. Insufficient sleep and malnutrition have been associated with a greater risk of developing infection following rhinovirus exposure; this is believed to be due to their effects on immune function. Breast feeding decreases the risk of acute otitis media and lower respiratory tract infections among other diseases, and it is recommended that breast feeding be continued when an infant has a cold. In the developed world breast feeding may not be protective against the common cold in and of itself.\n\nPathophysiology\nThe symptoms of the common cold are believed to be primarily related to the immune response to the virus. The mechanism of this immune response is virus-specific. For example, the rhinovirus is typically acquired by direct contact; it binds to humans via ICAM-1 receptors and the CDHR3 receptor through unknown mechanisms to trigger the release of inflammatory mediators. These inflammatory mediators then produce the symptoms. It does not generally cause damage to the nasal epithelium. The respiratory syncytial virus (RSV), on the other hand, is contracted by direct contact and airborne droplets. It then replicates in the nose and throat before spreading to the lower respiratory tract. RSV does cause epithelium damage. Human parainfluenza virus typically results in inflammation of the nose, throat, and bronchi. In young children, when it affects the trachea, it may produce the symptoms of croup, due to the small size of their airways.\n\nDiagnosis\nThe distinction between viral upper respiratory tract infections is loosely based on the location of symptoms, with the common cold affecting primarily the nose (rhinitis), throat (pharyngitis), and lungs (bronchitis). There can be significant overlap, and more than one area can be affected. Self-diagnosis is frequent. Isolation of the viral agent involved is rarely performed, and it is generally not possible to identify the virus type through symptoms.\n\nPrevention\nThe only useful ways to reduce the spread of cold viruses are physical and engineering measures such as using correct hand washing technique,  respirators, and improvement of indoor air. In the healthcare environment, gowns and disposable gloves are also used. Droplet precautions cannot reliably protect against inhalation of common-cold-laden aerosols. Instead, airborne precautions such as  respirators, ventilation, and HEPA\/high MERV filters, are the only reliable protection against cold-laden aerosols. Isolation or quarantine is not used as the disease is so widespread and symptoms are non-specific. There is no vaccine to protect against the common cold. Vaccination has proven difficult as there are so many viruses involved and because they mutate rapidly. Creation of a broadly effective vaccine is, therefore, highly improbable.\nRegular hand washing appears to be effective in reducing the transmission of cold viruses, especially among children. Whether the addition of antivirals or antibacterials to normal hand washing provides greater benefit is unknown. Wearing face masks when around people who are infected may be beneficial; however, there is insufficient evidence for maintaining a greater social distance.\nIt is unclear whether zinc supplements affect the likelihood of contracting a cold. Routine vitamin C supplements do not reduce the risk or severity of the common cold, though they may reduce its duration.\n\nManagement\nTreatments of the common cold primarily involve medications and other therapies for symptomatic relief. Getting plenty of rest, drinking fluids to maintain hydration, and gargling with warm salt water are reasonable conservative measures. Much of the benefit from symptomatic treatment is, however, attributed to the placebo effect. As of 2010, no medications or herbal remedies had been conclusively demonstrated to shorten the duration of infection.\n\nSymptomatic\nTreatments that may help with symptoms include pain medication and medications for fevers such as ibuprofen and acetaminophen (paracetamol). However, it is not clear whether acetaminophen helps with symptoms. It is not known if over-the-counter cough medications are effective for treating an acute cough. Cough medicines are not recommended for use in children due to a lack of evidence supporting effectiveness and the potential for harm. In 2009, Canada restricted the use of over-the-counter cough and cold medication in children six years and under due to concerns regarding risks and unproven benefits. The misuse of dextromethorphan (an over-the-counter cough medicine) has led to its ban in a number of countries. Intranasal corticosteroids have not been found to be useful.\nIn adults, short term use of nasal decongestants may have a small benefit. Antihistamines may improve symptoms in the first day or two; however, there is no longer-term benefit and they have adverse effects such as drowsiness. Other decongestants such as pseudoephedrine appear effective in adults. Combined oral analgesics, antihistaminics, and decongestants are generally effective for older children and adults. Ipratropium nasal spray may reduce the symptoms of a runny nose but has little effect on stuffiness. Ipratropium may also help with coughs in adults. The safety and effectiveness of nasal decongestant use in children is unclear.\nDue to lack of studies, it is not known whether increased fluid intake improves symptoms or shortens respiratory illness. As of 2017, heated and humidified air, such as via RhinoTherm, is of unclear benefit. One study has found chest vapor rub to provide some relief of nocturnal cough, congestion, and sleep difficulty.\nSome experts  advise against physical exercise if there are symptoms such as fever, widespread muscle aches or fatigue. It is regarded as safe to perform moderate exercise if the symptoms are confined to the head, including runny nose, nasal congestion, sneezing, or a minor sore throat. There is a popular belief that having a hot drink can help with cold symptoms, but evidence to support this is very limited.\n\nAntibiotics and antivirals\nAntibiotics have no effect against viral infections, including the common cold. Due to their side effects, antibiotics cause overall harm but nevertheless are still frequently prescribed. Some of the reasons that antibiotics are so commonly prescribed include people's expectations for them, physicians' desire to help, and the difficulty in excluding complications that may be amenable to antibiotics. There are no effective antiviral drugs for the common cold even though some preliminary research has shown benefits.Antibiotics may be beneficial in the case of secondary bacterial infection as acute bacterial rhinosinusitis (ABRS).The European Position Paper on Rhinosinusitis and Nasal Polyps (EPOS) consensus, recommends a combination of five signs and symptoms to assist in identifying acute bacterial rhinosinusitis - being likely when \u22653 of five criteria are present: Discolored discharge with a unilateral predominance; Severe local pain; Fever \u226538\u00b0C; Double sickening; Elevation of C-reactive protein (CRP) or erythrocyte sedimentation rate (ESR).\n\nZinc\nZinc supplements may shorten the duration of colds by up to 33% and reduce the severity of symptoms if supplementation begins within 24 hours of the onset of symptoms. Some zinc remedies directly applied to the inside of the nose have led to the loss of the sense of smell. A 2017 review did not recommend the use of zinc for the common cold for various reasons; whereas a 2017 and 2018 review both recommended the use of zinc, but also advocated further research on the topic.\n\nAlternative medicine\nWhile there are many alternative medicines and Chinese herbal medicines supposed to treat the common cold, there is insufficient scientific evidence to support their use. As of 2015, there is weak evidence to support nasal irrigation with saline. There is no firm evidence that Echinacea products or garlic provide any meaningful benefit in treating or preventing colds.\n\nVitamins C and D\nVitamin C supplementation does not affect the incidence of the common cold, but may reduce its duration. There is no conclusive evidence that vitamin D supplementation is efficacious in the prevention or treatment of respiratory tract infections.\n\nPrognosis\nThe common cold is generally mild and self-limiting with most symptoms generally improving in a week. In children, half of cases resolve in 10 days and 90% in 15 days. Severe complications, if they occur, are usually in the very old, the very young, or those who are immunosuppressed. Secondary bacterial infections may occur resulting in sinusitis, pharyngitis, or an ear infection. It is estimated that sinusitis occurs in 8% and ear infection in 30% of cases.\n\nEpidemiology\nThe common cold is the most common human disease and affects people all over the globe. Adults typically have two to three infections annually, and children may have six to ten colds a year (and up to twelve colds a year for school children). Rates of symptomatic infections increase in the elderly due to declining immunity.\n\nWeather\nA common misconception is that one can \"catch a cold\" merely through prolonged exposure to cold weather. Although it is now known that colds are viral infections, the prevalence of many such viruses are indeed seasonal, occurring more frequently during cold weather. The reason for the seasonality has not been conclusively determined. Possible explanations may include cold temperature-induced changes in the respiratory system, decreased immune response, and low humidity causing an increase in viral transmission rates, perhaps due to dry air allowing small viral droplets to disperse farther and stay in the air longer.\nThe apparent seasonality may also be due to social factors, such as people spending more time indoors near infected people, and especially children at school. Although normal exposure to cold does not increase one's risk of infection, severe exposure leading to significant reduction of body temperature (hypothermia) may put one at a greater risk for the common cold: although controversial, the majority of evidence suggests that it may increase susceptibility to infection.\n\nHistory\nWhile the cause of the common cold was identified in the 1950s, the disease appears to have been with humanity since its early history. Its symptoms and treatment are described in the Egyptian Ebers papyrus, the oldest existing medical text, written before the 16th century BCE. The name \"cold\" came into use in the 16th century, due to the similarity between its symptoms and those of exposure to cold weather.\nIn the United Kingdom, the Common Cold Unit (CCU) was set up by the Medical Research Council in 1946 and it was where the rhinovirus was discovered in 1956. In the 1970s, the CCU demonstrated that treatment with interferon during the incubation phase of rhinovirus infection protects somewhat against the disease, but no practical treatment could be developed. The unit was closed in 1989, two years after it completed research of zinc gluconate lozenges in the prevention and treatment of rhinovirus colds, the only successful treatment in the history of the unit.\n\nResearch directions\nAntivirals have been tested for effectiveness in the common cold; as of 2009, none had been both found effective and licensed for use. There are trials of the anti-viral drug pleconaril which shows promise against picornaviruses as well as trials of BTA-798. The oral form of pleconaril had safety issues and an aerosol form is being studied. The genomes of all known human rhinovirus strains have been sequenced.\n\nSocietal impact\nThe economic impact of the common cold is not well understood in much of the world. In the United States, the common cold leads to 75\u2013100 million physician visits annually at a conservative cost estimate of $7.7 billion per year. Americans spend $2.9 billion on over-the-counter drugs and another $400 million on prescription medicines for symptom relief. More than one-third of people who saw a doctor received an antibiotic prescription, which has implications for antibiotic resistance. An estimated 22\u2013189 million school days are missed annually due to a cold. As a result, parents missed 126 million workdays to stay home to care for their children. When added to the 150 million workdays missed by employees who have a cold, the total economic impact of cold-related work loss exceeds $20 billion per year. This accounts for 40% of time lost from work in the United States.\n\nReferences\nNotes\n\nBibliography\n\nEccles R, Weber O, eds. (2009). Common Cold (Illustrated ed.). Springer Science & Business Media. ISBN 978-3-7643-9912-2.\n\nExternal links\n\nCommon cold at Curlie","42":"Conductive hearing loss (CHL) occurs when there is a problem transferring sound waves anywhere along the pathway through the outer ear, tympanic membrane (eardrum), or middle ear (ossicles). If a conductive hearing loss occurs in conjunction with a sensorineural hearing loss, it is referred to as a mixed hearing loss. Depending upon the severity and nature of the conductive loss, this type of hearing impairment can often be treated with surgical intervention or pharmaceuticals to partially or, in some cases, fully restore hearing acuity to within normal range. However, cases of permanent or chronic conductive hearing loss may require other treatment modalities such as hearing aid devices to improve detection of sound and speech perception.\n\nCauses\nCommon causes of conductive hearing loss include:\n\nExternal ear\nCerumen (earwax) or foreign body in the external auditory canal\nOtitis externa, infection or irritation of the outer ear\nExostoses, abnormal growth of bone within the ear canal\nTumor of the ear canal\nCongenital stenosis or atresia of the external auditory canal (narrow or blocked ear canal).\nEar canal stenosis & atresia can exist independently or may result from congenital malformations of the auricle such as microtia or anotia.\nAcquired stenosis (narrowing) of the external auditory canal following surgery or radiotherapy\n\nMiddle ear\nFluid accumulation is the most common cause of conductive hearing loss in the middle ear, especially in children. Major causes are ear infections or conditions that block the eustachian tube, such as allergies or tumors. Blocking of the eustachian tube leads to decreased pressure in the middle ear relative to the external ear, and this causes decreased motion of both the ossicles and the tympanic membrane.\n\nAcute or Serous otitis media\nChronic suppurative otitis media (CSOM)\nPerforated eardrum\nTympanosclerosis or scarring of the eardrum\nCholesteatoma\nEustachian Tube Dysfunction, inflammation or mass within the nasal cavity, middle ear, or eustachian tube itself\nOtosclerosis, abnormal growth of bone in or near the middle ear\nMiddle ear tumour\nOssicular discontinuity as a consequence of infection or temporal bone trauma\nCongenital malformation of the ossicles. This can be an isolated phenomenon or can occur as part of a syndrome where development of the 1st and 2nd branchial arches is seen such as in Goldenhar syndrome, Treacher Collins syndrome, branchio-oto-renal syndrome etc.\nBarotrauma, unequal air pressures in the external and middle ear. This can temporarily occur, for example, by the environmental pressure changes as when shifting altitude, or inside a train going into a tunnel. It is managed by any of various methods of ear clearing manoeuvres to equalize the pressures, like swallowing, yawning, or the Valsalva manoeuvre. More severe barotrauma can lead to middle ear fluid or even permanent sensorineural hearing loss.\n\nInner ear\nThird window effect caused by:\n\nSuperior canal dehiscence \u2013 which may require surgical correction.\nEnlarged vestibular aqueduct\nLabyrinthine fistula\n\nPresentation\nConductive hearing loss makes all sounds seem faint or muffled. The hearing loss is usually worse in lower frequencies.\nCongenital conductive hearing loss is identified through newborn hearing screening or may be identified because the baby has microtia or other facial abnormalities. Conductive hearing loss developing during childhood is usually due to otitis media with effusion and may present with speech and language delay or difficulty hearing. Later onset of conductive hearing loss may have an obvious cause such as an ear infection, trauma or upper respiratory tract infection or may have an insidious onset related to chronic middle ear disease, otosclerosis or a tumour of the naso-pharynx. Earwax is a very common cause of a conductive hearing loss which may present suddenly when the wax blocks sound from getting through the external ear canal to the middle and inner ear.\n\nDiagnosis\nDiagnosis requires a detailed history, local examination of the ear, nose, throat and neck, and detailed hearing tests. In children a more detailed examination may be required if the hearing loss is congenital.\n\nOtoscopy\nExamination of the external ear canal and ear drum is important and may help identify problems located in the outer ear up to the tympanic membrane.\n\nDifferential testing\nFor basic screening, a conductive hearing loss can be identified using the Rinne test with a 256 Hz tuning fork. The Rinne test, in which a patient is asked to say whether a vibrating tuning fork is heard more loudly adjacent to the ear canal (air conduction) or touching the bone behind the ear (bone conduction), is negative indicating that bone conduction is more effective that air conduction. A normal, or positive, result, is when air conduction is more effective than bone conduction.\nWith a one-sided conductive component the combined use of both the Weber and Rinne tests is useful. If the Weber test is used, in which a vibrating tuning fork is touched to the midline of the forehead, the person will hear the sound more loudly in the affected ear because background noise does not mask the hearing on this side.\nThe following table compares sensorineural hearing loss to conductive:\n\nTympanometry\nTympanometry, or acoustic immitance testing, is a simple objective test of the ability of the middle ear to transmit sound waves from the outer ear to the middle ear and to the inner ear. This test is usually abnormal with conductive hearing loss.  A type B tympanogram reveals a flat response, due to fluid in the middle ear (otitis media), or an eardrum perforation. A type C tympanogram indicates negative middle ear pressure, which is commonly seen in eustachian tube dysfunction. A type As tympanogram indicates a shallow compliance of the middle ear, which is commonly seen in otosclerosis.\n\nAudiometry\nPure tone audiometry, a standardized hearing test over a set of frequencies from 250 Hz to 8000 Hz, may be conducted by a medical doctor, audiologist or audiometrist, with the result plotted separately for each ear on an audiogram. The shape of the plot reveals the degree and nature of hearing loss, distinguishing conductive hearing loss from other kinds of hearing loss. A conductive hearing loss is characterized by a difference of at least 15 decibels between the air conduction threshold and bone conduction threshold at the same frequency. On an audiogram, the \"x\" represents responses in the left ear at each frequency, while the \"o\" represents responses in right ear at each frequency.\n\nCT scan\nMost causes of conductive hearing loss can be identified by examination but if it is important to image the bones of the middle ear or inner ear then a CT scan is required. CT scan is useful in cases of congenital conductive hearing loss, chronic suppurative otitis media or cholesteatoma, ossicular damage or discontinuity, otosclerosis and third window dehiscence. Specific MRI scans can be used to identify cholesteatoma.\n\nPathophysiology\nManagement\nManagement falls into three modalities: surgical treatment, pharmaceutical treatment, and supportive, depending on the nature and location of the specific cause.\nIn cases of infection, antibiotics or antifungal medications are an option. Some conditions are amenable to surgical intervention such as middle ear fluid, cholesteatoma, and otosclerosis. If conductive hearing loss is due to head trauma, surgical repair is an option. If absence or deformation of ear structures cannot be corrected, or if the patient declines surgery, hearing aids which amplify sounds are a possible treatment option. Bone conduction hearing aids are useful as these deliver sound directly, through bone, to the cochlea or organ of hearing bypassing the pathology. These can be on a soft or hard headband or can be inserted surgically, a bone anchored hearing aid, of which there are several types. Conventional air conduction hearing aids can also be used.\n\nSee also\nHearing loss\nSensorineural hearing loss\n\nReferences\n\n\n== External links ==","43":"A computed tomography scan (CT scan; formerly called computed axial tomography scan or CAT scan) is a medical imaging technique used to obtain detailed internal images of the body. The personnel that perform CT scans are called radiographers or radiology technologists.\nCT scanners use a rotating X-ray tube and a row of detectors placed in a gantry to measure X-ray attenuations by different tissues inside the body. The multiple X-ray measurements taken from different angles are then processed on a computer using tomographic reconstruction algorithms to produce tomographic (cross-sectional) images (virtual \"slices\") of a body. CT scans can be used in patients with metallic implants or pacemakers, for whom magnetic resonance imaging (MRI) is contraindicated.\nSince its development in the 1970s, CT scanning has proven to be a versatile imaging technique. While CT is most prominently used in medical diagnosis, it can also be used to form images of non-living objects. The 1979 Nobel Prize in Physiology or Medicine was awarded jointly to South African-American physicist Allan MacLeod Cormack and British electrical engineer Godfrey Hounsfield \"for the development of computer-assisted tomography\".\n\nTypes\nOn the basis of image acquisition and procedures, various type of scanners are available in the market.\n\nSequential CT\nSequential CT, also known as step-and-shoot CT, is a type of scanning method in which the CT table moves stepwise. The table increments to a particular location and then stops which is followed by the X-ray tube rotation and acquisition of a slice. The table then increments again, and another slice is taken. The table movement stops while taking slices. This results in an increased time of scanning.\n\nSpiral CT\nSpinning tube, commonly called spiral CT, or helical CT, is an imaging technique in which an entire X-ray tube is spun around the central axis of the area being scanned. These are the dominant type of scanners on the market because they have been manufactured longer and offer a lower cost of production and purchase. The main limitation of this type of CT is the bulk and inertia of the equipment (X-ray tube assembly and detector array on the opposite side of the circle) which limits the speed at which the equipment can spin. Some designs use two X-ray sources and detector arrays offset by an angle, as a technique to improve temporal resolution.\n\nElectron beam tomography\nElectron beam tomography (EBT) is a specific form of CT in which a large enough X-ray tube is constructed so that only the path of the electrons, travelling between the cathode and anode of the X-ray tube, are spun using deflection coils. This type had a major advantage since sweep speeds can be much faster, allowing for less blurry imaging of moving structures, such as the heart and arteries. Fewer scanners of this design have been produced when compared with spinning tube types, mainly due to the higher cost associated with building a much larger X-ray tube and detector array and limited anatomical coverage.\n\nDual Energy CT\nDual Energy CT, also known as Spectral CT, is an advancement of Computed Tomography in which two energies are used to create two sets of data. A Dual Energy CT may employ Dual source, Single source with dual detector layer, Single source with energy switching methods to get two different sets of data.  \n\nDual source CT is an advanced scanner with a two X-ray tube detector system, unlike conventional single tube systems. These two detector systems are mounted on a single gantry at 90\u00b0 in the same plane. Dual Source CT scanners allow fast scanning with higher temporal resolution by acquiring a full CT slice in only half a rotation. Fast imaging reduces motion blurring at high heart rates and potentially allowing for shorter breath-hold time. This is particularly useful for ill patients having difficulty holding their breath or unable to take heart-rate lowering medication.\nSingle Source with Energy switching is another mode of Dual energy CT in which a single tube is operated at two different energies by switching the energies frequently.\n\nCT perfusion imaging\nCT perfusion imaging is a specific form of CT to assess flow through blood vessels whilst injecting a contrast agent. Blood flow, blood transit time, and organ blood volume, can all be calculated with reasonable sensitivity and specificity. This type of CT may be used on the heart, although sensitivity and specificity for detecting abnormalities are still lower than for other forms of CT. This may also be used on the brain, where CT perfusion imaging can often detect poor brain perfusion well before it is detected using a conventional spiral CT scan. This is better for stroke diagnosis than other CT types.\n\nPET CT\nPositron emission tomography\u2013computed tomography is a hybrid CT modality which combines, in a single gantry, a positron emission tomography (PET) scanner and an X-ray computed tomography (CT) scanner, to acquire sequential images from both devices in the same session, which are combined into a single superposed (co-registered) image. Thus, functional imaging obtained by PET, which depicts the spatial distribution of metabolic or biochemical activity in the body can be more precisely aligned or correlated with anatomic imaging obtained by CT scanning.\nPET-CT gives both anatomical and functional details of an organ under examination and is helpful in detecting different type of cancers.\n\nMedical use\nSince its introduction in the 1970s, CT has become an important tool in medical imaging to supplement conventional X-ray imaging and medical ultrasonography. It has more recently been used for preventive medicine or screening for disease, for example, CT colonography for people with a high risk of colon cancer, or full-motion heart scans for people with a high risk of heart disease. Several institutions offer full-body scans for the general population although this practice goes against the advice and official position of many professional organizations in the field primarily due to the radiation dose applied.\nThe use of CT scans has increased dramatically over the last two decades in many countries. An estimated 72 million scans were performed in the United States in 2007 and more than 80 million in 2015.\n\nHead\nCT scanning of the head is typically used to detect infarction (stroke), tumors, calcifications, haemorrhage, and bone trauma. Of the above, hypodense (dark) structures can indicate edema and infarction, hyperdense (bright) structures indicate calcifications and haemorrhage and bone trauma can be seen as disjunction in bone windows. Tumors can be detected by the swelling and anatomical distortion they cause, or by surrounding edema. CT scanning of the head is also used in CT-guided stereotactic surgery and radiosurgery for treatment of intracranial tumors, arteriovenous malformations, and other surgically treatable conditions using a device known as the N-localizer.\n\nNeck\nContrast CT is generally the initial study of choice for neck masses in adults. CT of the thyroid plays an important role in the evaluation of thyroid cancer. CT scan often incidentally finds thyroid abnormalities, and so is often the preferred investigation modality for thyroid abnormalities.\n\nLungs\nA CT scan can be used for detecting both acute and chronic changes in the lung parenchyma, the tissue of the lungs. It is particularly relevant here because normal two-dimensional X-rays do not show such defects. A variety of techniques are used, depending on the suspected abnormality. For evaluation of chronic interstitial processes such as emphysema, and fibrosis, thin sections with high spatial frequency reconstructions are used; often scans are performed both on inspiration and expiration. This special technique is called high resolution CT that produces a sampling of the lung, and not continuous images.\n\nBronchial wall thickening can be seen on lung CTs and generally (but not always) implies inflammation of the bronchi.\nAn incidentally found nodule in the absence of symptoms (sometimes referred to as an incidentaloma) may raise concerns that it might represent a tumor, either benign or malignant. Perhaps persuaded by fear, patients and doctors sometimes agree to an intensive schedule of CT scans, sometimes up to every three months and beyond the recommended guidelines, in an attempt to do surveillance on the nodules. However, established guidelines advise that patients without a prior history of cancer and whose solid nodules have not grown over a two-year period are unlikely to have any malignant cancer. For this reason, and because no research provides supporting evidence that intensive surveillance gives better outcomes, and because of risks associated with having CT scans, patients should not receive CT screening in excess of those recommended by established guidelines.\n\nAngiography\nComputed tomography angiography (CTA) is a type of contrast CT to visualize the arteries and veins throughout the body. This ranges from arteries serving the brain to those bringing blood to the lungs, kidneys, arms and legs. An example of this type of exam is CT pulmonary angiogram (CTPA) used to diagnose pulmonary embolism (PE). It employs computed tomography and an iodine-based contrast agent to obtain an image of the pulmonary arteries. CT scans can reduce the risk of angiography by providing clinicians with more information about the positioning and number of clots prior to the procedure.\n\nCardiac\nA CT scan of the heart is performed to gain knowledge about cardiac or coronary anatomy. Traditionally, cardiac CT scans are used to detect, diagnose, or follow up coronary artery disease. More recently CT has played a key role in the fast-evolving field of transcatheter structural heart interventions, more specifically in the transcatheter repair and replacement of heart valves.\nThe main forms of cardiac CT scanning are:\n\nCoronary CT angiography (CCTA): the use of CT to assess the coronary arteries of the heart. The subject receives an intravenous injection of radiocontrast, and then the heart is scanned using a high-speed CT scanner, allowing radiologists to assess the extent of occlusion in the coronary arteries, usually to diagnose coronary artery disease.\nCoronary CT calcium scan: also used for the assessment of severity of coronary artery disease. Specifically, it looks for calcium deposits in the coronary arteries that can narrow arteries and increase the risk of a heart attack. A typical coronary CT calcium scan is done without the use of radiocontrast, but it can possibly be done from contrast-enhanced images as well.\nTo better visualize the anatomy, post-processing of the images is common. Most common are multiplanar reconstructions (MPR) and volume rendering. For more complex anatomies and procedures, such as heart valve interventions, a true 3D reconstruction or a 3D print is created based on these CT images to gain a deeper understanding.\n\nAbdomen and pelvis\nCT is an accurate technique for diagnosis of abdominal diseases like Crohn's disease, GIT bleeding, and diagnosis and staging of cancer, as well as follow-up after cancer treatment to assess response. It is commonly used to investigate acute abdominal pain.\nNon-enhanced computed tomography is today the gold standard for diagnosing urinary stones. The size, volume and density of stones can be estimated to help clinicians guide further treatment; size is especially important in predicting spontaneous passage of a stone.\n\nAxial skeleton and extremities\nFor the axial skeleton and extremities, CT is often used to image complex fractures, especially ones around joints, because of its ability to reconstruct the area of interest in multiple planes. Fractures, ligamentous injuries, and dislocations can easily be recognized with a 0.2 mm resolution. With modern dual-energy CT scanners, new areas of use have been established, such as aiding in the diagnosis of gout.\n\nBiomechanical use\nCT is used in biomechanics to quickly reveal the geometry, anatomy, density and elastic moduli of biological tissues.\n\nOther uses\nIndustrial use\nIndustrial CT scanning (industrial computed tomography) is a process which uses X-ray equipment to produce 3D representations of components both externally and internally. Industrial CT scanning has been used in many areas of industry for internal inspection of components. Some of the key uses for CT scanning have been flaw detection, failure analysis, metrology, assembly analysis, image-based finite element methods and reverse engineering applications. CT scanning is also employed in the imaging and conservation of museum artifacts.\n\nAviation security\nCT scanning has also found an application in transport security (predominantly airport security) where it is currently used in a materials analysis context for explosives detection CTX (explosive-detection device) and is also under consideration for automated baggage\/parcel security scanning using computer vision based object recognition algorithms that target the detection of specific threat items based on 3D appearance (e.g. guns, knives, liquid containers). Its usage in airport security pioneered at Shannon Airport in March 2022 has ended the ban on liquids over 100 ml there, a move that Heathrow Airport plans for a full roll-out on 1 December 2022 and the TSA spent $781.2 million on an order for over 1,000 scanners, ready to go live in the summer.\n\nGeological use\nX-ray CT is used in geological studies to quickly reveal materials inside a drill core. Dense minerals such as pyrite and barite appear brighter and less dense components such as clay appear dull in CT images.\n\nCultural heritage use\nX-ray CT and micro-CT can also be used for the conservation and preservation of objects of cultural heritage. For many fragile objects, direct research and observation can be damaging and can degrade the object over time. Using CT scans, conservators and researchers are able to determine the material composition of the objects they are exploring, such as the position of ink along the layers of a scroll, without any additional harm. These scans have been optimal for research focused on the workings of the Antikythera mechanism or the text hidden inside the charred outer layers of the En-Gedi Scroll. However, they are not optimal for every object subject to these kinds of research questions, as there are certain artifacts like the Herculaneum papyri in which the material composition has very little variation along the inside of the object. After scanning these objects, computational methods can be employed to examine the insides of these objects, as was the case with the virtual unwrapping of the En-Gedi scroll and the Herculaneum papyri. Micro-CT has also proved useful for analyzing more recent artifacts such as still-sealed historic correspondence that employed the technique of letterlocking (complex folding and cuts) that provided a \"tamper-evident locking mechanism\". Further examples of use cases in archaeology is imaging the contents of sarcophagi or ceramics.\nRecently, CWI in Amsterdam has collaborated with Rijksmuseum to investigate art object inside details in the framework called IntACT.\n\nMicro organism research\nVaried types of fungus can degrade wood to different degrees, one Belgium research group has been used X-ray CT 3 dimension with sub-micron resolution unveiled fungi can penetrate micropores of 0.6 \u03bcm under certain conditions.\n\nTimber sawmill\nSawmills use industrial CT scanners to detect round defects, for instance knots, to improve total value of timber productions. Most sawmills are planning to incorporate this robust detection tool to improve productivity in the long run, however initial investment cost is high.\n\nInterpretation of results\nPresentation\nThe result of a CT scan is a volume of voxels, which may be presented to a human observer by various methods, which broadly fit into the following categories:\n\nSlices (of varying thickness). Thin slice is generally regarded as planes representing a thickness of less than 3 mm. Thick slice is generally regarded as planes representing a thickness between 3 mm and 5 mm.\nProjection, including maximum intensity projection and average intensity projection\nVolume rendering (VR)\nTechnically, all volume renderings become projections when viewed on a 2-dimensional display, making the distinction between projections and volume renderings a bit vague. The epitomes of volume rendering models feature a mix of for example coloring and shading in order to create realistic and observable representations.\nTwo-dimensional CT images are conventionally rendered so that the view is as though looking up at it from the patient's feet. Hence, the left side of the image is to the patient's right and vice versa, while anterior in the image also is the patient's anterior and vice versa. This left-right interchange corresponds to the view that physicians generally have in reality when positioned in front of patients.\n\nGrayscale\nPixels in an image obtained by CT scanning are displayed in terms of relative radiodensity. The pixel itself is displayed according to the mean attenuation of the tissue(s) that it corresponds to on a scale from +3,071 (most attenuating) to \u22121,024 (least attenuating) on the Hounsfield scale. A pixel is a two dimensional unit based on the matrix size and the field of view. When the CT slice thickness is also factored in, the unit is known as a voxel, which is a three-dimensional unit. Water has an attenuation of 0 Hounsfield units (HU), while air is \u22121,000 HU, cancellous bone is typically +400 HU, and cranial bone can reach 2,000 HU. The attenuation of metallic implants depends on the atomic number of the element used: Titanium usually has an amount of +1000 HU, iron steel can completely block the X-ray and is, therefore, responsible for well-known line-artifacts in computed tomograms. Artifacts are caused by abrupt transitions between low- and high-density materials, which results in data values that exceed the dynamic range of the processing electronics.\n\nWindowing\nCT data sets have a very high dynamic range which must be reduced for display or printing. This is typically done via a process of \"windowing\", which maps a range (the \"window\") of pixel values to a grayscale ramp. For example, CT images of the brain are commonly viewed with a window extending from 0 HU to 80 HU. Pixel values of 0 and lower, are displayed as black; values of 80 and higher are displayed as white; values within the window are displayed as a gray intensity proportional to position within the window. The window used for display must be matched to the X-ray density of the object of interest, in order to optimize the visible detail. Window width and window level parameters are used to control the windowing of a scan.\n\nMultiplanar reconstruction and projections\nMultiplanar reconstruction (MPR) is the process of converting data from one anatomical plane (usually transverse) to other planes. It can be used for thin slices as well as projections. Multiplanar reconstruction is possible as present CT scanners provide almost isotropic resolution.\nMPR is used almost in every scan. The spine is frequently examined with it. An image of the spine in axial plane can only show one vertebral bone at a time and cannot show its relation with other vertebral bones. By reformatting the data in other planes, visualization of the relative position can be achieved in sagittal and coronal plane.\nNew software allows the reconstruction of data in non-orthogonal (oblique) planes, which help in the visualization of organs which are not in orthogonal planes. It is better suited for visualization of the anatomical structure of the bronchi as they do not lie orthogonal to the direction of the scan.\nCurved-plane reconstruction (or curved planar reformation = CPR) is performed mainly for the evaluation of vessels. This type of reconstruction helps to straighten the bends in a vessel, thereby helping to visualize a whole vessel in a single image or in multiple images. After a vessel has been \"straightened\", measurements such as cross-sectional area and length can be made. This is helpful in preoperative assessment of a surgical procedure.\nFor 2D projections used in radiation therapy for quality assurance and planning of external beam radiotherapy, including digitally reconstructed radiographs, see Beam's eye view.\n\nVolume rendering\nA threshold value of radiodensity is set by the operator (e.g., a level that corresponds to bone). With the help of edge detection image processing algorithms a 3D model can be constructed from the initial data and displayed on screen. Various thresholds can be used to get multiple models, each anatomical component such as muscle, bone and cartilage can be differentiated on the basis of different colours given to them. However, this mode of operation cannot show interior structures.\nSurface rendering is limited technique as it displays only the surfaces that meet a particular threshold density, and which are towards the viewer. However, In volume rendering, transparency, colours and shading are used which makes it easy to present a volume in a single image. For example, Pelvic bones could be displayed as semi-transparent, so that, even viewing at an oblique angle one part of the image does not hide another.\n\nImage quality\nDose versus image quality\nAn important issue within radiology today is how to reduce the radiation dose during CT examinations without compromising the image quality. In general, higher radiation doses result in higher-resolution images, while lower doses lead to increased image noise and unsharp images. However, increased dosage raises the adverse side effects, including the risk of radiation-induced cancer \u2013 a four-phase abdominal CT gives the same radiation dose as 300 chest X-rays. Several methods that can reduce the exposure to ionizing radiation during a CT scan exist.\n\nNew software technology can significantly reduce the required radiation dose. New iterative tomographic reconstruction algorithms (e.g., iterative Sparse Asymptotic Minimum Variance) could offer super-resolution without requiring higher radiation dose.\nIndividualize the examination and adjust the radiation dose to the body type and body organ examined. Different body types and organs require different amounts of radiation.\nHigher resolution is not always suitable, such as detection of small pulmonary masses.\n\nArtifacts\nAlthough images produced by CT are generally faithful representations of the scanned volume, the technique is susceptible to a number of artifacts, such as the following:Chapters 3 and 5\n\nStreak artifact\nStreaks are often seen around materials that block most X-rays, such as metal or bone. Numerous factors contribute to these streaks: under sampling, photon starvation, motion, beam hardening, and Compton scatter. This type of artifact commonly occurs in the posterior fossa of the brain, or if there are metal implants. The streaks can be reduced using newer reconstruction techniques. Approaches such as metal artifact reduction (MAR) can also reduce this artifact. MAR techniques include spectral imaging, where CT images are taken with photons of different energy levels, and then synthesized into monochromatic images with special software such as GSI (Gemstone Spectral Imaging).\nPartial volume effect\nThis appears as \"blurring\" of edges. It is due to the scanner being unable to differentiate between a small amount of high-density material (e.g., bone) and a larger amount of lower density (e.g., cartilage). The reconstruction assumes that the X-ray attenuation within each voxel is homogeneous; this may not be the case at sharp edges. This is most commonly seen in the z-direction (craniocaudal direction), due to the conventional use of highly anisotropic voxels, which have a much lower out-of-plane resolution, than in-plane resolution. This can be partially overcome by scanning using thinner slices, or an isotropic acquisition on a modern scanner.\nRing artifact\nProbably the most common mechanical artifact, the image of one or many \"rings\" appears within an image. They are usually caused by the variations in the response from individual elements in a two dimensional X-ray detector due to defect or miscalibration. Ring artifacts can largely be reduced by intensity normalization, also referred to as flat field correction. Remaining rings can be suppressed by a transformation to polar space, where they become linear stripes. A comparative evaluation of ring artefact reduction on X-ray tomography images showed that the method of Sijbers and Postnov can effectively suppress ring artefacts.\nNoise\nThis appears as grain on the image and is caused by a low signal to noise ratio. This occurs more commonly when a thin slice thickness is used. It can also occur when the power supplied to the X-ray tube is insufficient to penetrate the anatomy.\nWindmill\nStreaking appearances can occur when the detectors intersect the reconstruction plane. This can be reduced with filters or a reduction in pitch.\nBeam hardening\nThis can give a \"cupped appearance\" when grayscale is visualized as height. It occurs because conventional sources, like X-ray tubes emit a polychromatic spectrum. Photons of higher photon energy levels are typically attenuated less. Because of this, the mean energy of the spectrum increases when passing the object, often described as getting \"harder\". This leads to an effect increasingly underestimating material thickness, if not corrected. Many algorithms exist to correct for this artifact. They can be divided into mono- and multi-material methods.\n\nAdvantages\nCT scanning has several advantages over traditional two-dimensional medical radiography. First, CT eliminates the superimposition of images of structures outside the area of interest. Second, CT scans have greater image resolution, enabling examination of finer details. CT can distinguish between tissues that differ in radiographic density by 1% or less. Third, CT scanning enables multiplanar reformatted imaging: scan data can be visualized in the transverse (or axial), coronal, or sagittal plane, depending on the diagnostic task.\nThe improved resolution of CT has permitted the development of new investigations. For example, CT angiography avoids the invasive insertion of a catheter. CT scanning can perform a virtual colonoscopy with greater accuracy and less discomfort for the patient than a traditional colonoscopy. Virtual colonography is far more accurate than a barium enema for detection of tumors and uses a lower radiation dose.\nCT is a moderate-to-high radiation diagnostic technique. The radiation dose for a particular examination depends on multiple factors: volume scanned, patient build, number and type of scan protocol, and desired resolution and image quality. Two helical CT scanning parameters, tube current and pitch, can be adjusted easily and have a profound effect on radiation. CT scanning is more accurate than two-dimensional radiographs in evaluating anterior interbody fusion, although they may still over-read the extent of fusion.\n\nAdverse effects\nCancer\nThe radiation used in CT scans can damage body cells, including DNA molecules, which can lead to radiation-induced cancer. The radiation doses received from CT scans is variable. Compared to the lowest dose X-ray techniques, CT scans can have 100 to 1,000 times higher dose than conventional X-rays. However, a lumbar spine X-ray has a similar dose as a head CT. Articles in the media often exaggerate the relative dose of CT by comparing the lowest-dose X-ray techniques (chest X-ray) with the highest-dose CT techniques. In general, a routine abdominal CT has a radiation dose similar to three years of average background radiation.\nLarge scale population-based studies have consistently demonstrated that low dose radiation from CT scans has impacts on cancer incidence in a variety of cancers. For example, in a large population-based cohort it was found that up to 4% of brain cancers were caused by CT scan radiation. Some experts project that in the future, between three and five percent of all cancers would result from medical imaging. An Australian study of 10.9 million people reported that the increased incidence of cancer after CT scan exposure in this cohort was mostly due to irradiation. In this group, one in every 1,800 CT scans was followed by an excess cancer. If the lifetime risk of developing cancer is 40% then the absolute risk rises to 40.05% after a CT. The risks of CT scan radiation are especially important in patients undergoing recurrent CT scans within a short time span of one to five years.\nSome experts note that CT scans are known to be \"overused,\" and \"there is distressingly little evidence of better health outcomes associated with the current high rate of scans.\" On the other hand, a recent paper analyzing the data of patients who received high cumulative doses showed a high degree of appropriate use. This creates an important issue of cancer risk to these patients. Moreover, a highly significant finding that was previously unreported is that some patients received >100 mSv dose from CT scans in a single day, which counteracts existing criticisms some investigators may have on the effects of protracted versus acute exposure.\nThere are contrarian views and the debate is ongoing. Some studies have shown that publications indicating an increased risk of cancer from typical doses of body CT scans are plagued with serious methodological limitations and several highly improbable results, concluding that no evidence indicates such low doses cause any long-term harm.\nOne study estimated that as many as 0.4% of cancers in the United States resulted from CT scans, and that this may have increased to as much as 1.5 to 2% based on the rate of CT use in 2007. Others dispute this estimate, as there is no consensus that the low levels of radiation used in CT scans cause damage. Lower radiation doses are used in many cases, such as in the investigation of renal colic.\nA person's age plays a significant role in the subsequent risk of cancer. Estimated lifetime cancer mortality risks from an abdominal CT of a one-year-old is 0.1%, or 1:1000 scans. The risk for someone who is 40 years old is half that of someone who is 20 years old with substantially less risk in the elderly. The International Commission on Radiological Protection estimates that the risk to a fetus being exposed to 10 mGy (a unit of radiation exposure) increases the rate of cancer before 20 years of age from 0.03% to 0.04% (for reference a CT pulmonary angiogram exposes a fetus to 4 mGy). A 2012 review did not find an association between medical radiation and cancer risk in children noting however the existence of limitations in the evidences over which the review is based. CT scans can be performed with different settings for lower exposure in children with most manufacturers of CT scans as of 2007 having this function built in. Furthermore, certain conditions can require children to be exposed to multiple CT scans. \nCurrent recommendations are to inform patients of the risks of CT scanning. However, employees of imaging centers tend not to communicate such risks unless patients ask.\n\nContrast reactions\nIn the United States half of CT scans are contrast CTs using intravenously injected radiocontrast agents. The most common reactions from these agents are mild, including nausea, vomiting, and an itching rash. Severe life-threatening reactions may rarely occur. Overall reactions occur in 1 to 3% with nonionic contrast and 4 to 12% of people with ionic contrast. Skin rashes may appear within a week to 3% of people.\nThe old radiocontrast agents caused anaphylaxis in 1% of cases while the newer, low-osmolar agents cause reactions in 0.01\u20130.04% of cases. Death occurs in about 2 to 30 people per 1,000,000 administrations, with newer agents being safer.\nThere is a higher risk of mortality in those who are female, elderly or in poor health, usually secondary to either anaphylaxis or acute kidney injury.\nThe contrast agent may induce contrast-induced nephropathy. This occurs in 2 to 7% of people who receive these agents, with greater risk in those who have preexisting kidney failure, preexisting diabetes, or reduced intravascular volume. People with mild kidney impairment are usually advised to ensure full hydration for several hours before and after the injection. For moderate kidney failure, the use of iodinated contrast should be avoided; this may mean using an alternative technique instead of CT. Those with severe kidney failure requiring dialysis require less strict precautions, as their kidneys have so little function remaining that any further damage would not be noticeable and the dialysis will remove the contrast agent; it is normally recommended, however, to arrange dialysis as soon as possible following contrast administration to minimize any adverse effects of the contrast.\nIn addition to the use of intravenous contrast, orally administered contrast agents are frequently used when examining the abdomen. These are frequently the same as the intravenous contrast agents, merely diluted to approximately 10% of the concentration. However, oral alternatives to iodinated contrast exist, such as very dilute (0.5\u20131% w\/v) barium sulfate suspensions. Dilute barium sulfate has the advantage that it does not cause allergic-type reactions or kidney failure, but cannot be used in patients with suspected bowel perforation or suspected bowel injury, as leakage of barium sulfate from damaged bowel can cause fatal peritonitis.\nSide effects from contrast agents, administered intravenously in some CT scans, might impair kidney performance in patients with kidney disease, although this risk is now believed to be lower than previously thought.\n\nScan dose\nThe table reports average radiation exposures; however, there can be a wide variation in radiation doses between similar scan types, where the highest dose could be as much as 22 times higher than the lowest dose. A typical plain film X-ray involves radiation dose of 0.01 to 0.15 mGy, while a typical CT can involve 10\u201320 mGy for specific organs, and can go up to 80 mGy for certain specialized CT scans.\nFor purposes of comparison, the world average dose rate from naturally occurring sources of background radiation is 2.4 mSv per year, equal for practical purposes in this application to 2.4 mGy per year. While there is some variation, most people (99%) received less than 7 mSv per year as background radiation. Medical imaging as of 2007 accounted for half of the radiation exposure of those in the United States with CT scans making up two thirds of this amount. In the United Kingdom it accounts for 15% of radiation exposure. The average radiation dose from medical sources is \u22480.6 mSv per person globally as of 2007. Those in the nuclear industry in the United States are limited to doses of 50 mSv a year and 100 mSv every 5 years.\nLead is the main material used by radiography personnel for shielding against scattered X-rays.\n\nRadiation dose units\nThe radiation dose reported in the gray or mGy unit is proportional to the amount of energy that the irradiated body part is expected to absorb, and the physical effect (such as DNA double strand breaks) on the cells' chemical bonds by X-ray radiation is proportional to that energy.\nThe sievert unit is used in the report of the effective dose. The sievert unit, in the context of CT scans, does not correspond to the actual radiation dose that the scanned body part absorbs but to another radiation dose of another scenario, the whole body absorbing the other radiation dose and the other radiation dose being of a magnitude, estimated to have the same probability to induce cancer as the CT scan. Thus, as is shown in the table above, the actual radiation that is absorbed by a scanned body part is often much larger than the effective dose suggests. A specific measure, termed the computed tomography dose index (CTDI), is commonly used as an estimate of the radiation absorbed dose for tissue within the scan region, and is automatically computed by medical CT scanners.\nThe equivalent dose is the effective dose of a case, in which the whole body would actually absorb the same radiation dose, and the sievert unit is used in its report. In the case of non-uniform radiation, or radiation given to only part of the body, which is common for CT examinations, using the local equivalent dose alone would overstate the biological risks to the entire organism.\n\nEffects of radiation\nMost adverse health effects of radiation exposure may be grouped in two general categories:\n\ndeterministic effects (harmful tissue reactions) due in large part to the killing\/malfunction of cells following high doses;\nstochastic effects, i.e., cancer and heritable effects involving either cancer development in exposed individuals owing to mutation of somatic cells or heritable disease in their offspring owing to mutation of reproductive (germ) cells.\nThe added lifetime risk of developing cancer by a single abdominal CT of 8 mSv is estimated to be 0.05%, or 1 one in 2,000.\nBecause of increased susceptibility of fetuses to radiation exposure, the radiation dosage of a CT scan is an important consideration in the choice of medical imaging in pregnancy.\n\nExcess doses\nIn October, 2009, the US Food and Drug Administration (FDA) initiated an investigation of brain perfusion CT (PCT) scans, based on radiation burns caused by incorrect settings at one particular facility for this particular type of CT scan. Over 200 patients were exposed to radiation at approximately eight times the expected dose for an 18-month period; over 40% of them lost patches of hair. This event prompted a call for increased CT quality assurance programs. It was noted that \"while unnecessary radiation exposure should be avoided, a medically needed CT scan obtained with appropriate acquisition parameter has benefits that outweigh the radiation risks.\" Similar problems have been reported at other centers. These incidents are believed to be due to human error.\n\nProcedure\nCT scan procedure varies according to the type of the study and the organ being imaged. The patient is made to lie on the CT table and the centering of the table is done according to the body part. The IV line is established in case of contrast-enhanced CT. After selecting proper and rate of contrast from the pressure injector, the scout is taken to localize and plan the scan. Once the plan is selected, the contrast is given. The raw data is processed according to the study and proper windowing is done to make scans easy to diagnose.\n\nPreparation\nPatient preparation may vary according to the type of scan. The general patient preparation includes.\n\nSigning the informed consent.\nRemoval of metallic objects and jewelry from the region of interest.\nChanging to the hospital gown according to hospital protocol.\nChecking of kidney function, especially creatinine and urea levels (in case of CECT).\n\nMechanism\nComputed tomography operates by using an X-ray generator that rotates around the object; X-ray detectors are positioned on the opposite side of the circle from the X-ray source. As the X-rays pass through the patient, they are attenuated differently by various tissues according to the tissue density. A visual representation of the raw data obtained is called a sinogram, yet it is not sufficient for interpretation. Once the scan data has been acquired, the data must be processed using a form of tomographic reconstruction, which produces a series of cross-sectional images. These cross-sectional images are made up of small units of pixels or voxels.\nPixels in an image obtained by CT scanning are displayed in terms of relative radiodensity. The pixel itself is displayed according to the mean attenuation of the tissue(s) that it corresponds to on a scale from +3,071 (most attenuating) to \u22121,024 (least attenuating) on the Hounsfield scale. A pixel is a two dimensional unit based on the matrix size and the field of view. When the CT slice thickness is also factored in, the unit is known as a voxel, which is a three-dimensional unit.\nWater has an attenuation of 0 Hounsfield units (HU), while air is \u22121,000 HU, cancellous bone is typically +400 HU, and cranial bone can reach 2,000 HU or more (os temporale) and can cause artifacts. The attenuation of metallic implants depends on the atomic number of the element used: Titanium usually has an amount of +1000 HU, iron steel can completely extinguish the X-ray and is, therefore, responsible for well-known line-artifacts in computed tomograms. Artifacts are caused by abrupt transitions between low- and high-density materials, which results in data values that exceed the dynamic range of the processing electronics. Two-dimensional CT images are conventionally rendered so that the view is as though looking up at it from the patient's feet. Hence, the left side of the image is to the patient's right and vice versa, while anterior in the image also is the patient's anterior and vice versa. This left-right interchange corresponds to the view that physicians generally have in reality when positioned in front of patients.\nInitially, the images generated in CT scans were in the transverse (axial) anatomical plane, perpendicular to the long axis of the body. Modern scanners allow the scan data to be reformatted as images in other planes. Digital geometry processing can generate a three-dimensional image of an object inside the body from a series of two-dimensional radiographic images taken by rotation around a fixed axis. These cross-sectional images are widely used for medical diagnosis and therapy.\n\nContrast\nContrast media used for X-ray CT, as well as for plain film X-ray, are called radiocontrasts. Radiocontrasts for CT are, in general, iodine-based. This is useful to highlight structures such as blood vessels that otherwise would be difficult to delineate from their surroundings. Using contrast material can also help to obtain functional information about tissues. Often, images are taken both with and without radiocontrast.\n\nHistory\nThe history of X-ray computed tomography goes back to at least 1917 with the mathematical theory of the Radon transform. In October 1963, William H. Oldendorf received a U.S. patent for a \"radiant energy apparatus for investigating selected areas of interior objects obscured by dense material\". The first commercially viable CT scanner was invented by Godfrey Hounsfield in 1972.\nIt is often claimed that revenues from the sales of The Beatles' records in the 1960s helped fund the development of the first CT scanner at EMI. The first production X-ray CT machines were in fact called EMI scanners.\n\nEtymology\nThe word tomography is derived from the Greek tome 'slice' and graphein 'to write'. Computed tomography was originally known as the \"EMI scan\" as it was developed in the early 1970s at a research branch of EMI, a company best known today for its music and recording business. It was later known as computed axial tomography (CAT or CT scan) and body section r\u00f6ntgenography.\nThe term CAT scan is no longer in technical use because current CT scans enable for multiplanar reconstructions. This makes CT scan the most appropriate term, which is used by radiologists in common vernacular as well as in textbooks and scientific papers.\nIn Medical Subject Headings (MeSH), computed axial tomography was used from 1977 to 1979, but the current indexing explicitly includes X-ray in the title.\nThe term sinogram was introduced by Paul Edholm and Bertil Jacobson in 1975.\n\nSociety and culture\nCampaigns\nIn response to increased concern by the public and the ongoing progress of best practices, the Alliance for Radiation Safety in Pediatric Imaging was formed within the Society for Pediatric Radiology. In concert with the American Society of Radiologic Technologists, the American College of Radiology and the American Association of Physicists in Medicine, the Society for Pediatric Radiology developed and launched the Image Gently Campaign which is designed to maintain high-quality imaging studies while using the lowest doses and best radiation safety practices available on pediatric patients. This initiative has been endorsed and applied by a growing list of various professional medical organizations around the world and has received support and assistance from companies that manufacture equipment used in Radiology.\nFollowing upon the success of the Image Gently campaign, the American College of Radiology, the Radiological Society of North America, the American Association of Physicists in Medicine and the American Society of Radiologic Technologists have launched a similar campaign to address this issue in the adult population called Image Wisely.\nThe World Health Organization and International Atomic Energy Agency (IAEA) of the United Nations have also been working in this area and have ongoing projects designed to broaden best practices and lower patient radiation dose.\n\nPrevalence\nUse of CT has increased dramatically over the last two decades. An estimated 72 million scans were performed in the United States in 2007, accounting for close to half of the total per-capita dose rate from radiologic and nuclear medicine procedures. Of the CT scans, six to eleven percent are done in children, an increase of seven to eightfold from 1980. Similar increases have been seen in Europe and Asia. In Calgary, Canada, 12.1% of people who present to the emergency with an urgent complaint received a CT scan, most commonly either of the head or of the abdomen. The percentage who received CT, however, varied markedly by the emergency physician who saw them from 1.8% to 25%. In the emergency department in the United States, CT or MRI imaging is done in 15% of people who present with injuries as of 2007 (up from 6% in 1998).\nThe increased use of CT scans has been the greatest in two fields: screening of adults (screening CT of the lung in smokers, virtual colonoscopy, CT cardiac screening, and whole-body CT in asymptomatic patients) and CT imaging of children. Shortening of the scanning time to around 1 second, eliminating the strict need for the subject to remain still or be sedated, is one of the main reasons for the large increase in the pediatric population (especially for the diagnosis of appendicitis). As of 2007, in the United States a proportion of CT scans are performed unnecessarily. Some estimates place this number at 30%. There are a number of reasons for this including: legal concerns, financial incentives, and desire by the public. For example, some healthy people avidly pay to receive full-body CT scans as screening. In that case, it is not at all clear that the benefits outweigh the risks and costs. Deciding whether and how to treat incidentalomas is complex, radiation exposure is not negligible, and the money for the scans involves opportunity cost.\n\nManufacturers\nMajor manufacturers of CT scanning devices and equipment are:\n\n GE HealthCare\n Siemens Healthineers\n Canon Medical Systems Corporation (formerly Toshiba Medical Systems)\n Koninklijke Philips N.V.\n Fujifilm Healthcare (formerly Hitachi Medical Systems)\n Neusoft Medical Systems\n United Imaging\n\nResearch\nPhoton-counting computed tomography is a CT technique currently under development. Typical CT scanners use energy integrating detectors; photons are measured as a voltage on a capacitor which is proportional to the X-rays detected. However, this technique is susceptible to noise and other factors which can affect the linearity of the voltage to X-ray intensity relationship. Photon counting detectors (PCDs) are still affected by noise but it does not change the measured counts of photons. PCDs have several potential advantages, including improving signal (and contrast) to noise ratios, reducing doses, improving spatial resolution, and through use of several energies, distinguishing multiple contrast agents. PCDs have only recently become feasible in CT scanners due to improvements in detector technologies that can cope with the volume and rate of data required. As of February 2016, photon counting CT is in use at three sites. Some early research has found the dose reduction potential of photon counting CT for breast imaging to be very promising. In view of recent findings of high cumulative doses to patients from recurrent CT scans, there has been a push for scanning technologies and techniques that reduce ionising radiation doses to patients to sub-milliSievert (sub-mSv in the literature) levels during the CT scan process, a goal that has been lingering.\n\nSee also\nReferences\nExternal links\n\nDevelopment of CT imaging\nCT Artefacts\u2014PPT by David Platten\nFiller A (2009-06-30). \"The History, Development and Impact of Computed Imaging in Neurological Diagnosis and Neurosurgery: CT, MRI, and DTI\". Nature Precedings: 1. doi:10.1038\/npre.2009.3267.4. ISSN 1756-0357.\nBoone JM, McCollough CH (2021). \"Computed tomography turns 50\". Physics Today. 74 (9): 34\u201340. Bibcode:2021PhT....74i..34B. doi:10.1063\/PT.3.4834. ISSN 0031-9228. S2CID 239718717.","44":"Cortical deafness is a rare form of sensorineural hearing loss caused by damage to the primary auditory cortex. Cortical deafness is an auditory disorder where the patient is unable to hear sounds but has no apparent damage to the structures of the ear (see  auditory system). It has been argued to be as the combination of auditory verbal agnosia and auditory agnosia. Patients with cortical deafness cannot hear any sounds, that is, they are not aware of sounds including non-speech, voices, and speech sounds. Although patients appear and feel completely deaf, they can still exhibit some reflex responses such as turning their head towards a loud sound.\n\nCause\nCortical deafness is caused by bilateral cortical lesions in the primary auditory cortex located in the temporal lobes of the brain. The ascending auditory pathways are damaged, causing a loss of perception of sound. Inner ear functions, however, remains intact. Cortical deafness is most often caused by stroke, but can also result from brain injury or birth defects. More specifically, a common cause is bilateral embolic stroke to the area of Heschl's gyri.\nIt is thought that cortical deafness could be a part of a spectrum of an overall cortical hearing disorder. In some cases, patients with cortical deafness have had recovery of some hearing function, resulting in partial auditory deficits such as auditory verbal agnosia. This syndrome might be difficult to distinguish from a bilateral temporal lesion such as described above.\n\nDiagnosis\nSince cortical deafness and auditory agnosia have many similarities, diagnosing the disorder proves to be difficult. Bilateral lesions near the primary auditory cortex in the temporal lobe are important criteria. Cortical deafness requires demonstration that brainstem auditory responses are normal, but cortical evoked potentials are impaired. Brainstem auditory evoked potentials, also called brainstem auditory evoked responses, show the neuronal activity in the auditory nerve, cochlear nucleus, superior olive, and inferior colliculus of the brainstem. They typically have a response latency of no more than six milliseconds with an amplitude of approximately one microvolt. The latency of the responses gives critical information: if cortical deafness is applicable, long latency responses are completely abolished and middle latency responses are either abolished or significantly impaired. In auditory agnosia, long and middle latency responses are preserved.\nAnother important aspect of cortical deafness that is often overlooked is that patients feel deaf. They are aware of their inability to hear environmental sounds, non-speech and speech sounds. Patients with auditory agnosia can be unaware of their deficit, and insist that they are not deaf. Verbal deafness and auditory agnosia are disorders of a selective, perceptive and associative nature whereas cortical deafness relies on the anatomic and functional disconnection of the auditory cortex from acoustic impulses.\n\nCase examples\nAlthough cortical deafness has very specific parameters of diagnosis, its causes on the other hand can vary tremendously. The following are three case studies with different reasons for cortical deafness.\n\nA case published in 2001 describes the patient as 20-year-old man referred for cochlear implants because of bilateral deafness following a motorcycle accident two years earlier. His CT shows hemorrhagic lesions involving both internal capsules. He was comatose for several weeks and awoke quadriparetic, cognitively impaired and completely deaf. He exhibited a response towards the occasional sudden, loud sound, however, by turning his head. Reading and writing capabilities were maintained, and he was able to communicate by lip-reading. His own speech was dysarthric, but comprehensible. Normal tympanograms and stapedial reflexes imply that the middle and inner ear remained functioning and the auditory nerve was intact. His auditory nerve was tested by evoking responses with normal auditory nerve potentials at 10 dB bilaterally. The results of the brainstem auditory evoked responses waves were normal, but an abnormal complex IV-V suggested that the pathways were functioning through the brainstem, but there was a lesion present in the mid-brain. With these findings, it was determined the patient had cortical deafness due to bilateral interruption of the ascending auditory pathway associated with hemorrhagic lesions of both internal capsules. Therefore, cochlear implantation was not performed.\nPublished in 1994, this patient was monitored over the course of almost 20 years after exhibiting signs of hearing impairment as an infant. Audiologic and related test results in concurrence with MRI confirmed bilateral absence of considerable portions of her temporal lobes resulting in cortical deafness. Although physiologic measures demonstrate normal peripheral hearing sensitivity, this patient's speech has the inflection and prosodic characteristics associated with profound peripheral hearing loss, and she is unable to understand spoken communication. Behaviorally obtained pure-tone thresholds were variable, ranging from normal to moderate hearing loss with normal middle ear muscle reflexes and normal ABRs to high- and low-intensity stimuli. Auditory middle latency and cortical evoked potentials were grossly abnormal, consistent with the central nature of cortical deafness. Because of her inability to communicate auditorily, this patient was ultimately taught American Sign Language and educated at the Louisiana School for the Deaf. At the completion of the case study, the patient was married and expecting a child.\nA more recent study, published in 2013 the patient described is a 56-year-old woman a history of hypertension, hypercholesterolemia, and multiple strokes who presented with a complaint of complete bilateral hearing loss. In March 2009, she experienced an acute right-sided insulotemporal intracerebral hemorrhage. Immediately after this event, the patient complained of hearing loss with the inability to hear all sounds except for severe bilateral tinnitus. Imaging revealed sequelae in the left cerebral cortex from her previous strokes. The new right-sided hemorrhage was centered on the posterior putamen with surrounding edema involving the posterior portion of the posterior limbs of the internal, external, and extreme capsules. Signal abnormalities extended into the right temporal lobe. The patient had no other neurologic deficits and spoke fluently, although with poor internal volume control of her voice. Otoscopic examination revealed normal-appearing external auditory canals, intact tympanic membranes bilaterally, and normal middle ear anatomy. Audiogram at that time showed bilateral profound hearing loss with no responses to pure-tone or speech testing.\nIn a case study by Sasidharan et al. (2020), a patient developed cortical deafness following bacterial meningitis at 5 months old. The case was evaluated when the patient was 7 years old. Objective tests showed normal peripheral hearing, but the patient did not respond to sounds during pure-tone audiometry. Late latency response tests showed absent bilateral responses, confirming cortical deafness. This case highlights that meningitis can lead to cortical deafness in addition to peripheral hearing loss.\n\nTreatment\nAuditory perception can improve with time. There seems to be a level of neuroplasticity that allows patients to recover the ability to perceive environmental and certain musical sounds.  Patients presenting with cortical hearing loss and no other associated symptoms recover to a variable degree, depending on the size and type of the cerebral lesion.  Patients whose symptoms include both motor deficits and aphasias often have larger lesions with an associated poorer prognosis in regard to functional status and recovery.\nCochlear or auditory brainstem implantation could also be treatment options. Electrical stimulation of the peripheral auditory system may result in improved sound perception or cortical remapping in patients with cortical deafness. However, hearing aids are an inappropriate answer for cases like these. Any auditory signal, regardless if has been amplified to normal or high intensities, is useless to a system unable to complete its processing. Ideally, patients should be directed toward resources to aid them in lip-reading, learning American Sign Language, as well as speech and occupational therapy. Patients should follow-up regularly to evaluate for any long-term recovery.\n\nHistory\nEarly reports, published in the late 19th century, describe patients with acute onset of deafness after experiencing symptoms described as apoplexy. The only means of definitive diagnosis in these reports were postmortem dissections. Subsequent cases throughout the 20th century reflect advancements in diagnoses of both hearing loss and stroke. With the advent of audiometric and electrophysiologic studies, investigators could diagnose cortical deafness with increasing precision. Advances in imaging techniques, such as MRI, greatly improved the diagnosis and localization of cerebral infarcts that coincide with primary or secondary auditory centers. Neurological and cognitive testing help to distinguish between total cortical deafness and auditory agnosia, resulting in the inability to perceive words, music, or specific environmental sounds.\n\nReferences\nFurther reading\nHood L, Berlin C, Allen P (1994). \"Cortical deafness: a longitudinal study\". Journal of the American Academy of Audiology. 5 (5): 330\u201342. PMID 7987023.\n\n\n== External links ==","45":"Corticosteroids are a class of steroid hormones that are produced in the adrenal cortex of vertebrates, as well as the synthetic analogues of these hormones. Two main classes of corticosteroids, glucocorticoids and mineralocorticoids, are involved in a wide range of physiological processes, including stress response, immune response, and regulation of inflammation, carbohydrate metabolism, protein catabolism, blood electrolyte levels, and behavior.\nSome common naturally occurring steroid hormones are cortisol (C21H30O5), corticosterone (C21H30O4), cortisone (C21H28O5) and aldosterone (C21H28O5) (cortisone and aldosterone are isomers). The main corticosteroids produced by the adrenal cortex are cortisol and aldosterone.\n\nClasses\nGlucocorticoids such as cortisol affect carbohydrate, fat, and protein metabolism, and have anti-inflammatory, immunosuppressive, anti-proliferative, and vasoconstrictive effects. Anti-inflammatory effects are mediated by blocking the action of inflammatory mediators (transrepression) and inducing anti-inflammatory mediators (transactivation). Immunosuppressive effects are mediated by suppressing delayed hypersensitivity reactions by direct action on T-lymphocytes. Anti-proliferative effects are mediated by inhibition of DNA synthesis and epidermal cell turnover. Vasoconstrictive effects are mediated by inhibiting the action of inflammatory mediators such as histamine.\nMineralocorticoids such as aldosterone are primarily involved in the regulation of electrolyte and water balance by modulating ion transport in the epithelial cells of the renal tubules of the kidney.\n\nMedical uses\nSynthetic pharmaceutical drugs with corticosteroid-like effects are used in a variety of conditions, ranging from hematological neoplasms to brain tumors or skin diseases. Dexamethasone and its derivatives are almost pure glucocorticoids, while prednisone and its derivatives have some mineralocorticoid action in addition to the glucocorticoid effect. Fludrocortisone (Florinef) is a synthetic mineralocorticoid. Hydrocortisone (cortisol) is typically used for replacement therapy, e.g. for adrenal insufficiency and congenital adrenal hyperplasia.\nMedical conditions treated with systemic corticosteroids:\n\nTopical formulations are also available for the skin, eyes (uveitis), lungs (asthma), nose (rhinitis), and bowels. Corticosteroids are also used supportively to prevent nausea, often in combination with 5-HT3 antagonists (e.g., ondansetron).\nTypical undesired effects of glucocorticoids present quite uniformly as drug-induced Cushing's syndrome. Typical mineralocorticoid side-effects are hypertension (abnormally high blood pressure), steroid induced diabetes mellitus, psychosis, poor sleep, hypokalemia (low potassium levels in the blood), hypernatremia (high sodium levels in the blood) without causing peripheral edema, metabolic alkalosis and connective tissue weakness. Wound healing or ulcer formation may be inhibited by the immunosuppressive effects.\nA variety of steroid medications, from anti-allergy nasal sprays (Nasonex, Flonase) to topical skin creams, to eye drops (Tobradex), to prednisone have been implicated in the development of central serous retinopathy (CSR).\nCorticosteroids have been widely used in treating people with traumatic brain injury. A systematic review identified 20 randomised controlled trials and included 12,303 participants, then compared patients who received corticosteroids with patients who received no treatment.  The authors recommended people with traumatic head injury should not be routinely treated with corticosteroids.\n\nPharmacology\nCorticosteroids act as agonists of the glucocorticoid receptor and\/or the mineralocorticoid receptor.\nIn addition to their corticosteroid activity, some corticosteroids may have some progestogenic activity and may produce sex-related side effects.\n\nPharmacogenetics\nAsthma\nPatients' response to inhaled corticosteroids has some basis in genetic variations. Two genes of interest are CHRH1 (corticotropin-releasing hormone receptor 1) and TBX21 (transcription factor T-bet). Both genes display some degree of polymorphic variation in humans, which may explain how some patients respond better to inhaled corticosteroid therapy than others. However, not all asthma patients respond to corticosteroids and large sub groups of asthma patients are corticosteroid resistant.\nA study funded by the Patient-Centered Outcomes Research Institute of children and teens with mild persistent asthma found that using the control inhaler as needed worked the same as daily use in improving asthma control, number of asthma flares, how well the lungs work, and quality of life. Children and teens using the inhaler as needed used about one-fourth the amount of corticosteroid medicine as children and teens using it daily.\n\nAdverse effects\nUse of corticosteroids has numerous side-effects, some of which may be severe:\n\nSevere amoebic colitis: Fulminant amoebic colitis is associated with high case fatality and can occur in patients infected with the parasite Entamoeba histolytica after exposure to corticosteroid medications.\nNeuropsychiatric: steroid psychosis, and anxiety, depression. Therapeutic doses may cause a feeling of artificial well-being (\"steroid euphoria\"). The neuropsychiatric effects are partly mediated by sensitization of the body to the actions of adrenaline. Therapeutically, the bulk of corticosteroid dose is given in the morning to mimic the body's diurnal rhythm; if given at night, the feeling of being energized will interfere with sleep. An extensive review is provided by Flores and Gumina.\nCardiovascular: Corticosteroids can cause sodium retention through a direct action on the kidney, in a manner analogous to the mineralocorticoid aldosterone. This can result in fluid retention and hypertension.\nMetabolic: Corticosteroids cause a movement of body fat to the face and torso, resulting in \"moon face\", \"buffalo hump\", and \"pot belly\" or \"beer belly\", and cause movement of body fat away from the limbs. This has been termed corticosteroid-induced lipodystrophy. Due to the diversion of amino-acids to glucose, they are considered anti-anabolic, and long term therapy can cause muscle wasting (muscle atrophy). Besides muscle atrophy, steroid myopathy includes muscle pains (myalgias), muscle weakness (typically of the proximal muscles), serum creatine kinase normal, EMG myopathic, and some have type II (fast-twitch\/glycolytic) fibre atrophy.\nEndocrine: By increasing the production of glucose from amino-acid breakdown and opposing the action of insulin, corticosteroids can cause hyperglycemia, insulin resistance and diabetes mellitus.\nSkeletal: Steroid-induced osteoporosis may be a side-effect of long-term corticosteroid use.  Use of inhaled corticosteroids among children with asthma may result in decreased height.\nGastro-intestinal: While cases of colitis have been reported, corticosteroids are often prescribed when the colitis, although due to suppression of the immune response to pathogens, should be considered only after ruling out infection or microbe\/fungal overgrowth in the gastrointestinal tract. While the evidence for corticosteroids causing peptic ulceration is relatively poor except for high doses taken for over a month, the majority of doctors as of 2010 still believe this is the case, and would consider protective prophylactic measures.\nEyes: chronic use may predispose to cataract and glaucoma. Clinical and experimental evidence indicates that corticosteroids can cause permanent eye damage by inducing central serous retinopathy (CSR, also known as central serous chorioretinopathy, CSC). This should be borne in mind when treating patients with optic neuritis. There is experimental and clinical evidence that, at least in optic neuritis speed of treatment initiation is important.\nVulnerability to infection: By suppressing immune reactions (which is one of the main reasons for their use in allergies), steroids may cause infections to flare up, notably candidiasis.\nPregnancy: Corticosteroids have a low but significant teratogenic effect, causing a few birth defects per 1,000 pregnant women treated. Corticosteroids are therefore contraindicated in pregnancy.\nHabituation: Topical steroid addiction (TSA) or red burning skin has been reported in long-term users of topical steroids (users who applied topical steroids to their skin over a period of weeks, months, or years). TSA is characterised by uncontrollable, spreading dermatitis and worsening skin inflammation which requires a stronger topical steroid to get the same result as the first prescription. When topical steroid medication is lost, the skin experiences redness, burning, itching, hot skin, swelling, and\/or oozing for a length of time. This is also called 'red skin syndrome' or 'topical steroid withdrawal' (TSW). After the withdrawal period is over the atopic dermatitis can cease or is less severe than it was before.\nIn children the short term use of steroids by mouth increases the risk of vomiting, behavioral changes, and sleeping problems.\n\nBiosynthesis\nThe corticosteroids are synthesized from cholesterol within the adrenal cortex. Most steroidogenic reactions are catalysed by enzymes of the cytochrome P450 family. They are located within the mitochondria and require adrenodoxin as a cofactor (except 21-hydroxylase and 17\u03b1-hydroxylase).\nAldosterone and corticosterone share the first part of their biosynthetic pathway. The last part is mediated either by the aldosterone synthase (for aldosterone) or by the 11\u03b2-hydroxylase (for corticosterone). These enzymes are nearly identical (they share 11\u03b2-hydroxylation and 18-hydroxylation functions), but aldosterone synthase is also able to perform an 18-oxidation. Moreover, aldosterone synthase is found within the zona glomerulosa at the outer edge of the adrenal cortex; 11\u03b2-hydroxylase is found in the zona fasciculata and zona glomerulosa.\n\nClassification\nBy chemical structure\nIn general, corticosteroids are grouped into four classes, based on chemical structure. Allergic reactions to one member of a class typically indicate an intolerance of all members of the class. This is known as the \"Coopman classification\".\nThe highlighted steroids are often used in the screening of allergies to topical steroids.\n\nGroup A \u2013 Hydrocortisone type\nHydrocortisone, hydrocortisone acetate, cortisone acetate, tixocortol pivalate, prednisolone, methylprednisolone, and prednisone.\n\nGroup B \u2013 Acetonides (and related substances)\nAmcinonide, budesonide, desonide, fluocinolone acetonide, fluocinonide, halcinonide, triamcinolone acetonide, and Deflazacort (O-isopropylidene derivative)\n\nGroup C \u2013 Betamethasone type\nBeclometasone, betamethasone, dexamethasone, fluocortolone, halometasone, and mometasone.\n\nGroup D \u2013 Esters\nGroup D1 \u2013 Halogenated (less labile)\nAlclometasone dipropionate, betamethasone dipropionate, betamethasone valerate, clobetasol propionate, clobetasone butyrate, fluprednidene acetate, and mometasone furoate.\n\nGroup D2 \u2013 Labile prodrug esters\nCiclesonide, cortisone acetate, hydrocortisone aceponate, hydrocortisone acetate, hydrocortisone buteprate, hydrocortisone butyrate, hydrocortisone valerate, prednicarbate, and tixocortol pivalate.\n\nBy route of administration\nTopical steroids\nFor use topically on the skin, eye, and mucous membranes.\nTopical corticosteroids are divided in potency classes I to IV in most countries (A to D in Japan). Seven categories are used in the United States to determine the level of potency of any given topical corticosteroid.\n\nInhaled steroids\nFor nasal mucosa, sinuses, bronchi, and lungs.\nThis group includes:\n\nFlunisolide\nFluticasone furoate\nFluticasone propionate\nTriamcinolone acetonide\nBeclomethasone dipropionate\nBudesonide\nMometasone furoate\nCiclesonide\nThere also exist certain combination preparations such as Advair Diskus in the United States, containing fluticasone propionate and salmeterol (a long-acting bronchodilator), and Symbicort, containing budesonide and formoterol fumarate dihydrate (another long-acting bronchodilator). They are both approved for use in children over 12 years old.\n\nOral forms\nSuch as prednisone, prednisolone, methylprednisolone, or dexamethasone.\n\nSystemic forms\nAvailable in injectables for intravenous and parenteral routes.\n\nHistory\nTadeusz Reichstein, Edward Calvin Kendall, and Philip Showalter Hench were awarded the Nobel Prize for Physiology and Medicine in 1950 for their work on hormones of the adrenal cortex, which culminated in the isolation of cortisone.\nInitially hailed as a miracle cure and liberally prescribed during the 1950s, steroid treatment brought about adverse events of such a magnitude that the next major category of anti-inflammatory drugs, the nonsteroidal anti-inflammatory drugs (NSAIDs), was so named in order to demarcate from the opprobrium.\nLewis Sarett of Merck & Co. was the first to synthesize cortisone, using a 36-step process that started with deoxycholic acid, which was extracted from ox bile. The low efficiency of converting deoxycholic acid into cortisone led to a cost of US$200 per gram in 1947. Russell Marker, at Syntex, discovered a much cheaper and more convenient starting material, diosgenin from wild Mexican yams. His conversion of diosgenin into progesterone by a four-step process now known as Marker degradation was an important step in mass production of all steroidal hormones, including cortisone and chemicals used in hormonal contraception.\nIn 1952, D.H. Peterson and H.C. Murray of Upjohn developed a process that used Rhizopus mold to oxidize progesterone into a compound that was readily converted to cortisone. The ability to cheaply synthesize large quantities of cortisone from the diosgenin in yams resulted in a rapid drop in price to US$6 per gram, falling to $0.46 per gram by 1980. Percy Julian's research also aided progress in the field. The exact nature of cortisone's anti-inflammatory action remained a mystery for years after, however, until the leukocyte adhesion cascade and the role of phospholipase A2 in the production of prostaglandins and leukotrienes was fully understood in the early 1980s.\nCorticosteroids were voted Allergen of the Year in 2005 by the American Contact Dermatitis Society.\n\nEtymology\nThe cortico- part of the name refers to the adrenal cortex, which makes these steroid hormones. Thus a corticosteroid is a \"cortex steroid\".\n\nSee also\nList of corticosteroids\nList of corticosteroid cyclic ketals\nList of corticosteroid esters\nList of steroid abbreviations\n\n\n== References ==","46":"Consciousness, at its simplest, is awareness of internal and external existence. However, its nature has led to millennia of analyses, explanations, and debate by philosophers, scientists, and theologians. Opinions differ about what exactly needs to be studied or even considered consciousness. In some explanations, it is synonymous with the mind, and at other times, an aspect of it. In the past, it was one's \"inner life\", the world of introspection, of private thought, imagination, and volition. Today, it often includes any kind of cognition, experience, feeling or perception. It may be awareness, awareness of awareness, metacognition, or self-awareness either continuously changing or not. The disparate range of research, notions and speculations raises a curiosity about whether the right questions are being asked.\nExamples of the range of descriptions, definitions or explanations are: ordered distinction between self and environment, simple wakefulness, one's sense of selfhood or soul explored by \"looking within\"; being a metaphorical \"stream\" of contents, or being a mental state, mental event, or mental process of the brain.\n\nEtymology\nThe words \"conscious\" and \"consciousness\" in the English language date to the 17th century, and the first recorded use of \"conscious\" as a simple adjective was applied figuratively to inanimate objects (\"the conscious Groves\", 1643).:\u200a175\u200a It derived from the Latin conscius (con- \"together\" and scio \"to know\") which meant \"knowing with\" or \"having joint or common knowledge with another\", especially as in sharing a secret. Thomas Hobbes in Leviathan (1651) wrote: \"Where two, or more men, know of one and the same fact, they are said to be Conscious of it one to another.\" There were also many occurrences in Latin writings of the phrase conscius sibi, which translates literally as \"knowing with oneself\", or in other words \"sharing knowledge with oneself about something\". This phrase has the figurative sense of \"knowing that one knows\", which is something like the modern English word \"conscious\", but it was rendered into English as \"conscious to oneself\" or \"conscious unto oneself\". For example, Archbishop Ussher wrote in 1613 of \"being so conscious unto myself of my great weakness\".\nThe Latin conscientia, literally 'knowledge-with', first appears in Roman juridical texts by writers such as Cicero. It means a kind of shared knowledge with moral value, specifically what a witness knows of someone else's deeds. Although Ren\u00e9 Descartes (1596\u20131650), writing in Latin, is generally taken to be the first philosopher to use conscientia in a way less like the traditional meaning and more like the way modern English speakers would use \"conscience\", his meaning is nowhere defined. In Search after Truth (Regul\u00e6 ad directionem ingenii ut et inquisitio veritatis per lumen naturale, Amsterdam 1701) he wrote the word with a gloss: conscienti\u00e2, vel interno testimonio (translatable as \"conscience, or internal testimony\"). It might mean the knowledge of the value of one\u2019s own thoughts.\n\nThe origin of the modern concept of consciousness is often attributed to John Locke who defined the word in his Essay Concerning Human Understanding, published in 1690, as \"the perception of what passes in a man's own mind\". The essay strongly influenced 18th-century British philosophy, and Locke's definition appeared in Samuel Johnson's celebrated Dictionary (1755).\nThe French term conscience is defined roughly like English \"consciousness\" in the 1753 volume of Diderot and d'Alembert's Encyclop\u00e9die as \"the opinion or internal feeling that we ourselves have from what we do\".\n\nProblem of definition\nScholars are divided  as to whether Aristotle had a concept of consciousness. He does not use any single word or terminology that is clearly similar to the phenomenon or concept defined by John Locke. Victor Caston contends that Aristotle did have a concept more clearly similar to perception.\nModern dictionary definitions of the word consciousness evolved over several centuries and reflect a range of seemingly related meanings, with some differences that have been controversial, such as the distinction between inward awareness and perception of the physical world, or the distinction between conscious and unconscious, or the notion of a mental entity or mental activity that is not physical.\nThe common-usage definitions of consciousness in Webster's Third New International Dictionary (1966) are as follows: \n\nawareness or perception of an inward psychological or spiritual fact; intuitively perceived knowledge of something in one's inner self\ninward awareness of an external object, state, or fact\nconcerned awareness; INTEREST, CONCERN\u2014often used with an attributive noun [e.g. class consciousness]\nthe state or activity that is characterized by sensation, emotion, volition, or thought; mind in the broadest possible sense; something in nature that is distinguished from the physical\nthe totality in psychology of sensations, perceptions, ideas, attitudes, and feelings of which an individual or a group is aware at any given time or within a particular time span\u2014compare STREAM OF CONSCIOUSNESS\nwaking life (as that to which one returns after sleep, trance, fever) wherein all one's mental powers have returned . . .\nthe part of mental life or psychic content in psychoanalysis that is immediately available to the ego\u2014compare PRECONSCIOUS, UNCONSCIOUS\nThe Cambridge English Dictionary defines consciousness as \"the state of understanding and realizing something\".\nThe Oxford Living Dictionary defines consciousness as \"[t]he state of being aware of and responsive to one's surroundings\", \"[a] person's awareness or perception of something\", and \"[t]he fact of awareness by the mind of itself and the world\".\nPhilosophers have attempted to clarify technical distinctions by using a jargon of their own. The corresponding entry in the Routledge Encyclopedia of Philosophy (1998) reads:\n\nConsciousness\nPhilosophers have used the term consciousness for four main topics: knowledge in general, intentionality, introspection (and the knowledge it specifically generates) and phenomenal experience... Something within one's mind is 'introspectively conscious' just in case one introspects it (or is poised to do so). Introspection is often thought to deliver one's primary knowledge of one's mental life. An experience or other mental entity is 'phenomenally conscious' just in case there is 'something it is like' for one to have it. The clearest examples are: perceptual experience, such as tastings and seeings; bodily-sensational experiences, such as those of pains, tickles and itches; imaginative experiences, such as those of one's own actions or perceptions; and streams of thought, as in the experience of thinking 'in words' or 'in images'. Introspection and phenomenality seem independent, or dissociable, although this is controversial.\n\nTraditional metaphors for mind\nDuring the early 19th century, the emerging field of geology inspired a popular metaphor that the mind likewise had hidden layers \"which recorded the past of the individual\".:\u200a3\u200a By 1875, most psychologists believed that \"consciousness was but a small part of mental life\",:\u200a3\u200a and this idea underlie the goal of Freudian therapy, to expose the unconscious layer of the mind.\nOther metaphors from various sciences inspired other analyses of the mind, for example: Johann Friedrich Herbart described ideas as being attracted and repulsed like magnets; John Stuart Mill developed the idea of \"mental chemistry\" and \"mental compounds\", and Edward B. Titchener sought the \"structure\" of the mind by analyzing its \"elements\". The abstract idea of states of consciousness mirrored the concept of states of matter. \nIn 1892, William James noted that the \"ambiguous word 'content' has been recently invented instead of 'object'\" and that the metaphor of mind as a container seemed to minimize the dualistic problem of how \"states of consciousness can know\" things, or objects;:\u200a465\u200a by 1899 psychologists were busily studying the \"contents of conscious experience by introspection and experiment\".:\u200a365\u200a Another popular metaphor was James's doctrine of the stream of consciousness, with continuity, fringes, and transitions.:\u200avii\u200a\nJames discussed the difficulties of describing and studying psychological phenomena, recognizing that commonly-used terminology was a necessary and acceptable starting point towards more precise, scientifically justified language. Prime examples were phrases like inner experience and personal consciousness:\n\nThe first and foremost concrete fact which every one will affirm to belong to his inner experience is the fact that consciousness of some sort goes on. 'States of mind' succeed each other in him. [...] But everyone knows what the terms mean [only] in a rough way; [...] When I say every 'state' or 'thought' is part of a personal consciousness, 'personal consciousness' is one of the terms in question. Its meaning we know so long as no one asks us to define it, but to give an accurate account of it is the most difficult of philosophic tasks. [...] The only states of consciousness that we naturally deal with are found in personal consciousnesses, minds, selves, concrete particular I's and you's. :\u200a152\u2013153\n\nFrom introspection to awareness\nPrior to the 20th century, philosophers treated the phenomenon of consciousness as the \"inner world [of] one's own mind\", and introspection was the mind \"attending to\" itself, an activity seemingly distinct from that of perceiving the 'outer world' and its physical phenomena. In 1892 William James noted the distinction along with doubts about the inward character of the mind:'Things' have been doubted, but thoughts and feelings have never been doubted. The outer world, but never the inner world, has been denied. Everyone assumes that we have direct introspective acquaintance with our thinking activity as such, with our consciousness as something inward and contrasted with the outer objects which it knows. Yet I must confess that for my part I cannot feel sure of this conclusion. [...] It seems as if consciousness as an inner activity were rather a postulate than a sensibly given fact...:\u200a467\u200a\nBy the 1960s, for many philosophers and psychologists who talked about consciousness, the word no longer meant the 'inner world' but an indefinite, large category called awareness, as in the following example:\n\nIt is difficult for modern Western man to grasp that the Greeks really had no concept of consciousness in that they did not class together phenomena as varied as problem solving, remembering, imagining, perceiving, feeling pain, dreaming, and acting on the grounds that all these are manifestations of being aware or being conscious.:\u200a4\u200a\nMany philosophers and scientists have been unhappy about the difficulty of producing a definition that does not involve circularity or fuzziness. In The Macmillan Dictionary of Psychology (1989 edition), Stuart Sutherland emphasized external awareness, and expressed a skeptical attitude more than a definition:\n\nConsciousness\u2014The having of perceptions, thoughts, and feelings; awareness. The term is impossible to define except in terms that are unintelligible without a grasp of what consciousness means. Many fall into the trap of equating consciousness with self-consciousness\u2014to be conscious it is only necessary to be aware of the external world. Consciousness is a fascinating but elusive phenomenon: it is impossible to specify what it is, what it does, or why it has evolved. Nothing worth reading has been written on it.\nUsing 'awareness', however, as a definition or synonym of consciousness is not a simple matter:\n\nIf awareness of the environment . . . is the criterion of consciousness, then even the protozoans are conscious. If awareness of awareness is required, then it is doubtful whether the great apes and human infants are conscious.\n\nInfluence on research\nMany philosophers have argued that consciousness is a unitary concept that is understood by the majority of people despite the difficulty philosophers have had defining it. Max Velmans proposed that the \"everyday understanding of consciousness\" uncontroversially \"refers to experience itself rather than any particular thing that we observe or experience\" and he added that consciousness \"is [therefore] exemplified by all the things that we observe or experience\",:\u200a4\u200a whether thoughts, feelings, or perceptions. Velmans noted however, as of 2009, that there was a deep level of \"confusion and internal division\" among experts about the phenomenon of consciousness, because researchers lacked \"a sufficiently well-specified use of the term...to agree that they are investigating the same thing\".:\u200a3\u200a He argued additionally that \"pre-existing theoretical commitments\" to competing explanations of consciousness might be a source of bias.\nWithin the \"modern consciousness studies\" community the technical phrase 'phenomenal consciousness' is a common synonym for all forms of awareness, or simply 'experience',:\u200a4\u200a without differentiating between inner and outer, or between higher and lower types. With advances in brain research, \"the presence or absence of experienced phenomena\":\u200a3\u200a of any kind underlies the work of those neuroscientists who seek \"to analyze the precise relation of conscious phenomenology to its associated information processing\" in the brain.:\u200a10\u200a This neuroscientific goal is to find the \"neural correlates of consciousness\" (NCC). One criticism of this goal is that it begins with a theoretical commitment to the neurological origin of all \"experienced phenomena\" whether inner or outer. Also, the fact that the easiest 'content of consciousness' to be so analyzed is \"the experienced three-dimensional world (the phenomenal world) beyond the body surface\":\u200a4\u200a invites another criticism, that most consciousness research since the 1990s, perhaps because of bias, has focused on processes of external perception.\nFrom a history of psychology perspective, Julian Jaynes rejected popular but \"superficial views of consciousness\":\u200a447\u200a especially those which equate it with \"that vaguest of terms, experience\".:\u200a8\u200a In 1976 he insisted that if not for introspection, which for decades had been ignored or taken for granted rather than explained, there could be no \"conception of what consciousness is\":\u200a18\u200a and in 1990, he reaffirmed the traditional idea of the phenomenon called 'consciousness', writing that \"its denotative definition is, as it was for Descartes, Locke, and Hume, what is introspectable\".:\u200a450\u200a Jaynes saw consciousness as an important but small part of human mentality, and he asserted: \"there can be no progress in the science of consciousness until ... what is introspectable [is] sharply distinguished\":\u200a447\u200a from the unconscious processes of cognition such as perception, reactive awareness and attention, and automatic forms of learning, problem-solving, and decision-making.:\u200a21-47\u200a\nThe cognitive science point of view\u2014with an inter-disciplinary perspective involving fields such as psychology, linguistics and anthropology\u2014requires no agreed definition of \"consciousness\" but studies the interaction of many processes besides perception. For some researchers, consciousness is linked to some kind of \"selfhood\", for example to certain pragmatic issues such as the feeling of agency and the effects of regret and action on experience of one's own body or social identity. Similarly Daniel Kahneman, who focused on systematic errors in perception, memory and decision-making, has differentiated between two kinds of mental processes, or cognitive \"systems\": the \"fast\" activities that are primary, automatic and \"cannot be turned off\",:\u200a22\u200a and the \"slow\", deliberate, effortful activities of a secondary system \"often associated with the subjective experience of agency, choice, and concentration\".:\u200a13\u200a Kahneman's two systems have been described as \"roughly corresponding to unconscious and conscious processes\".:\u200a8\u200a The two systems can interact, for example in sharing the control of attention.:\u200a22\u200a While System 1 can be impulsive, \"System 2 is in charge of self-control\",:\u200a26\u200a and \"When we think of ourselves, we identify with System 2, the conscious, reasoning self that has beliefs, makes choices, and decides what to think about and what to do.\":\u200a21\u200a\nSome have argued that we should eliminate the concept from our understanding of the mind, a position known as consciousness semanticism.\nIn medicine, a \"level of consciousness\" terminology is used to describe a patient's arousal and responsiveness, which can be seen as a continuum of states ranging from full alertness and comprehension, through disorientation, delirium, loss of meaningful communication, and finally loss of movement in response to painful stimuli. Issues of practical concern include how the level of consciousness can be assessed in severely ill, comatose, or anesthetized people, and how to treat conditions in which consciousness is impaired or disrupted. The degree or level of consciousness is measured by standardized behavior observation scales such as the Glasgow Coma Scale.\n\nPhilosophy of mind\nMost writers on the philosophy of consciousness have been concerned with defending a particular point of view, and have organized their material accordingly. For surveys, the most common approach is to follow a historical path by associating stances with the philosophers who are most strongly associated with them, for example, Descartes, Locke, Kant, etc. An alternative is to organize philosophical stances according to basic issues.\n\nCoherence of the concept\nPhilosophers differ from non-philosophers in their intuitions about what consciousness is. While most people have a strong intuition for the existence of what they refer to as consciousness, skeptics argue that this intuition is too narrow, either because the concept of consciousness is embedded in our intuitions, or because we all  are illusions. Gilbert Ryle, for example, argued that traditional understanding of consciousness depends on a Cartesian dualist outlook that improperly distinguishes between mind and body, or between mind and world. He proposed that we speak not of minds, bodies, and the world, but of entities, or identities, acting in the world. Thus, by speaking of \"consciousness\" we end up leading ourselves by thinking that there is any sort of thing as consciousness separated from behavioral and linguistic understandings.\n\nTypes\nNed Block argued that discussions on consciousness often failed to properly distinguish phenomenal (P-consciousness) from access (A-consciousness), though these terms had been used before Block. P-consciousness, according to Block, is raw experience: it is moving, colored forms, sounds, sensations, emotions and feelings with our bodies and responses at the center. These experiences, considered independently of any impact on behavior, are called qualia. A-consciousness, on the other hand, is the phenomenon whereby information in our minds is accessible for verbal report, reasoning, and the control of behavior. So, when we perceive, information about what we perceive is access conscious; when we introspect, information about our thoughts is access conscious; when we remember, information about the past is access conscious, and so on. Although some philosophers, such as Daniel Dennett, have disputed the validity of this distinction, others have broadly accepted it. David Chalmers has argued that A-consciousness can in principle be understood in mechanistic terms, but that understanding P-consciousness is much more challenging: he calls this the hard problem of consciousness.\nSome philosophers believe that Block's two types of consciousness are not the end of the story. William Lycan, for example, argued in his book Consciousness and Experience that at least eight clearly distinct types of consciousness can be identified (organism consciousness; control consciousness; consciousness of; state\/event consciousness; reportability; introspective consciousness; subjective consciousness; self-consciousness)\u2014and that even this list omits several more obscure forms.\nThere is also debate over whether or not A-consciousness and P-consciousness always coexist or if they can exist separately. Although P-consciousness without A-consciousness is more widely accepted, there have been some hypothetical examples of A without P. Block, for instance, suggests the case of a \"zombie\" that is computationally identical to a person but without any subjectivity. However, he remains somewhat skeptical concluding \"I don't know whether there are any actual cases of A-consciousness without P-consciousness, but I hope I have illustrated their conceptual possibility.\"\n\nDistinguishing consciousness from its contents\nSam Harris observes: \"At the level of your experience, you are not a body of cells, organelles, and atoms; you are consciousness and its ever-changing contents\". Seen in this way, consciousness is a subjectively experienced, ever-present field in which things (the contents of consciousness) come and go.\nChristopher Tricker argues that this field of consciousness is symbolized by the mythical bird that opens the Daoist classic the Zhuangzi. This bird's name is Of a Flock (peng \u9d6c), yet its back is countless thousands of miles across and its wings are like clouds arcing across the heavens. \"Like Of a Flock, whose wings arc across the heavens, the wings of your consciousness span to the horizon. At the same time, the wings of every other being's consciousness span to the horizon. You are of a flock, one bird among kin.\"\n\nMind\u2013body problem\nMental processes (such as consciousness) and physical processes (such as brain events) seem to be correlated, however the specific nature of the connection is unknown.\nThe first influential philosopher to discuss this question specifically was Descartes, and the answer he gave is known as mind\u2013body dualism. Descartes proposed that consciousness resides within an immaterial domain he called res cogitans (the realm of thought), in contrast to the domain of material things, which he called res extensa (the realm of extension). He suggested that the interaction between these two domains occurs inside the brain, perhaps in a small midline structure called the pineal gland.\nAlthough it is widely accepted that Descartes explained the problem cogently, few later philosophers have been happy with his solution, and his ideas about the pineal gland have especially been ridiculed. However, no alternative solution has gained general acceptance. Proposed solutions can be divided broadly into two categories: dualist solutions that maintain Descartes's rigid distinction between the realm of consciousness and the realm of matter but give different answers for how the two realms relate to each other; and monist solutions that maintain that there is really only one realm of being, of which consciousness and matter are both aspects. Each of these categories itself contains numerous variants. The two main types of dualism are substance dualism (which holds that the mind is formed of a distinct type of substance not governed by the laws of physics), and property dualism (which holds that the laws of physics are universally valid but cannot be used to explain the mind). The three main types of monism are physicalism (which holds that the mind consists of matter organized in a particular way), idealism (which holds that only thought or experience truly exists, and matter is merely an illusion), and neutral monism (which holds that both mind and matter are aspects of a distinct essence that is itself identical to neither of them). There are also, however, a large number of idiosyncratic theories that cannot cleanly be assigned to any of these schools of thought.\nSince the dawn of Newtonian science with its vision of simple mechanical principles governing the entire universe, some philosophers have been tempted by the idea that consciousness could be explained in purely physical terms. The first influential writer to propose such an idea explicitly was Julien Offray de La Mettrie, in his book Man a Machine (L'homme machine). His arguments, however, were very abstract. The most influential modern physical theories of consciousness are based on psychology and neuroscience. Theories proposed by neuroscientists such as Gerald Edelman and Antonio Damasio, and by philosophers such as Daniel Dennett, seek to explain consciousness in terms of neural events occurring within the brain. Many other neuroscientists, such as Christof Koch, have explored the neural basis of consciousness without attempting to frame all-encompassing global theories. At the same time, computer scientists working in the field of artificial intelligence have pursued the goal of creating digital computer programs that can simulate or embody consciousness.\nA few theoretical physicists have argued that classical physics is intrinsically incapable of explaining the holistic aspects of consciousness, but that quantum theory may provide the missing ingredients. Several theorists have therefore proposed quantum mind (QM) theories of consciousness. Notable theories falling into this category include the holonomic brain theory of Karl Pribram and David Bohm, and the Orch-OR theory formulated by Stuart Hameroff and Roger Penrose. Some of these QM theories offer descriptions of phenomenal consciousness, as well as QM interpretations of access consciousness. None of the quantum mechanical theories have been confirmed by experiment. Recent publications by G. Guerreshi, J. Cia, S. Popescu, and H. Briegel could falsify proposals such as those of Hameroff, which rely on quantum entanglement in protein. At the present time many scientists and philosophers consider the arguments for an important role of quantum phenomena to be unconvincing. Empirical evidence is against the notion of quantum consciousness, an experiment about wave function collapse led by Catalina Curceanu in 2022 suggests that quantum consciousness, as suggested by Roger Penrose and Stuart Hameroff, is highly implausible.\nApart from the general question of the \"hard problem\" of consciousness (which is, roughly speaking, the question of how mental experience can arise from a physical basis), a more specialized question is how to square the subjective notion that we are in control of our decisions (at least in some small measure) with the customary view of causality that subsequent events are caused by prior events. The topic of free will is the philosophical and scientific examination of this conundrum.\n\nProblem of other minds\nMany philosophers consider experience to be the essence of consciousness, and believe that experience can only fully be known from the inside, subjectively. The problem of other minds is a philosophical problem traditionally stated as the following epistemological question: Given that I can only observe the behavior of others, how can I know that others have minds? The problem of other minds is particularly acute for people who believe in the possibility of philosophical zombies, that is, people who think it is possible in principle to have an entity that is physically indistinguishable from a human being and behaves like a human being in every way but nevertheless lacks consciousness. Related issues have also been studied extensively by Greg Littmann of the University of Illinois, and by Colin Allen (a professor at the University of Pittsburgh) regarding the literature and research studying artificial intelligence in androids.\nThe most commonly given answer is that we attribute consciousness to other people because we see that they resemble us in appearance and behavior; we reason that if they look like us and act like us, they must be like us in other ways, including having experiences of the sort that we do. There are, however, a variety of problems with that explanation. For one thing, it seems to violate the principle of parsimony, by postulating an invisible entity that is not necessary to explain what we observe. Some philosophers, such as Daniel Dennett in a research paper titled \"The Unimagined Preposterousness of Zombies\", argue that people who give this explanation do not really understand what they are saying. More broadly, philosophers who do not accept the possibility of zombies generally believe that consciousness is reflected in behavior (including verbal behavior), and that we attribute consciousness on the basis of behavior. A more straightforward way of saying this is that we attribute experiences to people because of what they can do, including the fact that they can tell us about their experiences.\n\nScientific study\nFor many decades, consciousness as a research topic was avoided by the majority of mainstream scientists, because of a general feeling that a phenomenon defined in subjective terms could not properly be studied using objective experimental methods. In 1975 George Mandler published an influential psychological study which distinguished between slow, serial, and limited conscious processes and fast, parallel and extensive unconscious ones. The Science and Religion Forum 1984 annual conference, 'From Artificial Intelligence to Human Consciousness' identified the nature of consciousness as a matter for investigation; Donald Michie was a keynote speaker. Starting in the 1980s, an expanding community of neuroscientists and psychologists have associated themselves with a field called Consciousness Studies, giving rise to a stream of experimental work published in books, journals such as Consciousness and Cognition, Frontiers in Consciousness Research, Psyche, and the Journal of Consciousness Studies, along with regular conferences organized by groups such as the Association for the Scientific Study of Consciousness and the Society for Consciousness Studies.\nModern medical and psychological investigations into consciousness are based on psychological experiments (including, for example, the investigation of priming effects using subliminal stimuli), and on case studies of alterations in consciousness produced by trauma, illness, or drugs. Broadly viewed, scientific approaches are based on two core concepts. The first identifies the content of consciousness with the experiences that are reported by human subjects; the second makes use of the concept of consciousness that has been developed by neurologists and other medical professionals who deal with patients whose behavior is impaired. In either case, the ultimate goals are to develop techniques for assessing consciousness objectively in humans as well as other animals, and to understand the neural and psychological mechanisms that underlie it.\n\nMeasurement\nExperimental research on consciousness presents special difficulties, due to the lack of a universally accepted operational definition. In the majority of experiments that are specifically about consciousness, the subjects are human, and the criterion used is verbal report: in other words, subjects are asked to describe their experiences, and their descriptions are treated as observations of the contents of consciousness. For example, subjects who stare continuously at a Necker cube usually report that they experience it \"flipping\" between two 3D configurations, even though the stimulus itself remains the same. The objective is to understand the relationship between the conscious awareness of stimuli (as indicated by verbal report) and the effects the stimuli have on brain activity and behavior. In several paradigms, such as the technique of response priming, the behavior of subjects is clearly influenced by stimuli for which they report no awareness, and suitable experimental manipulations can lead to increasing priming effects despite decreasing prime identification (double dissociation).\nVerbal report is widely considered to be the most reliable indicator of consciousness, but it raises a number of issues. For one thing, if verbal reports are treated as observations, akin to observations in other branches of science, then the possibility arises that they may contain errors\u2014but it is difficult to make sense of the idea that subjects could be wrong about their own experiences, and even more difficult to see how such an error could be detected. Daniel Dennett has argued for an approach he calls heterophenomenology, which means treating verbal reports as stories that may or may not be true, but his ideas about how to do this have not been widely adopted. Another issue with verbal report as a criterion is that it restricts the field of study to humans who have language: this approach cannot be used to study consciousness in other species, pre-linguistic children, or people with types of brain damage that impair language. As a third issue, philosophers who dispute the validity of the Turing test may feel that it is possible, at least in principle, for verbal report to be dissociated from consciousness entirely: a philosophical zombie may give detailed verbal reports of awareness in the absence of any genuine awareness.\nAlthough verbal report is in practice the \"gold standard\" for ascribing consciousness, it is not the only possible criterion. In medicine, consciousness is assessed as a combination of verbal behavior, arousal, brain activity, and purposeful movement. The last three of these can be used as indicators of consciousness when verbal behavior is absent. The scientific literature regarding the neural bases of arousal and purposeful movement is very extensive. Their reliability as indicators of consciousness is disputed, however, due to numerous studies showing that alert human subjects can be induced to behave purposefully in a variety of ways in spite of reporting a complete lack of awareness. Studies related to the neuroscience of free will have also shown that the influence consciousness has on decision-making is not always straight-forward.\nAnother approach applies specifically to the study of self-awareness, that is, the ability to distinguish oneself from others. In the 1970s Gordon Gallup developed an operational test for self-awareness, known as the mirror test. The test examines whether animals are able to differentiate between seeing themselves in a mirror versus seeing other animals. The classic example involves placing a spot of coloring on the skin or fur near the individual's forehead and seeing if they attempt to remove it or at least touch the spot, thus indicating that they recognize that the individual they are seeing in the mirror is themselves. Humans (older than 18 months) and other great apes, bottlenose dolphins, orcas, pigeons, European magpies, and elephants have all been observed to pass this test.\n\nNeural correlates\nA major part of the scientific literature on consciousness consists of studies that examine the relationship between the experiences reported by subjects and the activity that simultaneously takes place in their brains\u2014that is, studies of the neural correlates of consciousness. The hope is to find that activity in a particular part of the brain, or a particular pattern of global brain activity, which will be strongly predictive of conscious awareness. Several brain imaging techniques, such as EEG and fMRI, have been used for physical measures of brain activity in these studies.\nAnother idea that has drawn attention for several decades is that consciousness is associated with high-frequency (gamma band) oscillations in brain activity. This idea arose from proposals in the 1980s, by Christof von der Malsburg and Wolf Singer, that gamma oscillations could solve the so-called binding problem, by linking information represented in different parts of the brain into a unified experience. Rodolfo Llin\u00e1s, for example, proposed that consciousness results from recurrent thalamo-cortical resonance where the specific thalamocortical systems (content) and the non-specific (centromedial thalamus) thalamocortical systems (context) interact in the gamma band frequency via synchronous oscillations.\nA number of studies have shown that activity in primary sensory areas of the brain is not sufficient to produce consciousness: it is possible for subjects to report a lack of awareness even when areas such as the primary visual cortex (V1) show clear electrical responses to a stimulus. Higher brain areas are seen as more promising, especially the prefrontal cortex, which is involved in a range of higher cognitive functions collectively known as executive functions. There is substantial evidence that a \"top-down\" flow of neural activity (i.e., activity propagating from the frontal cortex to sensory areas) is more predictive of conscious awareness than a \"bottom-up\" flow of activity. The prefrontal cortex is not the only candidate area, however: studies by Nikos Logothetis and his colleagues have shown, for example, that visually responsive neurons in parts of the temporal lobe reflect the visual perception in the situation when conflicting visual images are presented to different eyes (i.e., bistable percepts during binocular rivalry). Furthermore, top-down feedback from higher to lower visual brain areas may be weaker or absent in the peripheral visual field, as suggested by some experimental data and theoretical arguments; nevertheless humans can perceive visual inputs in the peripheral visual field arising from bottom-up V1 neural activities. Meanwhile, bottom-up V1 activities for the central visual fields can be vetoed, and thus made invisible to perception, by the top-down feedback, when these bottom-up signals are inconsistent with the brain's internal model of the visual world.\nModulation of neural responses may correlate with phenomenal experiences. In contrast to the raw electrical responses that do not correlate with consciousness, the modulation of these responses by other stimuli correlates surprisingly well with an important aspect of consciousness: namely with the phenomenal experience of stimulus intensity (brightness, contrast). In the research group of Danko Nikoli\u0107 it has been shown that some of the changes in the subjectively perceived brightness correlated with the modulation of firing rates while others correlated with the modulation of neural synchrony. An fMRI investigation suggested that these findings were strictly limited to the primary visual areas. This indicates that, in the primary visual areas, changes in firing rates and synchrony can be considered as neural correlates of qualia\u2014at least for some type of qualia.\nIn 2013, the perturbational complexity index (PCI) was proposed, a measure of the algorithmic complexity of the electrophysiological response of the cortex to transcranial magnetic stimulation. This measure was shown to be higher in individuals that are awake, in REM sleep or in a locked-in state than in those who are in deep sleep or in a vegetative state, making it potentially useful as a quantitative assessment of consciousness states.\nAssuming that not only humans but even some non-mammalian species are conscious, a number of evolutionary approaches to the problem of neural correlates of consciousness open up. For example, assuming that birds are conscious\u2014a common assumption among neuroscientists and ethologists due to the extensive cognitive repertoire of birds\u2014there are comparative neuroanatomical ways to validate some of the principal, currently competing, mammalian consciousness\u2013brain theories. The rationale for such a comparative study is that the avian brain deviates structurally from the mammalian brain. So how similar are they? What homologs can be identified? The general conclusion from the study by Butler, et al., is that some of the major theories for the mammalian brain also appear to be valid for the avian brain. The structures assumed to be critical for consciousness in mammalian brains have homologous counterparts in avian brains. Thus the main portions of the theories of Crick and Koch, Edelman and Tononi, and Cotterill  seem to be compatible with the assumption that birds are conscious. Edelman also differentiates between what he calls primary consciousness (which is a trait shared by humans and non-human animals) and higher-order consciousness as it appears in humans alone along with human language capacity. Certain aspects of the three theories, however, seem less easy to apply to the hypothesis of avian consciousness. For instance, the suggestion by Crick and Koch that layer 5 neurons of the mammalian brain have a special role, seems difficult to apply to the avian brain, since the avian homologs have a different morphology. Likewise, the theory of Eccles seems incompatible, since a structural homolog\/analogue to the dendron has not been found in avian brains. The assumption of an avian consciousness also brings the reptilian brain into focus. The reason is the structural continuity between avian and reptilian brains, meaning that the phylogenetic origin of consciousness may be earlier than suggested by many leading neuroscientists.\nJoaquin Fuster of UCLA has advocated the position of the importance of the prefrontal cortex in humans, along with the areas of Wernicke and Broca, as being of particular importance to the development of human language capacities neuro-anatomically necessary for the emergence of higher-order consciousness in humans.\nA study in 2016 looked at lesions in specific areas of the brainstem that were associated with coma and vegetative states. A small region of the rostral dorsolateral pontine tegmentum in the brainstem was suggested to drive consciousness through functional connectivity with two cortical regions, the left ventral anterior insular cortex, and the pregenual anterior cingulate cortex. These three regions may work together as a triad to maintain consciousness.\n\nModels\nA wide range of empirical theories of consciousness have been proposed. Adrian Doerig and colleagues list 13 notable theories, while Anil Seth and Tim Bayne list 22 notable theories.\nGlobal workspace theory (GWT) is a cognitive architecture and theory of consciousness proposed by the cognitive psychologist Bernard Baars in 1988. Baars explains the theory with the metaphor of a theater, with conscious processes represented by an illuminated stage. This theater integrates inputs from a variety of unconscious and otherwise autonomous networks in the brain and then broadcasts them to unconscious networks (represented in the metaphor by a broad, unlit \"audience\"). The theory has since been expanded upon by other scientists including cognitive neuroscientist Stanislas Dehaene and Lionel Naccache.\nIntegrated information theory (IIT) postulates that consciousness resides in the information being processed and arises once the information reaches a certain level of complexity. Additionally, IIT is one of the only leading theories of consciousness that attempts to create a 1:1 mapping between conscious states and precise, formal mathematical descriptions of those mental states. Proponents of this model suggest that it may provide a physical grounding for consciousness in neurons, as they provide the mechanism by which information is integrated.\nOrchestrated objective reduction (Orch OR) postulates that consciousness originates at the quantum level inside neurons. The mechanism is held to be a quantum process called objective reduction that is orchestrated by cellular structures called microtubules. However the details of the mechanism would go beyond current quantum theory.\nIn 2011, Graziano and Kastner proposed the \"attention schema\" theory of awareness. In that theory, specific cortical areas, notably in the superior temporal sulcus and the temporo-parietal junction, are used to build the construct of awareness and attribute it to other people. The same cortical machinery is also used to attribute awareness to oneself. Damage to these cortical regions can lead to deficits in consciousness such as hemispatial neglect. In the attention schema theory, the value of explaining the feature of awareness and attributing it to a person is to gain a useful predictive model of that person's attentional processing. Attention is a style of information processing in which a brain focuses its resources on a limited set of interrelated signals. Awareness, in this theory, is a useful, simplified schema that represents attentional states. To be aware of X is explained by constructing a model of one's attentional focus on X.\nThe entropic brain is a theory of conscious states informed by neuroimaging research with psychedelic drugs. The theory suggests that the brain in primary states such as rapid eye movement (REM) sleep, early psychosis and under the influence of psychedelic drugs, is in a disordered state; normal waking consciousness constrains some of this freedom and makes possible metacognitive functions such as internal self-administered reality testing and self-awareness. Criticism has included questioning whether the theory has been adequately tested.\nIn 2017, work by David Rudrauf and colleagues, including Karl Friston, applied the active inference paradigm to consciousness, a model of how sensory data is integrated with priors in a process of projective transformation. The authors argue that, while their model identifies a key relationship between computation and phenomenology, it does not completely solve the hard problem of consciousness or completely close the explanatory gap.\n\nBiological function and evolution\nThe emergence of consciousness during biological evolution remains a topic of ongoing scientific inquiry. The survival value of consciousness is still a matter of exploration and understanding. While consciousness appears to play a crucial role in human cognition, decision-making, and self-awareness, its adaptive significance across different species remains a subject of debate. \nSome people question whether consciousness has any survival value. Some argue that consciousness is a by-product of evolution. Thomas Henry Huxley for example defends in an essay titled \"On the Hypothesis that Animals are Automata, and its History\" an epiphenomenalist theory of consciousness, according to which consciousness is a causally inert effect of neural activity\u2014\"as the steam-whistle which accompanies the work of a locomotive engine is without influence upon its machinery\". To this William James objects in his essay Are We Automata? by stating an evolutionary argument for mind-brain interaction implying that if the preservation and development of consciousness in the biological evolution is a result of natural selection, it is plausible that consciousness has not only been influenced by neural processes, but has had a survival value itself; and it could only have had this if it had been efficacious. Karl Popper develops a similar evolutionary argument in the book The Self and Its Brain.\nOpinions are divided on when and how consciousness first arose. It has been argued that consciousness emerged (i) exclusively with the first humans, (ii) exclusively with the first mammals, (iii) independently in mammals and birds, or (iv) with the first reptiles. Other authors date the origins of consciousness to the first animals with nervous systems or early vertebrates in the Cambrian over 500 million years ago. Donald Griffin suggests in his book Animal Minds a gradual evolution of consciousness.\nRegarding the primary function of conscious processing, a recurring idea in recent theories is that phenomenal states somehow integrate neural activities and information-processing that would otherwise be independent. This has been called the integration consensus. Another example has been proposed by Gerald Edelman called dynamic core hypothesis which puts emphasis on reentrant connections that reciprocally link areas of the brain in a massively parallel manner. Edelman also stresses the importance of the evolutionary emergence of higher-order consciousness in humans from the historically older trait of primary consciousness which humans share with non-human animals (see Neural correlates section above). These theories of integrative function present solutions to two classic problems associated with consciousness: differentiation and unity. They show how our conscious experience can discriminate between a virtually unlimited number of different possible scenes and details (differentiation) because it integrates those details from our sensory systems, while the integrative nature of consciousness in this view easily explains how our experience can seem unified as one whole despite all of these individual parts. However, it remains unspecified which kinds of information are integrated in a conscious manner and which kinds can be integrated without consciousness. Nor is it explained what specific causal role conscious integration plays, nor why the same functionality cannot be achieved without consciousness. Obviously not all kinds of information are capable of being disseminated consciously (e.g., neural activity related to vegetative functions, reflexes, unconscious motor programs, low-level perceptual analyzes, etc.) and many kinds of information can be disseminated and combined with other kinds without consciousness, as in intersensory interactions such as the ventriloquism effect. Hence it remains unclear why any of it is conscious. For a review of the differences between conscious and unconscious integrations, see the article of Ezequiel Morsella.\nAs noted earlier, even among writers who consider consciousness to be well-defined, there is widespread dispute about which animals other than humans can be said to possess it. Edelman has described this distinction as that of humans possessing higher-order consciousness while sharing the trait of primary consciousness with non-human animals (see previous paragraph). Thus, any examination of the evolution of consciousness is faced with great difficulties. Nevertheless, some writers have argued that consciousness can be viewed from the standpoint of evolutionary biology as an adaptation in the sense of a trait that increases fitness. In his article \"Evolution of consciousness\", John Eccles argued that special anatomical and physical properties of the mammalian cerebral cortex gave rise to consciousness (\"[a] psychon ... linked to [a] dendron through quantum physics\"). Bernard Baars proposed that once in place, this \"recursive\" circuitry may have provided a basis for the subsequent development of many of the functions that consciousness facilitates in higher organisms. Peter Carruthers has put forth one such potential adaptive advantage gained by conscious creatures by suggesting that consciousness allows an individual to make distinctions between appearance and reality. This ability would enable a creature to recognize the likelihood that their perceptions are deceiving them (e.g. that water in the distance may be a mirage) and behave accordingly, and it could also facilitate the manipulation of others by recognizing how things appear to them for both cooperative and devious ends.\nOther philosophers, however, have suggested that consciousness would not be necessary for any functional advantage in evolutionary processes. No one has given a causal explanation, they argue, of why it would not be possible for a functionally equivalent non-conscious organism (i.e., a philosophical zombie) to achieve the very same survival advantages as a conscious organism. If evolutionary processes are blind to the difference between function F being performed by conscious organism O and non-conscious organism O*, it is unclear what adaptive advantage consciousness could provide. As a result, an exaptive explanation of consciousness has gained favor with some theorists that posit consciousness did not evolve as an adaptation but was an exaptation arising as a consequence of other developments such as increases in brain size or cortical rearrangement. Consciousness in this sense has been compared to the blind spot in the retina where it is not an adaption of the retina, but instead just a by-product of the way the retinal axons were wired. Several scholars including Pinker, Chomsky, Edelman, and Luria have indicated the importance of the emergence of human language as an important regulative mechanism of learning and memory in the context of the development of higher-order consciousness (see Neural correlates section above).\n\nAltered states\nThere are some brain states in which consciousness seems to be absent, including dreamless sleep or coma. There are also a variety of circumstances that can change the relationship between the mind and the world in less drastic ways, producing what are known as altered states of consciousness. Some altered states occur naturally; others can be produced by drugs or brain damage. Altered states can be accompanied by changes in thinking, disturbances in the sense of time, feelings of loss of control, changes in emotional expression, alternations in body image and changes in meaning or significance.\nThe two most widely accepted altered states are sleep and dreaming. Although dream sleep and non-dream sleep appear very similar to an outside observer, each is associated with a distinct pattern of brain activity, metabolic activity, and eye movement; each is also associated with a distinct pattern of experience and cognition. During ordinary non-dream sleep, people who are awakened report only vague and sketchy thoughts, and their experiences do not cohere into a continuous narrative. During dream sleep, in contrast, people who are awakened report rich and detailed experiences in which events form a continuous progression, which may however be interrupted by bizarre or fantastic intrusions. Thought processes during the dream state frequently show a high level of irrationality. Both dream and non-dream states are associated with severe disruption of memory: it usually disappears in seconds during the non-dream state, and in minutes after awakening from a dream unless actively refreshed.\nResearch conducted on the effects of partial epileptic seizures on consciousness found that patients who have partial epileptic seizures experience altered states of consciousness. In partial epileptic seizures, consciousness is impaired or lost while some aspects of consciousness, often automated behaviors, remain intact. Studies found that when measuring the qualitative features during partial epileptic seizures, patients exhibited an increase in arousal and became absorbed in the experience of the seizure, followed by difficulty in focusing and shifting attention.\nA variety of psychoactive drugs, including alcohol, have notable effects on consciousness. These range from a simple dulling of awareness produced by sedatives, to increases in the intensity of sensory qualities produced by stimulants, cannabis, empathogens\u2013entactogens such as MDMA (\"Ecstasy\"), or most notably by the class of drugs known as psychedelics. LSD, mescaline, psilocybin, dimethyltryptamine, and others in this group can produce major distortions of perception, including hallucinations; some users even describe their drug-induced experiences as mystical or spiritual in quality. The brain mechanisms underlying these effects are not as well understood as those induced by use of alcohol, but there is substantial evidence that alterations in the brain system that uses the chemical neurotransmitter serotonin play an essential role.\nThere has been some research into physiological changes in yogis and people who practise various techniques of meditation. Some research with brain waves during meditation has reported differences between those corresponding to ordinary relaxation and those corresponding to meditation. It has been disputed, however, whether there is enough evidence to count these as physiologically distinct states of consciousness.\nThe most extensive study of the characteristics of altered states of consciousness was made by psychologist Charles Tart in the 1960s and 1970s. Tart analyzed a state of consciousness as made up of a number of component processes, including exteroception (sensing the external world); interoception (sensing the body); input-processing (seeing meaning); emotions; memory; time sense; sense of identity; evaluation and cognitive processing; motor output; and interaction with the environment. Each of these, in his view, could be altered in multiple ways by drugs or other manipulations. The components that Tart identified have not, however, been validated by empirical studies. Research in this area has not yet reached firm conclusions, but a recent questionnaire-based study identified eleven significant factors contributing to drug-induced states of consciousness: experience of unity; spiritual experience; blissful state; insightfulness; disembodiment; impaired control and cognition; anxiety; complex imagery; elementary imagery; audio-visual synesthesia; and changed meaning of percepts.\n\nMedical aspects\nThe medical approach to consciousness is scientifically oriented. It derives from a need to treat people whose brain function has been impaired as a result of disease, brain damage, toxins, or drugs. In medicine, conceptual distinctions are considered useful to the degree that they can help to guide treatments. The medical approach focuses mostly on the amount of consciousness a person has: in medicine, consciousness is assessed as a \"level\" ranging from coma and brain death at the low end, to full alertness and purposeful responsiveness at the high end.\nConsciousness is of concern to patients and physicians, especially neurologists and anesthesiologists. Patients may have disorders of consciousness or may need to be anesthetized for a surgical procedure. Physicians may perform consciousness-related interventions such as instructing the patient to sleep, administering general anesthesia, or inducing medical coma. Also, bioethicists may be concerned with the ethical implications of consciousness in medical cases of patients such as the Karen Ann Quinlan case, while neuroscientists may study patients with impaired consciousness in hopes of gaining information about how the brain works.\n\nAssessment\nIn medicine, consciousness is examined using a set of procedures known as neuropsychological assessment. There are two commonly used methods for assessing the level of consciousness of a patient: a simple procedure that requires minimal training, and a more complex procedure that requires substantial expertise. The simple procedure begins by asking whether the patient is able to move and react to physical stimuli. If so, the next question is whether the patient can respond in a meaningful way to questions and commands. If so, the patient is asked for name, current location, and current day and time. A patient who can answer all of these questions is said to be \"alert and oriented times four\" (sometimes denoted \"A&Ox4\" on a medical chart), and is usually considered fully conscious.\nThe more complex procedure is known as a neurological examination, and is usually carried out by a neurologist in a hospital setting. A formal neurological examination runs through a precisely delineated series of tests, beginning with tests for basic sensorimotor reflexes, and culminating with tests for sophisticated use of language. The outcome may be summarized using the Glasgow Coma Scale, which yields a number in the range 3\u201315, with a score of 3 to 8 indicating coma, and 15 indicating full consciousness. The Glasgow Coma Scale has three subscales, measuring the best motor response (ranging from \"no motor response\" to \"obeys commands\"), the best eye response (ranging from \"no eye opening\" to \"eyes opening spontaneously\") and the best verbal response (ranging from \"no verbal response\" to \"fully oriented\"). There is also a simpler pediatric version of the scale, for children too young to be able to use language.\nIn 2013, an experimental procedure was developed to measure degrees of consciousness, the procedure involving stimulating the brain with a magnetic pulse, measuring resulting waves of electrical activity, and developing a consciousness score based on the complexity of the brain activity.\n\nDisorders\nMedical conditions that inhibit consciousness are considered disorders of consciousness. This category generally includes minimally conscious state and persistent vegetative state, but sometimes also includes the less severe locked-in syndrome and more severe chronic coma. Differential diagnosis of these disorders is an active area of biomedical research. Finally, brain death results in possible irreversible disruption of consciousness. While other conditions may cause a moderate deterioration (e.g., dementia and delirium) or transient interruption (e.g., grand mal and petit mal seizures) of consciousness, they are not included in this category.\n\nMedical experts increasingly view anosognosia as a disorder of consciousness. Anosognosia is a Greek-derived term meaning \"unawareness of disease\". This is a condition in which patients are disabled in some way, most commonly as a result of a stroke, but either misunderstand the nature of the problem or deny that there is anything wrong with them. The most frequently occurring form is seen in people who have experienced a stroke damaging the parietal lobe in the right hemisphere of the brain, giving rise to a syndrome known as hemispatial neglect, characterized by an inability to direct action or attention toward objects located to the left with respect to their bodies. Patients with hemispatial neglect are often paralyzed on the left side of the body, but sometimes deny being unable to move. When questioned about the obvious problem, the patient may avoid giving a direct answer, or may give an explanation that does not make sense. Patients with hemispatial neglect may also fail to recognize paralyzed parts of their bodies: one frequently mentioned case is of a man who repeatedly tried to throw his own paralyzed right leg out of the bed he was lying in, and when asked what he was doing, complained that somebody had put a dead leg into the bed with him. An even more striking type of anosognosia is Anton\u2013Babinski syndrome, a rarely occurring condition in which patients become blind but claim to be able to see normally, and persist in this claim in spite of all evidence to the contrary.\n\nOutside human adults\nIn children\nOf the eight types of consciousness in the Lycan classification, some are detectable in utero and others develop years after birth. Psychologist and educator William Foulkes studied children's dreams and concluded that prior to the shift in cognitive maturation that humans experience during ages five to seven, children lack the Lockean consciousness that Lycan had labeled \"introspective consciousness\" and that Foulkes labels \"self-reflection\". In a 2020 paper, Katherine Nelson and Robyn Fivush use \"autobiographical consciousness\" to label essentially the same faculty, and agree with Foulkes on the timing of this faculty's acquisition. Nelson and Fivush contend that \"language is the tool by which humans create a new, uniquely human form of consciousness, namely, autobiographical consciousness.\" Julian Jaynes had staked out these positions decades earlier. Citing the developmental steps that lead the infant to autobiographical consciousness, Nelson and Fivush point to the acquisition of \"theory of mind\", calling theory of mind \"necessary for autobiographical consciousness\" and defining it as \"understanding differences between one's own mind and others' minds in terms of beliefs, desires, emotions and thoughts.\" They write, \"The hallmark of theory of mind, the understanding of false belief, occurs ... at five to six years of age.\"\n\nIn animals\nThe topic of animal consciousness is beset by a number of difficulties. It poses the problem of other minds in an especially severe form, because non-human animals, lacking the ability to express human language, cannot tell humans about their experiences. Also, it is difficult to reason objectively about the question, because a denial that an animal is conscious is often taken to imply that it does not feel, its life has no value, and that harming it is not morally wrong. Descartes, for example, has sometimes been blamed for mistreatment of animals due to the fact that he believed only humans have a non-physical mind. Most people have a strong intuition that some animals, such as cats and dogs, are conscious, while others, such as insects, are not; but the sources of this intuition are not obvious, and are often based on personal interactions with pets and other animals they have observed.\n\nPhilosophers who consider subjective experience the essence of consciousness also generally believe, as a correlate, that the existence and nature of animal consciousness can never rigorously be known. Thomas Nagel spelled out this point of view in an influential essay titled \"What Is it Like to Be a Bat?\". He said that an organism is conscious \"if and only if there is something that it is like to be that organism\u2014something it is like for the organism\"; and he argued that no matter how much we know about an animal's brain and behavior, we can never really put ourselves into the mind of the animal and experience its world in the way it does itself. Other thinkers, such as Douglas Hofstadter, dismiss this argument as incoherent. Several psychologists and ethologists have argued for the existence of animal consciousness by describing a range of behaviors that appear to show animals holding beliefs about things they cannot directly perceive\u2014Donald Griffin's 2001 book Animal Minds reviews a substantial portion of the evidence.\nOn July 7, 2012, eminent scientists from different branches of neuroscience gathered at the University of Cambridge to celebrate the Francis Crick Memorial Conference, which deals with consciousness in humans and pre-linguistic consciousness in nonhuman animals. After the conference, they signed in the presence of Stephen Hawking, the 'Cambridge Declaration on Consciousness', which summarizes the most important findings of the survey:\n\"We decided to reach a consensus and make a statement directed to the public that is not scientific. It's obvious to everyone in this room that animals have consciousness, but it is not obvious to the rest of the world. It is not obvious to the rest of the Western world or the Far East. It is not obvious to the society.\"\n\"Convergent evidence indicates that non-human animals ..., including all mammals and birds, and other creatures, ... have the necessary neural substrates of consciousness and the capacity to exhibit intentional behaviors.\"\n\nIn artificial intelligence\nThe idea of an artifact made conscious is an ancient theme of mythology, appearing for example in the Greek myth of Pygmalion, who carved a statue that was magically brought to life, and in medieval Jewish stories of the Golem, a magically animated homunculus built of clay. However, the possibility of actually constructing a conscious machine was probably first discussed by Ada Lovelace, in a set of notes written in 1842 about the Analytical Engine invented by Charles Babbage, a precursor (never built) to modern electronic computers. Lovelace was essentially dismissive of the idea that a machine such as the Analytical Engine could think in a humanlike way. She wrote:\n\nIt is desirable to guard against the possibility of exaggerated ideas that might arise as to the powers of the Analytical Engine. ... The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform. It can follow analysis; but it has no power of anticipating any analytical relations or truths. Its province is to assist us in making available what we are already acquainted with.\nOne of the most influential contributions to this question was an essay written in 1950 by pioneering computer scientist Alan Turing, titled Computing Machinery and Intelligence. Turing disavowed any interest in terminology, saying that even \"Can machines think?\" is too loaded with spurious connotations to be meaningful; but he proposed to replace all such questions with a specific operational test, which has become known as the Turing test. To pass the test, a computer must be able to imitate a human well enough to fool interrogators. In his essay Turing discussed a variety of possible objections, and presented a counterargument to each of them. The Turing test is commonly cited in discussions of artificial intelligence as a proposed criterion for machine consciousness; it has provoked a great deal of philosophical debate. For example, Daniel Dennett and Douglas Hofstadter argue that anything capable of passing the Turing test is necessarily conscious, while David Chalmers argues that a philosophical zombie could pass the test, yet fail to be conscious. A third group of scholars have argued that with technological growth once machines begin to display any substantial signs of human-like behavior then the dichotomy (of human consciousness compared to human-like consciousness) becomes pass\u00e9 and issues of machine autonomy begin to prevail even as observed in its nascent form within contemporary industry and technology. J\u00fcrgen Schmidhuber argues that consciousness is the result of compression. As an agent sees representation of itself recurring in the environment, the compression of this representation can be called consciousness.\n\nIn a lively exchange over what has come to be referred to as \"the Chinese room argument\", John Searle sought to refute the claim of proponents of what he calls \"strong artificial intelligence (AI)\" that a computer program can be conscious, though he does agree with advocates of \"weak AI\" that computer programs can be formatted to \"simulate\" conscious states. His own view is that consciousness has subjective, first-person causal powers by being essentially intentional due to the way human brains function biologically; conscious persons can perform computations, but consciousness is not inherently computational the way computer programs are. To make a Turing machine that speaks Chinese, Searle imagines a room with one monolingual English speaker (Searle himself, in fact), a book that designates a combination of Chinese symbols to be output paired with Chinese symbol input, and boxes filled with Chinese symbols. In this case, the English speaker is acting as a computer and the rulebook as a program. Searle argues that with such a machine, he would be able to process the inputs to outputs perfectly without having any understanding of Chinese, nor having any idea what the questions and answers could possibly mean. If the experiment were done in English, since Searle knows English, he would be able to take questions and give answers without any algorithms for English questions, and he would be effectively aware of what was being said and the purposes it might serve. Searle would pass the Turing test of answering the questions in both languages, but he is only conscious of what he is doing when he speaks English. Another way of putting the argument is to say that computer programs can pass the Turing test for processing the syntax of a language, but that the syntax cannot lead to semantic meaning in the way strong AI advocates hoped.\nIn the literature concerning artificial intelligence, Searle's essay has been second only to Turing's in the volume of debate it has generated. Searle himself was vague about what extra ingredients it would take to make a machine conscious: all he proposed was that what was needed was \"causal powers\" of the sort that the brain has and that computers lack. But other thinkers sympathetic to his basic argument have suggested that the necessary (though perhaps still not sufficient) extra conditions may include the ability to pass not just the verbal version of the Turing test, but the robotic version, which requires grounding the robot's words in the robot's sensorimotor capacity to categorize and interact with the things in the world that its words are about, Turing-indistinguishably from a real person. Turing-scale robotics is an empirical branch of research on embodied cognition and situated cognition.\nIn 2014, Victor Argonov has suggested a non-Turing test for machine consciousness based on a machine's ability to produce philosophical judgments. He argues that a deterministic machine must be regarded as conscious if it is able to produce judgments on all problematic properties of consciousness (such as qualia or binding) having no innate (preloaded) philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures' consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. A positive result proves that a machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be caused by lack of the machine's intellect, not by absence of consciousness.\n\nStream of consciousness\nWilliam James is usually credited with popularizing the idea that human consciousness flows like a stream, in his Principles of Psychology of 1890.\nAccording to James, the \"stream of thought\" is governed by five characteristics: \n\nEvery thought tends to be part of a personal consciousness.\nWithin each personal consciousness thought is always changing.\nWithin each personal consciousness thought is sensibly continuous.\nIt always appears to deal with objects independent of itself.\nIt is interested in some parts of these objects to the exclusion of others.\nA similar concept appears in Buddhist philosophy, expressed by the Sanskrit term Citta-sa\u1e43t\u0101na, which is usually translated as mindstream or \"mental continuum\". Buddhist teachings describe that consciousness manifests moment to moment as sense impressions and mental phenomena that are continuously changing. The teachings list six triggers that can result in the generation of different mental events. These triggers are input from the five senses (seeing, hearing, smelling, tasting or touch sensations), or a thought (relating to the past, present or the future) that happen to arise in the mind. The mental events generated as a result of these triggers are: feelings, perceptions and intentions\/behaviour. The moment-by-moment manifestation of the mind-stream is said to happen in every person all the time. It even happens in a scientist who analyzes various phenomena in the world, or analyzes the material body including the organ brain. The manifestation of the mindstream is also described as being influenced by physical laws, biological laws, psychological laws, volitional laws, and universal laws. The purpose of the Buddhist practice of mindfulness is to understand the inherent nature of the consciousness and its characteristics.\n\nNarrative form\nIn the West, the primary impact of the idea has been on literature rather than science: \"stream of consciousness as a narrative mode\" means writing in a way that attempts to portray the moment-to-moment thoughts and experiences of a character. This technique perhaps had its beginnings in the monologues of Shakespeare's plays and reached its fullest development in the novels of James Joyce and Virginia Woolf, although it has also been used by many other noted writers.\nHere, for example, is a passage from Joyce's Ulysses about the thoughts of Molly Bloom:\n\nYes because he never did a thing like that before as ask to get his breakfast in bed with a couple of eggs since the City Arms hotel when he used to be pretending to be laid up with a sick voice doing his highness to make himself interesting for that old faggot Mrs Riordan that he thought he had a great leg of and she never left us a farthing all for masses for herself and her soul greatest miser ever was actually afraid to lay out 4d for her methylated spirit telling me all her ailments she had too much old chat in her about politics and earthquakes and the end of the world let us have a bit of fun first God help the world if all the women were her sort down on bathingsuits and lownecks of course nobody wanted her to wear them I suppose she was pious because no man would look at her twice I hope Ill never be like her a wonder she didnt want us to cover our faces but she was a well-educated woman certainly and her gabby talk about Mr Riordan here and Mr Riordan there I suppose he was glad to get shut of her.\n\nSpiritual approaches\nTo most philosophers, the word \"consciousness\" connotes the relationship between the mind and the world. To writers on spiritual or religious topics, it frequently connotes the relationship between the mind and God, or the relationship between the mind and deeper truths that are thought to be more fundamental than the physical world.\nThe Canadian psychiatrist Richard Maurice Bucke, author of the 1901 book Cosmic Consciousness: A Study in the Evolution of the Human Mind, distinguished between three types of consciousness: 'Simple Consciousness', awareness of the body, possessed by many animals; 'Self Consciousness', awareness of being aware, possessed only by humans; and 'Cosmic Consciousness', awareness of the life and order of the universe, possessed only by humans who have attained \"intellectual enlightenment or illumination\".\nAnother thorough account of the spiritual approach is Ken Wilber's 1977 book The Spectrum of Consciousness, a comparison of western and eastern ways of thinking about the mind. Wilber described consciousness as a spectrum with ordinary awareness at one end, and more profound types of awareness at higher levels.\nOther examples include the various levels of spiritual consciousness presented by Prem Saran Satsangi and Stuart Hameroff.\n\nSee also\nNotes\nReferences\nFurther reading\nExternal links\n\n Media related to Consciousness at Wikimedia Commons\nLibrary resources in your library and in other libraries about Consciousness\n Consciousness Studies at Wikibooks\n Quotations related to Consciousness at Wikiquote\n The dictionary definition of Consciousness at Wiktionary","47":"Deafblindness is the condition of little or no useful hearing and little or no useful sight. Different degrees of vision loss and auditory loss occur within each individual. Because of this inherent diversity, each deafblind individual's needs regarding lifestyle, communication, education, and work need to be addressed based on their degree of dual-modality deprivation, to improve their ability to live independently. In 1994, an estimated 35,000\u201340,000 United States residents were medically deafblind. Laura Bridgman was the first American deafblind person known to become well educated.  Helen Keller was a well-known example of an educated deafblind individual. To further her lifelong mission to help the deafblind community to expand its horizons and gain opportunities, the Helen Keller National Center for Deaf-Blind Youths and Adults (also called the Helen Keller National Center or HKNC), with a residential training program in Sands Point, New York, was established in 1967 by an act of Congress.\nThe deafblind community has its own culture, comparable to those of the deaf community. Members of the deafblind community have diverse backgrounds but are united by similar experiences and a shared, homogeneous understanding of what it means to be deafblind. Some deafblind individuals view their condition as a part of their identity.\n\nEpidemiology\nThe medical condition of deafblindness occurs in different forms. For some, this condition might happen congenitally from birth as a result of genetic defect, for others it happens suddenly due to a form of illness or accident that results in a modality deprivation of either vision or hearing, or both. A person might be born deaf and become blind at a later stage in life, or vice versa. In any given case of deafblindness, many possible onsets and causes of this condition exist; some happen gradually, others happen unexpectedly and suddenly. The diagnosis of deafblindness could be medically classified into specific types based on one's symptoms and causes.\nThe two overarching types of deafblindness are congenital and acquired.:\u200a36\u201374\u200a\nCongenital deafblindness: the condition of deafblindness from birth\n\nPregnancy complexities:\u200a50\u201367\u200a\nEffects of alcohol\/drugs\nFetal alcohol syndrome\nA result of prematurity\nCauses from illness\/infection\nRubella\nAIDS\nCytomegalovirus\nCongenital syphilis\nToxoplasmosis\nGenetic conditions (evident from birth):\u200a37\u201349\u200a\nAnomalies\/syndromes (numerous genetic defects may contribute to one's medical condition of deafblindness, of which some of more well-known syndromes are listed)\nCHARGE syndrome\nCochleosaccular degeneration with progressive cataracts\nDown syndrome\nMarshall syndrome\nCongenital rubella syndrome\nStickler syndrome\nTrisomy 13\nAcquired deafblindness: condition of deafblindness developed later in life\n\nGenetic conditions (evident at a later stage in life):\u200a37\u201349\u200a\nUsher syndrome\nAlport syndrome\nAge-related loss of modality (vision or auditory or both)\nIllness, such as meningitis\nSomatic injuries:\u200a68\u201374\u200a\nBrain damage\/trauma\nStroke\nPermanent physical damage (relating to vision or hearing)\n\nCommunication\nDeafblind people communicate in many different ways as determined by the nature of their condition, the age of onset, and what resources are available to them. For example, someone who grew up deaf and experienced vision loss later in life is likely to use a sign language (in a visually modified or tactile form). Others who grew up blind and later became deaf are more likely to use a tactile mode of spoken\/written language. Methods of communication include:\n\nUse of residual hearing (speaking clearly, hearing aids, or cochlear implants) or sight (signing within a restricted visual field, writing with large print)\nTactile signing, sign language, or a manual alphabet such as the American Manual Alphabet or Deaf-blind Alphabet (also known as \"two-hand manual\") with tactile or visual modifications\nInterpreting services (such as sign-language interpreters or communication aides)\nCommunication devices such as Tellatouch or its computerized versions known as the TeleBraille and Screen Braille Communicator.\nTadoma, a tactile modality\nSquare script, a method of writing along tactile guidelines\nProtactile, a tactile language related to American Sign Language in the Francosign language family\nMultisensory methods have been used to help deafblind people enhance their communication skills. These can be taught to very young children with developmental delays (to help with pre-intentional communication), young people with learning difficulties, and older people, including those with dementia. One such process is Tacpac.\nDeafblind people often use the assistance of people known as support-service providers (SSPs), who help the deafblind with tasks such as routine errands, guiding the deafblind through unfamiliar environments, and facilitating communication between the deafblind person and another person.\n\nTechnology\nBraille equipment includes a variety of multipurpose devices, which enhance access to distance communication. Some can be used as stand-alone devices connected via Wi-Fi, while others are paired with a mobile device to provide tactile access to e-mail, text messaging, and other modern communication resources. To receive Braille equipment, an eligible consumer must be proficient in Braille and must have access to the Internet or cellular telephone service.\nTelebraille does not have a computer communications modem, but does have a TTY (TDD) modem. It was designed as a TTY for deaf-blind people and is also useful for face-to-face conversation. It has two components: The sighted component is a modified SuperCom TTY device. It has a qwerty keyboard and a single-line LED display. The display is regular size and is not particularly suited to people with low vision. The SuperCom TTY can be connected directly to the telephone line using a conventional telephone jack or the telephone receiver can be coupled to the SuperCom on a cradle on top of the device. Text flows past the display in a continuous stream, like tickertape. The SuperCom is connected to the Braille portion of the device by a cable that is about 2 ft (0.6 m) long. The Braille display is about 15 characters in width, although a knockout allows additional characters to be installed, at considerable additional cost. The Telebraille is able to communicate in ASCII mode, but is not compatible with conventional computer modems. There is what looks like a RS-232 socket on the back of the Braille component, but the instructions for the Telebraille state that this jack is for \"future use\" and that no computer devices should be attached to it.\nA graphic Braille display can be used in sensing graphic data such as maps, images, and text data that require multiline display capabilities such spreadsheets and equations. Graphic braille displays available in the market are DV-2 (from KGS), Hyperbraille, and TACTISPLAY Table\/Walk (from Tactisplay Corp.). For example, TACTISPLAY Table can show 120*100 resolution refreshable braille graphics on one page.\n\nIn popular culture\nThe play The Miracle Worker (1959), which was adapted into the film The Miracle Worker (1962), recounts Anne Sullivan's efforts to draw Helen Keller from her world of blindness and deafness.\nThe Who\u2019s album Tommy (1969) tells one continuous life story about a deafblind mute boy named Tommy through songs.\nThe Bollywood film Black (2005) featured Rani Mukerji as a deafblind character named Michelle McNally.\nThe film Marie's Story (2014) relates the childhood and education of Marie Heurtin (1885\u20131921), a deafblind woman.\nHaben Girma, the first deafblind individual to graduate from Harvard Law School, released an autobiography entitled Haben: The Deafblind Woman Who Conquered Harvard Law (2019).\nFeeling Through (2019) is an American short drama film directed by Doug Roland that was the first film ever to star a deafblind actor (Robert Tarango) in a lead role; it is about a teenager and a deafblind man. It was nominated for the 2021 Academy Award for Best Live Action Short Film.\nThe Persistence of Vision is a 1978 novella by John Varley describing the life of a blind-and-deaf community.\n\nSee also\nTactile signing\nTangible symbol systems\nTommy (rock opera)\nWhite cane (used by blind people to assist them in walking)\n\nReferences\nExternal links\n Media related to Deafblindness at Wikimedia Commons\nThe National Center On Deaf-Blindness Official informational website on deafblindness in United States.\nThe Helen Keller National Center for Deaf-Blind Youths and Adults Helen Keller Services website catering for the deaf-blind and blind communities.\nWorld Federation of the Deafblind Website for worldwide information concerning deafblindness.\nAble Australia Informational website on deafblindness in Australia.\n\"Haben Girma Homepage\" About Haben Girma, the first deafblind Harvard Law School graduate.\nDeafblind UK is a national charity in the UK supporting people with sight and hearing loss to live the lives they want.\nSense is a national charity in England, Wales and Northern Ireland for everyone who is deafblind, there to help people communicate and experience the world.","48":"Deafness has varying definitions in cultural and medical contexts. In medical contexts, the meaning of deafness is hearing loss that precludes a person from understanding spoken language, an audiological condition. In this context it is written with a lower case d. It later came to be used in a cultural context to refer to those who primarily communicate through sign language regardless of hearing ability, often capitalized as Deaf and referred to as \"big D Deaf\" in speech and sign. The two definitions overlap but are not identical, as hearing loss includes cases that are not severe enough to impact spoken language comprehension, while cultural Deafness includes hearing people who use sign language, such as children of deaf adults.\n\nMedical context\nIn a medical context, deafness is defined as a degree of hearing difference such that a person is unable to understand speech, even in the presence of amplification. In profound deafness, even the highest intensity sounds produced by an audiometer (an instrument used to measure hearing by producing pure tone sounds through a range of frequencies) may not be detected. In total deafness, no sounds at all, regardless of amplification or method of production, can be heard.\n\nNeurologically, language is processed in the same areas of the brain whether one is deaf or hearing. The left hemisphere of the brain processes linguistic patterns whether by signed languages or by spoken languages.\nDeafness can be broken down into four different types of hearing loss: conductive hearing loss, sensorineural hearing loss, mixed hearing loss, and auditory neuropathy spectrum disorder. All of these forms of hearing loss cause an impairment in a person's hearing where they are not able to hear sounds correctly. These different types of hearing loss occur in different parts of the ear, which make it difficult for the information being heard to get sent to the brain properly. To break it down even further, there are three different levels of hearing loss. According to the CDC, the first level is mild hearing loss. This is when someone is still able to hear noises, but it is more difficult to hear the softer sounds. The second level is moderate hearing loss and this is when someone can hear almost nothing when someone is talking to them at a normal volume. The next level is severe hearing loss. Severe hearing loss is when someone can not hear any sounds when they are being produced at a normal level and they can only hear minimum sounds that are being produced at a loud level. The final level is profound hearing loss, which is when someone is not able to hear any sounds except for very loud ones.\nThere are millions of people in the world who are living with deafness or hearing impairments. Survey of Income and Program Participation (SIPP) indicate that fewer than 1 in 20 Americans are currently deaf or hard of hearing.  There are a lot of solutions available for people with hearing impairments. Some examples of solutions would be blinking lights on different things like their phones, alarms, and things that are important to alert them. Cochlear implants are an option too. Cochlear implants are surgically placed devices that stimulate the cochlear nerve in order to help the person hear. A cochlear implant is used instead of hearing aids in order to help when someone has difficulties understanding speech. A study by Anna Agostinelli et al., was done on four subjects with Single-Sided Deafness that use Cochlear Implants. This study showed their age, what made them lose their hearing, which ear was affected, and how long it has been since they had their Cochlear Implant activated. It was shown that the children had much improvement in their auditory use, Another study done by Shannon R. Culbertson et al.,  showed that children who had their activation at a younger age, had better auditory skill and perception. Children who had their activation earlier had a higher FLI (Functional Listening Index) score than those who had theirs activated later on. Functional Listening Index was developed by The Shepherd Centre. It is a 60- item scale that tracks the development of auditory skills from birth through 5 years of age for six categories: sound awareness, associating sound with meaning, comprehending simple spoken language, comprehending language in different listening conditions, listening through discourse and narratives, and advanced open listening set (Davis et al., 2015). Merv Hyde, Renee Punch, and Linda Komesaroff completed a study that says that parents have difficulties with making the decision to use Cochlear Implants for their child. A survey was done asking parents how they felt when making this decision. Many only made this decision due to feeling urgency with implanting their child. This can be a serious procedure, which comes with the risk of negative results. In the end, most of the parents felt that this was beneficial for their child.\n\nCultural context\nIn a cultural context, Deaf culture refers to a tight-knit cultural group of people whose primary language is signed, and who practice social and cultural norms which are distinct from those of the surrounding hearing community. This community does not automatically include all those who are clinically or legally deaf, nor does it exclude every hearing person. According to Baker and Padden, it includes any person who \"identifies him\/herself as a member of the Deaf community, and other members accept that person as a part of the community\", an example being children of deaf adults with normal hearing ability. It includes the set of social beliefs, behaviors, art, literary traditions, history, values, and shared institutions of communities that are influenced by deafness and which use sign languages as the main means of communication. While deafness is often included within the umbrella of disability, members of the Deaf community tend to view deafness as a difference in human experience or itself as a language minority.\nMany non-disabled people continue to assume that deaf people have no autonomy and fail to provide people with support beyond hearing aids, which is something that must be addressed. Different non-governmental organizations around the world have created programs towards closing the gap between deaf and non-disabled people in developing countries. As children, deaf people learn literacy differently than hearing children. They learn to speak and write, whereas hearing children naturally learn to speak and eventually learn to write later on. The Quota International organization with headquarters in the United States provided immense educational support in the Philippines, where it started providing free education to deaf children in the Leganes Resource Center for the Deaf. The Sounds Seekers British organization also provided support by offering audiology maintenance technology, to better assist those who are deaf in hard-to-reach places. The Nippon Foundation also supports deaf students at Gallaudet University and the National Technical Institute for the Deaf, through sponsoring international scholarships programs to encourage students to become future leaders in the deaf community. The more aid these organizations give to the deaf people, the more opportunities and resources disabled people must speak up about their struggles and goals that they aim to achieve. When more people understand how to leverage their privilege for the marginalized groups in the community, then we can build a more inclusive and tolerant environment for the generations that are yet to come.\n\nHistory\nThe first known record of sign language in history comes from Plato's Cratylus, written in the fifth century BCE. In a dialogue on the \"correctness of names\", Socrates says, \"Suppose that we had no voice or tongue, and wanted to communicate with one another, should we not, like the deaf and dumb, make signs with the hands and head and the rest of the body?\" His belief that deaf people possessed an innate intelligence for language put him at odds with his student Aristotle, who said, \"Those who are born deaf all become senseless and incapable of reason,\" and that \"it is impossible to reason without the ability to hear\".\nThis pronouncement would reverberate through the ages and it was not until the 17th century when manual alphabets began to emerge, as did various treatises on deaf education, such as Reducci\u00f3n de las letras y arte para ense\u00f1ar a hablar a los mudos ('Reduction of letters and art for teaching mute people to speak'), written by Juan Pablo Bonet in Madrid in 1620, and Didascalocophus, or, The deaf and dumb mans tutor, written by George Dalgarno in 1680.\nIn 1760, French philanthropic educator Charles-Michel de l'\u00c9p\u00e9e opened the world's first free school for the deaf. The school won approval for government funding in 1791 and became known as the \"Institution Nationale des Sourds-Muets \u00e0 Paris.\" The school inspired the opening of what is today known as the American School for the Deaf, the oldest permanent school for the deaf in the United States, and indirectly, Gallaudet University, the world's first school for the advanced education of the deaf and hard of hearing, and to date, the only higher education institution in which all programs and services are specifically designed to accommodate deaf and hard of hearing students.\n\nSchooling\nNicole M. Stephens and Jill Duncan say that parents often encounter difficulties when it comes time for them to choose an educational setting for their child. There are many things they consider when choosing that setting for them. Three things to consider would be the needs and abilities of the child, how the school can make accommodations for the child, and the environment itself. There are four themes that connect to eight sub-themes that the author refers to. Child-Centered connects to Inclusion and Additional Needs and Well-Being. Familial connects to Complex Processes, Information Input and Flow, and Caregiver perceptions of Education. School connects to School Systems and Personnel, and School Character. And finally On Reflection connects to No Regrets. It can be profitable for both the child and the parent to do trial and error with different schools. This can lead to the child being in the proper environment for them and their needs.\n\nSee also\nReferences\nExternal links\nWHO fact sheet on deafness and hearing loss\nGlobal audiology, International Society of Audiology\nInternational Ototoxicity Management Group","49":"A developed country, or advanced country, is a sovereign state that has a high quality of life, developed economy, and advanced technological infrastructure relative to other less industrialized nations. Most commonly, the criteria for evaluating the degree of economic development are the gross domestic product (GDP), gross national product (GNP), the per capita income, level of industrialization, amount of widespread infrastructure and general standard of living. Which criteria are to be used and which countries can be classified as being developed are subjects of debate. Different definitions of developed countries are provided by the International Monetary Fund and the World Bank; moreover, HDI ranking is used to reflect the composite index of life expectancy, education, and income per capita. In 2023, 40 countries fit all four criteria, while an additional 19 countries fit three out of four.\nDeveloped countries have generally more advanced post-industrial economies, meaning the service sector provides more wealth than the industrial sector. They are contrasted with developing countries, which are in the process of industrialisation or are pre-industrial and almost entirely agrarian, some of which might fall into the category of Least Developed Countries. As of 2023, advanced economies comprise 57.3% of global GDP based on nominal values and 41.1% of global GDP based on purchasing-power parity (PPP) according to the IMF.\n\nDefinition and criteria\nEconomic criteria have tended to dominate discussions. One such criterion is the income per capita; countries with the high gross domestic product (GDP) per capita would thus be described as developed countries. Another economic criterion is industrialisation; countries in which the tertiary and quaternary sectors of industry dominate would thus be described as developed. More recently, another measure, the Human Development Index (HDI), which combines an economic measure, national income, with other measures, indices for life expectancy and education has become prominent. This criterion would define developed countries as those with a very high (HDI) rating. The index, however, does not take into account several factors, such as the net wealth per capita or the relative quality of goods in a country. This situation tends to lower the ranking of some of the most advanced countries, such as the G7 members and others.\nAccording to the United Nations Statistics Division:\n\nThere is no established convention for the designation of \"developed\" and \"developing\" countries or areas in the United Nations system.\nAnd it notes that:\n\nThe designations \"developed\" and \"developing\" are intended for statistical convenience and do not necessarily express a judgement about the stage reached by a particular country or area in the development process.\nNevertheless, the UN Trade and Development considers that this categorization can continue to be applied:\n\nThe developed economies broadly comprise Northern America and Europe, Israel, Japan, the Republic of Korea, Australia, and New Zealand.\n\nSimilar terms\nTerms linked to the concept developed country include \"advanced country\", \"industrialized country\", \"more developed country\" (MDC), \"more economically developed country\" (MEDC), \"Global North country\", \"first world country\", and \"post-industrial country\". The term industrialized country may be somewhat ambiguous, as industrialisation is an ongoing process that is hard to define. The first industrialized country was the United Kingdom, followed by Belgium. Later it spread further to Germany, United States, France and other Western European countries. According to some economists such as Jeffrey Sachs, however, the current divide between the developed and developing world is largely a phenomenon of the 20th century.\nMathis Wackernagel calls the binary labeling of countries as \"neither descriptive nor explanatory. It is merely a thoughtless and destructive endorsement of GDP fetish. In reality, there are not two types of countries, but over 200 countries, all faced with the same laws of nature, yet each with unique features.\"\nA 2021 analysis proposes the term emerged to describe markets, economies, or countries that have graduated from emerging market status, but have not yet reached the level equivalent to developed countries. Multinational corporations from these emerging markets present unique patterns of overseas expansion and knowledge acquisition from foreign countries.\n\nEconomy lists by various criteria\nHuman Development Index (HDI)\nThe UN HDI is a statistical measure that gauges an economy's level of human development. While there is a strong correlation between having a high HDI score and being a prosperous economy, the UN points out that the HDI accounts for more than income or productivity. Unlike GDP per capita or per capita income, the HDI takes into account how income is turned \"into education and health opportunities and therefore into higher levels of human development.\"\nSince 1990, Norway (2001\u20132006, 2009\u20132019), Japan (1990\u20131991 and 1993), Canada (1992 and 1994\u20132000) and Iceland (2007\u20132008) have had the highest HDI score.\nThe following countries in the year 2022 are considered to be of \"very high human development\":\n\nWESP developed economies\nAccording to the United Nations Department of Economic and Social Affairs' World Economic Situation and Prospects report, the following 37 countries are classified as \"developed economies\" as of January 2024:\n31 countries in Europe:\n\ntwo countries in Northern America:\n\nfour countries in Asia and the Pacific:\n\nWorld Bank high-income economies\nAccording to the World Bank, the following 85 sovereign states and territories across are classified as \"high income\" economies, having a nominal GDP per capita in excess of $14,005 as of 2024:\nUnsovereign Territories are denoted with an asterisk (*).\n\nDevelopment Assistance Committee members\nThere are 29 OECD member countries and the European Union\u2014in the Development Assistance Committee (DAC), a group of the world's major donor countries that discusses issues surrounding development aid and poverty reduction in developing countries. The following OECD member countries are DAC members:\n23 countries in Europe:\n\ntwo countries in the Americas:\n\ntwo countries in Asia:\n\ntwo countries in Oceania:\n\nIMF advanced economies\nAccording to the International Monetary Fund, 41 countries and territories are officially listed as \"advanced economies\", with the addition of 7 microstates and dependencies modified by the CIA which were omitted from the IMF version:\n29 countries and dependencies in Europe classified by the IMF, 6 others given by the CIA:\n\nseven countries and territories in Asia:\n\nthree countries and territories in the Americas classified by the IMF, one territory given by the CIA :\n\ntwo countries in Oceania:\n\nd The CIA has modified an older version of the IMF's list of 38 Advanced Economies, noting that the IMF's Advanced Economies list \"would presumably also cover the following nine smaller countries of Andorra, Bermuda, Faroe Islands, Guernsey, Holy See, Jersey, Liechtenstein, Monaco, and San Marino[...]\". San Marino (2012) and Andorra (2021) were later included in the IMF's list.\n\nParis Club members\nThere are 22 permanent members in the Paris Club (French: Club de Paris), a group of officials from major creditor countries whose role is to find coordinated and sustainable solutions to the payment difficulties experienced by debtor countries.\n15 countries in Europe:\n\nthree countries in the Americas:\n\nthree countries in Asia:\n\none country in Oceania:\n\nComparative table (2024)\nComparative table of countries with a \"very high\" human development (0.800 or higher), according to UNDP; \"advanced\" economies, according to the IMF; \"high income\" economies, according to the World Bank; and income per capita (purchasing power parity) higher than $25,000, according to the IMF.\n\nSee also\nNotes\nReferences\nExternal links\n Quotations related to Developed country at Wikiquote\nIMF (advanced economies)\nThe World Factbook Archived 9 April 2008 at the Wayback Machine (developed countries)\nUnited Nations Statistics Division (definition)\nList of countries, United Nations Statistics Division (developed regions)\nWorld Bank (high-income economies)","50":"Decompression sickness (DCS; also called divers' disease, the bends, aerobullosis, and caisson disease) is a medical condition caused by dissolved gases emerging from solution as bubbles inside the body tissues during decompression. DCS most commonly occurs during or soon after a decompression ascent from underwater diving, but can also result from other causes of depressurisation, such as emerging from a caisson, decompression from saturation, flying in an unpressurised aircraft at high altitude, and extravehicular activity from spacecraft. DCS and arterial gas embolism are collectively referred to as decompression illness.\nSince bubbles can form in or migrate to any part of the body, DCS can produce many symptoms, and its effects may vary from joint pain and rashes to paralysis and death. DCS often causes air bubbles to settle in major joints like knees or elbows, causing individuals to bend over in excruciating pain, hence its common name, the bends. Individual susceptibility can vary from day to day, and different individuals under the same conditions may be affected differently or not at all. The classification of types of DCS according to symptoms has evolved since its original description in the 19th century. The severity of symptoms varies from barely noticeable to rapidly fatal.\nDecompression sickness can occur after an exposure to increased pressure while breathing a gas with a metabolically inert component, then decompressing too fast for it to be harmlessly eliminated through respiration, or by decompression by an upward excursion from a condition of saturation by the inert breathing gas components, or by a combination of these routes. Theoretical decompression risk is controlled by the tissue compartment with the highest inert gas concentration, which for decompression from saturation is the slowest tissue to outgas.\nThe risk of DCS can be managed through proper decompression procedures, and contracting the condition has become uncommon. Its potential severity has driven much research to prevent it, and divers almost universally use decompression schedules or dive computers to limit their exposure and to monitor their ascent speed. If DCS is suspected, it is treated by hyperbaric oxygen therapy in a recompression chamber. Where a chamber is not accessible within a reasonable time frame, in-water recompression may be indicated for a narrow range of presentations, if there are suitably skilled personnel and appropriate equipment available on site. Diagnosis is confirmed by a positive response to the treatment. Early treatment results in a significantly higher chance of successful recovery.\nDecompression sickness caused by a decompression from saturation can occur in decompression or upward excursions from saturation diving, ascent to high altitudes, and extravehicular activities in space. Treatment is recompression, and oxygen therapy.\n\nClassification\nDCS is classified by symptoms. The earliest descriptions of DCS used the terms: \"bends\" for joint or skeletal pain; \"chokes\" for breathing problems; and \"staggers\" for neurological problems. In 1960, Golding et al. introduced a simpler classification using the term \"Type I ('simple')\" for symptoms involving only the skin, musculoskeletal system, or lymphatic system, and \"Type II ('serious')\" for symptoms where other organs (such as the central nervous system) are involved. Type II DCS is considered more serious and usually has worse outcomes. This system, with minor modifications, may still be used today. Following changes to treatment methods, this classification is now much less useful in diagnosis, since neurological symptoms may develop after the initial presentation, and both Type I and Type II DCS have the same initial management.\n\nDecompression illness and dysbarism\nThe term dysbarism encompasses decompression sickness, arterial gas embolism, and barotrauma, whereas decompression sickness and arterial gas embolism are commonly classified together as decompression illness when a precise diagnosis cannot be made. DCS and arterial gas embolism are treated very similarly because they are both the result of gas bubbles in the body. The U.S. Navy prescribes identical treatment for Type II DCS and arterial gas embolism. Their spectra of symptoms also overlap, although the symptoms from arterial gas embolism are generally more severe because they often arise from an infarction (blockage of blood supply and tissue death).\n\nSigns and symptoms\nWhile bubbles can form anywhere in the body, DCS is most frequently observed in the shoulders, elbows, knees, and ankles. Joint pain (\"the bends\") accounts for about 60% to 70% of all altitude DCS cases, with the shoulder being the most common site for altitude and bounce diving, and the knees and hip joints for saturation and compressed air work. Neurological symptoms are present in 10% to 15% of DCS cases with headache and visual disturbances being the most common symptom. Skin manifestations are present in about 10% to 15% of cases. Pulmonary DCS (\"the chokes\") is very rare in divers and has been observed much less frequently in aviators since the introduction of oxygen pre-breathing protocols. The table below shows symptoms for different DCS types.\n\nFrequency\nThe relative frequencies of different symptoms of DCS observed by the U.S. Navy are as follows:\n\nOnset\nAlthough onset of DCS can occur rapidly after a dive, in more than half of all cases symptoms do not begin to appear for at least an hour. In extreme cases, symptoms may occur before the dive has been completed. The U.S. Navy and Technical Diving International, a leading technical diver training organization, have published a table that documents time to onset of first symptoms. The table does not differentiate between types of DCS, or types of symptom.\n\nCauses\nDCS is caused by a reduction in ambient pressure that results in the formation of bubbles of inert gases within tissues of the body. It may happen when leaving a high-pressure environment, ascending from depth, or ascending to altitude. A closely related condition of bubble formation in body tissues due to isobaric counterdiffusion can occur with no change of pressure.\n\nAscent from depth\nDCS is best known as a diving disorder that affects divers having breathed gas that is at a higher pressure than the surface pressure, owing to the pressure of the surrounding water. The risk of DCS increases when diving for extended periods or at greater depth, without ascending gradually and making the decompression stops needed to slowly reduce the excess pressure of inert gases dissolved in the body. The specific risk factors are not well understood and some divers may be more susceptible than others under identical conditions. DCS has been confirmed in rare cases of breath-holding divers who have made a sequence of many deep dives with short surface intervals, and may be the cause of the disease called taravana by South Pacific island natives who for centuries have dived by breath-holding for food and pearls.\nTwo principal factors control the risk of a diver developing DCS:\n\nthe rate and duration of gas absorption under pressure \u2013 the deeper or longer the dive the more gas is absorbed into body tissue in higher concentrations than normal (Henry's Law);\nthe rate and duration of outgassing on depressurization \u2013 the faster the ascent and the shorter the interval between dives the less time there is for absorbed gas to be offloaded safely through the lungs, causing these gases to come out of solution and form \"micro bubbles\" in the blood.\nEven when the change in pressure causes no immediate symptoms, rapid pressure change can cause permanent bone injury called dysbaric osteonecrosis (DON). DON can develop from a single exposure to rapid decompression.\n\nLeaving a high-pressure environment\nWhen workers leave a pressurized caisson or a mine that has been pressurized to keep water out, they will experience a significant reduction in ambient pressure. A similar pressure reduction occurs when astronauts exit a space vehicle to perform a space-walk or extra-vehicular activity, where the pressure in their spacesuit is lower than the pressure in the vehicle.\nThe original name for DCS was \"caisson disease\". This term was introduced in the 19th century, when caissons under pressure were used to keep water from flooding large engineering excavations below the water table, such as bridge supports and tunnels. Workers spending time in high ambient pressure conditions are at risk when they return to the lower pressure outside the caisson if the pressure is not reduced slowly. DCS was a major factor during construction of Eads Bridge, when 15 workers died from what was then a mysterious illness, and later during construction of the Brooklyn Bridge, where it incapacitated the project leader Washington Roebling. On the other side of the Manhattan island during construction of the Hudson River Tunnel, contractor's agent Ernest William Moir noted in 1889 that workers were dying due to decompression sickness; Moir pioneered the use of an airlock chamber for treatment.\n\nAscent to altitude and loss of pressure from a pressurised environment\nThe most common health risk on ascent to altitude is not decompression sickness but altitude sickness, or acute mountain sickness (AMS), which has an entirely different and unrelated set of causes and symptoms. AMS results not from the formation of bubbles from dissolved gasses in the body but from exposure to a low partial pressure of oxygen and alkalosis. However, passengers in unpressurized aircraft at high altitude may also be at some risk of DCS.\nAltitude DCS became a problem in the 1930s with the development of high-altitude balloon and aircraft flights but not as great a problem as AMS, which drove the development of pressurized cabins, which coincidentally controlled DCS. Commercial aircraft are now required to maintain the cabin at or below a pressure altitude of 2,400 m (7,900 ft) even when flying above 12,000 m (39,000 ft). Symptoms of DCS in healthy individuals are subsequently very rare unless there is a loss of pressurization or the individual has been diving recently. Divers who drive up a mountain or fly shortly after diving are at particular risk even in a pressurized aircraft because the regulatory cabin altitude of 2,400 m (7,900 ft) represents only 73% of sea level pressure.\nGenerally, the higher the altitude the greater the risk of altitude DCS but there is no specific, maximum, safe altitude below which it never occurs. There are very few symptoms at or below 5,500 m (18,000 ft) unless the person had predisposing medical conditions or had dived recently. There is a correlation between increased altitudes above 5,500 m (18,000 ft) and the frequency of altitude DCS but there is no direct relationship with the severity of the various types of DCS. A US Air Force study reports that there are few occurrences between 5,500 m (18,000 ft) and 7,500 m (24,600 ft) and 87% of incidents occurred at or above 7,500 m (24,600 ft).  High-altitude parachutists may reduce the risk of altitude DCS if they flush nitrogen from the body by pre-breathing pure oxygen. A similar procedure is used by astronauts and cosmonauts preparing for extravehicular activity in low pressure space suits.\n\nPredisposing factors\nAlthough the occurrence of DCS is not easily predictable, many predisposing factors are known. They may be considered as either environmental or individual. Decompression sickness and arterial gas embolism in recreational diving are associated with certain demographic, environmental, and dive style factors. A statistical study published in 2005 tested potential risk factors: age, gender, body mass index, smoking, asthma, diabetes, cardiovascular disease, previous decompression illness, years since certification, dives in the last year, number of diving days, number of dives in a repetitive series, last dive depth, nitrox use, and drysuit use. No significant associations with risk of decompression sickness or arterial gas embolism were found for asthma, diabetes, cardiovascular disease, smoking, or body mass index. Increased depth, previous DCI, larger number of consecutive days diving, and being male were associated with higher risk for decompression sickness and arterial gas embolism. Nitrox and drysuit use, greater frequency of diving in the past year, increasing age, and years since certification were associated with lower risk, possibly as indicators of more extensive training and experience.\n\nEnvironmental\nThe following environmental factors have been shown to increase the risk of DCS:\n\nthe magnitude of the pressure reduction ratio \u2013 a large pressure reduction ratio is more likely to cause DCS than a small one.\nrepetitive exposures \u2013 repetitive dives within a short period of time (a few hours) increase the risk of developing DCS. Repetitive ascents to altitudes above 5,500 metres (18,000 ft) within similar short periods increase the risk of developing altitude DCS.\nthe rate of ascent \u2013 the faster the ascent the greater the risk of developing DCS. The U.S. Navy Diving Manual indicates that ascent rates greater than about 20 m\/min (66 ft\/min) when diving increase the chance of DCS, while recreational dive tables such as the B\u00fchlmann tables require an ascent rate of 10 m\/min (33 ft\/min) with the last 6 m (20 ft) taking at least one minute.  An individual exposed to a rapid decompression (high rate of ascent) above 5,500 metres (18,000 ft) has a greater risk of altitude DCS than being exposed to the same altitude but at a lower rate of ascent.\nthe duration of exposure \u2013 the longer the duration of the dive, the greater is the risk of DCS. Longer flights, especially to altitudes of 5,500 m (18,000 ft) and above, carry a greater risk of altitude DCS.\nunderwater diving before flying \u2013 divers who ascend to altitude soon after a dive increase their risk of developing DCS even if the dive itself was within the dive table safe limits. Dive tables make provisions for post-dive time at surface level before flying to allow any residual excess nitrogen to outgas. However, the pressure maintained inside even a pressurized aircraft may be as low as the pressure equivalent to an altitude of 2,400 m (7,900 ft) above sea level. Therefore, the assumption that the dive table surface interval occurs at normal atmospheric pressure is invalidated by flying during that surface interval, and an otherwise-safe dive may then exceed the dive table limits.\ndiving before travelling to altitude \u2013 DCS can occur without flying if the person moves to a high-altitude location on land immediately after diving, for example, scuba divers in Eritrea who drive from the coast to the Asmara plateau at 2,400 m (7,900 ft) increase their risk of DCS.\ndiving at altitude \u2013 diving in water whose surface pressure is significantly below sea level pressure \u2013 for example, Lake Titicaca is at 3,800 m (12,500 ft). Versions of decompression tables for altitudes exceeding 300 m (980 ft), or dive computers with high-altitude settings or surface pressure sensors may be used to reduce this risk.\n\nIndividual\nThe following individual factors have been identified as possibly contributing to increased risk of DCS:\n\ndehydration \u2013 Studies by Walder concluded that decompression sickness could be reduced in aviators when the serum surface tension was raised by drinking isotonic saline, and the high surface tension of water is generally regarded as helpful in controlling bubble size. Maintaining proper hydration is recommended. There is no convincing evidence that overhydration has any benefits, and it is implicated in immersion pulmonary oedema.\npatent foramen ovale \u2013 a hole between the atrial chambers of the heart in the fetus is normally closed by a flap with the first breaths at birth. In about 20% of adults the flap does not completely seal, however, allowing blood through the hole when coughing or during activities that raise chest pressure. In diving, this can allow venous blood with microbubbles of inert gas to bypass the lungs, where the bubbles would otherwise be filtered out by the lung capillary system, and return directly to the arterial system (including arteries to the brain, spinal cord and heart). In the arterial system, bubbles (arterial gas embolism) are far more dangerous because they block circulation and cause infarction (tissue death, due to local loss of blood flow). In the brain, infarction results in stroke, and in the spinal cord it may result in paralysis.\na person's age \u2013 there are some reports indicating a higher risk of altitude DCS with increasing age.\nprevious injury \u2013 there is some indication that recent joint or limb injuries may predispose individuals to developing decompression-related bubbles.\nambient temperature \u2013 there is some evidence suggesting that individual exposure to very cold ambient temperatures may increase the risk of altitude DCS. Decompression sickness risk can be reduced by increased ambient temperature during decompression following dives in cold water, though risk is also increased by ingassing while the diver is warm and peripherally well-perfused, and decompressing when the diver is cold.\nbody type \u2013 typically, a person who has a high body fat content is at greater risk of DCS. This is because nitrogen is five times more soluble in fat than in water, leading to greater amounts of total body dissolved nitrogen during time at pressure. Fat represents about 15\u201325 percent of a healthy adult's body, but stores about half of the total amount of nitrogen (about 1 litre) at normal pressures.\nalcohol consumption \u2013 although alcohol consumption increases dehydration and therefore may increase susceptibility to DCS, a 2005 study found no evidence that alcohol consumption increases the incidence of DCS.\n\nMechanism\nDepressurisation causes inert gases, which were dissolved under higher pressure, to come out of physical solution and form gas bubbles within the body. These bubbles produce the symptoms of decompression sickness. Bubbles may form whenever the body experiences a reduction in pressure, but not all bubbles result in DCS. The amount of gas dissolved in a liquid is described by Henry's Law, which indicates that when the pressure of a gas in contact with a liquid is decreased, the amount of that gas dissolved in the liquid will also decrease proportionately.\nOn ascent from a dive, inert gas comes out of solution in a process called \"outgassing\" or \"offgassing\". Under normal conditions, most offgassing occurs by gas exchange in the lungs. If inert gas comes out of solution too quickly to allow outgassing in the lungs then bubbles may form in the blood or within the solid tissues of the body. The formation of bubbles in the skin or joints results in milder symptoms, while large numbers of bubbles in the venous blood can cause lung damage. The most severe types of DCS interrupt \u2013 and ultimately damage \u2013 spinal cord function, leading to paralysis, sensory dysfunction, or death. In the presence of a right-to-left shunt of the heart, such as a patent foramen ovale, venous bubbles may enter the arterial system, resulting in an arterial gas embolism. A similar effect, known as ebullism, may occur during explosive decompression, when water vapour forms bubbles in body fluids due to a dramatic reduction in environmental pressure.\n\nInert gases\nThe main inert gas in air is nitrogen, but nitrogen is not the only gas that can cause DCS. Breathing gas mixtures such as trimix and heliox include helium, which can also cause decompression sickness. Helium both enters and leaves the body faster than nitrogen, so different decompression schedules are required, but, since helium does not cause narcosis, it is preferred over nitrogen in gas mixtures for deep diving.\nThere is some debate as to the decompression requirements for helium during short-duration dives. Most divers do longer decompressions; however, some groups like the WKPP have been experimenting with the use of shorter decompression times by including deep stops. The balance of evidence as of 2020 does not indicate that deep stops increase decompression efficiency.\nAny inert gas that is breathed under pressure can form bubbles when the ambient pressure decreases. Very deep dives have been made using hydrogen\u2013oxygen mixtures (hydrox), but controlled decompression is still required to avoid DCS.\n\nIsobaric counterdiffusion\nDCS can also be caused at a constant ambient pressure when switching between gas mixtures containing different proportions of inert gas. This is known as isobaric counterdiffusion, and presents a problem for very deep dives. For example, after using a very helium-rich trimix at the deepest part of the dive, a diver will switch to mixtures containing progressively less helium and more oxygen and nitrogen during the ascent. Nitrogen diffuses into tissues 2.65 times slower than helium but is about 4.5 times more soluble. Switching between gas mixtures that have very different fractions of nitrogen and helium can result in \"fast\" tissues (those tissues that have a good blood supply) actually increasing their total inert gas loading. This is often found to provoke inner ear decompression sickness, as the ear seems particularly sensitive to this effect.\n\nBubble formation\nThe location of micronuclei or where bubbles initially form is not known. The most likely mechanisms for bubble formation are tribonucleation, when two surfaces make and break contact (such as in joints), and heterogeneous nucleation, where bubbles are created at a site based on a surface in contact with the liquid. Homogeneous nucleation, where bubbles form within the liquid itself is less likely because it requires much greater pressure differences than experienced in decompression. The spontaneous formation of nanobubbles on hydrophobic surfaces is a possible source of micronuclei, but it is not yet clear if these can grow large enough to cause symptoms as they are very stable.\nOnce microbubbles have formed, they can grow by either a reduction in pressure or by diffusion of gas into the gas from its surroundings. In the body, bubbles may be located within tissues or carried along with the bloodstream. The speed of blood flow within a blood vessel and the rate of delivery of blood to capillaries (perfusion) are the main factors that determine whether dissolved gas is taken up by tissue bubbles or circulation bubbles for bubble growth.\n\nPathophysiology\nThe primary provoking agent in decompression sickness is bubble formation from excess dissolved gases. Various hypotheses have been put forward for the nucleation and growth of bubbles in tissues, and for the level of supersaturation which will support bubble growth. The earliest bubble formation detected is subclinical intravascular bubbles detectable by doppler ultrasound in the venous systemic circulation. The presence of these \"silent\" bubbles is no guarantee that they will persist and grow to be symptomatic.\nVascular bubbles formed in the systemic capillaries may be trapped in the lung capillaries, temporarily blocking them. If this is severe, the symptom called \"chokes\" may occur. If the diver has a patent foramen ovale (or a shunt in the pulmonary circulation), bubbles may pass through it and bypass the pulmonary circulation to enter the arterial blood. If these bubbles are not absorbed in the arterial plasma and lodge in systemic capillaries they will block the flow of oxygenated blood to the tissues supplied by those capillaries, and those tissues will be starved of oxygen. Moon and Kisslo (1988) concluded that \"the evidence suggests that the risk of serious neurological DCI or early onset DCI is increased in divers with a resting right\u2013to-left shunt through a PFO. There is, at present, no evidence that PFO is related to mild or late onset bends. Bubbles form within other tissues as well as the blood vessels. Inert gas can diffuse into bubble nuclei between tissues. In this case, the bubbles can distort and permanently damage the tissue. As they grow, the bubbles may also compress nerves, causing pain. Extravascular or autochthonous[a] bubbles usually form in slow tissues such as joints, tendons and muscle sheaths. Direct expansion causes tissue damage, with the release of histamines and their associated affects. Biochemical damage may be as important as, or more important than mechanical effects.\nBubble size and growth may be affected by several factors \u2013 gas exchange with adjacent tissues, the presence of surfactants, coalescence and disintegration by collision. Vascular bubbles may cause direct blockage, aggregate platelets and red blood cells, and trigger the coagulation process, causing local and downstream clotting.\nArteries may be blocked by intravascular fat aggregation. Platelets accumulate in the vicinity of bubbles. Endothelial damage may be a mechanical effect of bubble pressure on the vessel walls, a toxic effect of stabilised platelet aggregates and possibly toxic effects due to the association of lipids with the air bubbles. Protein molecules may be denatured by reorientation of the secondary and tertiary structure when non-polar groups protrude into the bubble gas and hydrophilic groups remain in the surrounding blood, which may generate a cascade of pathophysiological events with consequent production of clinical signs of decompression sickness.\nThe physiological effects of a reduction in environmental pressure depend on the rate of bubble growth, the site, and surface activity. A sudden release of sufficient pressure in saturated tissue results in a complete disruption of cellular organelles, while a more gradual reduction in pressure may allow accumulation of a smaller number of larger bubbles, some of which may not produce clinical signs, but still cause physiological effects typical of a blood\/gas interface and mechanical effects. Gas is dissolved in all tissues, but decompression sickness is only clinically recognised in the central nervous system, bone, ears, teeth, skin and lungs.\nNecrosis has frequently been reported in the lower cervical, thoracic, and upper lumbar regions of the spinal cord. A catastrophic pressure reduction from saturation produces explosive mechanical disruption of cells by local effervescence, while a more gradual pressure loss tends to produce discrete bubbles accumulated in the white matter, surrounded by a protein layer. Typical acute spinal decompression injury occurs in the columns of white matter. Infarcts are characterised by a region of oedema, haemorrhage and early myelin degeneration, and are typically centred on small blood vessels. The lesions are generally discrete. Oedema usually extends to the adjacent grey matter. Microthrombi are found in the blood vessels associated with the infarcts.\nFollowing the acute changes there is an invasion of lipid phagocytes and degeneration of adjacent neural fibres with vascular hyperplasia at the edges of the infarcts. The lipid phagocytes are later replaced by a cellular reaction of astrocytes. Vessels in surrounding areas remain patent but are collagenised. Distribution of spinal cord lesions may be related to vascular supply. There is still uncertainty regarding the aetiology of decompression sickness damage to the spinal cord.\nDysbaric osteonecrosis lesions are typically bilateral and usually occur at both ends of the femur and at the proximal end of the humerus Symptoms are usually only present when a joint surface is involved, which typically does not occur until a long time after the causative exposure to a hyperbaric environment. The initial damage is attributed to the formation of bubbles, and one episode can be sufficient, however incidence is sporadic and generally associated with relatively long periods of hyperbaric exposure and aetiology is uncertain. Early identification of lesions by radiography is not possible, but over time areas of radiographic opacity develop in association with the damaged bone.\n\nDiagnosis\nDiagnosis of decompression sickness relies almost entirely on clinical presentation, as there are no laboratory tests that can incontrovertibly confirm or reject the diagnosis. Various blood tests have been proposed, but they are not specific for decompression sickness, they are of uncertain utility and are not in general use.\nDecompression sickness should be suspected if any of the symptoms associated with the condition occurs following a drop in pressure, in particular, within 24 hours of diving. In 1995, 95% of all cases reported to Divers Alert Network had shown symptoms within 24 hours. This window can be extended to 36 hours for ascent to altitude and 48 hours for prolonged exposure to altitude following diving. An alternative diagnosis should be suspected if severe symptoms begin more than six hours following decompression without an altitude exposure or if any symptom occurs more than 24 hours after surfacing. The diagnosis is confirmed if the symptoms are relieved by recompression. Although magnetic resonance imaging (MRI) or computed tomography (CT) can frequently identify bubbles in DCS, they are not as good at determining the diagnosis as a proper history of the event and description of the symptoms.\n\nTest of pressure\nThere is no gold standard for diagnosis, and DCI experts are rare. Most of the chambers open to treatment of recreational divers and reporting to Diver's Alert Network see fewer than 10 cases per year, making it difficult for the attending doctors to develop experience in diagnosis. A method used by commercial diving supervisors when considering whether to recompress as first aid when they have a chamber on site, is known as the test of pressure. The diver is checked for contraindications to recompression, and if none are present, recompressed. If the symptoms resolve or reduce during recompression, it is considered likely that a treatment schedule will be effective. The test is not entirely reliable, and both false positives and false negatives are possible, however in the commercial diving environment it is often considered worth treating when there is doubt, and very early recompression has a history of very high success rates and reduced number of treatments needed for complete resolution and minimal sequelae.\n\nDifferential diagnosis\nSymptoms of DCS and arterial gas embolism can be virtually indistinguishable. The most reliable way to tell the difference is based on the dive profile followed, as the probability of DCS depends on duration of exposure and magnitude of pressure, whereas AGE depends entirely on the performance of the ascent. In many cases it is not possible to distinguish between the two, but as the treatment is the same in such cases it does not usually matter.\nOther conditions which may be confused include skin symptoms. Cutis marmorata due to DCS may be confused with skin barotrauma due to dry suit squeeze, for which no treatment is necessary. Dry suit squeeze produces lines of redness with possible bruising where the skin was pinched between folds of the suit, while the mottled effect of cutis marmorata is usually on skin where there is subcutaneous fat, and has no linear pattern.\nTransient episodes of severe neurological incapacitation with rapid spontaneous recovery shortly after a dive may be attributed to hypothermia, but may actually be symptomatic of short term CNS involvement due to bubbles which form a short term gas embolism, then resolve, but which may leave residual problems which may cause relapses. These cases are thought to be under-diagnosed.\nInner ear decompression sickness (IEDCS) can be confused with inner ear barotrauma (IEBt), alternobaric vertigo, caloric vertigo and reverse squeeze. A history of difficulty in equalising the ears during the dive makes ear barotrauma more likely, but does not always eliminate the possibility of inner ear DCS, which is usually associated with deep, mixed gas dives with decompression stops. Both conditions may exist concurrently, and it can be difficult to distinguish whether a person has IEDCS, IEBt, or both.\nNumbness and tingling are associated with spinal DCS, but can also be caused by pressure on nerves (compression neurapraxia). In DCS the numbness or tingling is generally confined to one or a series of dermatomes, while pressure on a nerve tends to produce characteristic areas of numbness associated with the specific nerve on only one side of the body distal to the pressure point. A loss of strength or function is likely to be a medical emergency. A loss of feeling that lasts more than a minute or two indicates a need for immediate medical attention. It is only partial sensory changes, or paraesthesias, where this distinction between trivial and more serious injuries applies.\nLarge areas of numbness with associated weakness or paralysis, especially if a whole limb is affected, are indicative of probable brain involvement and require urgent medical attention. Paraesthesias or weakness involving a dermatome indicate probable spinal cord or spinal nerve root involvement. Although it is possible that this may have other causes, such as an injured intervertebral disk, these symptoms indicate an urgent need for medical assessment. In combination with weakness, paralysis or loss of bowel or bladder control, they indicate a medical emergency.\n\nPrevention\nUnderwater diving\nTo prevent the excess formation of bubbles that can lead to decompression sickness, divers limit their ascent rate\u2014the recommended ascent rate used by popular decompression models is about 10 metres (33 ft) per minute\u2014and follow a decompression schedule as necessary. This schedule may require the diver to ascend to a particular depth, and remain at that depth until sufficient inert gas has been eliminated from the body to allow further ascent. Each of these is termed a \"decompression stop\", and a schedule for a given bottom time and depth may contain one or more stops, or none at all. Dives that contain no decompression stops are called \"no-stop dives\", but divers usually schedule a short \"safety stop\" at 3 to 6 m (10 to 20 ft),  depending on the training agency or dive computer.\nThe decompression schedule may be derived from decompression tables, decompression software, or from dive computers, and these are generally based upon a mathematical model of the body's uptake and release of inert gas as pressure changes. These models, such as the B\u00fchlmann decompression algorithm, are modified to fit empirical data and provide a decompression schedule for a given depth and dive duration using a specified breathing gas mixture.\nSince divers on the surface after a dive may still have excess inert gas in their bodies, decompression from any subsequent dive before this excess is eliminated needs to modify the schedule to take account of the residual gas load from the previous dive. This will result in a shorter allowable time under water without obligatory decompression stops, or an increased decompression time during the subsequent dive. The total elimination of excess gas may take many hours, and tables will indicate the time at normal pressures that is required, which may be up to 18 hours.\nDecompression time can be significantly shortened by breathing mixtures containing much less inert gas during the decompression phase of the dive (or pure oxygen at stops in 6 metres (20 ft) of water or less). The reason is that the inert gas outgases at a rate proportional to the difference between the partial pressure of inert gas in the diver's body and its partial pressure in the breathing gas; whereas the likelihood of bubble formation depends on the difference between the inert gas partial pressure in the diver's body and the ambient pressure. Reduction in decompression requirements can also be gained by breathing a nitrox mix during the dive, since less nitrogen will be taken into the body than during the same dive done on air.\nFollowing a decompression schedule does not completely protect against DCS. The algorithms used are designed to reduce the probability of DCS to a very low level, but do not reduce it to zero. The mathematical implications of all current decompression models are that provided that no tissue is ingassing, longer decompression stops will decrease decompression risk, or at worst not increase it. Efficient decompression requires the diver to ascend fast enough to establish as high a decompression gradient, in as many tissues, as safely possible, without provoking the development of symptomatic bubbles. This is facilitated by the highest acceptably safe oxygen partial pressure in the breathing gas, and avoiding gas changes that could cause counterdiffusion bubble formation or growth. The development of schedules that are both safe and efficient has been complicated by the large number of variables and uncertainties, including personal variation in response under varying environmental conditions and workload, attributed to variations of body type, fitness and other risk factors.\n\nExposure to altitude\nOne of the most significant breakthroughs in the prevention of altitude DCS is oxygen pre-breathing. Breathing pure oxygen significantly reduces the nitrogen loads in body tissues by reducing the partial pressure of nitrogen in the lungs, which induces diffusion of nitrogen from the blood into the breathing gas, and this effect eventually lowers the concentration of nitrogen in the other tissues of the body. If continued for long enough, and without interruption, this provides effective protection upon exposure to low-barometric pressure environments. However, breathing pure oxygen during flight alone (ascent, en route, descent) does not decrease the risk of altitude DCS as the time required for ascent is generally not sufficient to significantly desaturate the slower tissues.\nPure aviator oxygen which has moisture removed to prevent freezing of valves at altitude is readily available and routinely used in general aviation mountain flying and at high altitudes. Most small general aviation aircraft are not pressurized, therefore oxygen use is an FAA requirement at higher altitudes.\nAlthough pure oxygen pre-breathing is an effective method to protect against altitude DCS, it is logistically complicated and expensive for the protection of civil aviation flyers, either commercial or private. Therefore, it is currently used only by military flight crews and astronauts for protection during high-altitude and space operations. It is also used by flight test crews involved with certifying aircraft, and may also be used for high-altitude parachute jumps.\nAstronauts aboard the International Space Station preparing for extra-vehicular activity (EVA) \"camp out\" at low atmospheric pressure, 10.2 psi (0.70 bar), spending eight sleeping hours in the Quest airlock chamber before their spacewalk. During the EVA they breathe 100% oxygen in their spacesuits, which operate at 4.3 psi (0.30 bar), although research has examined the possibility of using 100% O2 at 9.5 psi (0.66 bar) in the suits to lessen the pressure reduction, and hence the risk of DCS.\n\nTreatment\nRecompression on air was shown to be an effective treatment for minor DCS symptoms by Keays in 1909. Evidence of the effectiveness of recompression therapy utilizing oxygen was first shown by Yarbrough and Behnke, and has since become the standard of care for treatment of DCS. Recompression is normally carried out in a recompression chamber. At a dive site, a riskier alternative is in-water recompression.\nOxygen first aid has been used as an emergency treatment for diving injuries for years. Particularly if given within the first four hours of surfacing, it increases the success of recompression therapy as well as decreasing the number of recompression treatments required. Most fully closed-circuit diving rebreathers can deliver sustained high concentrations of oxygen-rich breathing gas and could be used as a means of supplying oxygen if dedicated equipment is not available.\nIt is beneficial to give fluids, as this helps reduce dehydration. It is no longer recommended to administer aspirin, unless advised to do so by medical personnel, as analgesics may mask symptoms. People should be made comfortable and placed in the supine position (horizontal), or the recovery position if vomiting occurs. In the past, both the Trendelenburg position and the left lateral decubitus position (Durant's maneuver) have been suggested as beneficial where air emboli are suspected, but are no longer recommended for extended periods, owing to concerns regarding cerebral edema.\n\nFirst aid\nAll cases of decompression sickness should be treated initially with the highest available concentration of oxygen until hyperbaric oxygen therapy (100% oxygen delivered in a hyperbaric chamber) can be provided. Mild cases of the \"bends\" and some skin symptoms may disappear during descent from high altitude; however, it is recommended that these cases still be evaluated. Neurological symptoms, pulmonary symptoms, and mottled or marbled skin lesions should be treated with hyperbaric oxygen therapy if seen within 10 to 14 days of development. Early recompression has a history of better outcomes and less treatment being needed.\nNormobaric oxygen administered at as close to 100% as practicable is known to be beneficial based on observed bubble reduction and symptom resolution. For this reason diver training in oxygen administration, and a system for administering a high percentage of inspired oxygen at quantities sufficient for plausible evacuation scenarios is desirable. Where oxygenation may be compromised the administration rate should be adjusted to ensure that the best practicable supplementation is maintained until supplies can be replenished.\nA horizontal position is preferable during evacuation if possible, with the recovery position recommended for unconscious divers, as there is evidence that inert gas washout is improved in horizontal subjects, and that large arterial bubbles tend to distribute towards the head in upright positions. A head down position is thought to be harmful in DCS.\nOral hydration is recommended in fully conscious persons, and fluids should ideally be isotonic, without alcohol, carbonation or caffeine, as diving is known to cause dehydration, and rehydration is known to reduce post-dive venous gas emboli.\nIntravascular rehydration is recommended if suitably competent responders are present. Glucose free isotonic crystalloid solutions are preferred. Case evidence shows that aggressive rehydration can be life-saving in severe cases.\nIf there are no contraindications, a non-steroidal anti-inflammatory drug along with hyperbatic oxygen is likely to improve rate of recovery. The most prominent NSAIDs are aspirin, ibuprofen, and naproxen; all available over the counter in most countries. Paracetamol (acetaminophen) is generally not considered an NSAID because it has only minor anti-inflammatory activity.Corticosteroids, pentoxyphylline, aspirin, lidocaine and nicergoline have been used in early management of DCS, but there is insufficient evidence on their effectiveness.\nDivers should be kept comfortably warm, as warm subjects are known to eliminate gas more quickly, but overheating aggravates neurological injury.\n\nDelay of recompression\nObservational evidence shows that outcomes after recompression are likely to be better after immediate recompression, which is only possible when on-site recompression is possible, although the 2004 workshop on decompression came to the conclusion that for cases with mild symptoms, a delay before recompression is unlikely to cause any worsening of long-term outcomes.\nIn more serious cases recompression should be done as soon as safely possible. There is some evidence that delays longer than six hours result in slower or less complete recovery, and the number of treatments required may be increased.\n\nTransport of a symptomatic diver\nExposing a case of decompression sickness to reduced ambient pressure will cause the bubbles to expand if not constrained by a rigid local tissue environment. This can aggravate the symptoms, and should be avoided if reasonably practicable. If a diver with DCS is transported by air, cabin pressure should be kept as close to sea level atmospheric pressure as possible, preferably not more than 150 m, either by cabin pressurisation or by remaining at low altitude throughout the flight. The risk of deterioration at higher altitudes must be considered against the risk of deterioration if not transported. Some divers with symptoms or signs of mild decompression sickness may be evacuated by pressurised commercial airliner for further treatment after a surface interval of at least 24 hours. The 2004 workshop considered it unlikely for this to cause a worse outcome. Most experience has been for short flights of less than two hours. There is little known about the effects of longer flights. Where possible, pre-flight and in-flight oxygen breathing at the highest available percentage is considered best practice. Similar precautions apply to surface transport through higher altitudes.\n\nIn-water recompression\nRecompression and hyperbaric oxygen administered in a recompression chamber is recognised as the definitive treatment for DCI, but when there is no readily available access to a suitable hyperbaric chamber, and if symptoms are significant or progressing, in-water recompression (IWR) with oxygen is a medically recognised option where a group of divers including the symptomatic diver already have relevant training and equipment that provides a sufficient understanding of the associated risks and allows the involved parties to collectively accept responsibility for a decision to proceed with IWR.\nIn-water recompression (IWR) or underwater oxygen treatment is the emergency treatment of decompression sickness by returning the diver underwater to help the gas bubbles in the tissues, which are causing the symptoms, to resolve. It is a procedure that exposes the diver to significant risk which should be compared with the risk associated with the other available options. Some authorities recommend that it is only to be used when the time to travel to the nearest recompression chamber is too long to save the victim's life, others take a more pragmatic approach, and accept that in some circumstances IWR is the best available option. The risks may not be justified for case of mild symptoms likely to resolve spontaneously, or for cases where the diver is likely to be unsafe in the water, but in-water recompression may be justified in cases where severe outcomes are likely, if conducted by a competent and suitably equipped team.\nCarrying out in-water recompression when there is a nearby recompression chamber or without suitable equipment and training is never a desirable option. The risk of the procedure is due to the diver suffering from DCS being seriously ill and may become paralysed, unconscious or stop breathing while under water. Any one of these events is likely to result in the diver drowning or asphyxiating or suffering further injury during a subsequent rescue to the surface. This risk can be reduced by improving airway security by using surface supplied gas and a helmet or full-face mask.\nSeveral schedules have been published for in-water recompression treatment, but little data on their efficacy is available.\nThe decision of whether or not to attempt IWR is dependent on identifying the diver whose condition is serious enough to justify the risk, but whose clinical condition does not indicate that the risk is unacceptable. The risk may not be justified for mild DCI, if spontaneous recovery is probable whether the diver is recompressed or not, and surface oxygen is indicated for these cases. However, in these cases the risk of the recompression is also low, and early abandonment is also unlikely to cause further harm.\n\nContraindications\nSome signs of decompression illness which suggest a risk of permanent injury are nevertheless considered contraindications for IWR.  Hearing loss and vertigo displayed in isolation with no other symptoms of DCI can have been caused by inner ear barotrauma rather than DCI, and inner ear barotrauma is generally considered a contraindication for recompression. Even when caused by DCI, vertigo can make in-water treatment hazardous if accompanied by nausea and vomiting. A diver with a deteriorating level of consciousness or with a persisting reduced level of consciousness should also not be recompressed in-water nor should a diver who does not want to go back down, or with a  history of oxygen toxicity in the preceding dives, or any physical injury or incapacitation which may make the procedure unsafe.\n\nDefinitive treatment\nThe duration of recompression treatment depends on the severity of symptoms, the dive history, the type of recompression therapy used and the patient's response to the treatment. One of the more frequently used treatment schedules is the US Navy Table 6, which provides hyperbaric oxygen therapy with a maximum pressure equivalent to 60 feet (18 m) of seawater (2.8 bar PO2) for a total time under pressure of 288 minutes, of which 240 minutes are on oxygen and the balance are air breaks to minimise the possibility of oxygen toxicity.\nA multiplace chamber is the preferred facility for treatment of decompression sickness as it allows direct physical access to the patient by medical personnel, but monoplace chambers are more widely available and should be used for treatment if a multiplace chamber is not available or transportation would cause significant delay in treatment, as the interval between onset of symptoms and recompression is important to the quality of recovery. It may be necessary to modify the optimum treatment schedule to allow use of a monoplace chamber, but this is usually better than delaying treatment. A US Navy treatment table 5 can be safely performed without air breaks if a built-in breathing system is not available. In most cases the patient can be adequately treated in a monoplace chamber at the receiving hospital.\n\nAltitude decompression sickness\nTreatment and management may vary depending on the grade or form of decompression sickness and the treating facility or organization. First aid at altitude is oxygen at the highest practicable concentration and earliest and largest practicable reduction in cabin altitude.\nGround-level 100% oxygen therapy is suggested for 2 hours following type-1 decompression sickness that occurs at altitude, if it resolves upon descent. In more severe cases, hyperbaric oxygen therapy following standard recompression protocols is indicated. Decompression sickness in aviation most commonly follows flights in non-pressurized aircraft, flights with cabin pressure fluctuations, or in individuals who fly after diving. Cases have also been reported after the use of altitude chambers. These are relatively rare clinical events.\n\nPrognosis\nImmediate treatment with 100% oxygen, followed by recompression in a hyperbaric chamber, will in most cases result in no long-term effects. However, permanent long-term injury from DCS is possible. Three-month follow-ups on diving accidents reported to DAN in 1987 showed 14.3% of the 268 divers surveyed had ongoing symptoms of Type II DCS, and 7% from Type I DCS. Long-term follow-ups showed similar results, with 16% having permanent neurological sequelae.\nLong term effects are dependent on both initial injury, and treatment. While almost all cases will resolve more quickly with treatment, milder cases may resolve adequately over time without recompression, where the damage is minor and the damage is not significantly aggravated by lack of treatment. In some cases the cost, inconvenience, and risk to the patient may make it appropriate not to evacuate to a hyperbaric treatment facility. These cases should be assessed by a specialist in diving medicine, which can generally be done remotely by telephone or internet.\nFor joint pain, the likely tissues affected depend on the symptoms, and the urgency of hyperbaric treatment will depend largely on the tissues involved.\n\nSharp, localised pain that is affected by movement suggests tendon or muscle injury, both of which will usually fully resolve with oxygen and anti-inflammatory medication.\nSharp, localised pain that is not affected by movement suggests local inflammation, which will also usually fully resolve with oxygen and anti-inflammatory medication.\nDeep, non-localised pain affected by movement suggests joint capsule tension, which is likely to fully resolve with oxygen and anti-inflammatory medication, though recompression will help it to resolve faster.\nDeep, non-localised pain not affected by movement suggests bone medulla involvement, with ischaemia due to blood vessel blockage and swelling inside the bone, which is mechanistically associated with osteonecrosis, and therefore it has been strongly recommended that these symptoms are treated with hyperbaric oxygen.\n\nEpidemiology\nThe incidence of decompression sickness is rare, estimated at 2.8 to 4 cases per 10,000 dives, with the risk 2.6 times greater for males than females. DCS affects approximately 1,000 U.S. scuba divers per year. In 1999, the Divers Alert Network (DAN) created \"Project Dive Exploration\" to collect data on dive profiles and incidents. From 1998 to 2002, they recorded 50,150 dives, from which 28 recompressions were required \u2013 although these will almost certainly contain incidents of arterial gas embolism (AGE) \u2013 a rate of about 0.05%.\nAround 2013, Honduras had the highest number of decompression-related deaths and disabilities in the world, caused by unsafe practices in lobster diving among the indigenous Miskito people, who face great economic pressures. At that time it was estimated that in the country over 2000 divers had been injured and 300 others had died since the 1970s.\n\nTimeline\n1670: Robert Boyle demonstrated that a reduction in ambient pressure could lead to bubble formation in living tissue. This description of a bubble forming in the eye of a viper subjected to a near vacuum was the first recorded description of decompression sickness.\n1769: Giovanni Morgagni described the post mortem findings of air in cerebral circulation and surmised that this was the cause of death.\n1840: Charles Pasley, who was involved in the recovery of the sunken warship HMS Royal George, commented that, of those having made frequent dives, \"not a man escaped the repeated attacks of rheumatism and cold\".\n1841: First documented case of decompression sickness, reported by a mining engineer who observed pain and muscle cramps among coal miners working in mine shafts air-pressurized to keep water out.\n1854: Decompression sickness reported and one resulting death of caisson workers on the Royal Albert Bridge.\n1867: Panamanian pearl divers using the revolutionary Sub Marine Explorer submersible repeatedly experienced \"fever\" due to rapid ascents. Continued sickness led to the vessel's abandonment in 1869.\n1870: Bauer published outcomes of 25 paralyzed caisson workers.\nFrom 1870 to 1910, all prominent features were established. Explanations at the time included: cold or exhaustion causing reflex spinal cord damage; electricity cause by friction on compression; or organ congestion; and vascular stasis caused by decompression. \n1871: The Eads Bridge in St Louis employed 352 compressed air workers including Alphonse Jaminet as the physician in charge. There were 30 seriously injured and 12 fatalities. Jaminet himself developed decompression sickness and his personal description was the first such recorded. According to Divers Alert Network, in its Inert Gas Exchange, Bubbles and Decompression Theory course, this is where \"bends\" was first used to refer to DCS.\n1872: The similarity between decompression sickness and iatrogenic air embolism as well as the relationship between inadequate decompression and decompression sickness was noted by Friedburg. He suggested that intravascular gas was released by rapid decompression and recommended: slow compression and decompression; four-hour working shifts; limit to maximum pressure of 44.1 psig (4 atm); using only healthy workers; and recompression treatment for severe cases.\n1873: Andrew Smith first used the term \"caisson disease\" describing 110 cases of decompression sickness as the physician in charge during construction of the Brooklyn Bridge. The project employed 600 compressed air workers. Recompression treatment was not used. The project chief engineer Washington Roebling had caisson disease, and endured the after-effects of the disease for the rest of his life. During this project, decompression sickness became known as \"The Grecian Bends\" or simply \"the bends\" because affected individuals characteristically bent forward at the hips: this is possibly reminiscent of a then popular women's fashion and dance maneuver known as the Grecian Bend.\n1890: During construction of the Hudson River Tunnel contractor's agent Ernest William Moir pioneered the use of an airlock chamber for treatment.\n1900: Leonard Hill used a frog model to prove that decompression causes bubbles and that recompression resolves them. Hill advocated linear or uniform decompression profiles. This type of decompression is used today by saturation divers. His work was financed by Augustus Siebe and the Siebe Gorman Company.\n1904: Tunnel building to and from Manhattan Island caused over 3,000 injuries and over 30 deaths which led to laws requiring PSI limits and decompression rules for \"sandhogs\" in the United States.\n1904: Siebe and Gorman in conjunction with Leonard Hill developed and produced a closed bell in which a diver can be decompressed at the surface. \n1908: \"The Prevention of Compressed Air Illness\" was published by JS Haldane, Boycott and Damant recommending staged decompression. These tables were accepted for use by the Royal Navy.\n1914\u201316: Experimental decompression chambers were in use on land and aboard ship.\n1924: The US Navy published the first standardized recompression procedure.\n1930s: Albert R Behnke separated the symptoms of Arterial Gas Embolism (AGE) from those of DCS.\n1935: Behnke et al. experimented with oxygen for recompression therapy.\n1937: Behnke introduced the \"no-stop\" decompression tables.\n1941: Altitude DCS is treated with hyperbaric oxygen for the first time.\n1944: US Navy published hyperbaric treatment tables \"Long Air Recompression Table with Oxygen\" and \"Short Oxygen Recompression Table\", both using 100% oxygen below 60 fsw (18 msw)\n1945: Field results showed that the 1944 oxygen treatment table was not yet satisfactory, so a series of tests were conducted by staff from the Navy Medical Research Institute and the Navy Experimental Diving Unit using human subjects to verify and modify the treatment tables. Tests were conducted using the 100-foot air-oxygen treatment table and the 100-foot air treatment table, which were found to be satisfactory. Other tables were extended until they produced satisfactory results. The resulting tables were used as the standard treatment for the next 20 years, and these tables and slight modifications were adopted by other navies and industry. Over time, evidence accumulated that the success of these table for severe decompression sickness was not very good.\n1957: Robert Workman established a new method for calculation of decompression requirements (M-values).\n1959: The \"SOS Decompression Meter\", a submersible mechanical device that simulated nitrogen uptake and release, was introduced.\n1960: FC Golding et al. split the classification of DCS into Type 1 and 2.\n1965: Low success rates of the existing US Navy treatment tables led to the development of the oxygen treatment table by Goodman and Workman in 1965, variations of which are still in general use as the definitive treatment for most cases of decompression sickness.\n1965: LeMessurier and Hills published a paper on A thermodynamic approach arising from a study on Torres Strait diving techniques which suggests that decompression by conventional models results in bubble formation which is then eliminated by re-dissolving at the decompression stops.\n1976 \u2013 M.P. Spencer showed that the sensitivity of decompression testing is increased by the use of ultrasonic methods which can detect mobile venous bubbles before symptoms of DCS emerge.\n1982: Paul K Weathersby, Louis D Homer and Edward T Flynn introduce survival analysis into the study of decompression sickness.\n1983: Orca produced the \"EDGE\", a personal dive computer, using a microprocessor to calculate nitrogen absorption for twelve tissue compartments.\n1984: Albert A B\u00fchlmann released his book \"Decompression\u2013Decompression Sickness\", which detailed his deterministic model for calculation of decompression schedules.\n1989: The advent of dive computers had not been widely accepted, but after the 1989 AAUS Dive computer workshop published a group consensus list of recommendations for the use of dive computers in scientific diving, most opposition to dive computers dissipated, numerous new models were introduced, the technology dramatically improved and dive computers became standard scuba diving equipment. Over time, some of the recommendations became irrelevant as the technology improved.\n2000: HydroSpace Engineering developed the HS Explorer, a Trimix computer with optional PO2 monitoring and twin decompression algorithms, Buhlmann, and the first full real time RGBM implementation.\n2001: The US Navy approved the use of Cochran NAVY decompression computer with the VVAL 18 Thalmann algorithm for Special Warfare operations.\nBy 2010: The use of dive computers for decompression status tracking was virtually ubiquitous among recreational divers and widespread in scientific diving.\n2018: A group of diving medical experts issued a consensus guideline on pre-hospital decompression sickness management and concluded that in-water recompression is a valid and effective emergency treatment where a chamber is not available, but is only appropriate in groups that have been trained and are competent in the skills required for IWR and have appropriate equipment.\n2023: The animal rights group, PETA, says that it has successfully lobbied the Navy to end a pair of studies that involved subjecting sheep to conditions that simulated surfacing quickly from a great depth, causing them pain and sometimes leaving the animals paralyzed or dead.\n\nSociety and culture\nEconomics\nIn the United States, it is common for medical insurance not to cover treatment for the bends that is the result of recreational diving. This is because scuba diving is considered an elective and \"high-risk\" activity and treatment for decompression sickness is expensive. A typical stay in a recompression chamber will easily cost several thousand dollars, even before emergency transportation is included.\nIn the United Kingdom, treatment of DCS is provided by the National Health Service. This may occur either at a specialised facility or at a hyperbaric centre based within a general hospital.\n\nOther animals\nAnimals may also contract DCS, especially those caught in nets and rapidly brought to the surface. It has been documented in loggerhead turtles and likely in prehistoric marine animals as well. Modern reptiles are susceptible to DCS, and there is some evidence that marine mammals such as cetaceans and seals may also be affected. AW Carlsen has suggested that the presence of a right-left shunt in the reptilian heart may account for the predisposition in the same way as a patent foramen ovale does in humans.\n\nFootnotes\nSee also\nDecompression (diving) \u2013 Pressure reduction and its effects during ascent from depth\nDecompression illness \u2013 Disorders arising from ambient pressure reduction\nDecompression theory \u2013 Theoretical modelling of decompression physiology\nDiving disorders \u2013 Physiological disorders resulting from underwater diving\nInner ear decompression sickness \u2013 Medical condition caused by inert gas bubbles forming out of solution\nTaravana \u2013 Decompression sickness after breath-hold diving\n\nNotes\n1. ^a  autochthonous: formed or originating in the place where found.\n\nReferences\nSources\nExternal links\n\nDivers Alert Network: diving medicine articles Archived 8 December 2005 at the Wayback Machine\nDive Tables from the NOAA\nCDC \u2013 Decompression Sickness and Tunnel Workers \u2013 NIOSH Workplace Safety and Health Topic\nPathophysiology of decompression and acute dysbaric disorders\n\"Decompression Sickness\" on Medscape\nNorwegian Diving- and Treatment Tables. Tables and guidelines for surface orientated diving on air and nitrox. Tables and guidelines for treatment of decompression illness. Jan Risberg \u2022 Andreas M\u00f8llerl\u00f8kken \u2022 Olav Sande Eftedal. 12.8.2019","51":"Dimenhydrinate, sold under the brand name Dramamine, among others, is an over-the-counter medication used to treat motion sickness and nausea. Dimenhydrinate is a theoclate salt composed of diphenhydramine (an ethanolamine derivative) and 8-chlorotheophylline (a chlorinated theophylline derivative) in a 1:1 ratio.\nDimenhydrinate was introduced to the market by G.D. Searle in 1949.\n\nMedical uses\nDimenhydrinate is an over-the-counter (OTC) first-generation antihistamine indicated for the prevention and relief of nausea and vomiting from a number of causes, including motion-sickness and post-operative nausea.\n\nSide effects\nCommon side effects may include:\n\nDrowsiness\nDry mouth, nose, or throat\nConstipation\nBlurred vision\nFeeling restless or excited (especially in children)\nContinuous and\/or cumulative use of anticholinergic medications, including first-generation antihistamines, is associated with higher risk of cognitive decline and dementia in older people.\n\nPharmacology\nDiphenhydramine is the primary constituent of dimen\u00adhydrinate and dictates the primary effect. The main differences relative to pure diphen\u00adhydramine are a lower potency due to being combined with 8-chloro\u00adtheo\u00adphylline (by weight, dimen\u00adhydrinate is between 53% and 55.5% diphen\u00adhydramine) and the fact that the stimulant properties of 8-chloro\u00adtheo\u00adphylline help reduce the side effect of drowsiness brought on by diphen\u00adhydramine. Diphen\u00adhydramine is itself an H1 receptor antagonist that demonstrates anticholinergic activity.\n\nPharmacokinetics\nThe diphenhydramine component requires about 2 hours to reach peak concentration after either oral or sublingual administration of diphen\u00adhydrinate, and has a half-life of 5\u200a\u2013\u200a6 hours in healthy adults.\n\nRecreational use\nDimenhydrinate is recreationally used as a deliriant. Slang terms for Dramamine used this way include \"drama\", \"dime\", \"dime tabs\", \"D-Q\", \"substance D\", \"d-house\", and \"drams\". Abusing Dramamine is sometimes referred to as Dramatizing or \"going a dime a dozen\", a reference to the amount of Dramamine tablets generally necessary for a trip.\nMany users report a side-effect profile consistent with tropane alkaloid (e.g. atropine) poisoning as both show antagonism of muscarinic acetylcholine receptors in both the central and autonomic nervous system, which inhibits various signal transduction pathways.\nOther CNS effects occur within the limbic system and hippocampus, causing confusion and temporary amnesia due to decreased acetylcholine signaling. Toxicology also manifests in the autonomic nervous system, primarily at the neuromuscular junction, resulting in ataxia and extrapyramidal side effects and the feeling of heaviness in the legs, and at sympathetic post-ganglionic junctions, causing urinary retention, pupil dilation, tachycardia, irregular urination, and dry red skin caused by decreased exocrine gland secretions, and mucous membranes. Considerable overdosage can lead to myocardial infarction (heart attack), serious ventricular arrhythmias, coma, and death. Such a side effect profile is thought to give ethanolamine-class antihistamines a relatively low abuse liability. An antidote that can be used for dimenhydrinate poisoning is physostigmine.\n\nHistory\nDimenhydrinate (then known as Compound 1694) was being tested as a potential treatment for hay fever and hives at Johns Hopkins Hospital in 1947 by allergists Dr. Leslie Gay and Dr. Paul Carliner. Among those who received the drug was a pregnant woman who had suffered from motion sickness her entire life. She remained symptom-free if she took dimenhydrinate a few minutes before boarding a trolley, whereas the placebo was ineffective. To confirm these findings, the following year, G.D. Searle & Co. conducted a trial in which dimenhydrinate or placebo was given to U.S. troops crossing the Atlantic during \"a rough passage\" in a converted freight ship, the General Ballou, for 10 days as a rescue therapy for sea sickness. The findings were positive, as were the findings of a second trial of mostly women on the ship's return voyage. Gay and Carliner announced their discovery at a meeting of the Johns Hopkins Medical Society on February 14, 1949, as well as in the Bulletin of The Johns Hopkins Hospital. The New York Times, the Baltimore Sun, and other national newspapers covered the discovery, and Dramamine was made available in drugstores later that year.\n\nBrand names\nDimenhydrinate is marketed under many brand names: in the U.S., Mexico, Turkey and Thailand as Dramamine; in Serbia as Dimigal; in Ukraine as Driminate; in Canada, Costa Rica, and India as Gravol; in Iceland as Gravamin; in Russia and Croatia as Dramina; in South Africa and Germany as Vomex; in Australia and Austria as Vertirosan; in Brazil as Dramin; in Colombia as Mareol; in Ecuador as Anautin; in Hungary as Daedalon; in Indonesia as Antimo; in Italy as Xamamina or Valontan; in Peru as Gravicoll; in Poland and Slovakia as Aviomarin; in Portugal as Viabom, Vomidrine, and Enjomin; in Spain as Biodramina; in Israel as Travamin; and in Pakistan as Gravinate.\n\nPopular culture\nModest Mouse produced a song titled \"Dramamine\" on their 1996 debut album This Is a Long Drive for Someone with Nothing to Think About. The song uses side effects of the drug as a metaphor for the deteriorating state of a personal relationship.\n\u201dThe Ending Of Dramamine\u201d is the opening track of the album How To Leave Town by Car Seat Headrest.\n\nReferences\nExternal links\n Media related to Dimenhydrinate at Wikimedia Commons","52":"Dexamethasone is a fluorinated glucocorticoid medication used to treat rheumatic problems, a number of skin diseases, severe allergies, asthma, chronic obstructive lung disease, croup, brain swelling, eye pain following eye surgery, superior vena cava syndrome (a complication of some forms of cancer), and along with antibiotics in tuberculosis. In adrenocortical insufficiency, it may be used in combination with a mineralocorticoid medication such as fludrocortisone. In preterm labor, it may be used to improve outcomes in the baby. It may be given by mouth, as an injection into a muscle, as an injection into a vein, as a topical cream or ointment for the skin or as a topical ophthalmic solution to the eye. The effects of dexamethasone are frequently seen within a day and last for about three days.\nThe long-term use of dexamethasone may result in thrush, bone loss, cataracts, easy bruising, or muscle weakness. It is in pregnancy category C in the United States, meaning that it should only be used when the benefits are predicted to be greater than the risks. In Australia, the oral use is category A, meaning it has been frequently used in pregnancy and not been found to cause problems to the baby. It should not be taken when breastfeeding. Dexamethasone has anti-inflammatory and immunosuppressant effects.\nDexamethasone was first synthesized in 1957 by Philip Showalter Hench and was approved for medical use in 1958. It is on the World Health Organization's List of Essential Medicines. In 2021, it was the 260th most commonly prescribed medication in the United States, with more than 1 million prescriptions. It is available as a generic medication.\n\nMedical uses\nAnti-inflammatory\nDexamethasone is used to treat many inflammatory and autoimmune disorders, such as rheumatoid arthritis and bronchospasm. Idiopathic thrombocytopenic purpura, a decrease in numbers of platelets due to an immune problem, responds to 40 mg daily for four days; it may be administered in 14-day cycles. It is unclear whether dexamethasone in this condition is significantly better than other glucocorticoids.\nIt is also given in small amounts before and\/or after some forms of dental surgery, such as the extraction of the wisdom teeth, an operation which often causes puffy, swollen cheeks.\nDexamethasone is commonly given as a treatment for croup in children, as a single dose can reduce the swelling of the airway to improve breathing and reduce discomfort.\nIt is injected into the heel when treating plantar fasciitis, sometimes in conjunction with triamcinolone acetonide.\nIt is useful to counteract allergic anaphylactic shock, if given in high doses.\nIt is present in certain eye drops \u2013 particularly after eye surgery \u2013 and as a nasal spray, and certain ear drops (can be combined with an antibiotic and an antifungal). Dexamethasone intravitreal steroid implants have been approved by the US Food and Drug Administration (FDA) to treat ocular conditions such as diabetic macular edema, central retinal vein occlusion, and uveitis. However, the evidence is poor quality relating to the treatment of uveitis, with the potential side effects (cataract progression and raised intraocular pressure) being significant, and the benefits not certainly greater than standard treatment. Dexamethasone has also been used with antibiotics to treat acute endophthalmitis.\nDexamethasone is used in transvenous screw-in cardiac pacing leads to minimize the inflammatory response of the myocardium. The steroid is released into the myocardium as soon as the screw is extended and can play a significant role in minimizing the acute pacing threshold due to the reduction of inflammatory response. The typical quantity present in a lead tip is less than 1.0 mg.\nDexamethasone may be administered before antibiotics in cases of bacterial meningitis. Gram-negative bacteria \u2014 to which the causative agent of bacterial meningitis, neisseria meningitidis, belongs \u2014 have highly immunogenic lipopolysaccharides as a component of their cell membrane and trigger a strong inflammatory response. Pre-administration of dexamethasone before the administration of antibiotics act to reduce that response, thus reducing hearing loss and neurological damage.\n\nCancer\nPeople with cancer undergoing chemotherapy are often given dexamethasone to counteract certain side effects of their antitumor treatments. Dexamethasone can increase the antiemetic effect of 5-HT3 receptor antagonists, such as ondansetron. The exact mechanism of this interaction is not well-defined, but it has been theorized that this effect may be due to, among many other causes, inhibition of prostaglandin synthesis, anti-inflammatory effects, immunosuppressive effects, decreased release of endogenous opioids, or a combination of the aforementioned.\nIn brain tumors (primary or metastatic), dexamethasone is used to counteract the development of edema, which could eventually compress other brain structures. It is also given in cord compression, where a tumor is compressing the spinal cord. Evidence on the safety and efficacy of using dexamethasone to treat malignant brain tumors is not clear.\nDexamethasone is also used as a direct chemotherapeutic agent in certain hematological malignancies, especially in the treatment of multiple myeloma, in which dexamethasone is given alone or in combination with other chemotherapeutic drugs, including most commonly with thalidomide (Thal-dex), lenalidomide, bortezomib (Velcade, Vel-dex), or a combination of doxorubicin (Adriamycin) and vincristine or bortezomib\/lenalidomide\/dexamethasone.\n\nCOVID-19\nDexamethasone is recommended by the National Health Service in the UK and the National Institutes of Health (NIH) in the US for people with COVID-19 who need either mechanical ventilation or supplemental oxygen (without ventilation).\nThe Infectious Diseases Society of America (IDSA) guideline panel suggests the use of glucocorticoids for people with severe COVID-19, defined as people with SpO2 \u226494% on room air, and those who require supplemental oxygen, mechanical ventilation, or extracorporeal membrane oxygenation (ECMO). The IDSA recommends against the use of glucocorticoids for those with COVID-19 without hypoxemia requiring supplemental oxygen.\nThe World Health Organization (WHO) recommends systemic corticosteroids rather than no systemic corticosteroids for the treatment of people with COVID-19 (strong recommendation, based on moderate certainty evidence). The WHO suggests not to use corticosteroids in the treatment of people with non-severe COVID-19 (conditional recommendation, based on low certainty evidence).\nThe Oxford University RECOVERY Trial issued a press release announcing preliminary results that the drug could reduce deaths by about a third in participants on ventilators and by about a fifth in participants on oxygen; it did not benefit people who did not require respiratory support. A meta-analysis of seven clinical trials of critically ill COVID-19 participants, each treated with one of three different corticosteroids found a statistically significant reduction in death. The largest reduction was obtained with dexamethasone (36% compared to placebo).\nIn September 2020, the European Medicines Agency (EMA) endorsed the use of dexamethasone in adults and adolescents, from twelve years of age and weighing at least 40 kilograms (88 lb), who require supplemental oxygen therapy. Dexamethasone can be taken by mouth or given as an injection or infusion (drip) into a vein.\nIn November 2020, the Public Health Agency of Canada's Clinical Pharmacology Task Group recommended dexamethasone for hospitalized patients requiring mechanical ventilation. Although dexamethasone, and other glucocorticoids, reduce mortality in COVID-19 they have also been associated with an increased risk of secondary infections, secondary infections being a significant issue in critically ill COVID-19 patients.\nThe mechanism of action of dexamethasone involves suppression of late-stage interferon type I programs in severe COVID-19 patients.\n\nSurgery\nDexamethasone is used fairly regularly to prevent postoperative nausea and vomiting, manage pain (potentially reduce the amount of pain medications needed), and to help reduce the amount of time spent in the hospital following surgery. It is often given as a single dose during surgery by intravenous administration. The adverse effects of taking steroids after surgery on wound healing, blood sugar levels, and in people with diabetes are not completely understood, however it is likely that dexamethasone does not increase the risk of postoperative infections.\n\nEndocrine\nDexamethasone is the treatment for the very rare disorder of glucocorticoid resistance.\nIn adrenal insufficiency and Addison's disease, dexamethasone is prescribed when the patient does not respond well to prednisone or methylprednisolone.\nIt can be used in congenital adrenal hyperplasia in older adolescents and adults to suppress adrenocorticotropic hormone (ACTH) production. It is typically given at night.\n\nPregnancy\nDexamethasone may be given to women at risk of delivering prematurely to promote maturation of the fetus's lungs. This administration, given from one day to one week before delivery, has been associated with low birth weight, although not with increased rates of neonatal death.\nDexamethasone has also been used during pregnancy as an off-label prenatal treatment for the symptoms of congenital adrenal hyperplasia (CAH) in female babies. CAH causes a variety of physical abnormalities, notably ambiguous genitalia. Early prenatal CAH treatment has been shown to reduce some CAH symptoms, but it does not treat the underlying congenital disorder. This use is controversial: it is inadequately studied, only around one in ten of the fetuses of women treated are at risk of the condition, and serious adverse events have been documented. Experimental use of dexamethasone in pregnancy for fetal CAH treatment was discontinued in Sweden when one in five cases had adverse events.\nA small clinical trial found long-term effects on verbal working memory among the small group of children treated prenatally, but the small number of test subjects means the study cannot be considered definitive.\n\nHigh-altitude illnesses\nDexamethasone is used in the treatment of high-altitude cerebral edema (HACE), as well as high-altitude pulmonary edema (HAPE). It is commonly carried on mountain-climbing expeditions to help climbers deal with complications of altitude sickness.\n\nNausea and vomiting\nIntravenous dexamethasone is effective for prevention of nausea and vomiting in people who had surgery and whose post-operative pain was treated with long-acting spinal or epidural spinal opioids.\nThe combination of dexamethasone and a 5-HT3 receptor antagonist such as ondansetron is more effective than a 5-HT3 receptor antagonist alone in preventing postoperative nausea and vomiting.\n\nSore throat\nA single dose of dexamethasone or another steroid speeds improvement of a sore throat.\n\nContraindications\nContraindications of dexamethasone include, but are not limited to:\n\nUncontrolled infections\nKnown hypersensitivity to dexamethasone\nCerebral malaria\nSystemic fungal infection\nConcurrent treatment with live virus vaccines (including smallpox vaccine)\n\nAdverse effects\nThe exact incidence of the adverse effects of dexamethasone is not available, hence estimates have been made as to the incidence of the adverse effects below based on the adverse effects of related corticosteroids and on available documentation on dexamethasone.\n\nCommon\nUnknown frequency\nWithdrawal\nSudden withdrawal after long-term treatment with corticosteroids can lead to:\n\nInteractions\nKnown drug interactions include:\n\nInducers of hepatic microsomal enzymes such as barbiturates, phenytoin, and rifampicin can reduce the half-life of dexamethasone.\nCotreatment with oral contraceptives can increase its volume of distribution.\n\nPharmacology\nPharmacodynamics\nAs a glucocorticoid, dexamethasone is an agonist of the glucocorticoid receptor (GR). It is highly selective for the GR over the mineralocorticoid receptor (MR), and in relation to this, has minimal mineralocorticoid activity. This is in contrast to endogenous corticosteroids like cortisol, which bind to and activate both the GR and the MR. Dexamethasone is 25 times more potent than hydrocortisone (cortisol) as a glucocorticoid. Its affinity (Ki) for the GR was about 1.2 nM in one study.\nThe activation of the GR by dexamethasone results in dose-dependent suppression of the hypothalamic\u2013pituitary\u2013adrenal axis (HPA axis) and of production of endogenous corticosteroids by the adrenal glands, thereby reducing circulating endogenous concentrations of corticosteroids like cortisol and corticosterone.\nDexamethasone poorly penetrates the blood\u2013brain barrier into the central nervous system due to binding to P-glycoprotein. However, higher doses of dexamethasone override the export capacity of P-glycoprotein and enter the brain to produce central activation of GRs. In conjunction with the suppression of endogenous corticosteroids by dexamethasone, this results in skewed ratios of activation of peripheral versus central GRs as well as skewed ratios of activation of GRs versus MRs when compared to non-synthetic corticosteroids. These differences can have significant clinical relevance.\n\nChemistry\nDexamethasone is a synthetic pregnane corticosteroid and derivative of cortisol (hydrocortisone) and is also known as 1-dehydro-9\u03b1-fluoro-16\u03b1-methylhydrocortisone or as 9\u03b1-fluoro-11\u03b2,17\u03b1,21-trihydroxy-16\u03b1-methylpregna-1,4-diene-3,20-dione. The molecular and crystal structure of dexamethasone has been determined by X-ray crystallography. It is a stereoisomer of betamethasone, the two compounds differing only in the spatial configuration of the methyl group at position 16 (see steroid nomenclature).\n\nSynthesis\nTo synthesize dexamethasone, 16\u03b2-methylprednisolone acetate is dehydrated to the 9,11-dehydro derivative. This is then reacted with a source of hypobromite, such as basic N-bromosuccinimide, to form the 9\u03b1-bromo-11\u03b2-hydrin derivative, which is then ring-closed to an epoxide. A ring-opening reaction with hydrogen fluoride in tetrahydrofuran gives dexamethasone.\n\nHistory\nDexamethasone was first synthesized by Philip Showalter Hench in 1957. It was introduced for medical use in 1958.\nOn 16 June 2020, the RECOVERY Trial announced preliminary results stating that dexamethasone improves survival rates of hospitalized patients with COVID-19 receiving oxygen or on a ventilator. Benefits were only observed in patients requiring respiratory support; those who did not require breathing support saw a worse survival rate than the control group, although the difference may have been due to chance.\nA preprint containing the full dataset was published on 22 June 2020, and demand for dexamethasone surged after publication of the preprint. The preliminary report was published in The New England Journal of Medicine on 18 July 2020. The final report was published in February 2021.\nThe World Health Organization (WHO) states that dexamethasone should be reserved for seriously ill and critical patients receiving COVID-19 treatment in a hospital setting, and the WHO Director-General stated that \"WHO emphasizes that dexamethasone should only be used for patients with severe or critical disease, under close clinical supervision. There is no evidence this drug works for patients with mild disease or as a preventative measure, and it could cause harm.\" In July 2020, the WHO stated they are in the process of updating treatment guidelines to include dexamethasone or other steroids. In September 2020, the WHO released updated guidance on using corticosteroids for COVID-19.\nIn July 2020, the European Medicines Agency (EMA) started reviewing results from the RECOVERY study arm that involved the use of dexamethasone in the treatment of patients with COVID-19 admitted to the hospital to provide an opinion on the results and in particular the potential use of dexamethasone for the treatment of adults with COVID-19. In September 2020, the EMA received an application for marketing authorization of dexamethasone for COVID-19.\n\nSociety and culture\nPrice\nDexamethasone is inexpensive. In the United States a month of medication is typically priced less than US$25. In India, a course of treatment for preterm labor is about US$0.50. The drug is available in most areas of the world.\n\nNonmedical use\nDexamethasone is given in legal Bangladesh brothels to prostitutes not yet of legal age, causing weight gain aimed at making them appear older and healthier to customers and police.\nDexamethasone and most glucocorticoids are banned by sporting bodies including the World Anti-Doping Agency.\n\nVeterinary use\nCombined with marbofloxacin CAS number 115550-35-1and clotrimazole, dexamethasone is available under the name Aurizon, CAS number 50-02-2, and used to treat difficult ear infections, especially in dogs. It can also be combined with trichlormethiazide to treat horses with swelling of distal limbs and general bruising.\n\nReferences\nExternal links\n\n\"Dexamethasone Ophthalmic\". MedlinePlus.\n\"Dexamethasone Injection\". MedlinePlus.","53":"In healthcare, a differential diagnosis (DDx) is a method of analysis that distinguishes a particular disease or condition from others that present with similar clinical features. Differential diagnostic procedures are used by clinicians to diagnose the specific disease in a patient, or, at least, to consider any imminently life-threatening conditions. Often, each individual option of a possible disease is called a differential diagnosis (e.g., acute bronchitis could be a differential diagnosis in the evaluation of a cough, even if the final diagnosis is common cold).\nMore generally, a differential diagnostic procedure is a systematic diagnostic method used to identify the presence of a disease entity where multiple alternatives are possible. This method may employ algorithms, akin to the process of elimination, or at least a process of obtaining information that decreases the \"probabilities\" of candidate conditions to negligible levels, by using evidence such as symptoms, patient history, and medical knowledge to adjust epistemic confidences in the mind of the diagnostician (or, for computerized or computer-assisted diagnosis, the software of the system).\nDifferential diagnosis can be regarded as implementing aspects of the hypothetico-deductive method, in the sense that the potential presence of candidate diseases or conditions can be viewed as hypotheses that clinicians further determine as being true or false.\nA differential diagnosis is also commonly used within the field of psychiatry\/psychology, where two different diagnoses can be attached to a patient who is exhibiting symptoms that could fit into either diagnosis. For example, a patient who has been diagnosed with bipolar disorder may also be given a differential diagnosis of borderline personality disorder, given the similarity in the symptoms of both conditions.\nStrategies used in preparing a differential diagnosis list vary with the experience of the healthcare provider. While novice providers may work systemically to assess all possible explanations for a patient's concerns, those with more experience often draw on clinical experience and pattern recognition to protect the patient from delays, risks, and cost of inefficient strategies or tests. Effective providers utilize an evidence-based approach, complementing their clinical experience with knowledge from clinical research.\n\nGeneral components\nA differential diagnosis has four general steps. The clinician will:\n\nGather relevant information about the patient and create a symptoms list.\nList possible causes (candidate conditions) for the symptoms. The list need not be in writing.\nPrioritize the list by balancing the risks of a diagnosis with the probability.  These are subjective, not objective parameters.\nPerform tests to determine the actual diagnosis.  This is known by the colloquial phrase \"to Rule Out\".  Even after the process, the diagnosis is not clear.  The clinician again considers the risks and may treat them empirically, often called \"Educated Best Guess.\"\nA mnemonic to help in considering multiple possible pathological processes is VINDICATEM:\n\nVascular\nInflammatory \/ Infectious\nNeoplastic\nDegenerative \/ Deficiency \/ Drugs\nIdiopathic \/ Intoxication \/ Iatrogenic\nCongenital\nAutoimmune \/ Allergic \/ Anatomic\nTraumatic\nEndocrine \/ Environmental\nMetabolic\n\nSpecific methods\nThere are several methods for differential diagnostic procedures and several variants among those. Furthermore, a differential diagnostic procedure can be used concomitantly or alternately with protocols, guidelines, or other diagnostic procedures (such as pattern recognition or using medical algorithms).\nFor example, in case of medical emergency, there may not be enough time to do any detailed calculations or estimations of different probabilities, in which case the ABC protocol (airway, breathing and circulation) may be more appropriate. Later, when the situation is less acute, a more comprehensive differential diagnostic procedure may be adopted.\nThe differential diagnostic procedure may be simplified if a \"pathognomonic\" sign or symptom is found (in which case it is almost certain that the target condition is present) or in the absence of a sine qua non sign or symptom (in which case it is almost certain that the target condition is absent).\nA diagnostician can be selective, considering first those disorders that are more likely (a probabilistic approach), more serious if left undiagnosed and untreated (a prognostic approach), or more responsive to treatment if offered (a pragmatic approach). Since the subjective probability of the presence of a condition is never exactly 100% or 0%, the differential diagnostic procedure may aim at specifying these various probabilities to form indications for further action.\nThe following are two methods of differential diagnosis, being based on epidemiology and likelihood ratios, respectively.\n\nEpidemiology-based method\nOne method of performing a differential diagnosis by epidemiology aims to estimate the probability of each candidate condition by comparing their probabilities to have occurred in the first place in the individual. It is based on probabilities related both to the presentation (such as pain) and probabilities of the various candidate conditions (such as diseases).\n\nTheory\nThe statistical basis for differential diagnosis is Bayes' theorem. As an analogy, when a die has landed the outcome is certain by 100%, but the probability that it Would Have Occurred in the First Place (hereafter abbreviated WHOIFP) is still 1\/6. In the same way, the probability that a presentation or condition would have occurred in the first place in an individual (WHOIFPI) is not same as the probability that the presentation or condition has occurred in the individual, because the presentation has occurred by 100% certainty in the individual. Yet, the contributive probability fractions of each condition are assumed the same, relatively:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  \n                    \n                      Pr\n                      (\n                      \n                        Presentation is caused by condition in individual\n                      \n                      )\n                    \n                    \n                      Pr\n                      (\n                      \n                        Presentation has occurred in individual\n                      \n                      )\n                    \n                  \n                \n                =\n                \n                  \n                    \n                      Pr\n                      (\n                      \n                        Presentation WHOIFPI by condition\n                      \n                      )\n                    \n                    \n                      Pr\n                      (\n                      \n                        Presentation WHOIFPI\n                      \n                      )\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&{\\frac {\\Pr({\\text{Presentation is caused by condition in individual}})}{\\Pr({\\text{Presentation has occurred in individual}})}}={\\frac {\\Pr({\\text{Presentation WHOIFPI by condition}})}{\\Pr({\\text{Presentation WHOIFPI}})}}\\end{aligned}}}\n  \n\nwhere:\n\nPr(Presentation is caused by condition in individual) is the probability that the presentation is caused by condition in the individual; condition without further specification refers to any candidate condition\nPr(Presentation has occurred in individual) is the probability that the presentation has occurred in the individual, which can be perceived and thereby set at 100%\nPr(Presentation WHOIFPI by condition) is the probability that the presentation Would Have Occurred in the First Place in the Individual by condition\nPr(Presentation WHOIFPI) is the probability that the presentation Would Have Occurred in the First Place in the Individual\nWhen an individual presents with a symptom or sign, Pr(Presentation has occurred in individual) is 100% and can therefore be replaced by 1, and can be ignored since division by 1 does not make any difference:\n\n  \n    \n      \n        Pr\n        (\n        \n          Presentation is caused by condition in individual\n        \n        )\n        =\n        \n          \n            \n              Pr\n              (\n              \n                Presentation WHOIFPI by condition\n              \n              )\n            \n            \n              Pr\n              (\n              \n                Presentation WHOIFPI\n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\Pr({\\text{Presentation is caused by condition in individual}})={\\frac {\\Pr({\\text{Presentation WHOIFPI by condition}})}{\\Pr({\\text{Presentation WHOIFPI}})}}}\n  \n\nThe total probability of the presentation to have occurred in the individual can be approximated as the sum of the individual candidate conditions:\n\n  \n    \n      \n        \n          \n            \n              \n                Pr\n                (\n                \n                  Presentation WHOIFPI\n                \n                )\n              \n              \n                \n                =\n                Pr\n                (\n                \n                  Presentation WHOIFPI by condition 1\n                \n                )\n              \n            \n            \n              \n              \n                \n                \n\n                \n                +\n                Pr\n                (\n                \n                  Presentation WHOIFPI by condition 2\n                \n                )\n              \n            \n            \n              \n              \n                \n                \n\n                \n                +\n                Pr\n                (\n                \n                  Presentation WHOIFPI by condition 3\n                \n                )\n                +\n                \n                  etc.\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\Pr({\\text{Presentation WHOIFPI}})&=\\Pr({\\text{Presentation WHOIFPI by condition 1}})\\\\&{}+\\Pr({\\text{Presentation WHOIFPI by condition 2}})\\\\&{}+\\Pr({\\text{Presentation WHOIFPI by condition 3}})+{\\text{etc.}}\\end{aligned}}}\n  \n\nAlso, the probability of the presentation to have been caused by any candidate condition is proportional to the probability of the condition, depending on what rate it causes the presentation:\n\n  \n    \n      \n        Pr\n        (\n        \n          Presentation WHOIFPI by condition\n        \n        )\n        =\n        Pr\n        (\n        \n          Condition WHOIFPI\n        \n        )\n        \u22c5\n        \n          r\n          \n            \n              condition\n            \n            \u2192\n            \n              presentation\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\Pr({\\text{Presentation WHOIFPI by condition}})=\\Pr({\\text{Condition WHOIFPI}})\\cdot r_{{\\text{condition}}\\rightarrow {\\text{presentation}}},}\n  \n\nwhere:\n\nPr(Presentation WHOIFPI by condition) is the probability that the presentation Would Have Occurred in the First Place in the Individual by condition\nPr(Condition WHOIFPI) is the probability that the condition Would Have Occurred in the First Place in the Individual\nrCondition \u2192 presentation is the rate at which a condition causes the presentation, that is, the fraction of people with conditions that manifests with the presentation.\nThe probability that a condition would have occurred in the first place in an individual is approximately equal to that of a population that is as similar to the individual as possible except for the current presentation, compensated where possible by relative risks given by known risk factor that distinguish the individual from the population:\n\n  \n    \n      \n        Pr\n        (\n        \n          Condition WHOIFPI\n        \n        )\n        \u2248\n        R\n        \n          R\n          \n            condition\n          \n        \n        \u22c5\n        Pr\n        (\n        \n          Condition in population\n        \n        )\n        ,\n      \n    \n    {\\displaystyle \\Pr({\\text{Condition WHOIFPI}})\\approx RR_{\\text{condition}}\\cdot \\Pr({\\text{Condition in population}}),}\n  \n\nwhere:\n\nPr(Condition WHOIFPI) is the probability that the condition Would Have Occurred in the First Place in the Individual\nRRcondition is the relative risk for condition conferred by known risk factors in the individual that are not present in the population\nPr(Condition in population) is the probability that the condition occurs in a population that is as similar to the individual as possible except for the presentation\nThe following table demonstrates how these relations can be made for a series of candidate conditions:\n\nOne additional \"candidate condition\" is the instance of there being no abnormality, and the presentation is only a (usually relatively unlikely) appearance of a basically normal state. Its probability in the population (P(No abnormality in population)) is complementary to the sum of probabilities of \"abnormal\" candidate conditions.\n\nExample\nThis example case demonstrates how this method is applied but does not represent a guideline for handling similar real-world cases. Also, the example uses relatively specified numbers with sometimes several decimals, while in reality, there are often simply rough estimations, such as of likelihoods being very high, high, low or very low, but still using the general principles of the method.\nFor an individual (who becomes the \"patient\" in this example), a blood test of, for example, serum calcium shows a result above the standard reference range, which, by most definitions, classifies as hypercalcemia, which becomes the \"presentation\" in this case. A clinician (who becomes the \"diagnostician\" in this example), who does not currently see the patient, gets to know about his finding.\nBy practical reasons, the clinician considers that there is enough test indication to have a look at the patient's medical records. For simplicity, let's say that the only information given in the medical records is a family history of primary hyperparathyroidism (here abbreviated as PH), which may explain the finding of hypercalcemia. For this patient, let's say that the resultant hereditary risk factor is estimated to confer a relative risk of 10 (RRPH = 10).\nThe clinician considers that there is enough motivation to perform a differential diagnostic procedure for the finding of hypercalcemia. The main causes of hypercalcemia are primary hyperparathyroidism (PH) and cancer, so for simplicity, the list of candidate conditions that the clinician could think of can be given as:\n\nPrimary hyperparathyroidism (PH)\nCancer\nOther diseases that the clinician could think of (which is simply termed \"other conditions\" for the rest of this example)\nNo disease (or no abnormality), and the finding is caused entirely by statistical variability\nThe probability that 'primary hyperparathyroidism' (PH) would have occurred in the first place in the individual (P(PH WHOIFPI)) can be calculated as follows:\nLet's say that the last blood test taken by the patient was half a year ago and was normal and that the incidence of primary hyperparathyroidism in a general population appropriately matches the individual (except for the presentation and mentioned heredity) is 1 in 4000 per year. Ignoring more detailed retrospective analyses (such as including speed of disease progress and lag time of medical diagnosis), the time-at-risk for having developed primary hyperparathyroidism can roughly be regarded as being the last half-year because a previously developed hypercalcemia would probably have been caught up by the previous blood test. This corresponds to a probability of primary hyperparathyroidism (PH) in the population of:\n\n  \n    \n      \n        Pr\n        (\n        \n          PH in population\n        \n        )\n        =\n        0.5\n        \n           years\n        \n        \u22c5\n        \n          \n            1\n            4000 per year\n          \n        \n        =\n        \n          \n            1\n            8000\n          \n        \n      \n    \n    {\\displaystyle \\Pr({\\text{PH in population}})=0.5{\\text{ years}}\\cdot {\\frac {1}{\\text{4000 per year}}}={\\frac {1}{8000}}}\n  \n\nWith the relative risk conferred from the family history, the probability that primary hyperparathyroidism (PH) would have occurred in the first place in the individual given from the currently available information becomes:\n\n  \n    \n      \n        Pr\n        (\n        \n          PH WHOIFPI\n        \n        )\n        \u2248\n        R\n        \n          R\n          \n            P\n            H\n          \n        \n        \u22c5\n        Pr\n        (\n        \n          PH in population\n        \n        )\n        =\n        10\n        \u22c5\n        \n          \n            1\n            8000\n          \n        \n        =\n        \n          \n            1\n            800\n          \n        \n        =\n        0.00125\n      \n    \n    {\\displaystyle \\Pr({\\text{PH WHOIFPI}})\\approx RR_{PH}\\cdot \\Pr({\\text{PH in population}})=10\\cdot {\\frac {1}{8000}}={\\frac {1}{800}}=0.00125}\n  \n\nPrimary hyperparathyroidism can be assumed to cause hypercalcemia essentially 100% of the time (rPH \u2192 hypercalcemia = 1), so this independently calculated probability of primary hyperparathyroidism (PH) can be assumed to be the same as the probability of being a cause of the presentation:\n\n  \n    \n      \n        \n          \n            \n              \n                Pr\n                (\n                \n                  Hypercalcemia WHOIFPI by PH\n                \n                )\n              \n              \n                \n                =\n                Pr\n                (\n                \n                  PH WHOIFPI\n                \n                )\n                \u22c5\n                \n                  r\n                  \n                    \n                      PH\n                    \n                    \u2192\n                    \n                      hypercalcemia\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                0.00125\n                \u22c5\n                1\n                =\n                0.00125\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\Pr({\\text{Hypercalcemia WHOIFPI by PH}})&=\\Pr({\\text{PH WHOIFPI}})\\cdot r_{{\\text{PH}}\\rightarrow {\\text{hypercalcemia}}}\\\\&=0.00125\\cdot 1=0.00125\\end{aligned}}}\n  \n\nFor cancer, the same time-at-risk is assumed for simplicity, and let's say that the incidence of cancer in the area is estimated at 1 in 250 per year, giving a population probability of cancer of:\n\n  \n    \n      \n        Pr\n        (\n        \n          cancer in population\n        \n        )\n        =\n        0.5\n        \n           years\n        \n        \u22c5\n        \n          \n            1\n            250 per year\n          \n        \n        =\n        \n          \n            1\n            500\n          \n        \n      \n    \n    {\\displaystyle \\Pr({\\text{cancer in population}})=0.5{\\text{ years}}\\cdot {\\frac {1}{\\text{250 per year}}}={\\frac {1}{500}}}\n  \n\nFor simplicity, let's say that any association between a family history of primary hyperparathyroidism and risk of cancer is ignored, so the relative risk for the individual to have contracted cancer in the first place is similar to that of the population (RRcancer = 1):\n\n  \n    \n      \n        Pr\n        (\n        \n          cancer WHOIFPI\n        \n        )\n        \u2248\n        R\n        \n          R\n          \n            cancer\n          \n        \n        \u22c5\n        Pr\n        (\n        \n          cancer in population\n        \n        )\n        =\n        1\n        \u22c5\n        \n          \n            1\n            500\n          \n        \n        =\n        \n          \n            1\n            500\n          \n        \n        =\n        0.002.\n      \n    \n    {\\displaystyle \\Pr({\\text{cancer WHOIFPI}})\\approx RR_{\\text{cancer}}\\cdot \\Pr({\\text{cancer in population}})=1\\cdot {\\frac {1}{500}}={\\frac {1}{500}}=0.002.}\n  \n\nHowever, hypercalcemia only occurs in, very approximately, 10% of cancers, (rcancer \u2192 hypercalcemia = 0.1), so:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                Pr\n                (\n                \n                  Hypercalcemia WHOIFPI by cancer\n                \n                )\n              \n            \n            \n              \n                =\n              \n              \n                \n                Pr\n                (\n                \n                  cancer WHOIFPI\n                \n                )\n                \u22c5\n                \n                  r\n                  \n                    \n                      cancer\n                    \n                    \u2192\n                    \n                      hypercalcemia\n                    \n                  \n                \n              \n            \n            \n              \n                =\n              \n              \n                0.002\n                \u22c5\n                0.1\n                =\n                0.0002.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\Pr({\\text{Hypercalcemia WHOIFPI by cancer}})\\\\=&\\Pr({\\text{cancer WHOIFPI}})\\cdot r_{{\\text{cancer}}\\rightarrow {\\text{hypercalcemia}}}\\\\=&0.002\\cdot 0.1=0.0002.\\end{aligned}}}\n  \n\nThe probabilities that hypercalcemia would have occurred in the first place by other candidate conditions can be calculated in a similar manner. However, for simplicity, let's say that the probability that any of these would have occurred in the first place is calculated at 0.0005 in this example.\nFor the instance of there being no disease, the corresponding probability in the population is complementary to the sum of probabilities for other conditions:\n\n  \n    \n      \n        \n          \n            \n              \n                Pr\n                (\n                \n                  no disease in population\n                \n                )\n              \n              \n                \n                =\n                1\n                \u2212\n                Pr\n                (\n                \n                  PH in population\n                \n                )\n                \u2212\n                Pr\n                (\n                \n                  cancer in population\n                \n                )\n              \n            \n            \n              \n              \n                \n                \n\n                \n                \n                \u2212\n                Pr\n                (\n                \n                  other conditions in population\n                \n                )\n              \n            \n            \n              \n              \n                \n                \n\n                \n                =\n                0.997.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\Pr({\\text{no disease in population}})&=1-\\Pr({\\text{PH in population}})-\\Pr({\\text{cancer in population}})\\\\&{}\\quad -\\Pr({\\text{other conditions in population}})\\\\&{}=0.997.\\end{aligned}}}\n  \n\nThe probability that the individual would be healthy in the first place can be assumed to be the same:\n\n  \n    \n      \n        Pr\n        (\n        \n          no disease WHOIFPI\n        \n        )\n        =\n        0.997.\n        \n      \n    \n    {\\displaystyle \\Pr({\\text{no disease WHOIFPI}})=0.997.\\,}\n  \n\nThe rate at which the case of no abnormal condition still ends up in measurement of serum calcium of being above the standard reference range (thereby classifying as hypercalcemia) is, by the definition of standard reference range, less than 2.5%. However, this probability can be further specified by considering how much the measurement deviates from the mean in the standard reference range. Let's say that the serum calcium measurement was 1.30 mmol\/L, which, with a standard reference range established at 1.05 to 1.25 mmol\/L, corresponds to a standard score of 3 and a corresponding probability of 0.14% that such degree of hypercalcemia would have occurred in the first place in the case of no abnormality:\n\n  \n    \n      \n        \n          r\n          \n            \n              no disease\n            \n            \u2192\n            \n              hypercalcemia\n            \n          \n        \n        =\n        0.0014\n      \n    \n    {\\displaystyle r_{{\\text{no disease}}\\rightarrow {\\text{hypercalcemia}}}=0.0014}\n  \n\nSubsequently, the probability that hypercalcemia would have resulted from no disease can be calculated as:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                Pr\n                (\n                \n                  Hypercalcemia WHOIFPI by no disease\n                \n                )\n              \n            \n            \n              \n                =\n              \n              \n                \n                Pr\n                (\n                \n                  no disease WHOIFPI\n                \n                )\n                \u22c5\n                \n                  r\n                  \n                    \n                      no disease\n                    \n                    \u2192\n                    \n                      hypercalcemia\n                    \n                  \n                \n              \n            \n            \n              \n                =\n              \n              \n                0.997\n                \u22c5\n                0.0014\n                \u2248\n                0.0014\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\Pr({\\text{Hypercalcemia WHOIFPI by no disease}})\\\\=&\\Pr({\\text{no disease WHOIFPI}})\\cdot r_{{\\text{no disease}}\\rightarrow {\\text{hypercalcemia}}}\\\\=&0.997\\cdot 0.0014\\approx 0.0014\\end{aligned}}}\n  \n\nThe probability that hypercalcemia would have occurred in the first place in the individual can thus be calculated as:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                Pr\n                (\n                \n                  hypercalcemia WHOIFPI\n                \n                )\n              \n            \n            \n              \n                =\n              \n              \n                \n                Pr\n                (\n                \n                  hypercalcemia WHOIFPI by PH\n                \n                )\n                +\n                Pr\n                (\n                \n                  hypercalcemia WHOIFPI by cancer\n                \n                )\n              \n            \n            \n              \n              \n                \n                \n\n                \n                +\n                Pr\n                (\n                \n                  hypercalcemia WHOIFPI by other conditions\n                \n                )\n                +\n                Pr\n                (\n                \n                  hypercalcemia WHOIFPI by no disease\n                \n                )\n              \n            \n            \n              \n                =\n              \n              \n                0.00125\n                +\n                0.0002\n                +\n                0.0005\n                +\n                0.0014\n                =\n                0.00335\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\Pr({\\text{hypercalcemia WHOIFPI}})\\\\=&\\Pr({\\text{hypercalcemia WHOIFPI by PH}})+\\Pr({\\text{hypercalcemia WHOIFPI by cancer}})\\\\&{}+\\Pr({\\text{hypercalcemia WHOIFPI by other conditions}})+\\Pr({\\text{hypercalcemia WHOIFPI by no disease}})\\\\=&0.00125+0.0002+0.0005+0.0014=0.00335\\end{aligned}}}\n  \n\nSubsequently, the probability that hypercalcemia is caused by primary hyperparathyroidism (PH) in the individual can be calculated as:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                Pr\n                (\n                \n                  hypercalcemia is caused by PH in individual\n                \n                )\n              \n            \n            \n              \n                =\n              \n              \n                \n                  \n                    \n                      Pr\n                      (\n                      \n                        hypercalcemia WHOIFPI by PH\n                      \n                      )\n                    \n                    \n                      Pr\n                      (\n                      \n                        hypercalcemia WHOIFPI\n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n                =\n              \n              \n                \n                  \n                    0.00125\n                    0.00335\n                  \n                \n                =\n                0.373\n                =\n                37.3\n                %\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\Pr({\\text{hypercalcemia is caused by PH in individual}})\\\\=&{\\frac {\\Pr({\\text{hypercalcemia WHOIFPI by PH}})}{\\Pr({\\text{hypercalcemia WHOIFPI}})}}\\\\=&{\\frac {0.00125}{0.00335}}=0.373=37.3\\%\\end{aligned}}}\n  \n\nSimilarly, the probability that hypercalcemia is caused by cancer in the individual can be calculated as:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                Pr\n                (\n                \n                  hypercalcemia is caused by cancer in individual\n                \n                )\n              \n            \n            \n              \n                =\n              \n              \n                \n                  \n                    \n                      Pr\n                      (\n                      \n                        hypercalcemia WHOIFPI by cancer\n                      \n                      )\n                    \n                    \n                      Pr\n                      (\n                      \n                        hypercalcemia WHOIFPI\n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n                =\n              \n              \n                \n                  \n                    0.0002\n                    0.00335\n                  \n                \n                =\n                0.060\n                =\n                6.0\n                %\n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\Pr({\\text{hypercalcemia is caused by cancer in individual}})\\\\=&{\\frac {\\Pr({\\text{hypercalcemia WHOIFPI by cancer}})}{\\Pr({\\text{hypercalcemia WHOIFPI}})}}\\\\=&{\\frac {0.0002}{0.00335}}=0.060=6.0\\%,\\end{aligned}}}\n  \n\nand for other candidate conditions:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                Pr\n                (\n                \n                  hypercalcemia is caused by other conditions in individual\n                \n                )\n              \n            \n            \n              \n                =\n              \n              \n                \n                  \n                    \n                      Pr\n                      (\n                      \n                        hypercalcemia WHOIFPI by other conditions\n                      \n                      )\n                    \n                    \n                      Pr\n                      (\n                      \n                        hypercalcemia WHOIFPI\n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n                =\n              \n              \n                \n                  \n                    0.0005\n                    0.00335\n                  \n                \n                =\n                0.149\n                =\n                14.9\n                %\n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\Pr({\\text{hypercalcemia is caused by other conditions in individual}})\\\\=&{\\frac {\\Pr({\\text{hypercalcemia WHOIFPI by other conditions}})}{\\Pr({\\text{hypercalcemia WHOIFPI}})}}\\\\=&{\\frac {0.0005}{0.00335}}=0.149=14.9\\%,\\end{aligned}}}\n  \n\nand the probability that there actually is no disease:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                Pr\n                (\n                \n                  hypercalcemia is present despite no disease in individual\n                \n                )\n              \n            \n            \n              \n                =\n              \n              \n                \n                  \n                    \n                      Pr\n                      (\n                      \n                        hypercalcemia WHOIFPI by no disease\n                      \n                      )\n                    \n                    \n                      Pr\n                      (\n                      \n                        hypercalcemia WHOIFPI\n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n                =\n              \n              \n                \n                  \n                    0.0014\n                    0.00335\n                  \n                \n                =\n                0.418\n                =\n                41.8\n                %\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\Pr({\\text{hypercalcemia is present despite no disease in individual}})\\\\=&{\\frac {\\Pr({\\text{hypercalcemia WHOIFPI by no disease}})}{\\Pr({\\text{hypercalcemia WHOIFPI}})}}\\\\=&{\\frac {0.0014}{0.00335}}=0.418=41.8\\%\\end{aligned}}}\n  \n\nFor clarification, these calculations are given as the table in the method description:\n\nThus, this method estimates that the probability that the hypercalcemia is caused by primary hyperparathyroidism, cancer, other conditions or no disease at all are 37.3%, 6.0%, 14.9%, and 41.8%, respectively, which may be used in estimating further test indications.\nThis case is continued in the example of the method described in the next section.\n\nLikelihood ratio-based method\nThe procedure of differential diagnosis can become extremely complex when fully taking additional tests and treatments into consideration. One method that is somewhat a tradeoff between being clinically perfect and being relatively simple to calculate is one that uses likelihood ratios to derive subsequent post-test likelihoods.\n\nTheory\nThe initial likelihoods for each candidate condition can be estimated by various methods, such as:\n\nBy epidemiology as described in the previous section.\nBy clinic-specific pattern recognition, such as statistically knowing that patients coming into a particular clinic with a particular complaint statistically has a particular likelihood of each candidate condition.\nOne method of estimating likelihoods even after further tests uses likelihood ratios (which is derived from sensitivities and specificities) as a multiplication factor after each test or procedure. In an ideal world, sensitivities and specificities would be established for all tests for all possible pathological conditions. In reality, however, these parameters may only be established for one of the candidate conditions. Multiplying with likelihood ratios necessitates conversion of likelihoods from probabilities to odds in favor (hereafter simply termed \"odds\") by:\n\n  \n    \n      \n        \n          odds\n        \n        =\n        \n          \n            probability\n            \n              1\n              \u2212\n              \n                probability\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{odds}}={\\frac {\\text{probability}}{1-{\\text{probability}}}}}\n  \n\nHowever, only the candidate conditions with known likelihood ratio need this conversion. After multiplication, conversion back to probability is calculated by:\n\n  \n    \n      \n        \n          probability\n        \n        =\n        \n          \n            odds\n            \n              \n                odds\n              \n              +\n              1\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{probability}}={\\frac {\\text{odds}}{{\\text{odds}}+1}}}\n  \n\nThe rest of the candidate conditions (for which there is no established likelihood ratio for the test at hand) can, for simplicity, be adjusted by subsequently multiplying all candidate conditions with a common factor to again yield a sum of 100%.\nThe resulting probabilities are used for estimating the indications for further medical tests, treatments or other actions. If there is an indication for an additional test, and it returns with a result, then the procedure is repeated using the likelihood ratio of the additional test. With updated probabilities for each of the candidate conditions, the indications for further tests, treatments, or other actions change as well, and so the procedure can be repeated until an endpoint where there no longer is any indication for currently performing further actions. Such an endpoint mainly occurs when one candidate condition becomes so certain that no test can be found that is powerful enough to change the relative probability profile enough to motivate any change in further actions. Tactics for reaching such an endpoint with as few tests as possible includes making tests with high specificity for conditions of already outstandingly high-profile-relative probability, because the high likelihood ratio positive for such tests is very high, bringing all less likely conditions to relatively lower probabilities. Alternatively, tests with high sensitivity for competing candidate conditions have a high likelihood ratio negative, potentially bringing the probabilities for competing candidate conditions to negligible levels. If such negligible probabilities are achieved, the clinician can rule out these conditions, and continue the differential diagnostic procedure with only the remaining candidate conditions.\n\nExample\nThis example continues for the same patient as in the example for the epidemiology-based method. As with the previous example of epidemiology-based method, this example case is made to demonstrate how this method is applied but does not represent a guideline for handling similar real-world cases. Also, the example uses relatively specified numbers, while in reality, there are often just rough estimations. In this example, the probabilities for each candidate condition were established by an epidemiology-based method to be as follows:\n\nThese percentages could also have been established by experience at the particular clinic by knowing that these are the percentages for final diagnosis for people presenting to the clinic with hypercalcemia and having a family history of primary hyperparathyroidism.\nThe condition of highest profile-relative probability (except \"no disease\") is primary hyperparathyroidism (PH), but cancer is still of major concern, because if it is the actual causative condition for the hypercalcemia, then the choice of whether to treat or not likely means life or death for the patient, in effect potentially putting the indication at a similar level for further tests for both of these conditions.\nHere, let's say that the clinician considers the profile-relative probabilities of being of enough concern to indicate sending the patient a call for a clinician visit, with an additional visit to the medical laboratory for an additional blood test complemented with further analyses, including parathyroid hormone for the suspicion of primary hyperparathyroidism.\nFor simplicity, let's say that the clinician first receives the blood test (in formulas abbreviated as \"BT\") result for the parathyroid hormone analysis and that it showed a parathyroid hormone level that is elevated relative to what would be expected by the calcium level.\nSuch a constellation can be estimated to have a sensitivity of approximately 70% and a specificity of approximately 90% for primary hyperparathyroidism. This confers a likelihood ratio positive of 7 for primary hyperparathyroidism.\nThe probability of primary hyperparathyroidism is now termed Pre-BTPH because it corresponds to before the blood test (Latin preposition prae means before). It was estimated at 37.3%, corresponding to an odds of 0.595. With the likelihood ratio positive of 7 for the blood test, the post-test odds is calculated as:\n\n  \n    \n      \n        Odds\n        \u2061\n        (\n        \n          \n            PostBT\n          \n          \n            P\n            H\n          \n        \n        )\n        =\n        Odds\n        \u2061\n        (\n        \n          \n            PreBT\n          \n          \n            P\n            H\n          \n        \n        )\n        \u22c5\n        L\n        H\n        (\n        B\n        T\n        )\n        =\n        0.595\n        \u22c5\n        7\n        =\n        4.16\n        ,\n      \n    \n    {\\displaystyle \\operatorname {Odds} ({\\text{PostBT}}_{PH})=\\operatorname {Odds} ({\\text{PreBT}}_{PH})\\cdot LH(BT)=0.595\\cdot 7=4.16,}\n  \n\nwhere:\n\nOdds(PostBTPH) is the odds for primary hyperparathyroidism after the blood test for parathyroid hormone\nOdds(PreBTPH is the odds in favor of primary hyperparathyroidism before the blood test for parathyroid hormone\nLH(BT) is the likelihood ratio positive for the blood test for parathyroid hormone\nAn Odds(PostBTPH) of 4.16 is again converted to the corresponding probability by:\n\n  \n    \n      \n        Pr\n        (\n        \n          \n            PostBT\n          \n          \n            P\n            H\n          \n        \n        )\n        =\n        \n          \n            \n              Odds\n              \u2061\n              (\n              \n                \n                  PostBT\n                \n                \n                  P\n                  H\n                \n              \n              )\n            \n            \n              Odds\n              \u2061\n              (\n              \n                \n                  PostBT\n                \n                \n                  P\n                  H\n                \n              \n              )\n              +\n              1\n            \n          \n        \n        =\n        \n          \n            4.16\n            \n              4.16\n              +\n              1\n            \n          \n        \n        =\n        0.806\n        =\n        80.6\n        %\n      \n    \n    {\\displaystyle \\Pr({\\text{PostBT}}_{PH})={\\frac {\\operatorname {Odds} ({\\text{PostBT}}_{PH})}{\\operatorname {Odds} ({\\text{PostBT}}_{PH})+1}}={\\frac {4.16}{4.16+1}}=0.806=80.6\\%}\n  \n\nThe sum of the probabilities for the rest of the candidate conditions should therefore be:\n\n  \n    \n      \n        Pr\n        (\n        \n          \n            PostBT\n          \n          \n            r\n            e\n            s\n            t\n          \n        \n        )\n        =\n        100\n        %\n        \u2212\n        80.6\n        %\n        =\n        19.4\n        %\n      \n    \n    {\\displaystyle \\Pr({\\text{PostBT}}_{rest})=100\\%-80.6\\%=19.4\\%}\n  \n\nBefore the blood test for parathyroid hormone, the sum of their probabilities were:\n\n  \n    \n      \n        Pr\n        (\n        \n          \n            PreBT\n          \n          \n            rest\n          \n        \n        )\n        =\n        6.0\n        %\n        +\n        14.9\n        %\n        +\n        41.8\n        %\n        =\n        62.7\n        %\n      \n    \n    {\\displaystyle \\Pr({\\text{PreBT}}_{\\text{rest}})=6.0\\%+14.9\\%+41.8\\%=62.7\\%}\n  \n\nTherefore, to conform to a sum of 100% for all candidate conditions, each of the other candidates must be multiplied by a correcting factor:\n\n  \n    \n      \n        \n          Correcting factor\n        \n        =\n        \n          \n            \n              Pr\n              (\n              \n                \n                  PostBT\n                \n                \n                  rest\n                \n              \n              )\n            \n            \n              Pr\n              (\n              \n                \n                  PreBT\n                \n                \n                  rest\n                \n              \n              )\n            \n          \n        \n        =\n        \n          \n            19.4\n            62.7\n          \n        \n        =\n        0.309\n      \n    \n    {\\displaystyle {\\text{Correcting factor}}={\\frac {\\Pr({\\text{PostBT}}_{\\text{rest}})}{\\Pr({\\text{PreBT}}_{\\text{rest}})}}={\\frac {19.4}{62.7}}=0.309}\n  \n\nFor example, the probability of cancer after the test is calculated as:\n\n  \n    \n      \n        Pr\n        (\n        \n          \n            PostBT\n          \n          \n            cancer\n          \n        \n        )\n        =\n        Pr\n        (\n        \n          \n            PreBT\n          \n          \n            cancer\n          \n        \n        )\n        \u22c5\n        \n          Correcting factor\n        \n        =\n        6.0\n        %\n        \u22c5\n        0.309\n        =\n        1.9\n        %\n      \n    \n    {\\displaystyle \\Pr({\\text{PostBT}}_{\\text{cancer}})=\\Pr({\\text{PreBT}}_{\\text{cancer}})\\cdot {\\text{Correcting factor}}=6.0\\%\\cdot 0.309=1.9\\%}\n  \n\nThe probabilities for each candidate conditions before and after the blood test are given in following table:\n\nThese \"new\" percentages, including a profile-relative probability of 80% for primary hyperparathyroidism, underlie any indications for further tests, treatments, or other actions. In this case, let's say that the clinician continues the plan for the patient to attend a clinician visit for a further checkup, especially focused on primary hyperparathyroidism.\nA clinician visit can, theoretically, be regarded as a series of tests, including both questions in a medical history, as well as components of a physical examination, where the post-test probability of a previous test, can be used as the pre-test probability of the next. The indications for choosing the next test are dynamically influenced by the results of previous tests.\nLet's say that the patient in this example is revealed to have at least some of the symptoms and signs of depression, bone pain, joint pain or constipation of more severity than what would be expected by the hypercalcemia itself, supporting the suspicion of primary hyperparathyroidism, and let's say that the likelihood ratios for the tests, when multiplied together, roughly results in a product of 6 for primary hyperparathyroidism.\nThe presence of unspecific pathologic symptoms and signs in the history and examination are often concurrently indicative of cancer as well, and let's say that the tests gave an overall likelihood ratio estimated at 1.5 for cancer. For other conditions, as well as the instance of not having any disease at all, let's say that it is unknown how they are affected by the tests at hand, as often happens in reality. This gives the following results for the history and physical examination (abbreviated as P&E):\n\nThese probabilities after the history and examination may make the physician confident enough to plan the patient for surgery for a parathyroidectomy to resect the affected tissue.\nAt this point, the probability of \"other conditions\" is so low that the physician cannot think of any test for them that could make a difference that would be substantial enough to form an indication for such a test, and the physician thereby practically regards \"other conditions\" as ruled out, in this case not primarily by any specific test for such other conditions that were negative, but rather by the absence of positive tests so far.\nFor \"cancer\", the cutoff at which to confidently regard it as ruled out maybe more stringent because of severe consequences of missing it, so the physician may consider that at least a histopathologic examination of the resected tissue is indicated.\nThis case is continued in the example of Combinations in the corresponding section below.\n\nCoverage of candidate conditions\nThe validity of both the initial estimation of probabilities by epidemiology and further workup by likelihood ratios are dependent on the inclusion of candidate conditions that are responsible for a large part as possible of the probability of having developed the condition, and it is clinically important to include those where relatively fast initiation of therapy is most likely to result in the greatest benefit. If an important candidate condition is missed, no method of differential diagnosis will supply the correct conclusion. The need to find more candidate conditions for inclusion increases with the increasing severity of the presentation itself. For example, if the only presentation is a deviating laboratory parameter and all common harmful underlying conditions have been ruled out, then it may be acceptable to stop finding more candidate conditions, but this would much more likely be unacceptable if the presentation would have been severe pain.\n\nCombinations\nIf two conditions get high post-test probabilities, especially if the sum of the probabilities for conditions with known likelihood ratios becomes higher than 100%, then the actual condition is a combination of the two. In such cases, that combined condition can be added to the list of candidate conditions, and the calculations should start over from the beginning.\nTo continue the example used above, let's say that the history and physical examination were indicative of cancer as well, with a likelihood ratio of 3, giving an Odds(PostH&E) of 0.057, corresponding to a P(PostH&E) of 5.4%. This would correspond to a \"Sum of known P(PostH&E)\" of 101.5%. This is an indication for considering a combination of primary hyperparathyroidism and cancer, such as, in this case, a parathyroid hormone-producing parathyroid carcinoma. A recalculation may therefore be needed, with the first two conditions being separated into \"primary hyperparathyroidism without cancer\", \"cancer without primary hyperparathyroidism\" as well as \"combined primary hyperparathyroidism and cancer\", and likelihood ratios being applied to each condition separately. In this case, however, tissue has already been resected, wherein a histopathologic examination can be performed that includes the possibility of parathyroid carcinoma in the examination (which may entail appropriate sample staining).\nLet's say that the histopathologic examination confirms primary hyperparathyroidism, but also showed a malignant pattern. By an initial method by epidemiology, the incidence of parathyroid carcinoma is estimated at 1 in 6 million people per year, giving a very low probability before taking any tests into consideration. In comparison, the probability that non-malignant primary hyperparathyroidism would have occurred at the same time as an unrelated non-carcinoma cancer that presents with malignant cells in the parathyroid gland is calculated by multiplying the probabilities of the two. The resultant probability is, however, much smaller than the 1 in 6 million. Therefore, the probability of parathyroid carcinoma may still be close to 100% after histopathologic examination despite the low probability of occurring in the first place.\n\nMachine differential diagnosis\nMachine differential diagnosis is the use of computer software to partly or fully make a differential diagnosis. It may be regarded as an application of artificial intelligence.  Alternatively, it may be seen as \"augmented intelligence\" if it meets the FDA criteria, namely that (1) it reveals the underlying data, (2) reveals the underlying logic, and (3) leaves the clinician in charge to shape and make the decision. Machine learning AI is generally seen as a device by the FDA, whereas augmented intelligence applications are not.\nMany studies demonstrate improvement of quality of care and reduction of medical errors by using such decision support systems. Some of these systems are designed for a specific medical problem such as schizophrenia, Lyme disease or ventilator-associated pneumonia. Others are designed to cover all major clinical and diagnostic findings to assist physicians with faster and more accurate diagnosis.\nHowever, these tools all still require advanced medical skills to rate symptoms and choose additional tests to deduce the probabilities of different diagnoses. Machine differential diagnosis is also currently unable to diagnose multiple concurrent disorders. Their usage by non-experts is therefore not a substitute for professional diagnosis.\n\nHistory\nThe method of differential diagnosis was first suggested for use in the diagnosis of mental disorders by Emil Kraepelin. It is more systematic than the old-fashioned method of diagnosis by gestalt (impression).\n\nAlternative medical meanings\n\"Differential diagnosis\" is also used more loosely to refer simply to a list of the most common causes of a given symptom, to a list of disorders similar to a given disorder, or to such lists when they are annotated with advice on how to narrow the list down (French's Index of Differential Diagnosis is an example). Thus, a differential diagnosis in this sense is medical information specially organized to aid in diagnosis.\n\nUsage apart from in medicine\nMethods similar to those of differential diagnostic processes in medicine are also used by biological taxonomists to identify and classify organisms, living and extinct. For example, after finding an unknown species, there can first be a listing of all potential species, followed by ruling out of one by one until, optimally, only one potential choice remains.\nSimilar procedures may be used by plant and maintenance engineers and automotive mechanics and used to be used in diagnosing faulty electronic circuitry.\n\nIn popular culture\nIn the American television medical drama House, the main protagonist Dr. Gregory House leads a team of diagnosticians who regularly use differential diagnostics procedures.\n\nSee also\nComorbidity\nDiagnosis of exclusion\nDual diagnosis\nGender-bias in medical diagnosis\nList of medical symptoms\n\n\n== References ==","54":"Diplopia is the simultaneous perception of two images of a single object that may be displaced horizontally or vertically in relation to each other. Also called double vision, it is a loss of visual focus under regular conditions, and is often voluntary. However, when occurring involuntarily, it results from impaired function of the extraocular muscles, where both eyes are still functional, but they cannot turn to target the desired object. Problems with these muscles may be due to mechanical problems, disorders of the neuromuscular junction, disorders of the cranial nerves (III, IV, and VI) that innervate the muscles, and occasionally disorders involving the supranuclear oculomotor pathways or ingestion of toxins.\nDiplopia can be one of the first signs of a systemic disease, particularly to a muscular or neurological process, and it may disrupt a person's balance, movement, or reading abilities.\n\nCauses\nDiplopia has a diverse range of ophthalmologic, infectious, autoimmune, neurological, and neoplastic causes:\n\nDiagnosis\nDiplopia is diagnosed mainly by information from the patient. Doctors may use blood tests, physical examinations, computed tomography (CT), or magnetic resonance imaging (MRI) to find the underlying cause.\n\nClassification\nOne of the first steps in diagnosing diplopia is often to see whether one of two major classifications may be eliminated. That involves blocking one eye to see which symptoms are evident in each eye alone. Persisting blurry or double vision with one eye closed is classified as monocular diplopia.\n\nBinocular\nBinocular diplopia is the other one in which the blurring of vision occurs only when the patient looks through both eyes simultaneously. It is common and occurs in approximately 10.0% to 40.0% of zygomatic complex injuries. Furthermore, diplopia may be transient or persistent. Inadequate diagnosis and treatment at improper times and tethering or fibrosis of muscles may lead to persistent diplopia.\nBinocular diplopia is double vision arising as a result of strabismus(in layman's terms \"cross-eyed\"), the misalignment of the two eyes relative to each other, either esotropia (inward) or exotropia (outward). In such a case while the fovea of one eye is directed at the object of regard, the fovea of the other is directed elsewhere, and the image of the object of regard falls on an extrafoveal area of the retina. Acute diplopia is a diagnostic challenge. The most common cause of acute diplopia are ocular motor nerve palsies (OMP).\nThe brain calculates the visual direction of an object based upon the position of its image relative to the fovea. Images falling on the fovea are seen as being directly ahead, while those falling on retina outside the fovea may be seen as above, below, right, or left of straight ahead depending upon the area of retina stimulated. Thus, when the eyes are misaligned, the brain perceives two images of one target object, as the target object simultaneously stimulates different, noncorresponding, retinal areas in either eye, thus producing double vision.\nThis correlation of particular areas of the retina in one eye with the same areas in the other is known as retinal correspondence. This relationship also gives rise to an associated phenomenon of binocular diplopia, although one that is rarely noted by those experiencing diplopia. Because the fovea of one eye corresponds to the fovea of the other, images falling on the two foveae are projected to the same point in space. Thus, when the eyes are misaligned, two different objects will be perceived as superimposed in the same space. This phenomenon is known as 'visual confusion'.\nThe brain naturally guards against double vision. In an attempt to avoid double vision, the brain can sometimes ignore the image from one eye, a process known as suppression. The ability to suppress is to be found particularly in childhood when the brain is still developing. Thus, those with childhood strabismus almost never complain of diplopia, while adults who develop strabismus almost always do. While this ability to suppress might seem an entirely positive adaptation to strabismus, in the developing child, this can prevent the proper development of vision in the affected eye, resulting in amblyopia. Some adults are also able to suppress their diplopia, but their suppression is rarely as deep or as effective and takes much longer to establish, thus they are not at risk of permanently compromising their vision. In some cases, diplopia disappears without medical intervention, but in other cases, the cause of the double vision may still be present.\nCertain people with diplopia who cannot achieve fusion and yet do not suppress may display a certain type of spasm-like irregular movement of the eyes in the vicinity of the fixation point (see: Horror fusionis).\n\nMonocular\nDiplopia can also occur when viewing with only one eye; this is called monocular diplopia, or where the patient perceives more than two images, monocular polyopia.  While serious causes rarely may be behind monocular diplopia symptoms, this is much less often the case than with binocular diplopia. The differential diagnosis of multiple image perception includes the consideration of such conditions as corneal surface keratoconus, subluxation of the lens, a structural defect within the eye, a lesion in the anterior visual cortex, or nonorganic conditions, but diffraction-based (rather than geometrical) optical models have shown that common optical conditions, especially astigmatism, can also produce this symptom.\n\nTemporary\nTemporary binocular diplopia can be caused by alcohol intoxication or head injuries, such as concussion (if temporary double vision does not resolve quickly, one should see an optometrist or ophthalmologist immediately). It can also be a side effect of benzodiazepines or opioids, particularly if used in larger doses for recreation, the antiepileptic drugs phenytoin, zonisamide and lamotrigine. As well as the hypnotic drug zolpidem and the dissociative drugs ketamine and dextromethorphan. Temporary diplopia can also be caused by tired or strained eye muscles. If diplopia appears with other symptoms such as fatigue and acute or chronic pain, the patient should see an ophthalmologist immediately.\n\nVoluntary\nSome people are able to consciously uncouple their eyes, either by overfocusing closely (i.e., going cross-eyed) or unfocusing.  Also, while looking at one object behind another object, the foremost object's image is doubled (for example, placing one's finger in front of one's face while reading text on a computer monitor). In this sense, double vision is neither dangerous nor harmful, and may even be enjoyable. It makes viewing stereograms possible.\nMonocular diplopia may be induced in many individuals, even those with normal eyesight, with simple defocusing experiments involving fine, high-contrast lines.\n\nTreatment\nThe appropriate treatment for binocular diplopia depends upon the cause of the condition producing the symptoms. Efforts must first be made to identify and treat the underlying cause of the problem. Treatment options include eye exercises, wearing an eye patch on alternative eyes, prism correction, and in more extreme situations, surgery or botulinum toxin.  If your provider diagnoses swelling or inflammation of, or around the nerve, medicines called corticosteroids may be used.\nSometimes, the condition disappears without treatment. If you have diabetes, you'll be advised to keep tight control of your blood sugar level.\nThe provider may prescribe an eye patch to relieve the double vision. The patch can be removed after the nerve heals.\nSurgery or special glasses (prisms) may be advised if there is no recovery in 6 to 12 months.\nIf diplopia turns out to be intractable, it can be managed as last resort by obscuring part of the patient's field of view. This approach is outlined in the article on diplopia occurring in association with a condition called horror fusionis.\n\nSee also\nBinocular vision\nMonocular rivalry\nOrthoptics\n\nReferences\nFurther reading\nFraine L (2012). \"Nonsurgical management of diplopia\". The American Orthoptic Journal. 62: 13\u201318. doi:10.3368\/aoj.62.1.13. PMID 23002469. S2CID 218549699.\n\nExternal links\n\nDeciphering Diplopia","55":"The Diseases Database is a free website that provides information about the relationships between medical conditions, symptoms, and medications. The database is run by Medical Object Oriented Software Enterprises Ltd, a company based in London.\nThe site's stated aim is \"education, background reading and general interest\" with an intended audience \"physicians, other clinical healthcare workers and students of these professions\". The editor of the site is stated as Malcolm H Duncan, a UK qualified medical doctor.\n\nOrganization\nThe Diseases Database is based on a collection of about 8,500 concepts, called \"items\", related to human medicine including diseases, drugs, symptoms, physical signs and abnormal laboratory results.\nIn order to link items to both each other and external information resources three sets of metadata are modelled within the database.\n\nItems are assigned various relationships e.g. diabetes mellitus type 2 is labelled \"a risk factor for\" ischaemic heart disease. More formally the database employs an entity-attribute-value model with items populating both entity and value slots. Relationships may be read in either direction e.g. the assertion \"myocardial infarction {may cause} chest pain\" has the corollary \"chest pain {may be caused by} myocardial infarction\". Such relationships aggregate within the database and allow lists to be retrieved - e.g. a list of items which may cause chest pain, and a list of items which may be caused by myocardial infarction.\nMost items are assigned topic specific hyperlinks to Web resources which include Online Mendelian Inheritance in Man, eMedicine and Wikipedia.\nMost items are mapped to concepts within the Unified Medical Language System (UMLS). UMLS links enable the display of short text definitions or Medical Subject Heading (MeSH) scope notes for the majority of items on the database.\nThe UMLS map also enables links to and from other medical classifications and terminologies e.g. ICD-9 and SNOMED.\n\nReferences\nExternal links\n\nOfficial website","56":"Dizziness is an imprecise term that can refer to a sense of disorientation in space, vertigo, or lightheadedness. It can also refer to disequilibrium or a non-specific feeling, such as giddiness or foolishness.\nDizziness is a common medical complaint, affecting 20\u201330% of persons. Dizziness is broken down into four main subtypes: vertigo (~25\u201350%), disequilibrium (less than ~15%), presyncope (less than ~15%), and nonspecific dizziness (~10%).\n\nVertigo is the sensation of spinning or having one's surroundings spin about them. Many people find vertigo very disturbing and often report associated nausea and vomiting.\nPresyncope describes lightheadedness or feeling faint; the name relates to syncope, which is actually fainting.\nDisequilibrium is the sensation of being off balance and is most often characterized by frequent falls in a specific direction.  This condition is not often associated with nausea or vomiting.\nNon-specific dizziness may be psychiatric in origin.  It is a diagnosis of exclusion and can sometimes be brought about by hyperventilation.\n\nMechanism and causes\nMany conditions cause dizziness because multiple parts of the body are required for maintaining balance including the inner ear, eyes, muscles, skeleton, and the nervous system. Thus dizziness can be caused by a variety of problems and may reflect a focal process (such as one affecting balance or coordination) or a diffuse one (such as a toxic exposure or low perfusion state).\nCommon causes of dizziness include:\n\nInadequate blood supply to the brain due to:\nA sudden fall in blood pressure\nHeart problems or artery blockages\nAnaemias, such as vitamin B12 deficiency anemia, iron deficiency anemia\nLoss or distortion of vision or visual cues\nStanding too quickly\/prolonged standing\nDisorders of the inner ear\nDehydration\nDistortion of brain\/nervous function by medications such as anticonvulsants and sedatives\nDysfunction of cervical proprioception\nSide effects from other prescription drugs, such as proton-pump inhibitors or Coumadin (warfarin)\n\nDiagnosis\nDifferential diagnosis\nDizziness may occur from an abnormality involving the brain (in particular the brainstem or cerebellum), inner ear, eyes, heart, vascular system, fluid or blood volume, spinal cord, peripheral nerves, or body electrolytes. Dizziness can accompany certain serious events, such as a concussion or brain bleed, epilepsy and seizures (convulsions), stroke, and cases of meningitis and encephalitis. However, the most common subcategories can be broken down as follows: 40% peripheral vestibular dysfunction, 10% central nervous system lesion, 15% psychiatric disorder, 25% presyncope\/disequilibrium, and 10% nonspecific dizziness. Some vestibular pathologies have symptoms that are comorbid with mental disorders.\nWhile traditional medical teaching has focused on determining the cause of dizziness based on the category (such as vertigo vs. presyncope),  research published in 2017 suggests that this analysis is of limited clinical utility.\nMedical conditions that often have dizziness as a symptom include:\n\nBenign paroxysmal positional vertigo\nM\u00e9ni\u00e8re's disease\nLabyrinthitis\nOtitis media\nBrain tumor\nAcoustic neuroma\nMotion sickness\nRamsay Hunt syndrome\nFatal Familial Insomnia\nMigraine\nMultiple sclerosis\nPregnancy\nLow blood pressure (hypotension)\nLow blood oxygen content (hypoxemia)\nHeart attack\nIron deficiency (anemia)\nVitamin B12 deficiency\nLow blood sugar (hypoglycemia)\nHormonal changes (e.g. thyroid disease, menstruation, pregnancy)\nPanic disorder\nHyperventilation\nAnxiety\nDepression\nAge-diminished visual, balance, and perception of spatial orientation abilities\nStroke; cause of isolated dizziness in 0.7% of people who present to the emergency department\n\nEpidemiology\nAbout 20\u201330% of the population report to have experienced dizziness at some point in 2008.\n\nDisequilibrium\nIn medicine, disequilibrium refers to impaired equilibrioception that can be characterised as a sensation of impending fall or of the need to obtain external assistance for proper locomotion. It is sometimes described as a feeling of improper tilt of the floor, or as a sense of floating. This sensation can originate in the inner ear or other motion sensors, or in the central nervous system. Neurologic disorders tend to cause constant vertigo or disequilibrium and usually have other symptoms of neurologic dysfunction associated with the vertigo. Many medications used to treat seizures, depression, anxiety, and pain affect the vestibular system and the central nervous system which can cause the symptom of disequilibrium.\n\nSee also\nReferences\nExternal links\n\nDizzytimes.com (Archived 2020-05-27 at the Wayback Machine)\u2014Online community for people with vertigo and dizziness\nDysautonomia Youth Network of America, Inc.","57":"The Dix\u2013Hallpike or Nyl\u00e9n\u2013B\u00e1r\u00e1ny test is a diagnostic maneuver from the group of rotation tests used to identify benign paroxysmal positional vertigo (BPPV).\n\nProcedure\nWhen performing the Dix\u2013Hallpike test, patients are lowered quickly to a supine position (lying horizontally with the face and torso facing up) with the neck extended 30 degrees below horizontal by the clinician performing the maneuver.\nThe Dix\u2013Hallpike and the side-lying testing position have yielded similar results. As such, the side-lying position can be used if the Dix\u2013Hallpike cannot be performed easily.\nSteps: \n\nThe examiner looks for nystagmus (usually accompanied  by vertigo). In BPPV, the nystagmus typically occurs in A or B only, and is torsional\u2014the fast phase beating toward the lower ear. Its onset is usually delayed a few seconds, and it lasts 10\u201320 seconds. As the patient is returned to the upright position, transient nystagmus may occur in the opposite direction. Both nystagmus and vertigo typically decrease on repeat testing.\n\nInterpretation\nPositive test result\nA positive test is indicated by patient report of a reproduction of vertigo and clinician observation of nystagmus (involuntary eye movement).\nFor some patients, this maneuver may be contraindicated, and a modification may be needed that also targets the posterior semicircular canal. Such patients include those who are too anxious about eliciting the uncomfortable symptoms of vertigo, and those who may not have the range of motion necessary to comfortably be in a supine position. The modification involves the patient moving from a seated position to side-lying without their head extending off the examination table, such as with Dix\u2013Hallpike. The head is rotated 45 degrees away from the side being tested, and the eyes are examined for nystagmus.\n\nNegative test\nIf the test is negative, it makes benign positional vertigo a less likely diagnosis and central nervous system involvement should be considered.\n\nAdvantages\nAlthough there are alternative methods to administering the test, Cohen proposes advantages to the classic maneuver. The test can be easily administered by a single examiner, which prevents the need for external aid. Due to the position of the subject and the examiner, nystagmus, if present, can be observed directly by the examiner.\n\nLimitations\nThe negative predictive value of this test is not 100%. Some patients with a history of BPPV will not have a positive test result. The estimated sensitivity is 79%, along with an estimated specificity of 75%.\nThe test may need to be performed more than once, as it is not always easy to demonstrate observable nystagmus that is typical of BPPV. Also, the test results can be affected by the speed with which the maneuver is conducted and the plane of the occiput.\nThere are several disadvantages proposed by Cohen for the classic maneuver. Patients may be too tense, for fear of producing vertigo symptoms, which can prevent the necessary brisk passive movements for the test. A subject must have adequate cervical spine range of motion to allow neck extension, as well as trunk and hip range of motion to lie supine. From the previous point, the use of this maneuver can be limited by musculoskeletal and obesity issues in a subject.\n\nPrecautions and contraindications\nIn rare cases a patient may be unable or unwilling to participate in the Dix\u2013Hallpike test due to physical limitations. In these circumstances the side-lying test or other alternative tests may be used.\nPrecautions\n\nThe Dix\u2013Hallpike maneuver places a degree of stress on the patient's lower back; therefore, a cautious approach must be taken with patients who are suffering from back pain.\nSevere respiratory or cardiac problems may not allow a patient to tolerate the maneuver. For example a patient with orthopnoea may not be able to participate in the procedure, as the patient may have troubling breathing when lying down.\nAbsolute contraindications \n\nNeck surgery\nSevere rheumatoid arthritis\nAtlantoaxial and occipitoatlantal instability\nAplasia of odontoid process\nCervical myelopathy\nCervical radiculopathy\nCarotid sinus syncope\nVascular dissection syndromes\n\nSee also\nTilt table test\nEpley maneuver \u2013 used to treat BPPV\n\nFootnotes\nExternal links\nOverview and diagrams at dizziness-and-balance.com Archived 2012-03-07 at the Wayback Machine\nvideo of Dix\u2013Hallpike test","58":"A dive computer, personal decompression computer or decompression meter is a device used by an underwater diver to measure the elapsed time and depth during a dive and use this data to calculate and display an ascent profile which, according to the programmed decompression algorithm, will give a low risk of decompression sickness. A secondary function is to record the dive profile, warn the diver when certain events occur, and provide useful information about the environment.\nMost dive computers use real-time ambient pressure input to a decompression algorithm to indicate the remaining time to the no-stop limit, and after that has passed, the minimum decompression required to surface with an acceptable risk of decompression sickness. Several algorithms have been used, and various personal conservatism factors may be available. Some dive computers allow for gas switching during the dive, and some monitor the pressure remaining in the scuba cylinders. Audible alarms may be available to warn the diver when exceeding the no-stop limit, the maximum operating depth for the gas mixture, the recommended ascent rate, decompression ceiling, or other limit beyond which risk increases significantly.\nThe display provides data to allow the diver to avoid decompression, or to decompress relatively safely, and includes depth and duration of the dive. This must be displayed clearly, legibly, and unambiguously at all light levels. Several additional functions and displays may be available for interest and convenience, such as water temperature and compass direction, and it may be possible to download the data from the dives to a personal computer via cable or wireless connection. Data recorded by a dive computer may be of great value to the investigators in a diving accident, and may allow the cause of an accident to be discovered.\nDive computers may be wrist-mounted or fitted to a console with the submersible pressure gauge. A dive computer is perceived by recreational scuba divers and service providers to be one of the most important items of safety equipment.  It is one of the most expensive pieces of diving equipment owned by most divers. Use by professional scuba divers is also common, but use by surface-supplied divers is less widespread, as the diver's depth is monitored at the surface by pneumofathometer and decompression is controlled by the diving supervisor. Some freedivers use another type of dive computer to record their dive profiles and give them useful information which can make their dives safer and more efficient, and some computers can provide both functions, but require the user to select which function is required.\n\nPurpose\nThe primary purpose of a decompression computer is to facilitate safe decompression by an underwater diver breathing a suitable gas at ambient pressure, by providing information based on the recent pressure exposure history of the diver that allows an ascent with acceptably low risk of developing decompression sickness. Dive computers address the same problem as decompression tables, but are able to perform a continuous calculation of the theoretical partial pressure of inert gases in the body based on the actual depth and time profile of the diver and the decompression model used by the computer. As the dive computer automatically measures depth and time, it is able to warn of excessive ascent rates and missed decompression stops and the diver has less reason to carry a separate dive watch and depth gauge. Many dive computers also provide additional information to the diver including ambient temperature, partial pressure of oxygen in the breathing gas at ambient pressure, accumulated oxygen toxicity exposure data, a computer-readable dive log, and the pressure of the remaining breathing gas in the diving cylinder. This recorded information can be used for the diver's personal log of their activities or as important information in medical review or legal cases following  diving accidents.\nBecause of the computer's ability to continually re-calculate based on changing data, the diver benefits by being able to remain underwater for longer periods at acceptable risk. For example, a recreational diver who plans to stay within \"no-decompression stop\" limits can in many cases simply ascend a few feet each minute, while continuing the dive, and still remain within reasonably safe limits, rather than adhering to a pre-planned bottom time and then ascending directly. Multi-level dives can be pre-planned with traditional dive tables or personal computer and smartphone apps, or on the fly using waterproof dive tables, but the additional calculations become complex, and the plan may be cumbersome to follow, and the risk of errors rises with profile complexity. Computers allow for a certain amount of spontaneity during the dive, and automatically take into account deviations from the dive plan.\n\nDive computers are used to safely calculate decompression schedules in recreational, scientific, and military diving operations. There is no reason to assume that they cannot be valuable tools for commercial diving operations, especially on multi-level dives.\n\nComponents\nSome components are common to all models of dive computer as they are essential to the basic function:\n\nambient pressure transducer\npressure sensor\nComponent that converts ambient pressure to an electrical signal Piezoresistive pressure sensors are frequently used for this purpose.\n\nanalog-to-digital converter\nComponent that converts the voltage output from the pressure transducer to a binary signal that can be processed by the computer.\n\nbuttons\nUser input interface in the form of push-buttons or external contacts which accept manual input from the user to set the user preferences and select display options.\n\nclock\nCircuitry that synchronises the steps of the processor and keeps track of elapsed time. It may also keep track of time of day.\n\ndisplay\nScreen to present the results of computation to the diver in real time.\n\nfaceplate\nThe transparent glass or plastic window covering the screen. Tempered glass and synthetic sapphire are most scratch resistant, but brittle, and can fracture on impact, causing the housing to leak, which can destroy the electronics. These materials are popular on wristwatch style units, which are expected to be worn out of the water. The larger units are more likely to be worn only while diving, and the more impact resistant polycarbonate faceplates used for these computers are more sensitive to scratching, but are less likely to flood. Disposable transparent self-adhesive faceplate protectors are available for some models.\n\nhousing\nThe waterproof container in which the other components are installed to protect them from the environment. Three basic form factors are used: Wristwtch, wrist or console mount circular (puck), Rectangular or contoured wrist mount, and housed smartphone.\n\nmicroprocessor\nThe logic-processing microcircuitry that converts the input signals into real time output data modelling the diver's decompression status using the chosen algorithm and other input data.\n\npower supply\nThe battery that provides electrical power to run the device. It may be rechargeable, or user replaceable, or may require replacement by an authorised agent or the manufacturer.\n\nrandom access memory (RAM)\nTemporary storage for the variable data and results of computation.\n\nread only memory (ROM)\nNon-volatile memory containing the program and constants used in the algorithm.\n\nstrap\nBand used to secure the housing to the user's wrist. Several types may be used. Double straps may be used for greater security. An alternative to straps is console mounting, usually limited to puck form factor recreational dive computers.\n\ntemperature sensor\nComponent that measures the temperature of the pressure transducer to compensate for temperature variations. The output may be recorded and displayed, but the primary function is to allow accurate pressure measurement. \n\nAdditional components may be necessary for additional or extended features and functionality.\n\naccelerometer\nused to detect directional tapping input and pitch and tilt angles.\n\nBluetooth hardware\nUsed for communication with smartphones or personal computers to upload data and download firmware updates.\n\nbuzzer\nUsed to provide audible and vibratory alarms.\n\nexternal electrical contacts\nMay be used for several purposes at the surface, including battery charging and communication with a personal computer.\n\nGPS receiver\nUsed for position identification at the surface.\n\nfluxgate compass\nUsed to provide compass functionality for navigation.\n\ninfrared data transfer hardware\nUsed for date transfer to and from personal computer.\n\nlight sensor\nUsed to provide automatic display intensity\n\nscreen protector\nTransparent sacrificial film or lens covering the screen to protect the screen against scratches.\n\nultrasonic communications hardware\nUsed for wireless communications with pressure sensors on gas cylinders for gas-integrated systems, and in some cases, other peripherals.\n\nwatertight electrical connections\nTo receive input from oxygen cells, and to communicate with electronically controlled rebreathers.\n\nwireless charging coil\nUsed to recharge the battery.\n\nFunction\nDive computers are battery-powered computers within a watertight and pressure resistant case. These computers track the dive profile by measuring time and pressure. All dive computers measure the ambient pressure to model the concentration of gases in the tissues of the diver. More advanced dive computers provide additional measured data and user input into the calculations, for example, the water temperature, gas composition, altitude of the water surface, or the remaining pressure in the diving cylinder. Dive computers suitable for calculating decompression for rebreather diving need to measure the oxygen partial pressure in the breathing loop. A dive computer may be used as the control unit for an electronically controlled closed circuit rebreather, in which case it will calculate oxygen partial pressure in the loop using the output from more than one oxygen sensor.\nThe computer uses the pressure and time input in a decompression algorithm to estimate the partial pressure of inert gases that have been dissolved in the diver's tissues. Based on these calculations, the computer estimates when a risk-free direct ascent to the surface is no longer possible, and what decompression stops would be needed based on the profile of the dive up to that time and recent hyperbaric exposures which may have left residual dissolved gases in the diver.\nMany dive computers are able to produce a low risk decompression schedule for dives that take place at altitude, which requires longer decompression than for the same profile at sea level, because the computers measure the atmospheric pressure before the dive and take this into account in the algorithm. Many dive computers continuously monitor the pressure as long as the battery has a charge, so when divers travel before or after diving and particularly when they fly, they should transport their dive computer with them in the same pressure regime (carry on baggage, not checked in and carried in the hold) so that the computer can measure the pressure profile that their body has undergone and take it into account in consequent dives. Older computers that are powered down completely when switched off will not benefit by this process.\nMany computers have some way for the user to adjust decompression conservatism. This may be by way of a personal factor, which makes an undisclosed change to the algorithm arbitrarily decided by the manufacturer, or the setting of gradient factors, a way of reducing the permitted supersaturation of tissue compartments by specific ratios, which is well defined in the literature, leaving the responsibility for making informed decisions on personal safety to the diver.\n\nAlgorithms\nThe decompression algorithms used in dive computers vary between manufacturers and computer models. Examples of decompression algorithms are the B\u00fchlmann algorithms and their variants, the Thalmann VVAL18 Exponential\/Linear model, the Varying Permeability Model, and the Reduced Gradient Bubble Model. The proprietary names for the algorithms do not always clearly describe the actual decompression model. The algorithm may be a variation of one of the standard algorithms, for example, several versions of the  B\u00fchlmann decompression algorithm are in use. The algorithm used may be an important consideration in the choice of a dive computer. Dive computers using the same internal electronics and algorithms may be marketed under a variety of brand names.\nThe algorithm used is intended to inform the diver of a decompression profile that will keep the risk of decompression sickness (DCS) to an acceptable level. Researchers use experimental diving programmes or data that has been recorded from previous dives to validate an algorithm. The dive computer measures depth and time, then uses the algorithm to determine decompression requirements or estimate remaining no-stop times at the current depth. An algorithm takes into account the magnitude of pressure reduction, breathing gas changes, repetitive exposures, rate of ascent, and time at altitude. Algorithms are not able to reliably account for age, previous injury, ambient temperature, body type, alcohol consumption, dehydration, and other factors such as patent foramen ovale, because the effects of these factors have not been experimentally quantified, though some may attempt to compensate for these by factoring in user input, and for diver peripheral temperature and workload by having sensors that monitor ambient temperature and cylinder pressure changes as a proxy. Water temperature is known to be a poor proxy for body temperature, as it does not account for the effectiveness of the diving suit or heat generated by work or active heating systems.\nAs of 2009, the newest dive computers on the market used:\n\nLiquivision X1: V-Planner Live: VPM-B Varying Permeability Model and GAP for X1: B\u00fchlmann GF (Buhlman with Gradient Factors)\nMares: Mares-Wienke Reduced Gradient Bubble Model\nPelagic Pressure Systems: modified Haldanean\/DSAT Database or B\u00fchlmann ZH-L16C(called Z+)\nSeiko: B\u00fchlmann ZH-L12 as modified by Randy Bohrer.\nSuunto: Suunto-Wienke Reduced Gradient Bubble Model. The Suunto folded RGBM is not a true RGBM algorithm, which would be computationally intensive, but a Haldanean model with additional bubble limitation factors.\nUwatec: B\u00fchlmann ZH-L8 \/ADT (Adaptive), MB (Micro Bubble), PMG (Predictive Multigas), B\u00fchlmann ZH-L16 DD (Trimix)\nHeinrichs Weikamp OSTC and DR5: B\u00fchlmann ZH-L16 and B\u00fchlmann ZH-L16 plus Erik Baker's gradient factors deep stop algorithm both for open circuit and fixed set point closed circuit rebreather.\nAs of 2012:\n\nCochran EMC-20H: 20-tissue Haldanean model.\nCochran VVAL-18: nine-tissue Haldanean model with exponential ongasing and linear offgassing.\nDelta P: 16-tissue Haldanean model with VGM (variable gradient model, i.e., the tolerated supersaturation levels change during the dive as a function of the profile, but no details are provided as to how this is done).\nMares: ten-tissue Haldanean model with RGBM; the RGBM part of the model adjusts gradient limits in multiple-dive scenarios through undisclosed \"reduction factors\".:\u200a16\u201320\u200a\nSuunto: nine-tissue Haldanean model with RGBM; the RGBM part of the model adjusts gradient limits in multiple-dive scenarios through undisclosed \"reduction factors\".:\u200a16\u201320\u200a\nUwatec: ZH-L8 ADT (Adaptive), MB (Micro Bubble), PMG (Predictive Multigas), ZH-L16 DD (Trimix).\nAs of 2019:\n\nAqualung: Pelagic Z+ \u2013 a proprietary algorithm based on B\u00fchlmann ZH-L16C algorithm.\nCressi: Haldane and Wienke RGBM algorithm.\nGarmin: B\u00fchlmann ZH-L16C algorithm.\nOceanic: Dual Algorithm: Pelagic Z+ (ZH-L16C) and Pelagic DSAT.\nScubaPro: ZH-L8 ADT (Adaptive), MB (Micro Bubble), PMG (Predictive Multigas), ZH-L16 DD (Trimix).\nShearwater: B\u00fchlmann ZH-L16C with user selectable gradient factors or optional VPM-B and VPM-B\/GFS.\nAs of 2021:\n\nAqualung: Pelagic Z+ \u2013 a proprietary algorithm developed by Dr. John E. Lewis, based on B\u00fchlmann ZH-L16C algorithm. Conservatism may be adjusted by altitude setting, deep stops, and safety stops.\nAtomic: \"Recreational RGBM\" based on the Wienke model, using user input of age, selected risk level, and exertion level to adjust conservatism.\nCressi: RGBM. User settings for conservatism and optional deep and safety stops.\nGarmin: B\u00fchlmann ZH-L16C, with a choice of three preset conservatism settings or customisable gradient factors, and customisable safety stops.\nMares: RGBM or B\u00fchlmann ZH-L16C GF (Gradient Factor) depending on model. Preset and customisable conservatism settings.\nOceanic: User option of dual algorithms: Pelagic Z+ (ZH-L16C) and Pelagic DSAT.\nOceans: B\u00fchlmann ZH-L16C GF (Gradient Factor). Preset conservatism settings.\nRatio: B\u00fchlmann ZH-L16B and VPM-B, user settable Gradient Factors (GFL\/GFH) for B\u00fchlmann and user settable Bubble Radius for VPM.\nScubaPro: ZH-L16 ADT MB PMG. Predictive multi-gas modified algorithm, with various conservatism options with user inputs of experience level, age and physical condition, which are assumed to have some influence on gas elimination rate. Input from breathing rate, skin temperature and heart rate monitor is also available and can be used by the algorithm to estimate a workload condition, which is used to modify the algorithm.\nShearwater: B\u00fchlmann ZH-L16C with optional VPM-B, VPM-B\/GFS and DCIEM. The standard package is B\u00fchlmann with user selectable gradient factors, and the option to enable VPM software which may be used in open-circuit tech and rebreather modes, or enable DCIEM which may be used in air and single-gas nitrox modes. VPM-B\/GFS is a combination of the two models which applies the ceiling from the more conservative model for each stop. The current decompression ceiling may be displayed as an option and the algorithm will calculate decompression at any depth below the ceiling. The GFS option is a hybrid that automatically chooses the decompression ceiling from the more conservative of the VPM-B profile and a B\u00fchlmann ZH-L16C profile. For the B\u00fchlmann profile a single gradient factor is used, adjustable over a range of 70% (most conservative) to 99% (least conservative), the default is 90%.  The DCIEM model differs from ZH-L16C and VPM which are parallel models and assume that all compartments are exposed to ambient partial pressures and no gas interchange occurs between compartments. A serial model assumes that the diffusion takes place through a series of compartments, and only one is exposed to the ambient partial pressures.\nSuunto: RGBM based algorithm with conservatism settings, known to be a comparatively conservative algorithm. There are various versions used in different models. The technical computers use an algorithm that claims flexibility through the use of continuous decompression, which means the current ceiling is displayed instead of a stop depth.\nRGBM\nTechnical RGBM\nFused RGBM: for deep diving, switches between \"RGBM\" and \"Technical RGBM\" for open circuit and rebreather dives to a maximum of 150 m\nFused RGBM 2\nB\u00fchlmann 16 GF (Gradient Factor) based on ZH-L16C\nAs of 2023:\n\nShearwater Research has supplied dive computers to the US Navy with an exponential\/linear algorithm bases on the Thalman algorithm since Cochran Undersea Technology closed down after the death of the owner. This algorithm is not as of 2024 available to the general public on Shearwater computers, although the algorithm is freely available and known to be lower risk than the Buhlmann algorithm for mixed gas and constant set-point CCR diving at deeper depths, which is the primary market for Shearwater products.\n\nDisplay information\nDive computers provide a variety of visual dive information to the diver, usually on a LCD or OLED display. More than one screen arrangement may be selectable during a dive, and the primary screen will display by default and contain the safety critical data. Secondary screens are usually selected by pressing one or two buttons one or more times, and may be transient or remain visible until another screen is selected. All safety critical information should be visible on any screen that will not automatically revert within a short period, as the diver may forget how to get back to it and this may put them as significant risk. Some computers use a scroll through system which tends to require more button pushes, but is easier to remember, as eventually the right screen will turn up, others may use a wider selection of buttons, which is quicker when the sequence is known, but easier to forget or become confused, and may demand more of the diver's attention, :\nMost dive computers display the following basic dive profile and no-stop status information during the dive. This information includes safety critical information, and is usually displayed on the default underwater display, and some may be shown on all underwater displays:\n\nCurrent depth (derived from ambient pressure).\nMaximum depth reached on the current dive.\nNo-stop time, the time remaining at the current depth without the need for decompression stops on ascent.\nElapsed dive time of the current dive.\nMany dive computers also display additional information. Some of this is safety-critical for decompression, and would usually be displayed on all screens available underwater, or have a timed default return to the primary screen: Most of the non-critical information is likely to be useful on at least some dives, and may be displayed on a secondary screen layout which can be selected during the dive.\n\nTotal ascent time, or time to surface (TTS) assuming immediate ascent at recommended rate, and decompression stops as indicated. When multiple gases are enabled in the computer, the time to surface may be predicted based on the optimum gas being selected, during ascent, but the actual time to surface will depend on the actual gas selected, and may be longer than the displayed value. This does not invalidate the decompression calculation, which accounts for the actual exposure and gas selected.\nRequired decompression stop depth and time, also assuming immediate ascent at recommended rate. The depth and duration of the first stop are usually displayed prominently.\nAmbient temperature,(actually temperature of the pressure transducer). This may be a default display or a user selected setting, and may not be on the primary display, as it is not safety-critical information.\nCurrent ascent rate. This may be displayed as an actual speed of ascent, or a relative rate compared to the recommended rate.\nDive profile (often not displayed during the dive, but transmitted to a personal computer). Not a safety-critical information, so usually on a temporary secondary display if available\nGas mixture in use, as selected by the user.\nOxygen partial pressure at current depth, based on selected gas mixture.\nCumulative oxygen toxicity exposure (CNS), computed from measured pressure and time and selected gas mixture.\nBattery charge status or low battery warning.\nTime of day, often with a 12hour or 24 hour format option.\nCompass heading, using a flux gate sensor, with tilt corrections. When available this is usually combined with displays of all safety critical data, so that it does not have to automatically revert to the primary display layout.\nA few computers will display additional information on decompression status after the no-stop limit has been exceeded. These data may be selected as optional display settings by the diver, and may require a more comprehensive understanding of decompression theory and modelling than provided by recreational diver training. They are intended as information that may help a technical diver make a more informed decision while dealing with a contingency that affects decompression risk. \n\nAt depth + 5 minutes, (@+5), shows the effect on time to surface of remaining at the current depth on the current breathing gas for five more minutes. The display will show the amended TTS.\nDelta + 5 (\u0394+5) is the change in time to surface if remaining at the same depth on the same gas for 5 minutes longer. This value will be positive if ingassing, negative if outgassing, and 0 if the extra exposure has no net effect on computed decompression obligation. This is useful for multi-level dives, where it helps estimate whether there will still be enough breathing gas for the ascent.\nDecompression ceiling, the depth at which calculated supersaturation of the controlling tissue is at the maximum permissible level according to the algorithm. This is the shallowest depth to which the diver can ascend with acceptable decompression risk according to the chosen constraints. This depth will be equal to or shallower than the current obligatory stop depth and deeper than the next obligatory stop. When decompression is completed, the ceiling will be zero.\nCurrent gradient factor (GF99), an indication of the diver's current proximity to the baseline M-value of the algorithm in the limiting tissue. If it exceeds 100% then the diver is oversaturated according to the algorithm's least conservative setting. This value will slowly decrease at each decompression stop, and increase during the ascent to the next stop. This functionality may be useful in a contingency when the diver needs to exit the water as soon as possible but at a reasonable decompression risk. Responsible use of this feature requires a good understanding of the theory of decompression and how it is modeled by the computer.\nSurfacing gradient factor, The calculated gradient factor for the controlling tissue if the diver were to surface directly from the current depth, without any stops. The figure shown is a percentage of the calculated M-value at that stage of the dive. If it exceeds 99%, the risk of DCS is higher than for the baseline M-value, and if lowe, then the risk is lower than for the baseline M-value, When indicated decompression clears, it will be at the GF-Hi value the diver selected, This is an optional way of monitoring decompression status which could be useful in an emergency.\nSome computers, known as air-integrated, or gas-integrated, are designed to display information from a diving cylinder pressure sensor, such as:\n\nGas pressure.\nEstimated remaining air time (RAT) based on available gas, rate of gas consumption and ascent time.\nSome computers can provide a real time display of the oxygen partial pressure in the rebreather. This requires an input from an oxygen cell. These computers will also calculate cumulative oxygen toxicity exposure based on measured partial pressure.\nSome computers can display a graph of the current tissue saturation for several tissue compartments, according to the algorithm in use.\nSome information, which has no practical use during a dive, is only shown at the surface to avoid an information overload of the diver during the dive:\n\n\"Time to Fly\" display showing when the diver can safely board an airplane.\nDesaturation time, the estimated time required to return all tissues to surface pressure dissolved gas equilibrium.\nA log of key information about previous dives \u2013 date, start time, maximum depth, duration, and possibly others.\nMaximum non-decompression bottom times for subsequent dives based on the estimated residual concentration of the inert gases in the tissues.\nDive planning functions (no decompression time based on current tissue loads and user-selected depth and breathing gas).\nWarnings and alarms may include:\n\nMaximum operating depth exceeded\nNo decompression limit approaching\nNo decompression limit exceeded\nExcessive ascent rate\nDecompression ceiling violation\nOmitted decompression\nLow cylinder pressure (where applicable)\nOxygen partial pressure high or low\nMaximum depth violation\n\nAudible information\nMany dive computers have warning buzzers that warn the diver of events such as:\n\nExcessive ascent rates.\nMissed decompression stops.\nMaximum operation depth exceeded.\nOxygen toxicity limits exceeded.\nDecompression ceiling violation, or stop depth violation\nSome buzzers can be turned off to avoid the noise.\n\nData sampling, storage and upload\nData sampling rates generally range from once per second to once per 30 seconds, though there have been cases where a sampling rate as low as once in 180 seconds has been used. This rate may be user selectable. Depth resolution of the display generally ranges between 1m and 0.1m. The recording format for depth over the sampling interval could be maximum depth, depth at the sampling time, or the average depth over the interval. For a small interval these will not make a significant difference to the calculated decompression status of the diver, and are the values at the point where the computer is carried by the diver, which is usually a wrist or suspended on a console, and may vary in depth differently to the depth of the demand valve, which determines breathing gas pressure, which is the relevant pressure for decompression computation.\nTemperature resolution for data records varies between 0.1 \u00b0C to 1 \u00b0C. Accuracy is generally not specified, and there is often a lag of minutes as the sensor temperature changes to follow the water temperature. Temperature is measured at the pressure sensor, and is needed primarily to provide correct pressure data, so it is not a high priority for decompression monitoring to give the precise ambient temperature in real time.\nData storage is limited by internal memory, and the amount of data generated depends on the sampling rate. Capacity may be specified in hours of run time, number of dives recorded, or both. Values of up to 100 hours were available by 2010. This may be influenced by sampling rate selected by the diver.\nBy 2010, most dive computers had the ability to upload the data to a PC or smartphone, by cable, infrared or Bluetooth wireless connection.\n\nSpecial purpose dive computers\nSome dive computers are able to calculate decompression schedules for breathing gases other than air, such as nitrox, pure oxygen, trimix or heliox. The more basic nitrox dive computers only support one or two gas mixes for each dive. Others support many different mixes. When multiple gases are supported, there may be an option to set those which will be carried on the dive as active, which sets the computer to calculate the decompression schedule and time to surface based on the assumption that the active gases will be used when they are optimal for decompression. Calculation of tissue gas loads will generally follow the gas actually selected by the diver, unless there is multiple cylinder pressure monitoring to enable automatic gas selection by the computer.\nMost dive computers calculate decompression for open circuit scuba where the proportions of the breathing gases are constant for each mix: these are \"constant fraction\" dive computers. Other dive computers are designed to model the gases in closed circuit scuba (diving rebreathers), which maintain constant partial pressures of gases by varying the proportions of gases in the mixture: these are \"constant partial pressure\" dive computers. These may be switched over to constant fraction mode if the diver bails out to open circuit. There are also dive computers which monitor oxygen partial pressure in real time in combination with a user nominated diluent mixture to provide a real-time updated mix analysis which is then used in the decompression algorithm to provide decompression information.\n\nFreediving computers\nA freediving computer, or general purpose dive computer in freediving mode, will record breath hold dive details automatically while the diver is underwater, and the length of the surface interval between dives. It records each dive, so there is a record of the number of dives. This is useful to ensure adequate surface interval to clear carbon dioxide buildup.\nSurface interval times are also useful to monitor to avoid taravana, the freediving decompression sickness. A dive computer is also the most effective way to notify the diver of the depth at which free-fall should start by a free-fall alarm. monitoring descent and ascent speed, and verifying maximum depth are also useful when training for efficiency. \nTwo types of freediving computer are available, the ones that are dedicated to freediving, and those that are also scuba decompression computers, with a freediving mode. A stopwatch is useful for timing static apnea, rechargeable batteries are an option in some models, and GPS can be useful for spearfishers who wish to mark a place and return to it later. A few models offer a heart rate monitor.\n\nAdditional functionality and features\nSome dive computers provide additional functionality, generally a subset of those listed below:\n\nBreathing gas oxygen analyser\nElectronic compass\nGas blending calculator\nGlobal navigation satellite receiver (only works at the surface)\nLight-meter\nLunar phase indicator (useful for estimating tidal conditions)\nMagnetometer (for detecting ferrous metal)\nPitch and roll angle\nStopwatch\nTime of day in a second time zone\nTime to surface after another 5 minutes at current depth on current gas.\nGauge mode (overrides decompression monitoring, and just records and displays depth and time and leaves the diver to control decompression by following tables). Selecting gauge mode may reset the tissue saturation records to default, which invalidates any further decompression calculations until the diver has fully desaturated.\nAir integration (AI), also known as gas integration: \u2013 Some dive computers are designed to measure, display, and monitor pressure in one or more diving cylinders. The computer is either connected to the first stage by a high pressure hose, or uses a wireless pressure transmitter on the regulator first stage to provide a wireless data signal indicating remaining cylinder pressure, The signals are encoded to eliminate the risk of one diver's computer picking up a signal from another diver's transducer, or interference from other sources. Some dive computers can receive a signal from more than one remote pressure transducer. The Ratio iX3M Tech and others can process and display pressures from up to 10 transmitters.\nWorkload modification of decompression algorithm based on gas consumption rate from integrated gas pressure monitor.\nHeart rate monitor from remote transducer. This can also be used to modify the decompression algorithm to allow for an assumed workload.\nGraphic display of calculated tissue compartment inert gas tensions during and after the dive.\nIndication of computed decompression ceiling in addition to the more usual next stop depth. The effects on decompression risk of following the ceiling rather than remaining below the stop depth is not known, but stop depths are arbitrarily chosen for the calculation of decompression tables, and time spent at any depth below the indicated ceiling depth is processed by the same algorithm.\nDisplay of supersaturation of limiting tissue as a percentage of M-value in the event of an immediate ascent. This is an indicator of decompression risk in the event of an emergency ascent.\nDisplay of current supersaturation of limiting tissue as a percentage of M-value during ascent. This is an indication of decompression stress and risk in real time.\nMultiple active gases for open circuit and closed circuit diluent.\nDeactivation of gas options during dive in case of lost gas. This will trigger the computer to recalculate the estimated time to surface without the deactivated gases.\nDefinition of a new gas during the dive to allow calculations for decompression on gas supplied by another diver.\nBattery charge status.\nAlternative decompression algorithms.\nFeatures and accessories of some models:\n\nPiezo-electric buttons (no moving parts)\nUser input by directional tapping \nRechargeable batteries.\nWireless charging.\nOptional battery types. For example the Shearwater Perdix and Petrel 2 can use 1.5V alkaline cells or 3.6V lithium cells provided they have the same physical format (AA).\nUser changeable batteries.\nBattery redundancy.\nUser selected display colours (useful for the colour-blind), and variable brightness.\nScreen inversion for ambidextrous use of units with plug-in cable connections for oxygen monitors.\nMask or mouthpiece mounted head-up display. (NERD)\nWireless downloading of dive log data.\nFirmware upgrades over the Internet via Bluetooth or USB cable from smart phone or personal computer.\nDisplay prompts for changing settings.\nTwin straps or bungee straps for improved security.\nStrap extensions for wristwatch format computers to allow for fitting over the forearm on bulky diving suits.\nAftermarket straps, for improved security.\nScreen protectors, in the form of a self-adhesive transparent plastic film or a rigid transparent plastic cover.\nSoftware for downloading, display and analysis of logged data. Most downloadable dive computers have a proprietary application, and many can also interface with open source software such as Subsurface. Some can down and upload via a smartphone to the cloud.\n\nHoused smartphones\nSmartphones in underwater housings running a decopression monitoring app may be able to take photos or video as well, provided the housing is suitable.\n\nSafety and reliability\nThe ease of use of dive computers can allow divers to perform complex dives with little planning. Divers may rely on the computer instead of dive planning and monitoring. Dive computers are intended to reduce risk of decompression sickness, and allow easier monitoring of the dive profile. Where present, breathing gas integration allows easier monitoring of remaining gas supply, and warnings can alert the diver to some high risk situations, but the diver remains responsible for planning and safe execution of the dive plan. The computer cannot guarantee safety, and only monitors a fraction of the situation. The diver must remain aware of the rest by personal observation and attention to the ongoing situation. A dive computer can also fail during a dive, due to malfunction or misuse.\n\nFailure modes and probability of failure\nIt is possible for a dive computer to malfunction during a dive. Manufacturers are not obliged to publish reliability statistics, and generally only include a warning in the user manual that they are used at the diver's own risk. Reliability has markedly improved over time, particularly for the hardware.\n\nHardware failures\nMechanical and electrical failures: \n\nLeaks allowing ingress of water to the electronic components, may be caused by:\nCracked faceplate, which is more likely with hard, scratch-resistant glass and sapphire used on watch format units. They are strong, but brittle, and can shatter under impact with a sufficiently hard point contact.\nSeal failures can occur at joints, probably most often at the battery closure, as it is usually the most often disturbed. Computers with user serviceable batteries often use a double O-ring barrel seal to provide a more reliable seal.\nButton failures are one of the more frequent problems, some models are particularly susceptible. Occasionally the failure is in the form of leaks, but more often the switch fails open, which is sometimes a fatigue problem. Pressure sensitive switches with no moving parts are sometimes used to avoid this problem.\nCircuitry failures, other than switch failures, often due to water or battery leaks causing internal corrosion.\nBattery failure, such as running down unexpectedly, leaking, or failing to charge properly. Internal rechargeable batteries exchange a lower risk of water leaks for a higher risk of battery degradation over time.\nNon-rechargeable lithium batteries can explode if incorrectly used in a dive computer with charging facilities.\n\nSoftware failures and reliability issues\nThere have been several instances where dive computers have been recalled due to significant safety issues in the software or factory calibration. Earlier dive computers had to have software upgrades at the factory or an approved agent. This has changed and as of 2024, it is common to be able to update firmware over the internet, via bluetooth or a similar procedure.\nA series of Uwatec Aladin Air X NitrOx dive computers made in 1995 was recalled in 2003 due to faulty software which miscalculated desaturation time, leading to at least seven cases of DCS attributed to their use. This is not the only recall for faulty software or calibration, Suunto D6 and D9s were recalled in 2006, Oceanic Versa Pro 2A in 2006, and  Dacor Darwin computers in 2005, but no injuries were reported, and the units were recalled relatively soon after the problems were reported. The Uwatec Aladin Air X Nitrox recall occurred during a class action suit and after several related lawsuits against the company and several alleged cover-ups, starting as early as 1996. The case was settled on the eve of trial.\n\nInherent risk\nThe main problem in establishing decompression algorithms for both dive computers and production of decompression tables, is that the gas absorption and release under pressure in the human body is still not completely understood. Furthermore, the risk of decompression sickness also depends on the physiology, fitness, condition and health of the individual diver. The safety record of most dive computers indicates that when used according to the manufacturer's instructions, and within the recommended depth range, the risk of decompression sickness is low.\nPersonal settings to adjust conservatism of the algorithm are available for most dive compters. They may be input as undisclosed personal factors, as reductions to M-values by a fixed ratio, by gradient factor, or by selecting a bubble size limit in VPM and RGBM models. The personal settings for recreational computers tend to be additional to the conservatism factors programmed into the algorithm by the manufacturer. Technical diving computers tend to allow a wider range of choice at the user's discretion, and provide warnings that the diver should ensure that they understand what they are doing and the associated risk before adjusting from the moderately conservative factory settings.\n\nHuman error\nMany dive computers have menus, various selectable options and various display modes, which are controlled by a small number of buttons. Control of the computer display differs between manufacturers and in some cases between models by the same manufacturer. The diver may need information not displayed on the default screen during a dive, and the button sequence to access the information may not be immediately obvious. If the diver becomes familiar with the control of the computer on dives where the information is not critical before relying on it for more challenging dives there is less risk of confusion which may lead to an accident.\nMost dive computers are supplied with default factory settings for algorithm conservatism, and maximum oxygen partial pressure, which are acceptably safe in the opinion of the manufacturer's legal advisors. Some of these may be changed to user preferences, which will affect risk. The user manual will generally provide instructions for adjusting and resetting to factory default, with some information on how to choose appropriate user settings. Responsibility for appropriate use of user settings lies with the user who makes or authorises the settings. There is a risk of the user making inappropriate choices due to lack of understanding or input error.\nIn some cases it can be easy to select the wrong setting by accidentally double pressing the same button with cold fingers encased in thick gloves. The process of correcting the setting can be unfamiliar and take a considerably greater number of buttons pressed at a time when there are other important matters to attend to. An example of this type of error would be accidentally selecting oxygen as the breathing gas instead of a travel gas because oxygen is at the top of the gas options list. This is an error that must be corrected as soon as possible as it will set off alarms and cause unsafe decompression calculation errors. Confirmation messages during gas switches can reduce the risk of user error at the cost of an extra button press.\n\nManagement and mitigation strategies\nIf the diver has been monitoring decompression status and is within the no-decompression limits, a computer failure can be acceptably managed by simply surfacing at the recommended ascent rate, and if possible, doing a short safety stop near the surface. If, however the computer could fail while the diver has a decompression obligation, or cannot make a direct ascent, some form of backup is prudent. The dive computer can be considered safety-critical equipment when there is a significant decompression obligation, as failure without some form of backup system can expose the diver to a risk of severe injury or death.\nThe diver may carry a backup dive computer. The probability of both failing at the same time is orders of magnitude lower. Use of a backup which is the same model as the primary simplifies use and reduces the probability of user error, particularly under stress, but makes the equipment redundancy less statistically independent. Statistics for failure rates of dive computers do not appear to be publicly available.\nIf diving to a well regulated buddy system where both divers follow closely matched dive profiles, using the same gases, the buddy's dive computer may be sufficient backup.\nA dive profile can be planned before the dive, and followed closely to allow reversion to the planned schedule if the computer fails. This implies the availability of a backup timer and depth gauge, or the schedule will be useless. It also requires the diver to follow the planned profile conservatively. \nSome organisations such as the American Academy of Underwater Sciences have recommended that a dive plan should be established before the dive and then followed throughout the dive unless the dive is aborted. This dive plan should be within the limits of the decompression tables to increase the margin of safety, and to provide a backup decompression schedule based on the dive tables in case the computer fails underwater. The disadvantage of this extremely conservative use of dive computers is that when used this way, the dive computer is merely used as a bottom timer, and the advantages of real time computation of decompression status \u2013 the original purpose of dive computers \u2013 are sacrificed. This recommendation is not in the 2018 version of the AAUS Standards for Scientific diving: Manual.\nA diver wishing to further reduce the risk of decompression sickness can take additional precautionary measures, such as one or more of: \n\nUse a dive computer with a relatively conservative decompression model.\nInduce additional conservatism in the algorithm by selecting a more conservative personal setting or using a higher altitude setting than the actual dive altitude indicates.\nAdd additional deep safety stops during a deep dive (the efficacy of this approach has not been supported by experiment)\nMake a slow ascent. This will reduce decompression stress in the earlier parts of the ascent, but will make the total time to surface longer if the decompression stress later in the ascent is not to be increased.\nAdd additional shallow safety stops, or stay longer at the stops than required by the computer\nHave a long surface interval between dives. This will decrease risk provided the outgassing calculations of the algorithm are accurate or conservative.\nIf using a backup computer, run one on a low conservatism setting as an indication of fastest acceptable risk ascent for an emergency, and the other at the diver's preferred conservatism for personally acceptable risk when there is no contingency and no rush to surface. The diver can always elect to do more decompression than indicated as necessary by the computer for a lower risk of decompression sickness without incurring a penalty for later dives. Some dive computers can be set to a different gradient factor during a dive, which has the same effect if the diver can remember under stress how to make the adjustment, and some computers can be set to display the maximum tissue supersaturation value for an immediate ascent.\nContinue to breathe oxygen enriched gas after surfacing, either in the water while waiting for the boat, after exiting the water, or both.\n\nManagement of violations\nViolations of the safety limits as indicated by the computer display may occur during a dive for various reasons, including user error and circumstances beyond the diver's control. How this is handled depends on the decompression model, how the algorithm implements the model, and how the manufacturer chooses to interpret and apply the violation criteria.\nMany computers go into a \"lockout mode\" for 24 to 48 hours if the diver violates the safety limits set by the manufacturer, to discourage continued diving after what the manufacturer deems an unsafe dive. Once in lockout mode, these computers will not function until the lockout period has ended. This is usually a reasonable response if lockout is initiated after the dive, as the algorithm will have been used out of scope and the manufacturer will reasonably prefer to avoid further responsibility for its use until tissues can be considered desaturated. When lockout happens underwater it will leave the diver without any decompression information at the time when it is most needed. For example, the Apeks Quantum will stop displaying the depth if the 100 m depth limit is exceeded, but will lock out 5 minutes after surfacing for a missed decompression stop. The Scubapro\/Uwatec Galileo technical trimix computer will switch to gauge mode at 155 m after a warning, after which the diver will get no decompression information. Other computers, for example Delta P's VR3, Cochran NAVY, and the Shearwater range will continue to function, providing 'best guess' functionality while warning the diver that a stop has been missed, or a ceiling violated.\nSome dive computers are extremely sensitive to violations of indicated decompression stop depth. The HS Explorer is programmed to credit time spent even slightly (0.1 metre) above the indicated stop depth at only 1\/60 of the nominal rate. There is no theoretical or experimental basis claimed as justification for this hard limit. Others, such as the Shearwater Perdix, will fully credit any decompression done below the calculated decompression ceiling, which may be displayed as a user selectable option, and is always equal to or shallower than the indicated stop depth. This strategy is supported by the mathematics of the model, but little experimental evidence is available on the practical consequences, so a warning is provided. A violation of the computed decompression ceiling elicits an alarm, which self cancels if the diver immediately descends below the ceiling. The Ratio iX3M will provide a warning if the indicated stop depth is violated by 0.1 m or more, but it is not clear how the algorithm is affected. In many cases the user manual does not provide information on how sensitive the algorithm is to precise depth, what penalties may be incurred by minor discrepancies, or what theoretical basis justifies the penalty. Over-reaction to stop depth violation puts the diver at an unnecessary disadvantage if there is an urgent need to surface, and no computer can guarantee freedom from decompression sickness even if the displayed surfacing profile is followed exactly.\nMore complex functionality is accompanied by more complex code, which is more likely to include undiscovered errors, particularly in non-critical functions, where testing may not be so rigorous. The trend is to be able to download firmware updates online to eliminate bugs as they are found and corrected. In earlier computers, some errors required factory recall.\nThere are circumstances in which a lockout on surfacing is not an appropriate, helpful, safe or reasonable response. If a cave diver surfaces inside a cave, and the computer locks out following a violation, the diver may be in a position where they have no option but to make the return dive without the information the computer could reasonably be expected to provide, putting the diver at considerably more severe risk than strictly necessary. This is a very rare occurrence, but it is a failure that a backup computer cannot mitigate. Depending on circumstances and the specific computer, it may be possible to set it to gauge mode, which would at least provide depth and time data.\n\nRedundancy\nA single computer shared between divers cannot accurately record the dive profile of the second diver, and therefore their decompression status will be unreliable and probably inaccurate. In the event of computer malfunction during a dive, the buddy's computer record may be the best available estimate of decompression status, and has been used as a guide for decompression in emergencies. Further diving after an ascent in these conditions exposes the diver to an unknown additional risk. Some divers carry a backup computer to allow for this possibility. The backup computer will carry the full recent pressure exposure history, and continued diving after a malfunction of one computer will not affect risk provided that the second computer continues to function correctly. It is also possible to set the conservatism on the backup computer to allow for the fastest acceptable ascent in case of an emergency, with the primary computer set for the diver's preferred risk level if this feature is not available on the computer. Under normal circumstances the primary computer will be used to control ascent rate.\n\nHistory\nIn 1951 the Office of Naval Research funded a project with the Scripps Institution of Oceanography for the theoretical design of a prototype decompression computer. Two years later, two Scripps researchers, Groves and Monk, published a paper specifying the required functionalities for a decompression device to be carried by the diver: It must calculate decompression during a multilevel dive; it must take into account residual nitrogen loading from previous dives; and, based on this information, specify a safe ascent profile with better resolution than decompression tables. They suggested using an electrical analog computer to measure decompression and air consumption.\n\nPneumatic analogues\nThe prototype mechanical analogue Foxboro Decomputer Mark I, was produced by the Foxboro Company in 1955, and evaluated by the US Navy Experimental Diving Unit in 1957. The Mark 1 simulated two tissues using five calibrated porous ceramic flow resistors and five bellows actuators to drive a needle which indicated decompression risk during an ascent by moving towards a red zone on the display dial. The US Navy found the device to be too inconsistent.\nThe first recreational mechanical analogue dive computer, the \"decompression meter\" was designed by the Italians De Sanctis & Alinari in 1959 and built by their company named SOS, which also made depth gauges. The decompression meter was distributed directly by SOS and also by scuba diving equipment firms such as Scubapro and Cressi. It was very simple in principle: a waterproof bladder filled with gas inside the casing bled into a smaller chamber through a semi-porous ceramic flow resistor to simulate a single tissue in- and out-gassing. The chamber pressure was measured by a bourdon tube gauge, calibrated to indicate decompression status. The device functioned so poorly that it was eventually nicknamed \"bendomatic\".\nIn 1965, R. A. Stubbs and D. J. Kidd applied their decompression model to a pneumatic analogue decompression computer, and in 1967 Brian Hills reported development of a pneumatic analogue decompression computer modelling the thermodynamic decompression model. It modelled phase equilibration instead of the more commonly used limited supersaturation criteria and was intended as an instrument for on-site control of decompression of a diver based on real-time output from the device. Hills considered the model to be conservative.\nSeveral mechanical analogue decompression meters were subsequently made, some with several bladders for simulating the effect on various body tissues, but they were sidelined with the arrival of electronic computers.\nThe Canadian DCIEM pneumatic analogue computer of 1962 simulated four tissues, approximating the DCIEM tables of the time.\nThe 1973 GE Decometer by General Electric used semi-permeable silicone membranes instead of ceramic flow resistors, which allowed deeper dives.\nThe Farallon Decomputer of 1975 by Farallon Industries, California simulated two tissues, but produced results very different from the US Navy tables of the time, and was withdrawn a year later.\n\nElectrical analogues\nAt the same time as the mechanical simulators, electrical analog simulators were being developed, in which tissues were simulated by a network of resistors and capacitors, but these were found to be unstable with temperature fluctuations, and required calibration before use. They were also bulky and heavy because of the size of the batteries needed. The first analogue electronic decompression meter was the Tracor, completed in 1963 by Texas Research Associates.\n\nDigital\nThe first digital dive computer was a laboratory model, the XDC-1, based on a desktop electronic calculator, converted to run a DCIEM four-tissue algorithm by Kidd and Stubbs in 1975. It used pneumofathometer depth input from surface-supplied divers.\nFrom 1976 the diving equipment company Dacor developed and marketed a digital dive computer which used a table lookup based on stored US Navy tables rather than a real-time tissue gas saturation model. The Dacor Dive Computer (DDC), displayed output on light-emitting diodes for: current depth; elapsed dive time; surface interval; maximum depth of the dive; repetitive dive data; ascent rate, with a warning for exceeding 20 metres per minute; warning when no-decompression limit is reached; battery low warning light; and required decompression.\nThe Canadian company CTF Systems Inc. then developed the XDC-2 or CyberDiver II (1980), which also used table lookup, and the XDC-3, also known as CyberDiverIII, which used microprocessors, measured cylinder pressure using a high-pressure hose, calculated tissue loadings using the Kidd-Stubbs model, and remaining no-stop time. It had an LED matrix display, but was limited by the power supply, as the four 9 V batteries only lasted for 4 hours and it weighed 1.2 kg. About 700 of the XDC models were sold from 1979 to 1982.\nIn 1979 the XDC-4 could already be used with mixed gases and different decompression models using a multiprocessor system, but was too expensive to make an impact on the market.\nIn 1982\/1983, the Hans Hass-DecoBrain I, designed by Divetronic AG, a Swiss start-up, became the first decompression diving computer, capable of displaying the information that today's diving computers do. It worked with a stored decompression table. The DecoBrain II was based on Albert A. B\u00fchlmann's 16 compartment (ZH-L12) tissue model, which J\u00fcrg Hermann, an electronic engineer, implemented in 1981 on one of Intel's first single-chip microcontrollers as part of his thesis at the Swiss Federal Institute of Technology.\nThe 1984 Orca Edge was an early example of a dive computer. Designed by Craig Barshinger, Karl Huggins and Paul Heinmiller, the EDGE did not display a decompression plan, but instead showed the ceiling or the so-called \"safe-ascent-depth\". A drawback was that if the diver was faced by a ceiling, he did not know how long he would have to decompress. The Edge's large, unique display, however, featuring 12 tissue bars permitted an experienced user to make a reasonable estimate of his or her decompression obligation.\nIn the 1980s the technology quickly improved. In 1983 the Orca Edge became available as the first commercially viable dive computer. The model was based on the US Navy dive tables but did not calculate a decompression plan. However, production capacity was only one unit a day.\nIn 1984 the US Navy diving computer (UDC) which was based on a 9 tissue model of Edward D. Thalmann of the Naval Experimental Diving Unit (NEDU), Panama City, who developed the US Navy tables. Divetronic AG completed the UDC development \u2013 as it had been started by the chief engineer Kirk Jennings of the Naval Ocean System Center, Hawaii, and Thalmann of the NEDU \u2013 by adapting the Deco Brain for US Navy warfare use and for their 9-tissue MK-15 mixed gas model under an R&D contract of the US Navy.\nOrca Industries continued to refine their technology with the release of the Skinny-dipper in 1987 to do calculations for repetitive diving. They later released the Delphi computer in 1989 that included calculations for diving at altitude as well as profile recording.\nIn 1986 the Finnish company, Suunto, released the SME-ML. This computer had a simple design, with all the information on display. It was easy to use and was able to store 10 hours of dives, which could be accessed any time. The SME-ML used a 9 compartment algorithm used for the US Navy tables, with tissues half times from 2.5 to 480 minutes. Battery life was up to 1500 hours, maximum depth 60 m.\nIn 1987 Swiss company UWATEC entered the market with the Aladin, which was a bulky and fairly rugged grey device with quite a small screen, a maximum depth of 100 metres, and an ascent rate of 10 metres per minute. It stored data for 5 dives and had a user replaceable 3.6 V battery, which lasted for around 800 dives. For some time it was the most commonly seen dive computer, particularly in Europe. Later versions had a battery which had to be changed by the manufacturer and an inaccurate battery charge indicator, but the brand remained popular.\nThe c1989 Dacor Microbrain Pro Plus claimed to have the first integrated dive planning function, the first EEPROM storing full dive data  for the last three dives, basic data for 9999 dives, and recorded maximum depth achieved, cumulative total dive time, and total number of dives. The LCD provides a graphic indication of remaining no-decompression time.\n\nGeneral acceptance\nEven by 1989, the advent of dive computers had not met with what might be considered widespread acceptance. Combined with the general mistrust, at the time, of taking a piece of electronics that your life might depend upon underwater, there were also objections expressed ranging from dive resorts felt that the increased bottom time would upset their boat and meal schedules, to that experienced divers felt that the increased bottom time would, regardless of the claims, result in many more cases of decompression sickness. Understanding the need for clear communication and debate, Michael Lang of the California State University at San Diego and Bill Hamilton of Hamilton Research Ltd. brought together, under the auspices of the American Academy of Underwater Sciences a diverse group that included most of the dive computer designers and manufacturers, some of the best known hyperbaric medicine theorists and practitioners, representatives from the recreational diving agencies, the cave diving community and the scientific diving community.\nThe basic issue was made clear by Andrew A. Pilmanis in his introductory remarks: \"It is apparent that dive computers are here to stay, but are still in the early stages of development. From this perspective, this workshop can begin the process of establishing standard evaluation procedures for assuring safe and effective utilization of dive computers in scientific diving.\"\nAfter meeting for two days the conferees were still in, \"the early stages of development,\" and the \"process of establishing standard evaluation procedures for assuring safe and effective utilization of dive computers in scientific diving,\" had not really begun. University of Rhode Island diving safety officer Phillip Sharkey and Orca Edge's Director of Research and Development, Paul Heinmiller, prepared a 12-point proposal that they invited the diving safety officers in attendance to discuss at an evening closed meeting. Those attending included Jim Stewart (Scripps Institution of Oceanography), Lee Somers (University of Michigan), Mark Flahan (San Diego State University), Woody Southerland (Duke University), John Heine (Moss Landing Marine Laboratories), Glen Egstrom (University of California, Los Angeles), John Duffy (California Department of Fish and Game), and James Corry (United States Secret Service). Over the course of several hours the suggestion prepared by Sharkey and Heinmiller was edited and turned into the following 13 recommendations:\n\nOnly those makes and models of dive computers specifically approved by the Diving Control Board may be used.\nAny diver desiring the approval to use a dive computer as a means of determining decompression status must apply to the Diving Control Board, complete an appropriate practical training session and pass a written examination.\nEach diver relying on a dive computer to plan dives and indicate or determine decompression status must have his own unit.\nOn any given dive, both divers in the buddy pair must follow the most conservative dive computer.\nIf the dive computer fails at any time during the dive, the dive must be terminated and appropriate surfacing procedures should be initiated immediately.\nA diver should not dive for 18 hours before activating a dive computer to use it to control his diving.\nOnce the dive computer is in use, it must not be switched off until it indicates complete outgassing has occurred or 18 hours have elapsed, whichever comes first.\nWhen using a dive computer, non-emergency ascents are to be at the rate specified for the make and model of dive computer being used.\nAscent rates shall not exceed 40 fsw\/min in the last 60 fsw.\nWhenever practical, divers using a dive computer should make a stop between 10 and 30 feet for 5 minutes, especially for dives below 60 fsw.\nOnly 1 dive on the dive computer in which the NDL of the tables or dive computer has been exceeded may be made in any 18-hour period.\nRepetitive and multi-level diving procedures should start the dive, or series of dives, at the maximum planned depth, followed by subsequent dives of shallower exposures.\nMultiple deep dives require special consideration.\nAs recorded in \"Session 9: General discussion and concluding remarks:\" \n\nMike Lang next lead the group discussion to reach consensus on the guidelines for use of dive computers. These 13 points had been thoroughly discussed and compiled the night before, so that most of the additional comments were for clarification and precision. The following items are the guidelines for use of dive computers for the scientific diving community. It was again reinforced that almost all of these guidelines were also applicable to the diving community at large.\nAfter the AAUS workshop most opposition to dive computers dissipated, numerous new models were introduced, the technology dramatically improved and dive computers soon became standard scuba diving equipment. Over time, some of the 13 recommendations became irrelevant, as more recent dive computers continue running while they have battery power, and switching them off mainly turns off the display.\n\nFurther development\nc1996, Mares marketed a dive computer with spoken audio output, produced by Benemec Oy of Finland.\nc2000, HydroSpace Engineering developed the HS Explorer, a Trimix computer with optional PO2 monitoring and twin decompression algorithms, B\u00fchlmann, and the first full RGBM implementation.\nIn 2001, the US Navy approved the use of Cochran NAVY decompression computer with the VVAL 18 Thalmann algorithm for Special Warfare operations.\nIn 2008, the Underwater Digital Interface (UDI) was released to the market. This dive computer, based on the RGBM model, includes a digital compass, an underwater communication system that enables divers to transmit preset text messages, and a distress signal with homing capabilities.\nBy 2010 the use of dive computers for decompression status tracking was virtually ubiquitous among recreational divers and widespread in scientific diving. 50 models by 14 manufacturers were available in the UK.\nThe variety and number of additional functions available has increased over the years.\nWristwatch format housings have become common. They are compact and can also serve as daily wear wristwatches, but the display area is limited by the size of the unit and may be difficult to read for divers with poorer vision, and control buttons are necessarily small and may be awkward to use with thick gloves. Battery life may also be limited by the available volume.\n\nSmartphone housings\nWaterproof housings are marketed which use a smartphone, depth and temperature sensors and a decompression app to provide dive computer capabilities. Depth ratings vary, but 80msw is claimed for some. Bluetooth wireless communications have been used for communication between the smartphone and external sensors. Specifications may not mention any validation tests or compliance with standards relevant to diving equipment. A variety of features are offered based on the smartphone platform. Android and iOS operating systems are supported.\n\nValidation\nVerification is the determination that a dive computer functions correctly, in that it correctly executes its programmed algorithm, and this would be a standard quality assurance procedure by the manufacturer, while validation confirms that the algorithm provides the accepted level of risk. The risk of the decompression algorithms programmed into dive computers may be assessed in several ways, including tests on human subjects, monitored pilot programs, comparison to dive profiles with known decompression sickness risk, and comparison to risk models.\n\nPerformance of dive computers exposed to profiles with known human subject results.\nStudies (2004) at the University of Southern California's Catalina hyperbaric chamber ran dive computers against a group of dive profiles that have been tested with human subjects, or have a large number of operational dives on record.\nThe dive computers were immersed in water inside the chamber and the profiles were run. Remaining no-decompression times, or required total decompression times, were recorded from each computer 1 min prior to departure from each depth in the profile. The results for a 40 msw \"low risk\" multi-level no-decompression dive from the PADI\/DSAT RDP test series provided a range of 26 min of no-decompression time remaining to 15 min of required decompression time for the computers tested. The computers which indicated required decompression may be regarded as conservative: following the decompression profile of a conservative algorithm or setting will expose the diver to a reduced risk of decompression, but the magnitude of the reduction is unknown. Conversely the more aggressive indications of the computers showing a considerable amount of remaining no-decompression time will expose the diver to a greater risk than the fairly conservative PADI\/DSAT schedule, of unknown magnitude.\n\nComparative assessment and validation\nEvaluation of decompression algorithms could be done without the need for tests on human subjects by establishing a set of previously tested dive profiles with a known risk of decompression sickness. This could provide a rudimentary baseline for dive computer comparisons. As of 2012, the accuracy of temperature and depth measurements from computers may lack consistency between models making this type of research difficult.\n\nAccuracy of displayed data\nEuropean standard \"EN13319:2000 Diving accessories - Depth gauges and combined depth and time measuring devices - Functional and safety requirements, test methods\", specifies functional and safety requirements and accuracy standards for depth and time measurement in dive computers and other instruments measuring water depth by ambient pressure. It does not apply to any other data which may be displayed or used by the instrument.\nTemperature data are used to correct pressure sensor output, which is non-linear with temperature, and are not as important as pressure for the decompression algorithm, so a lesser level of accuracy is required. A study published in 2021 examined the response time, accuracy and precision of water temperature measurement computers and found that 9 of 12 models were accurate within 0.5 \u00b0C given sufficient time for the temperature to stabilise, using downloaded data from open water and wet chamber dives in fresh- and seawater. High ambient air temperature is known to affect temperature profiles for several minutes into a dive, depending on the location of the pressure sensor, as the heat transfer from computer body to the water is slowed by factors such as poor thermal conductivity of a plastic housing, internal heat generation, and mounting the sensor orifice in contact with the insulation of the diving suit. An edge-mounted sensor in a small metal housing will follow ambient temperature changes much faster than a base mounted sensor in a large, thick-walled plastic housing, while both provide accurate pressure signals.\nAn earlier survey of 49 models of decompression computer published in 2012 showed a wide range of error in displayed depth and temperature. Temperature measurement is primarily used to ensure correct processing of the depth transducer signal, so measuring the temperature of the pressure transducer is appropriate, and the slow response to external ambient temperature is not relevant to this function, provided that the pressure signal is correctly processed.\nNearly all of the tested computers recorded depths greater than the actual pressure would indicate, and were markedly inaccurate (up to 5%) for some of the computers. There was considerable variability in permitted no-stop bottom times, but for square profile exposures, the computer-generated values tended to be more conservative than tables at depths shallower than 30 m, but less conservative at 30\u201350 m. The no-stop limits generated by the computers were compared to the no-stop limits of the DCIEM and RNPL tables. Variation from applied depth pressure measured in a decompression chamber, where accuracy of pressure measurement instrumentation is periodically calibrated to fairly high precision (\u00b10.25%), showed errors from -0.5 to +2m, with a tendency to increase with depth.\nThere appeared to be a tendency for models of computer by the same manufacturer to display a similar variance in displayed pressure, which the researchers interpreted as suggesting that the offset could be a deliberate design criterion, but could also be an artifact of using similar components and software by the manufacturer. The importance of these errors for decompression purposes is unknown, as ambient pressure, which is measured directly, but not displayed, is used for decompression calculations. Depth is calculated as a function of pressure, and does not take into account density variations in the water column. Actual linear distance below the surface is more relevant for scientific measurement, while displayed depth is more relevant to forensic examinations of dive computers, and for divers using the computer in gauge mode with standard decompression tables, which are usually set up for pressure in feet or metres of water column .\n\nErgonomic considerations\nIf the diver cannot effectively use the dive computer during a dive it is of no value except as a dive profile recorder.\nTo effectively use the device the ergonomic aspects of the display and control input system (User interface) are important. Misunderstanding of the displayed data and inability to make necessary inputs can lead to life-threatening problems underwater. The operating manual is not available for reference during the dive, so either the diver must learn and practice the use of the specific unit before using it in complex situations, or the operation must be sufficiently intuitive that it can be worked out on the spot, by a diver who may be under stress at the time. Although several manufacturers claim that their units are simple and intuitive to operate, the number of functions, layout of the display, and sequence of button pressing is markedly different between different manufacturers, and even between different models by the same manufacturer. Number of buttons that may need to be pressed during a dive generally varies between two and four, and the layout and sequence of pressing buttons can become complicated. Experience using one model may be of little use preparing the diver to use a different model, and a significant relearning stage may be necessary. Previous experience may even be a disadvantage when the knowledge of one system may confuse the diver who needs to use a different system under stress. Both technical and ergonomic aspects of the dive computer are important for diver safety. Underwater legibility of the display may vary significantly with underwater conditions and the visual acuity of the individual diver. If labels identifying output data and menu choices are not legible at the time they are needed, they do not help. Legibility is strongly influenced by text size, font, brightness, and contrast. Colour can help in recognition of meaning, such as distinguishing between normal and abnormal conditions, but may detract from legibility, particularly for the colour-blind, and a blinking display demands attention to a warning or alarm, but is distracting from other information.\nSeveral criteria have been identified as important ergonomic considerations:\n\nEase of reading critical data, including:\nNo decompression time remaining\nCurrent depth\nElapsed time since the beginning of the dive (run time)\nIf decompression is required, total time to surface, and depth of the next required decompression stop\nIf gas integration is the only way to monitor the remaining gas supply, the remaining gas pressure.\nEase of reading and accessibility of the primary screen display. Misinterpretation of the display data can be very dangerous. This can occur for various reasons, including lack of identifying information and poor legibility. Ease of returning to the primary screen from alternative display options is also important. If the diver cannot remember how to get back to the screen which displays safety-critical information, their safety may be severely compromised. Divers may not fully understand and remember the operating instructions, as they tend to be complicated. Under stress complicated procedures are more likely to be forgotten or misapplied. Alternative screens may revert to the primary screen automatically after a time sufficient to read the auxiliary information. Critical information may be displayed on all stable screen options during a dive as a compromise. It is preferable for the data to be visible by default, and not require illumination by a dive light or internal lighting that needs a button pressed to light up. Some manufacturers offer similar functionality in optional compact and larger screen formats.\nEase of use and understanding of the user manual.\nEase of reading and clarity of meaning of warnings. These may be provided by simple symbol displays, by audible alarms, flashing displays, text messages, colour coding or combinations of these. Alarms should clearly indicate the problem, so the diver need not waste time trying to work out what is at fault, and can take immediate action to correct the problem.\nHead-up displays can be used to provide the diver with a view of critical information which is always visible. These can be mounted on the mask, or on the mouthpiece assembly. Head-up displays require special near-eye 0ptics to allow correct focus on the display. In conditions of very low visibility, a head-up display has the advantage that the diver's ability to see the display is not affected by turbidity. It also lets the diver monitor all displayed dive data without interrupting their work.\nFor more technical applications, ease of making gas switches to both preset gas mixes carried by the diver, and non-preset mixes, which might be supplied by another diver.\nEase of accessing alternative screen data, much of which is not directly important for safety, but may affect the success of the dive in other ways, like use of compass features.\nLegibility of the display under various ambient conditions of visibility and lighting, and for varying visual acuity of the diver, which may include fogging of the mask or even loss of the mask.\n\nForm factor\nThere are four commonly used form factors:\n\nWristwach housings are compact, light, and may be used as daily use wristwatches. Freediving computers are usually in this format, but it is also popular for scuba.\nCircular housings (puck) fit into most diving instrument consoles and have optional wrist mounts. The shape and size were determined by the common sizes of analog mechanical diving instruments (depth gauges and pressure gauges) which commonly used console mounts.\nRectangular housings (brick) are intended for wrist mounting, using a larger screen size for easier-to-read displays or more information on the screen. They often have a curved back surface for stability, and two straps for security. There are also asymmetrically styled housings with similar characteristics and features. This may be the most popular format for technical diving.\nSmartphone housings are sized to accept a reasonable range of phone models and are therefore necessarily larger. The display area is inherently large, and display quality depends on the smartphone used. They may be wrist mounted or carried in the hand with a safety lanyard, particularly if they are also used as cameras.\n\nManufacturing and performance standards\nStandards relevant in the European Union:\n\nWhen a dive computer is integrated with a cylinder pressure gauge it has to be certified according to EN250 (respiratory equipment) and the PPE Directive becomes mandatory.\nThe EMC directive (89\/336\/EEC) for electrical appliances, requires that they do not cause electrical interference, and are not susceptible to it.\nEN13319:2000: covers equipment for measuring depth and time, but explicitly excludes monitoring of decompression obligation.\nPPE Directive 89\/686\/EEC is intended to harmonize products to provide a high level of protection and safety, but dive computers are not listed in the directive under section 3.11 - additional requirements specific to particular risks \u2013  safety devices for diving equipment. Several other classes of diving equipment such as respiratory equipment (EN250:2002), buoyancy compensators (EN1809:1999), combined buoyancy and rescue devices (EN12628:2001), respiratory equipment for compressed nitrox and oxygen (EN13949:2004), rebreathers (EN14143:2004), and dry suits (EN14225-2:2005) fall under the PPE directive.)\nThe general quality assurance standard ISO9001\n\nOperational considerations for use in commercial diving operations\nTheir acceptance of dive computers for use in commercial diving varies between countries and industrial sectors. Validation criteria have been a major obstacle to acceptance of diving computers for commercial diving. Millions of recreational and scientific dives each year are successful and without incident, but the use of dive computers remains prohibited for commercial diving operations in several jurisdictions because the algorithms used cannot be guaranteed safe to use, and the legislative bodies who can authorise their use have a duty of care to workers. Manufacturers do not want to invest in the expensive and tedious process of official validation, while regulatory bodies will not accept dive computers until a validation process has been documented.\nVerification is the determination that a dive computer functions correctly, in that it correctly executes its programmed algorithm, while validation confirms that the algorithm provides the accepted level of risk.\nIf the decompression algorithm used in a series of dive computers is considered to be acceptable\nfor commercial diving operations, with or without additional usage guidelines, then there are\noperational issues that need to be considered:\n\nThe computer must be simple to operate or it will probably not be accepted.\nThe display must be easily read in low visibility conditions to be effectively used.\nThe display must be clear and easily understood, even if the diver is influenced by nitrogen narcosis, to reduce the risk of confusion and poor decisions.\nThe decompression algorithm should be adjustable to more conservative settings, as some divers may want a more conservative profile.\nThe dive computer must be easy to download to collect profile data so that analysis of dives can be done.\n\nRebreather control and monitor hardware\nThe functional requirements of an electronically controlled closed circuit rebreather are very similar to the functions and capacity of technical diving decompression computers for rebreather diving, and some rebreather manufacturers use dive computer hardware repackaged by dive computer manufacturers as rebreather control and monitoring units. The software may be modified to provide the display of multiple oxygen cell readings, warnings, alarms and voting logic, and the dive computer hardware may be hard-wired to the rebreather control hardware.\n\nBottom timer\nA bottom timer, or dive timer, is an electronic device that records the depth at specific time intervals during a dive, and displays current depth, maximum depth, elapsed time and may also display water temperature and average depth. It does not calculate decompression data at all, and is equivalent to gauge mode on many dive computers.\n\nTraining and certification\nThe approach to training in the use of a dive computer has changed over time. Originally a dive computer was considered special equipment, and the user was responsible for ensuring that they knew how to use it correctly. The AAUS recommendations from the Dive Computer Workshop of 1989 stipulated passing a written exam before scientific divers should be allowed to use personal dive computers in the field. As they became more common, and the usual way of monitoring the dive, minimal instruction on the use of the computer became integrated into dive training as part of the training for a given certification. This is complicated by the probability of more than one model being used by the learners on a given course, except where the school supplied the computers.  Since late 2009, it has been an option for PADI Open Water Diver courses, to do a dive computer section in place of learning to use the dive tables. A booklet is supplied on how to use and select a dive computer. SDI was an early adopter of use of dive computers in training from entry level, and offers the course named SDI Computer Diver intended for divers certified through agencies which used traditional dive tables for planning during their training, and have not been formally trained in the use of dive computers.\nIn 2024 Scuba Schools International (SSI) announced a training program called \"Computer Diver\" which covers the basic functionality, setup and operation of dive computers. The training is considered appropriate for ages 10 and up, to a maximum depth of 30 m, and is expected to take 3 to 6 hours.\nSimilarly, PADI schools offer a course called \"Computer Diving Specialist\", which has a prerequisite certification of PADI Open Water Diver, the minimum level certification for autonomous recreational diving. The course comprises up to three classroom sessions and an optional open water dive for a beginner with a minimum experience of four open water dives limited to 18 metres to become a specialist by PADI standards.\nThere is a wide variation in detail of operation for each manufacturer, and in many cases between the models available from each manufacturer, so only the basic information and principles are portable between models, and significant relearning is required to be able to use a new computer safely. This situation could be improved by an internationally accepted standard for user interfaces for critical functions. The information required to safely operate most dive computers is normally extracted from the owner's manual by the user, and in many cases from videos freely available on the internet. In most such situations there is no competence assessment, and the user finds out by trial and error, while diving, what they have failed to understand or remember.\n\nManufacturers\nAqua Lung\/La Spirotechnique markets under Aqualung, Apeks, Oceanic, Aeris, and former brands of U.S. Divers, SeaQuest, Technisub, plus owns Pelagic Pressure Systems\nBenemec Oy, marketed by A.P.Valves (Buddy) and Mares\nCitizen Watch\nCochran Undersea Technology (Cochran) was the supplier of dive computers to the US Navy. They were programmed with US Navy specified algorithm based on the Thalmann algorithm. Cochran has closed down after the death of the founder, and the US Navy has been using Shearwater Research computers programmed with the decompression model specified by the Navy since then.\nDeepblu\nDelta P Technology (VR2)\nDivesoft (Liberty)\nGarmin\nHeinrichsWeikamp (Open source)\nHTM Sports: Dacor and Mares\nHydroSpace Engineering(HSE)\nLiquivision\nOceans (Oceans S1 Supersonic)\nPelagic Pressure Systems, Acquired by Aqua Lung in May 2015,\nRatio Computers\nScubapro-UWATEC owned by Johnson Outdoors\nSeiko, also marketed by:\nCressi, Dive Rite, Scubapro, Tusa, Zeagle\nShearwater Research (Shearwater)\nSherwood Scuba\nSuunto\nTechnical Dive Computers\nUemis\nUnderwater Technology Center\nVR Technology (VR3)\n\nValue\nAlong with delayed surface marker buoys, dive computers stood out in a 2018 survey of European recreational divers and diving service providers as being perceived as highly important safety equipment.\n\nSee also\nReferences\nFurther reading\nBlogg, S.L.; Lang, M.A.; M\u00f8llerl\u00f8kken, A., eds. (2012). \"Proceedings of the Validation of Dive Computers Workshop\". European Underwater and Baromedical Society Symposium, August 24, 2011. Gdansk. Trondheim: Norwegian University of Science and Technology.\nAzzopardi, Elaine; Sayer, Martin D.J. (24\u201329 September 2012). \"Not all are created equal: operational variability in 49 models of diving computer\". In Lobel, Lisa Kerr; Seller, Diana L. (eds.). Diving For Science 2012, Proceedings of the American Academy of Underwater Scientists 31st Scientific Sympoium. Monterey, CA: American Academy of Underwater Sciences. pp. 36\u201340. ISBN 978-0-9800423-6-8. Retrieved 22 May 2021.\nFraedrich, Doug (December 2018). \"Validation of algorithms used in commercial off-the-shelf dive computer\". Diving and Hyperbaric Medicine. 48 (4): 252\u2013258. doi:10.28920\/dhm48.4.252-258. PMC 6355308. PMID 30517958.\n\nExternal links\n Media related to Dive computers at Wikimedia Commons","59":"The Dix\u2013Hallpike or Nyl\u00e9n\u2013B\u00e1r\u00e1ny test is a diagnostic maneuver from the group of rotation tests used to identify benign paroxysmal positional vertigo (BPPV).\n\nProcedure\nWhen performing the Dix\u2013Hallpike test, patients are lowered quickly to a supine position (lying horizontally with the face and torso facing up) with the neck extended 30 degrees below horizontal by the clinician performing the maneuver.\nThe Dix\u2013Hallpike and the side-lying testing position have yielded similar results. As such, the side-lying position can be used if the Dix\u2013Hallpike cannot be performed easily.\nSteps: \n\nThe examiner looks for nystagmus (usually accompanied  by vertigo). In BPPV, the nystagmus typically occurs in A or B only, and is torsional\u2014the fast phase beating toward the lower ear. Its onset is usually delayed a few seconds, and it lasts 10\u201320 seconds. As the patient is returned to the upright position, transient nystagmus may occur in the opposite direction. Both nystagmus and vertigo typically decrease on repeat testing.\n\nInterpretation\nPositive test result\nA positive test is indicated by patient report of a reproduction of vertigo and clinician observation of nystagmus (involuntary eye movement).\nFor some patients, this maneuver may be contraindicated, and a modification may be needed that also targets the posterior semicircular canal. Such patients include those who are too anxious about eliciting the uncomfortable symptoms of vertigo, and those who may not have the range of motion necessary to comfortably be in a supine position. The modification involves the patient moving from a seated position to side-lying without their head extending off the examination table, such as with Dix\u2013Hallpike. The head is rotated 45 degrees away from the side being tested, and the eyes are examined for nystagmus.\n\nNegative test\nIf the test is negative, it makes benign positional vertigo a less likely diagnosis and central nervous system involvement should be considered.\n\nAdvantages\nAlthough there are alternative methods to administering the test, Cohen proposes advantages to the classic maneuver. The test can be easily administered by a single examiner, which prevents the need for external aid. Due to the position of the subject and the examiner, nystagmus, if present, can be observed directly by the examiner.\n\nLimitations\nThe negative predictive value of this test is not 100%. Some patients with a history of BPPV will not have a positive test result. The estimated sensitivity is 79%, along with an estimated specificity of 75%.\nThe test may need to be performed more than once, as it is not always easy to demonstrate observable nystagmus that is typical of BPPV. Also, the test results can be affected by the speed with which the maneuver is conducted and the plane of the occiput.\nThere are several disadvantages proposed by Cohen for the classic maneuver. Patients may be too tense, for fear of producing vertigo symptoms, which can prevent the necessary brisk passive movements for the test. A subject must have adequate cervical spine range of motion to allow neck extension, as well as trunk and hip range of motion to lie supine. From the previous point, the use of this maneuver can be limited by musculoskeletal and obesity issues in a subject.\n\nPrecautions and contraindications\nIn rare cases a patient may be unable or unwilling to participate in the Dix\u2013Hallpike test due to physical limitations. In these circumstances the side-lying test or other alternative tests may be used.\nPrecautions\n\nThe Dix\u2013Hallpike maneuver places a degree of stress on the patient's lower back; therefore, a cautious approach must be taken with patients who are suffering from back pain.\nSevere respiratory or cardiac problems may not allow a patient to tolerate the maneuver. For example a patient with orthopnoea may not be able to participate in the procedure, as the patient may have troubling breathing when lying down.\nAbsolute contraindications \n\nNeck surgery\nSevere rheumatoid arthritis\nAtlantoaxial and occipitoatlantal instability\nAplasia of odontoid process\nCervical myelopathy\nCervical radiculopathy\nCarotid sinus syncope\nVascular dissection syndromes\n\nSee also\nTilt table test\nEpley maneuver \u2013 used to treat BPPV\n\nFootnotes\nExternal links\nOverview and diagrams at dizziness-and-balance.com Archived 2012-03-07 at the Wayback Machine\nvideo of Dix\u2013Hallpike test","60":"Dizziness is an imprecise term that can refer to a sense of disorientation in space, vertigo, or lightheadedness. It can also refer to disequilibrium or a non-specific feeling, such as giddiness or foolishness.\nDizziness is a common medical complaint, affecting 20\u201330% of persons. Dizziness is broken down into four main subtypes: vertigo (~25\u201350%), disequilibrium (less than ~15%), presyncope (less than ~15%), and nonspecific dizziness (~10%).\n\nVertigo is the sensation of spinning or having one's surroundings spin about them. Many people find vertigo very disturbing and often report associated nausea and vomiting.\nPresyncope describes lightheadedness or feeling faint; the name relates to syncope, which is actually fainting.\nDisequilibrium is the sensation of being off balance and is most often characterized by frequent falls in a specific direction.  This condition is not often associated with nausea or vomiting.\nNon-specific dizziness may be psychiatric in origin.  It is a diagnosis of exclusion and can sometimes be brought about by hyperventilation.\n\nMechanism and causes\nMany conditions cause dizziness because multiple parts of the body are required for maintaining balance including the inner ear, eyes, muscles, skeleton, and the nervous system. Thus dizziness can be caused by a variety of problems and may reflect a focal process (such as one affecting balance or coordination) or a diffuse one (such as a toxic exposure or low perfusion state).\nCommon causes of dizziness include:\n\nInadequate blood supply to the brain due to:\nA sudden fall in blood pressure\nHeart problems or artery blockages\nAnaemias, such as vitamin B12 deficiency anemia, iron deficiency anemia\nLoss or distortion of vision or visual cues\nStanding too quickly\/prolonged standing\nDisorders of the inner ear\nDehydration\nDistortion of brain\/nervous function by medications such as anticonvulsants and sedatives\nDysfunction of cervical proprioception\nSide effects from other prescription drugs, such as proton-pump inhibitors or Coumadin (warfarin)\n\nDiagnosis\nDifferential diagnosis\nDizziness may occur from an abnormality involving the brain (in particular the brainstem or cerebellum), inner ear, eyes, heart, vascular system, fluid or blood volume, spinal cord, peripheral nerves, or body electrolytes. Dizziness can accompany certain serious events, such as a concussion or brain bleed, epilepsy and seizures (convulsions), stroke, and cases of meningitis and encephalitis. However, the most common subcategories can be broken down as follows: 40% peripheral vestibular dysfunction, 10% central nervous system lesion, 15% psychiatric disorder, 25% presyncope\/disequilibrium, and 10% nonspecific dizziness. Some vestibular pathologies have symptoms that are comorbid with mental disorders.\nWhile traditional medical teaching has focused on determining the cause of dizziness based on the category (such as vertigo vs. presyncope),  research published in 2017 suggests that this analysis is of limited clinical utility.\nMedical conditions that often have dizziness as a symptom include:\n\nBenign paroxysmal positional vertigo\nM\u00e9ni\u00e8re's disease\nLabyrinthitis\nOtitis media\nBrain tumor\nAcoustic neuroma\nMotion sickness\nRamsay Hunt syndrome\nFatal Familial Insomnia\nMigraine\nMultiple sclerosis\nPregnancy\nLow blood pressure (hypotension)\nLow blood oxygen content (hypoxemia)\nHeart attack\nIron deficiency (anemia)\nVitamin B12 deficiency\nLow blood sugar (hypoglycemia)\nHormonal changes (e.g. thyroid disease, menstruation, pregnancy)\nPanic disorder\nHyperventilation\nAnxiety\nDepression\nAge-diminished visual, balance, and perception of spatial orientation abilities\nStroke; cause of isolated dizziness in 0.7% of people who present to the emergency department\n\nEpidemiology\nAbout 20\u201330% of the population report to have experienced dizziness at some point in 2008.\n\nDisequilibrium\nIn medicine, disequilibrium refers to impaired equilibrioception that can be characterised as a sensation of impending fall or of the need to obtain external assistance for proper locomotion. It is sometimes described as a feeling of improper tilt of the floor, or as a sense of floating. This sensation can originate in the inner ear or other motion sensors, or in the central nervous system. Neurologic disorders tend to cause constant vertigo or disequilibrium and usually have other symptoms of neurologic dysfunction associated with the vertigo. Many medications used to treat seizures, depression, anxiety, and pain affect the vestibular system and the central nervous system which can cause the symptom of disequilibrium.\n\nSee also\nReferences\nExternal links\n\nDizzytimes.com (Archived 2020-05-27 at the Wayback Machine)\u2014Online community for people with vertigo and dizziness\nDysautonomia Youth Network of America, Inc.","61":"A digital object identifier (DOI) is a persistent identifier or handle used to uniquely identify various objects, standardized by the International Organization for Standardization (ISO). DOIs are an implementation of the Handle System; they also fit within the URI system (Uniform Resource Identifier). They are widely used to identify academic, professional, and government information, such as journal articles, research reports, data sets, and official publications. \nA DOI aims to resolve to its target, the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL where the object is located. Thus, by being actionable and interoperable, a DOI differs from ISBNs or ISRCs which are identifiers only. The DOI system uses the indecs Content Model for representing metadata.\nThe DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI should provide a more stable link than directly using its URL. But if its URL changes, the publisher must update the metadata for the DOI to maintain the link to the URL. It is the publisher's responsibility to update the DOI database. If they fail to do so, the DOI resolves to a dead link, leaving the DOI useless.\nThe developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000. Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs. The DOI system is implemented through a federation of registration agencies coordinated by the IDF. By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations, and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.\n\nNomenclature and syntax\nA DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.\n\nprefix\/suffix\nThe prefix identifies the registrant of the identifier and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a number greater than or equal to 1000, whose limit depends only on the total number of registrants. The prefix may be further subdivided with periods, like 10.NNNN.N.\nFor example, in the DOI name 10.1000\/182, the prefix is 10.1000 and the suffix is 182. The \"10\" part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace, and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).\nDOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works such as licenses, parties to a transaction, etc.\nThe names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.\n\nDisplay\nThe official DOI Handbook explicitly states that DOIs should be displayed on screens and in print in the format doi:10.1000\/182.\nContrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https:\/\/doi.org\/10.1000\/182) instead of the officially specified format (for example, doi:10.1000\/182) This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL\u2014providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.\nThe CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyperlinked to its appropriate URL\u2014the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window\/tab in their browser in order to go to the appropriate page for the document the DOI represents.\nSince DOI is a namespace within the Handle System, it is semantically correct to represent it as the URI info:doi\/10.1000\/182.\n\nContent\nMajor content of the DOI system currently includes:\n\nScholarly materials (journal articles, books, ebooks, etc.) through Crossref, a consortium of around 3,000 publishers; Airiti, a leading provider of Chinese and Taiwanese electronic academic journals; and the Japan Link Center (JaLC) an organization providing link management and DOI assignment for electronic academic journals in Japanese.\nResearch datasets through DataCite, a consortium of leading research libraries, technical information providers, and scientific data centers;\nEuropean Union official publications through the EU publications office;\nThe Chinese National Knowledge Infrastructure project at Tsinghua University and the Institute of Scientific and Technical Information of China (ISTIC), two initiatives sponsored by the Chinese government.\nPermanent global identifiers for both commercial and non-commercial audio\/visual content titles, edits, and manifestations through the Entertainment ID Registry, commonly known as EIDR.\nIn the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.\nOther registries include Crossref and the multilingual European DOI Registration Agency (mEDRA). Since 2015, RFCs can be referenced as doi:10.17487\/rfc....\n\nFeatures and benefits\nThe IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated (although when the publisher of a journal changes, sometimes all the DOIs will be changed, with the old DOIs no longer working). It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.\nThe Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system. DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request. However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents, that would have been available for no additional fee from alternative locations.\nThe indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.\nThe International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.\n\nComparison with other identifier schemes\nA DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.\nA DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.\nThe DOI system offers persistent, semantically interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing both social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. \nThe comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as \"identifiers\" does not mean that they can be compared easily. Other \"identifier systems\" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. \nThe DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).\nA DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.\n\nResolution\nDOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.\nTo resolve a DOI name, it may be input to a DOI resolver, such as doi.org.\nAnother approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as https:\/\/doi.org\/ (preferred) or http:\/\/dx.doi.org\/, both of which support HTTPS. For example, the DOI 10.1000\/182 can be included in a reference or hyperlink as https:\/\/doi.org\/10.1000\/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.\nOther DOI resolvers and HTTP Proxies include the Handle System and PANGAEA. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http:\/\/doai.io. This service is unusual in that it tries to find a non-paywalled (often author archived) version of a title and redirects the user to that instead of the publisher's version. Since then, other open-access favoring DOI resolvers have been created, notably https:\/\/oadoi.org\/ in October 2016 (later Unpaywall). While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).\nAn alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs, which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.\n\nIDF organizational structure\nThe International DOI Foundation (IDF), a non-profit organization created in 1997, is the governance body of the DOI system. It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.\nThe IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.\nRegistration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.\nRegistration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.\n\nStandardization\nThe DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46\/SC9. The Draft International Standard ISO\/DIS 26324, Information and documentation \u2013 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot, which was approved by 100% of those voting in a ballot closing on 15 November 2010. The final standard was published on 23 April 2012.\nDOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi\/ is the infoURI Namespace of Digital Object Identifiers.\nThe DOI syntax is a NISO standard, first standardized in 2000, ANSI\/NISO Z39.84-2005 Syntax for the Digital Object Identifier.\nThe maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:\n\nURN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000\/1 rather than the simpler doi:10.1000\/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.\n\nSee also\nNotes\nReferences\nExternal links\n\nOfficial website\nShort DOI \u2013 DOI Foundation service for converting long DOIs to shorter equivalents\nFactsheet: DOI System and Internet Identifier Specifications\nCrossRef DOI lookup","62":"eMedicine is an online clinical medical knowledge base founded in 1996 by doctors Scott Plantz and Jonathan Adler, and computer engineer Jeffrey Berezin. The eMedicine website consists of approximately 6,800 medical topic review articles, each of which is associated with a clinical subspecialty \"textbook\".  The knowledge base includes over 25,000 clinically multimedia files.\nEach article is authored by board certified specialists in the subspecialty to which the article belongs and undergoes three levels of physician peer-review, plus review by a Doctor of Pharmacy. The article's authors are identified with their current faculty appointments.  Each article is updated yearly, or more frequently as changes in practice occur, and the date is published on the article.  eMedicine.com was sold to WebMD in January, 2006 and is available as the Medscape Reference.\n\nHistory\nPlantz, Adler and Berezin evolved the concept for eMedicine.com in 1996 and deployed the initial site via Boston Medical Publishing, Inc., a corporation in which Plantz and Adler were principals. A Group Publishing System 1 (GPS 1) was developed that allowed large numbers of contributors to collaborate simultaneously. That system was first used to create a knowledge base in emergency medicine with 600 contributing MDs creating over 630 chapters in just over a year.  In 1997 eMedicine.com, Inc. was legally spun off from Boston Medical Publishing. eMedicine attracted angel-level investment from Tenet Healthcare in 1999 and a significant VC investment in 2000 (Omnicom Group, HIG Capital). \nSeveral years were spent creating the tables of contents, recruiting expert physicians and in the creation of the additional 6,100+ medical and surgical articles.  The majority of operations were based out of the Omaha, Nebraska, office. \nIn the early 2000s Plantz and Lorenzo also spearheaded an alliance with the University of Nebraska Medical Center to accredit eMedicine content for physician, nursing, and pharmacy continuing education.\nIn 2005, eMedicine entered into discussions for acquisition.  The board of directors unanimously recommended approval for sale of the company to WebMD.  The sale was completed in January 2006.\nThe site is free to use, requiring only registration. eMedicine content could also be accessed as an e-book, and could be downloaded into a palm top device.\n\nStatPearls\nIn 2018, the founders of eMedicine began StatPearls, an international collaboration to improve medical education, with stated goal of creating a free, comprehensive, online medical educational database and providing medical workers with affordable continuing medical education credits.\n\nUsage among specialists\nIn 2012 Volsky et al. evaluated the most frequently used internet information sources by the public, (1) identifying the three most frequently referenced Internet sources; (2) comparing the content accuracy of each of the three sources and (3) ascertaining user-friendliness of each site; and (4) informing practitioners and patients of the quality of available information. They found Wikipedia, eMedicine, and MedlinePlus (United States National Library of Medicine (NLM)\/National Institutes of Health (NIH) were the most referenced sources. For content accuracy, eMedicine scored highest (84%; p<0.05) over MedlinePlus (49%) and Wikipedia (46%). The highest incidence of errors and omissions per article was found in Wikipedia (0.98\u00b10.19), twice more than eMedicine (0.42\u00b10.19; p<0.05). Errors were similar between MedlinePlus and both eMedicine and Wikipedia. On ratings for user interface, which incorporated Flesch-Kinkaid Reading Level and Flesch Reading Ease, MedlinePlus was the most user-friendly (4.3\u00b10.29). This was nearly twice that of eMedicine (2.4\u00b10.26) and slightly greater than Wikipedia (3.7\u00b10.3). All differences were significant (p<0.05). There were seven topics for which articles were not available on MedlinePlus. They concluded \"Knowledge of the quality of available information on the Internet improves pediatric otolaryngologists' ability to counsel parents. The top web search results for pediatric otolaryngology diagnoses are Wikipedia, MedlinePlus, and eMedicine. Online information varies in quality, with a 46\u201384% concordance with current textbooks. eMedicine has the most accurate, comprehensive content and fewest errors, but is more challenging to read and navigate. Both Wikipedia and MedlinePlus have lower content accuracy and more errors; however, MedlinePlus is simplest of all to read, at a 9th grade level.\nSignificantly for eMedicine, Laraway and Rogers used PubMed, Medline Medical Journals.com and eMedicine as primary sources of information. This is significant because Medline is the compendium of all NIH-sponsored research.  eMedicine is made up of articles translating the body of current research in Medline into clinical practice guidelines from the perspective of each subspeciality.\nCao, Liu, Simpson, et al revealed that Medline and eMedicine were used as primary resources in developing the online system AskHERMES. Physicians were asked to solve complex clinical problems using three different sources of information: AskHermes, Google and UpToDate.  Surveys of the physicians who used all three systems were asked to score the three systems by ease of use, quality of answer, time spent, and overall performance.\nA 2009 study showed that \"89.1% of ophthalmologist respondents accessed peer-reviewed material online, including Emedicine (60.2%).\"\nA 2007 study showed that 12% of radiology residents used eMedicine as their first source when doing research on the Internet.\nA 2005 study ranking 114 sites rated it the second-highest Internet-based source of information for pediatric neuro-oncology, after the site of the National Cancer Institute.\nA 2002 study described the site's coverage of dermatology as \"excellent and comprehensive.\"\nIn 2000 an article in the Journal of Ear Nose and Throat by AD Meyers from the University of Colorado School of Medicine, Denver, Colorado, announced the unveiling of the ENT textbook online at emedicine.com.\n\nReferences\nExternal links\nOfficial website","63":"Otology is a branch of medicine which studies normal, pathological anatomy and physiology of the ear (hearing. Otology also studies vestibular sensory systems, related structures and functions), as well as their diseases, diagnosis and treatment. Otologic surgery generally refers to surgery of the middle ear and mastoid related to chronic otitis media, such as tympanoplasty, or ear drum surgery, ossiculoplasty, or surgery of the hearing bones, and mastoidectomy.  Otology also includes surgical treatment of conductive hearing loss, such as stapedectomy surgery for otosclerosis.\nNeurotology(a related field of medicine and subspecialty of otolaryngology) is the study of diseases of the inner ear, which can lead to hearing and balance disorders.  Neurotologic surgery generally refers to surgery of the inner ear, or surgery that involves entering the inner ear with risk to the hearing and balance organs, including:labyrinthectomy, cochlear implant surgery, and surgery for tumors of the temporal bone, such as intracanalicular acoustic neuromas.  Neurotology is expanded to include surgery of the lateral skull base to treat intracranial tumors related to the ear and surrounding nerve and vascular structures, such as large cerebellar pontine angle acoustic neuromas, glomus jugulare tumors and facial nerve tumors.\nSome of the concerns of otology include:\n\nidentifying the underlying mechanisms of M\u00e9ni\u00e8re's disease,\nfinding the causes of tinnitus and developing treatment methods,\ndefining the development and progression of otitis media\nRelated concerns of neurotology include:\n\nstudying signal processing in the cochlear implant patient,\ninvestigating postural control areas and vestibulo-ocular mechanisms.\nstudying the genetics of acoustic neuromas in patients with neurofibromatosis, to better understanding how to treat these tumors and prevent their growth.\n\nSee also\nAudiology \u2013 Branch of science that studies hearing, balance, and related disorders\nEar Research Foundation \u2013 Research institute in USAPages displaying wikidata descriptions as a fallback\nNeurotology \u2013 Head and neck surgery (otorhinolaryngology) subspecialty\nStenvers projection \u2013 Radiological technique\nTWJ Foundation \u2013 UK health charityPages displaying wikidata descriptions as a fallback\n\n\n== References ==","64":"Ear pain, also known as earache or otalgia, is pain in the ear. Primary ear pain is pain that originates from the ear. Secondary ear pain is a type of referred pain, meaning that the source of the pain differs from the location where the pain is felt.\nMost causes of ear pain are non-life-threatening. Primary ear pain is more common than secondary ear pain, and it is often due to infection or injury. The conditions that cause secondary (referred) ear pain are broad and range from temporomandibular joint syndrome to inflammation of the throat.\nIn general, the reason for ear pain can be discovered by taking a thorough history of all symptoms and performing a physical examination, without need for imaging tools like a CT scan. However, further testing may be needed if red flags are present like hearing loss, dizziness, ringing in the ear or unexpected weight loss.\nManagement of ear pain depends on the cause. If there is a bacterial infection, antibiotics are sometimes recommended and over the counter pain medications can help control discomfort. Some causes of ear pain require a procedure or surgery.\n83 percent of children have at least one episode of a middle ear infection by three years of age.\n\nSigns and symptoms\nEar pain can present in one or both ears. It may or may not be accompanied by other symptoms such as fever, sensation of the world spinning, ear itchiness, or a sense of fullness in the ear. The pain may or may not worsen with chewing. The pain may also be continuous or intermittent.\nEar pain due to an infection is the most common in children and can occur in babies. Adults may need further evaluation if they have hearing loss, dizziness or ringing in the ear. Additional red flags include diabetes, a weakened immune system, swelling seen on the outer ear, or swelling along the jaw.\n\nCauses\nEar pain has a variety of causes, the majority of which are not life-threatening. Ear pain can originate from a part of the ear itself, known as primary ear pain, or from an anatomic structure outside the ear that is perceived as pain within the ear, known as secondary ear pain. Secondary ear pain is a type of referred pain, meaning that the source of the pain differs from the location where the pain is felt. Primary ear pain is more common in children, whereas secondary (referred) pain is more common in adults.\nPrimary ear pain is most commonly caused by infection or injury to one of the parts of the ear.\n\nExternal ear\nMany conditions involving the external ear will be visible to the naked eye.  Because the external ear is the most exposed portion of the ear, it is vulnerable to trauma or environmental exposures.  Blunt trauma, such as a blow to the ear, can result in a hematoma, or collection of blood between the cartilage and perichondrium of the ear. This type of injury is particularly common in contact sports such as wrestling and boxing. Environmental injuries include sunburn, frostbite, or contact dermatitis.\nLess common causes of external ear pain include:\n\nAuricular Cellulitis: a superficial infection of the ear that may be precipitated by trauma, an insect bite, or ear piercing\nPerichondritis: infection of the perichondrium, or fascia surrounding the ear cartilage, which can develop as a complication of untreated auricular cellulitis. It is important to identify and treat perichondritis with antibiotics to avoid permanent ear deformities.\nRelapsing polychondritis: a systemic inflammatory condition involving cartilage in many parts of the body, but often including the cartilage of both ears. The severity and prognosis of the disease varies widely.\n\nOtitis externa\nOtitis externa, also known as \"swimmer's ear\", is a cellulitis of the external ear canal. In North America, 98% of cases are caused by bacteria, and the most common causative organisms are Pseudomonas and Staph aureus. Risk factors include exposure to excessive moisture (e.g. from swimming or a warm climate) and disruption of the protective cerumen barrier, which can result from aggressive ear cleaning or placing objects in the ear.\nMalignant otitis externa is a rare and potentially life-threatening complication of otitis externa in which the infection spreads from the ear canal into the surrounding skull base, hence becoming an osteomyelitis. It occurs largely in diabetic patients. It is very rare in children, though can be seen in immunocompromised children and adults. Pseudomonas is the most common causative organism. The pain tends to be more severe than in uncomplicated otitis externa, and laboratory studies often reveal elevated inflammatory markers (ESR and\/or CRP). The infection may extend to cranial nerves, or rarely to the meninges or brain. Examination of the ear canal may reveal granulation tissue in the inferior canal. It is treated with several weeks of IV and oral antibiotics, usually fluoroquinolones.\n\nMechanical obstruction\nEarwax impaction: results in 12 million medical visits annually in the United States. Cerumen impaction may cause ear pain, but it can also prevent thorough examination of the ear and identification of an alternate source of pain.\nForeign body: commonly include insects or small objects like beads\n\nLess common\nHerpes zoster: varicella zoster virus can reactivate in an area that includes the ear. Reactivation can produce pain and visible vesicles within the ear canal and, when combined with facial paralysis due to facial nerve involvement, is called Ramsay Hunt syndrome.\nTumors: the most common ear canal tumor is squamous cell carcinoma. Symptoms can resemble those of otitis externa, and cancer should be considered if the symptoms are not improving on appropriate treatment.\n\nMiddle and inner ear\nAcute otitis media\nAcute otitis media is an infection of the middle ear. More than 80% of children experience at least one episode of otitis media by age 3 years.  Acute otitis media is also most common in these first 3 years of life, though older children may also experience it. The most common causative bacteria are Streptococcus pneumoniae, Haemophilus influenzae, and Moraxella catarrhalis.  Otitis media often occurs with or following cold symptoms. The diagnosis is made by the combination of symptoms and examination of the tympanic membrane for redness, bulging, and\/or a middle ear effusion (collection of fluid within the middle ear).\nComplications of otitis media include hearing loss, facial nerve paralysis, or extension of infection to surrounding anatomic structures, including: \n\nMastoiditis: infection of the air cells in the mastoid process, the area of the skull located right behind the ear\nPetrositis: infection of the petrous portion of the temporal bone\nLabyrinthitis\nMeningitis\nSubdural abscess\nBrain abscess\nCerebral venous sinus thrombosis\n\nTrauma\nBarotrauma: results from changes in atmospheric pressure that occur, for example, when descending in a plane or deep diving. As atmospheric pressure increases with descent, the eustachian tube collapses due to pressure within the middle ear being less than the external pressure, which causes pain. In severe cases, middle ear hemorrhage or tympanic membrane rupture can result.\nTympanic membrane rupture: disruption of the eardrum. This can be caused by a blow to the ear, blast injury, barotrauma, or direct penetration of the tympanic membrane by an object entering the ear.\nNoxacusis (Pain Hyperacusis): causes pain in the ear when exposed to noise that typically does not cause pain. This has been thought to be caused by type II nerves responding to damage of the outer hair cells.\n\nReferred ear pain\nA variety of conditions can cause irritation of one of the nerves that provides sensation to the ear.\nConditions causing irritation the trigeminal nerve (cranial nerve V):\n\nTemporomandibular joint syndrome: inflammation or abnormal movements of the joint between the jaw and skull. These disorders are most common in women of childbearing age, and are uncommon in children younger than 10 years old.\nMyofascial pain syndrome: pain in the muscles involved in chewing. There may be certain parts of the muscles or tendons (connective tissue connecting the muscles to bones) that are especially painful when pressed\nTrigeminal neuralgia: attacks of shooting pain down the face that may be triggered by touching the face or temperature changes\nDental pain from cavities or an abscess\nOral cavity carcinoma\nConditions causing irritation of the facial nerve (cranial nerve VII) or glossopharyngeal nerve (cranial nerve IX):\n\nTonsillitis: infection\/inflammation of the tonsils\nPost-tonsillectomy: pain following surgical removal of the tonsils\nPharyngitis: infection\/inflammation of the throat\nSinusitis\nParotitis: inflammation of the parotid gland, the salivary gland right in front of the ear\nCarcinoma of the oropharynx (base of tongue, soft palate, pharyngeal wall, tonsils)\nConditions causing irritation of the vagus nerve (cranial nerve X):\n\nGERD\nMyocardial ischemia (inadequate oxygen supply to the heart muscle)\nConditions causing irritation of cervical nerves C2-C3:\n\nCervical spine trauma, arthritis (joint inflammation), or tumor\nTemporal arteritis: an autoimmune disorder leading to inflammation of the temporal artery, a large artery in the head. This condition tends to occur in adults older than 50.\n\nPathophysiology\nPrimary ear pain\nThe ear can be anatomically divided into the external ear, the external auditory canal, the middle ear, and the inner ear. These three are indistinguishable in terms of the pain experienced.\n\nSecondary ear pain\nMany different nerves provide sensation to the various parts of the ear, including cranial nerves V (trigeminal), VII (facial), IX (glossopharyngeal), and X (vagus), and the great auricular nerve (cervical nerves C2-C3). These nerves also supply other parts of the body, from the mouth to the chest and abdomen. Irritation of these nerves in another part of the body has the potential to produce pain in the ear. This is called referred pain. Irritation of the trigeminal nerve (cranial nerve V) is the most common cause of referred ear pain.\n\nDiagnostic\nWhile some disorders may require specific imaging or testing, most etiologies of ear pain are diagnosed clinically. Because the differential for ear pain is so broad, there is no consensus on the best diagnostic framework to use. One approach is to differentiate by time course, as primary causes of ear pain are typically more acute in nature, while secondary causes of ear pain are more chronic.\nAcute causes may be further distinguished by the presence of fever (indicating an underlying infection) or the absence of fever (suggesting a structural problem, such as such as trauma or other injury to the ear). Etiologies leading to chronic pain may be broken down by the presence or absence of worrisome clinical features, also known as red flags.\nOne red flag is the presence of one or multiple risk factors including smoking, heavy alcohol use (greater than 3.5 drinks per day), diabetes, coronary artery disease, and older age (greater than 50). These factors increase the risk of having a serious cause of ear pain, like cancer or a serious infection. In particular, second hand smoke may increase risk of acute otitis media in children. In addition, swimming is the most significant risk factor for otitis externae, though other risk factors include high humidity in the ear canal, eczema and\/or ear trauma.\nIf red flags are present it may be necessary to do additional workup such as a CT scan or biopsy to rule out a more dangerous diagnosis. Such diagnoses include malignant (or necrotizing) otitis externa, mastoiditis, temporal arteritis, and cancer. While the presence of a red flag does raise suspicion for one of these four disease, it does not guarantee a diagnosis as any one symptom can be seen in a variety of situations. For example, jaw claudication can be seen in temporal arteritis, but also in TMJ dysfunction.\nIf there are no red flags, other sources of referred ear pain become more likely and are reasonable to pursue.\n\n*Indicates a \"Can't Miss\" diagnosis or a red flag.\n\nManagement\nManagement of ear pain depends on the underlying cause.\n\nAntibiotics\nWhile not all causes of ear pain are treated with antibiotics, those caused by bacterial infections of the ear are usually treated with antibiotics known to cover the common bacterial organisms for that type of infection. Many bacterial ear infections are treated with cleaning of the area, topical or systemic antibiotics, and oral analgesics for comfort. Some types of bacterial ear infections can benefit from warm compresses included in the treatment. Some of the causes of ear pain that are typically treated with either a topical or systemic antibiotic include:\n\nUncomplicated acute bacterial otitis externa (AOE). For symptoms that are not responsive to treatment within 10 days, a physician should evaluate for necrotizing external otitis.\nAcute otitis media (AOM) self-resolves within 24\u201348 hours in 80% of cases. If it does not self-resolve, AOM thought to be caused by bacteria is treated with systemic antibiotics. If symptoms do not respond to a week of treatment, a physician should evaluate for mastoiditis.\nAcute folliculitis.\nAuricular cellulitis.\nSuppurative otitis media. There is also a risk for tympanic membrane rupture.\nPerichondritis. An otorhinolaryngologist should also evaluate it and if a foreign body is present in the cartilage, this foreign body should be removed. If there is cartilage involvement, then more advance care with hospitalization is needed.\nSinusitis can cause secondary ear pain. Treating the underlying sinusitis will treat the ear pain. (See sinusitis.)\nSome bacterial infections may require a more advanced treatment with evaluation by otorhinolaryngology, IV antibiotics, and hospital admission.\n\nNecrotizing external otitis is potentially fatal and should be evaluated by an otorhinolaryngologist with admission to the hospital and IV antibiotics.(See otitis externa.)\nAcute mastoiditis is treated with admission to the hospital, otorhinolaryngology consultation and empiric IV antibiotics. Cases with intracranial involvement are treated with a mastoidectomy with myringotomy.\nChondritis.\n\nProcedures\nSome causes of ear pain require procedural management alone, by a health professional, or in addition to antibiotic therapy.\n\nKeratosis obturans is treated with removal of impacted desquamated keratin debris in the ear canal.\nChronic perichondritis and chondritis that continues to be symptomatic despite appropriate antibiotic management may require surgical debridement. Surgical drainage could be required.\nBullous myringitis leads to the development of bullae on the tympanic membrane that can be punctured to give pain relief.\nForeign body in the ear canal can cause pain and be treated with careful removal.\nInfected sebaceous cyst is treated with incision and drainage of the cysts, oral antibiotics and otorhinolaryngology assessment.\n\nOther\nGiven the variety of causes of ear pain, some causes require treatment other than antibiotics and procedures.\n\nRelapsing polychondritis is an autoimmune disease treated with immunomodulating medications (medications that help modulate the immune system).\nTemporomandibular joint dysfunction can lead to secondary ear pain and can be initially treated with a soft food diet, NSAIDs, application of a heat pack, massage of local area, and a referral to a dentist.\nMyofascial pain syndromes are initially treated with NSAIDs and physical therapy. Local anesthetic injection into the muscle trigger point can be considered in severe cases.\nGlossopharyngeal neuralgia is treated with carbamazepine.\n\nEpidemiology\n2\/3 of people presenting with ear pain were diagnosed with some sort of primary otalgia and 1\/3 were diagnosed with some sort secondary otalgia.\nA common cause of primary otalgia is ear infection called otitis media, meaning an infection behind the eardrum. The peak age for children to get acute otitis media is ages 6\u201324 months. One review paper wrote that 83% of children had at least one episode of acute otitis media by 3 years of age. Worldwide, there are 709 millions cases of acute otitis media every year. Hearing loss globally due to ear infection is estimated to be 30 people in every 10,000. Around the world there is around 21,000 to 28,000 deaths due to complications from ear infections. These complications include brain abscesses and meningitis.\nOtitis externae peaks at age 7\u201312 years of age and around 10% of people has had it at least once in their lives.\nCerumen impaction occurs in 1 out of every 10 children, 1 in every 20 adults and 1 in every 3 elderly citizens.\nBarotrauma occurs around 1 in every 1000 people.\nOf people presenting with ear pain, only 3% was diagnosed with eustachian tube dysfunction.\n\nHistory\nNot much was known about ear pain and acute otitis media before the 17th century. It was a common phenomenon with no treatment. That changed when the otoscope was invented in the 1840s by Anton von Troeltsh in Germany.  Another shift came with the invention of antibiotics. Before antibiotics was introduced there use to be a high rate of ear infections spreading to the bone around the ear, but that is now considered a rare complication.\n\nSociety and culture\nThere was previously a strong tradition of treating acute otitis media with amoxicillin. One quote from the 1980s shows this sentiment by saying \"any child with an earache has an acute amoxicillin deficiency\". However, people started realizing that using antibiotics too much can cause bacteria to gain resistance. Increasing resistance makes antibiotics less effective. The term antibiotic stewardship is then used to describe the systematic effort to educate antibiotic prescribers to only give these medications when they are warranted. In particular to children, most ear pain resolves by itself with no complications. There are guidelines in place to help determine when antibiotics for ear pain are needed in children.\nThe ear itself played a role in treatment via acupuncture, also known as auriculotherapy. It was believed that acupuncture of the ear could be used to correct other pain or disorders in the body. Such practices may have started as far back as the Stone Age. The first documentation of auriculotherapy in Europe was in the 1600s. One physician described stimulating the ear by burning or scarring to treat sciatic pain, while another physician applied this treatment for toothache. Paul Nogier is known as the father of ear acupuncture for his theory that parts of the ear corresponds to other areas of the body in a reliable fashion.\n\nResearch\nThere are currently studies going on delivering antibiotics directly into the middle ear.\n\nReferences\n\n\n== External links ==","65":"Dopamine (DA, a contraction of 3,4-dihydroxyphenethylamine) is a neuromodulatory molecule that plays several important roles in cells. It is an organic chemical of the catecholamine and phenethylamine families. Dopamine constitutes about 80% of the catecholamine content in the brain. It is an amine synthesized by removing a carboxyl group from a molecule of its precursor chemical, L-DOPA, which is synthesized in the brain and kidneys. Dopamine is also synthesized in plants and most animals. In the brain, dopamine functions as a neurotransmitter\u2014a chemical released by neurons (nerve cells) to send signals to other nerve cells. Neurotransmitters are synthesized in specific regions of the brain, but affect many regions systemically. The brain includes several distinct dopamine pathways, one of which plays a major role in the motivational component of reward-motivated behavior. The anticipation of most types of rewards increases the level of dopamine in the brain, and many addictive drugs increase dopamine release or block its reuptake into neurons following release. Other brain dopamine pathways are involved in motor control and in controlling the release of various hormones. These pathways and cell groups form a dopamine system which is neuromodulatory.\nIn popular culture and media, dopamine is often portrayed as the main chemical of pleasure, but the current opinion in pharmacology is that dopamine instead confers motivational salience; in other words, dopamine signals the perceived motivational prominence (i.e., the desirability or aversiveness) of an outcome, which in turn propels the organism's behavior toward or away from achieving that outcome.\nOutside the central nervous system, dopamine functions primarily as a local paracrine messenger. In blood vessels, it inhibits norepinephrine release and acts as a vasodilator; in the kidneys, it increases sodium excretion and urine output; in the pancreas, it reduces insulin production; in the digestive system, it reduces gastrointestinal motility and protects intestinal mucosa; and in the immune system, it reduces the activity of lymphocytes. With the exception of the blood vessels, dopamine in each of these peripheral systems is synthesized locally and exerts its effects near the cells that release it.\nSeveral important diseases of the nervous system are associated with dysfunctions of the dopamine system, and some of the key medications used to treat them work by altering the effects of dopamine. Parkinson's disease, a degenerative condition causing tremor and motor impairment, is caused by a loss of dopamine-secreting neurons in an area of the midbrain called the substantia nigra. Its metabolic precursor L-DOPA can be manufactured; Levodopa, a pure form of L-DOPA, is the most widely used treatment for Parkinson's. There is evidence that schizophrenia involves altered levels of dopamine activity, and most antipsychotic drugs used to treat this are dopamine antagonists which reduce dopamine activity. Similar dopamine antagonist drugs are also some of the most effective anti-nausea agents. Restless legs syndrome and attention deficit hyperactivity disorder (ADHD) are associated with decreased dopamine activity. Dopaminergic stimulants can be addictive in high doses, but some are used at lower doses to treat ADHD. Dopamine itself is available as a manufactured medication for intravenous injection. It is useful in the treatment of severe heart failure or cardiogenic shock. In newborn babies it may be used for hypotension and septic shock.\n\nStructure\nA dopamine molecule consists of a catechol structure (a benzene ring with two hydroxyl side groups) with one amine group        attached via an ethyl chain. As such, dopamine is the simplest possible catecholamine, a family that also includes the neurotransmitters norepinephrine and epinephrine. The presence of a benzene ring with this amine attachment makes it a substituted phenethylamine, a family that includes numerous psychoactive drugs.\nLike most amines, dopamine is an organic base. As a base, it is generally protonated in acidic environments (in an acid-base reaction). The protonated form is highly water-soluble and relatively stable, but can become oxidized if exposed to oxygen or other oxidants. In basic environments, dopamine is not protonated. In this free base form, it is less water-soluble and also more highly reactive. Because of the increased stability and water-solubility of the protonated form, dopamine is supplied for chemical or pharmaceutical use as dopamine hydrochloride\u2014that is, the hydrochloride salt that is created when dopamine is combined with hydrochloric acid. In dry form, dopamine hydrochloride is a fine powder which is white to yellow in color.\n\nBiochemistry\nSynthesis\nDopamine is synthesized in a restricted set of cell types, mainly neurons and cells in the medulla of the adrenal glands. The primary and minor metabolic pathways respectively are:\n\nPrimary: L-Phenylalanine \u2192 L-Tyrosine \u2192 L-DOPA \u2192 Dopamine\nMinor: L-Phenylalanine \u2192 L-Tyrosine \u2192 p-Tyramine \u2192 Dopamine\nMinor: L-Phenylalanine \u2192 m-Tyrosine \u2192 m-Tyramine  \u2192 Dopamine\nThe direct precursor of dopamine, L-DOPA, can be synthesized indirectly from the essential amino acid phenylalanine or directly from the non-essential amino acid tyrosine. These amino acids are found in nearly every protein and so are readily available in food, with tyrosine being the most common. Although dopamine is also found in many types of food, it is incapable of crossing the blood\u2013brain barrier that surrounds and protects the brain. It must therefore be synthesized inside the brain to perform its neuronal activity.\nL-Phenylalanine is converted into L-tyrosine by the enzyme phenylalanine hydroxylase, with molecular oxygen (O2) and tetrahydrobiopterin as cofactors. L-Tyrosine is converted into L-DOPA by the enzyme tyrosine hydroxylase, with tetrahydrobiopterin, O2, and iron (Fe2+) as cofactors. L-DOPA is converted into dopamine by the enzyme aromatic L-amino acid decarboxylase (also known as DOPA decarboxylase), with pyridoxal phosphate as the cofactor.\nDopamine itself is used as precursor in the synthesis of the neurotransmitters norepinephrine and epinephrine. Dopamine is converted into norepinephrine by the enzyme dopamine \u03b2-hydroxylase, with O2 and L-ascorbic acid as cofactors. Norepinephrine is converted into epinephrine by the enzyme phenylethanolamine N-methyltransferase with S-adenosyl-L-methionine as the cofactor.\nSome of the cofactors also require their own synthesis. Deficiency in any required amino acid or cofactor can impair the synthesis of dopamine, norepinephrine, and epinephrine.\n\nDegradation\nDopamine is broken down into inactive metabolites by a set of enzymes\u2014monoamine oxidase (MAO), catechol-O-methyl transferase (COMT), and aldehyde dehydrogenase (ALDH), acting in sequence. Both isoforms of monoamine oxidase, MAO-A and MAO-B, effectively metabolize dopamine. Different breakdown pathways exist but the main end-product is homovanillic acid (HVA), which has no known biological activity. From the bloodstream, homovanillic acid is filtered out by the kidneys and then excreted in the urine. The two primary metabolic routes that convert dopamine into HVA are:\n\nDopamine \u2192 DOPAL \u2192 DOPAC \u2192 HVA \u2013 catalyzed by MAO, ALDH, and COMT respectively\nDopamine \u2192 3-Methoxytyramine \u2192 HVA \u2013 catalyzed by COMT and MAO+ALDH respectively\nIn clinical research on schizophrenia, measurements of homovanillic acid in plasma have been used to estimate levels of dopamine activity in the brain. A difficulty in this approach however, is separating the high level of plasma homovanillic acid contributed by the metabolism of norepinephrine.\nAlthough dopamine is normally broken down by an oxidoreductase enzyme, it is also susceptible to oxidation by direct reaction with oxygen, yielding quinones plus various free radicals as products. The rate of oxidation can be increased by the presence of ferric iron or other factors. Quinones and free radicals produced by autoxidation of dopamine can poison cells, and there is evidence that this mechanism may contribute to the cell loss that occurs in Parkinson's disease and other conditions.\n\nFunctions\nCellular effects\nDopamine exerts its effects by binding to and activating cell surface receptors. In humans, dopamine has a high binding affinity at dopamine receptors and human trace amine-associated receptor 1 (hTAAR1). In mammals, five subtypes of dopamine receptors have been identified, labeled from D1 to D5. All of them function as metabotropic, G protein-coupled receptors, meaning that they exert their effects via a complex second messenger system. These receptors can be divided into two families, known as D1-like and D2-like. For receptors located on neurons in the nervous system, the ultimate effect of D1-like activation (D1 and D5) can be excitation (via opening of sodium channels) or inhibition (via opening of potassium channels); the ultimate effect of D2-like activation (D2, D3, and D4) is usually inhibition of the target neuron. Consequently, it is incorrect to describe dopamine itself as either excitatory or inhibitory: its effect on a target neuron depends on which types of receptors are present on the membrane of that neuron and on the internal responses of that neuron to the second messenger cAMP. D1 receptors are the most numerous dopamine receptors in the human nervous system; D2 receptors are next; D3, D4, and D5 receptors are present at significantly lower levels.\n\nStorage, release, and reuptake\nInside the brain, dopamine functions as a neurotransmitter and neuromodulator, and is controlled by a set of mechanisms common to all monoamine neurotransmitters. After synthesis, dopamine is transported from the cytosol into synaptic vesicles by a solute carrier\u2014a vesicular monoamine transporter, VMAT2. Dopamine is stored in these vesicles until it is ejected into the synaptic cleft. In most cases, the release of dopamine occurs through a process called exocytosis which is caused by action potentials, but it can also be caused by the activity of an intracellular trace amine-associated receptor, TAAR1. TAAR1 is a high-affinity receptor for dopamine, trace amines, and certain substituted amphetamines that is located along membranes in the intracellular milieu of the presynaptic cell; activation of the receptor can regulate dopamine signaling by inducing dopamine reuptake inhibition and efflux as well as by inhibiting neuronal firing through a diverse set of mechanisms.\nOnce in the synapse, dopamine binds to and activates dopamine receptors. These can be postsynaptic dopamine receptors, which are located on dendrites (the postsynaptic neuron), or presynaptic autoreceptors (e.g., the D2sh and presynaptic D3 receptors), which are located on the membrane of an axon terminal (the presynaptic neuron). After the postsynaptic neuron elicits an action potential, dopamine molecules quickly become unbound from their receptors. They are then absorbed back into the presynaptic cell, via reuptake mediated either by the dopamine transporter or by the plasma membrane monoamine transporter. Once back in the cytosol, dopamine can either be broken down by a monoamine oxidase or repackaged into vesicles by VMAT2, making it available for future release.\nIn the brain the level of extracellular dopamine is modulated by two mechanisms: phasic and tonic transmission. Phasic dopamine release, like most neurotransmitter release in the nervous system, is driven directly by action potentials in the dopamine-containing cells. Tonic dopamine transmission occurs when small amounts of dopamine are released without being preceded by presynaptic action potentials. Tonic transmission is regulated by a variety of factors, including the activity of other neurons and neurotransmitter reuptake.\n\nCentral nervous system\nInside the brain, dopamine plays important roles in executive functions, motor control, motivation, arousal, reinforcement, and reward, as well as lower-level functions including lactation, sexual gratification, and nausea. The dopaminergic cell groups and pathways make up the dopamine system which is neuromodulatory.\nDopaminergic neurons (dopamine-producing nerve cells) are comparatively few in number\u2014a total of around 400,000 in the human brain\u2014and their cell bodies are confined in groups to a few relatively small brain areas. However their axons project to many other brain areas, and they exert powerful effects on their targets. These dopaminergic cell groups were first mapped in 1964 by Annica Dahlstr\u00f6m and Kjell Fuxe, who assigned them labels starting with the letter \"A\" (for \"aminergic\"). In their scheme, areas A1 through A7 contain the neurotransmitter norepinephrine, whereas A8 through A14 contain dopamine. The dopaminergic areas they identified are the substantia nigra (groups 8 and 9); the ventral tegmental area (group 10); the posterior hypothalamus (group 11); the arcuate nucleus (group 12); the zona incerta (group 13) and the periventricular nucleus (group 14).\nThe substantia nigra is a small midbrain area that forms a component of the basal ganglia. This has two parts\u2014an input area called the pars reticulata and an output area called the pars compacta. The dopaminergic neurons are found mainly in the pars compacta (cell group A8) and nearby (group A9). In humans, the projection of dopaminergic neurons from the substantia nigra pars compacta to the dorsal striatum, termed the nigrostriatal pathway, plays a significant role in the control of motor function and in learning new motor skills. These neurons are especially vulnerable to damage, and when a large number of them die, the result is a parkinsonian syndrome.\nThe ventral tegmental area (VTA) is another midbrain area. The most prominent group of VTA dopaminergic neurons projects to the prefrontal cortex via the mesocortical pathway and another smaller group projects to the nucleus accumbens via the mesolimbic pathway. Together, these two pathways are collectively termed the mesocorticolimbic projection. The VTA also sends dopaminergic projections to the amygdala, cingulate gyrus, hippocampus, and olfactory bulb. Mesocorticolimbic neurons play a central role in reward and other aspects of motivation. Accumulating literature shows that dopamine also plays a crucial role in aversive learning through its effects on a number of brain regions.\nThe posterior hypothalamus has dopamine neurons that project to the spinal cord, but their function is not well established. There is some evidence that pathology in this area plays a role in restless legs syndrome, a condition in which people have difficulty sleeping due to an overwhelming compulsion to constantly move parts of the body, especially the legs.\nThe arcuate nucleus and the periventricular nucleus of the hypothalamus have dopamine neurons that form an important projection\u2014the tuberoinfundibular pathway which goes to the pituitary gland, where it influences the secretion of the hormone prolactin. Dopamine is the primary neuroendocrine inhibitor of the secretion of prolactin from the anterior pituitary gland. Dopamine produced by neurons in the arcuate nucleus is secreted into the hypophyseal portal system of the median eminence, which supplies the pituitary gland. The prolactin cells that produce prolactin, in the absence of dopamine, secrete prolactin continuously; dopamine inhibits this secretion.\nThe zona incerta, grouped between the arcuate and periventricular nuclei, projects to several areas of the hypothalamus, and participates in the control of gonadotropin-releasing hormone, which is necessary to activate the development of the male and female reproductive systems, following puberty.\nAn additional group of dopamine-secreting neurons is found in the retina of the eye. These neurons are amacrine cells, meaning that they have no axons. They release dopamine into the extracellular medium, and are specifically active during daylight hours, becoming silent at night. This retinal dopamine acts to enhance the activity of cone cells in the retina while suppressing rod cells\u2014the result is to increase sensitivity to color and contrast during bright light conditions, at the cost of reduced sensitivity when the light is dim.\n\nBasal ganglia\nThe largest and most important sources of dopamine in the vertebrate brain are the substantia nigra and ventral tegmental area. Both structures are components of the midbrain, closely related to each other and functionally similar in many respects. The largest component of the basal ganglia is the striatum. The substantia nigra sends a dopaminergic projection to the dorsal striatum, while the ventral tegmental area sends a similar type of dopaminergic projection to the ventral striatum.\nProgress in understanding the functions of the basal ganglia has been slow. The most popular hypotheses, broadly stated, propose that the basal ganglia play a central role in action selection. The action selection theory in its simplest form proposes that when a person or animal is in a situation where several behaviors are possible, activity in the basal ganglia determines which of them is executed, by releasing that response from inhibition while continuing to inhibit other motor systems that if activated would generate competing behaviors. Thus the basal ganglia, in this concept, are responsible for initiating behaviors, but not for determining the details of how they are carried out. In other words, they essentially form a decision-making system.\nThe basal ganglia can be divided into several sectors, and each is involved in controlling particular types of actions. The ventral sector of the basal ganglia (containing the ventral striatum and ventral tegmental area) operates at the highest level of the hierarchy, selecting actions at the whole-organism level. The dorsal sectors (containing the dorsal striatum and substantia nigra) operate at lower levels, selecting the specific muscles and movements that are used to implement a given behavior pattern.\nDopamine contributes to the action selection process in at least two important ways. First, it sets the \"threshold\" for initiating actions. The higher the level of dopamine activity, the lower the impetus required to evoke a given behavior. As a consequence, high levels of dopamine lead to high levels of motor activity and impulsive behavior; low levels of dopamine lead to torpor and slowed reactions. Parkinson's disease, in which dopamine levels in the substantia nigra circuit are greatly reduced, is characterized by stiffness and difficulty initiating movement\u2014however, when people with the disease are confronted with strong stimuli such as a serious threat, their reactions can be as vigorous as those of a healthy person. In the opposite direction, drugs that increase dopamine release, such as cocaine or amphetamine, can produce heightened levels of activity, including, at the extreme, psychomotor agitation and stereotyped movements.\nThe second important effect of dopamine is as a \"teaching\" signal. When an action is followed by an increase in dopamine activity, the basal ganglia circuit is altered in a way that makes the same response easier to evoke when similar situations arise in the future. This is a form of operant conditioning, in which dopamine plays the role of a reward signal.\n\nReward\nIn the language used to discuss the reward system, reward is the attractive and motivational property of a stimulus that induces appetitive behavior (also known as approach behavior) and consummatory behavior. A rewarding stimulus is one that can induce the organism to approach it and choose to consume it. Pleasure, learning (e.g., classical and operant conditioning), and approach behavior are the three main functions of reward. As an aspect of reward, pleasure provides a definition of reward; however, while all pleasurable stimuli are rewarding, not all rewarding stimuli are pleasurable (e.g., extrinsic rewards like money). The motivational or desirable aspect of rewarding stimuli is reflected by the approach behavior that they induce, whereas the pleasure from intrinsic rewards results from consuming them after acquiring them. A neuropsychological model which distinguishes these two components of an intrinsically rewarding stimulus is the incentive salience model, where \"wanting\" or desire (less commonly, \"seeking\") corresponds to appetitive or approach behavior while \"liking\" or pleasure corresponds to consummatory behavior. In human drug addicts, \"wanting\" becomes dissociated with \"liking\" as the desire to use an addictive drug increases, while the pleasure obtained from consuming it decreases due to drug tolerance.\nWithin the brain, dopamine functions partly as a global reward signal. An initial dopamine response to a rewarding stimulus encodes information about the salience, value, and context of a reward. In the context of reward-related learning, dopamine also functions as a reward prediction error signal, that is, the degree to which the value of a reward is unexpected. According to this hypothesis proposed by Montague, Dayan, and Sejnowski, rewards that are expected do not produce a second phasic dopamine response in certain dopaminergic cells, but rewards that are unexpected, or greater than expected, produce a short-lasting increase in synaptic dopamine, whereas the omission of an expected reward actually causes dopamine release to drop below its background level. The \"prediction error\" hypothesis has drawn particular interest from computational neuroscientists, because an influential computational-learning method known as temporal difference learning makes heavy use of a signal that encodes prediction error. This confluence of theory and data has led to a fertile interaction between neuroscientists and computer scientists interested in machine learning.\nEvidence from microelectrode recordings from the brains of animals shows that dopamine neurons in the ventral tegmental area (VTA) and substantia nigra are strongly activated by a wide variety of rewarding events. These reward-responsive dopamine neurons in the VTA and substantia nigra are crucial for reward-related cognition and serve as the central component of the reward system. The function of dopamine varies in each axonal projection from the VTA and substantia nigra; for example, the VTA\u2013nucleus accumbens shell projection assigns incentive salience (\"want\") to rewarding stimuli and its associated cues, the VTA\u2013prefrontal cortex projection updates the value of different goals in accordance with their incentive salience, the VTA\u2013amygdala and VTA\u2013hippocampus projections mediate the consolidation of reward-related memories, and both the VTA\u2013nucleus accumbens core and substantia nigra\u2013dorsal striatum pathways are involved in learning motor responses that facilitate the acquisition of rewarding stimuli. Some activity within the VTA dopaminergic projections appears to be associated with reward prediction as well.\n\nPleasure\nWhile dopamine has a central role in causing \"wanting,\" associated with the appetitive or approach behavioral responses to rewarding stimuli, detailed studies have shown that dopamine cannot simply be equated with hedonic \"liking\" or pleasure, as reflected in the consummatory behavioral response. Dopamine neurotransmission is involved in some but not all aspects of pleasure-related cognition, since pleasure centers have been identified both within the dopamine system (i.e., nucleus accumbens shell) and outside the dopamine system (i.e., ventral pallidum and parabrachial nucleus). For example, direct electrical stimulation of dopamine pathways, using electrodes implanted in the brain, is experienced as pleasurable, and many types of animals are willing to work to obtain it. Antipsychotic drugs reduce dopamine levels and tend to cause anhedonia, a diminished ability to experience pleasure. Many types of pleasurable experiences\u2014such as sexual intercourse, eating, and playing video games\u2014increase dopamine release. All addictive drugs directly or indirectly affect dopamine neurotransmission in the nucleus accumbens; these drugs increase drug \"wanting\", leading to compulsive drug use, when repeatedly taken in high doses, presumably through the sensitization of incentive-salience. Drugs that increase synaptic dopamine concentrations include psychostimulants such as methamphetamine and cocaine. These produce increases in \"wanting\" behaviors, but do not greatly alter expressions of pleasure or change levels of satiation. However, opiate drugs such as heroin and morphine produce increases in expressions of \"liking\" and \"wanting\" behaviors. Moreover, animals in which the ventral tegmental dopamine system has been rendered inactive do not seek food, and will starve to death if left to themselves, but if food is placed in their mouths they will consume it and show expressions indicative of pleasure.\nA clinical study from January 2019 that assessed the effect of a dopamine precursor (levodopa), dopamine antagonist (risperidone), and a placebo on reward responses to music \u2013 including the degree of pleasure experienced during musical chills, as measured by changes in electrodermal activity as well as subjective ratings \u2013 found that the manipulation of dopamine neurotransmission bidirectionally regulates pleasure cognition (specifically, the hedonic impact of music) in human subjects. This research demonstrated that increased dopamine neurotransmission acts as a sine qua non condition for pleasurable hedonic reactions to music in humans.\nA study published in Nature in 1998 found evidence that playing video games releases dopamine in the human striatum. This dopamine is associated with learning, behavior reinforcement, attention, and sensorimotor integration. Researchers used positron emission tomography scans and 11C-labelled raclopride to track dopamine levels in the brain during goal-directed motor tasks and found that dopamine release was positively correlated with task performance and was greatest in the ventral striatum. This was the first study to demonstrate the behavioral conditions under which dopamine is released in humans. It highlights the ability of positron emission tomography to detect neurotransmitter fluxes during changes in behavior. According to research, potentially problematic video game use is related to personality traits such as low self-esteem and low self-efficacy, anxiety, aggression, and clinical symptoms of depression and anxiety disorders. Additionally, the reasons individuals play video games vary and may include coping, socialization, and personal satisfaction. The DSM-5 defines Internet Gaming Disorder as a mental disorder closely related to Gambling Disorder. This has been supported by some researchers but has also caused controversy.\n\nOutside the central nervous system\nDopamine does not cross the blood\u2013brain barrier, so its synthesis and functions in peripheral areas are to a large degree independent of its synthesis and functions in the brain. A substantial amount of dopamine circulates in the bloodstream, but its functions there are not entirely clear. Dopamine is found in blood plasma at levels comparable to those of epinephrine, but in humans, over 95% of the dopamine in the plasma is in the form of dopamine sulfate, a conjugate produced by the enzyme sulfotransferase 1A3\/1A4 acting on free dopamine. The bulk of this dopamine sulfate is produced in the mesenteric organs. The production of dopamine sulfate is thought to be a mechanism for detoxifying dopamine that is ingested as food or produced by the digestive process\u2014levels in the plasma typically rise more than fifty-fold after a meal. Dopamine sulfate has no known biological functions and is excreted in urine.\nThe relatively small quantity of unconjugated dopamine in the bloodstream may be produced by the sympathetic nervous system, the digestive system, or possibly other organs. It may act on dopamine receptors in peripheral tissues, or be metabolized, or be converted to norepinephrine by the enzyme dopamine beta hydroxylase, which is released into the bloodstream by the adrenal medulla. Some dopamine receptors are located in the walls of arteries, where they act as a vasodilator and an inhibitor of norepinephrine release from postganglionic sympathetic nerves terminals (dopamine can inhibit norepinephrine release by acting on presynaptic dopamine receptors, and also on presynaptic \u03b1-1 receptors, like norepinephrine itself). These responses might be activated by dopamine released from the carotid body under conditions of low oxygen, but whether arterial dopamine receptors perform other biologically useful functions is not known.\nBeyond its role in modulating blood flow, there are several peripheral systems in which dopamine circulates within a limited area and performs an exocrine or paracrine function. The peripheral systems in which dopamine plays an important role include the immune system, the kidneys and the pancreas.\n\nImmune system\nIn the immune system dopamine acts upon receptors present on immune cells, especially lymphocytes. Dopamine can also affect immune cells in the spleen, bone marrow, and circulatory system. In addition, dopamine can be synthesized and released by immune cells themselves. The main effect of dopamine on lymphocytes is to reduce their activation level. The functional significance of this system is unclear, but it affords a possible route for interactions between the nervous system and immune system, and may be relevant to some autoimmune disorders.\n\nKidneys\nThe renal dopaminergic system is located in the cells of the nephron in the kidney, where all subtypes of dopamine receptors are present. Dopamine is also synthesized there, by tubule cells, and discharged into the tubular fluid. Its actions include increasing the blood supply to the kidneys, increasing the glomerular filtration rate, and increasing the excretion of sodium in the urine. Hence, defects in renal dopamine function can lead to reduced sodium excretion and consequently result in the development of high blood pressure. There is strong evidence that faults in the production of dopamine or in the receptors can result in a number of pathologies including oxidative stress, edema, and either genetic or essential hypertension. Oxidative stress can itself cause hypertension. Defects in the system can also be caused by genetic factors or high blood pressure.\n\nPancreas\nIn the pancreas the role of dopamine is somewhat complex. The pancreas consists of two parts, an exocrine and an endocrine component. The exocrine part synthesizes and secretes digestive enzymes and other substances, including dopamine, into the small intestine. The function of this secreted dopamine after it enters the small intestine is not clearly established\u2014the possibilities include protecting the intestinal mucosa from damage and reducing gastrointestinal motility (the rate at which content moves through the digestive system).\nThe pancreatic islets make up the endocrine part of the pancreas, and synthesize and secrete hormones including insulin into the bloodstream. There is evidence that the beta cells in the islets that synthesize insulin contain dopamine receptors, and that dopamine acts to reduce the amount of insulin they release. The source of their dopamine input is not clearly established\u2014it may come from dopamine that circulates in the bloodstream and derives from the sympathetic nervous system, or it may be synthesized locally by other types of pancreatic cells.\n\nMedical uses\nDopamine as a manufactured medication is sold under the trade names Intropin, Dopastat, and Revimine, among others. It is on the World Health Organization's List of Essential Medicines. It is most commonly used as a stimulant drug in the treatment of severe low blood pressure, slow heart rate, and cardiac arrest. It is especially important in treating these in newborn infants. It is given intravenously. Since the half-life of dopamine in plasma is very short\u2014approximately one minute in adults, two minutes in newborn infants and up to five minutes in preterm infants\u2014it is usually given in a continuous intravenous drip rather than a single injection.\nIts effects, depending on dosage, include an increase in sodium excretion by the kidneys, an increase in urine output, an increase in heart rate, and an increase in blood pressure. At low doses it acts through the sympathetic nervous system to increase heart muscle contraction force and heart rate, thereby increasing cardiac output and blood pressure. Higher doses also cause vasoconstriction that further increases blood pressure. Older literature also describes very low doses thought to improve kidney function without other consequences, but recent reviews have concluded that doses at such low levels are not effective and may sometimes be harmful. While some effects result from stimulation of dopamine receptors, the prominent cardiovascular effects result from dopamine acting at \u03b11, \u03b21, and \u03b22 adrenergic receptors.\nSide effects of dopamine include negative effects on kidney function and irregular heartbeats. The LD50, or lethal dose which is expected to prove fatal in 50% of the population, has been found to be: 59 mg\/kg (mouse; administered intravenously); 95 mg\/kg (mouse; administered intraperitoneally); 163 mg\/kg (rat; administered intraperitoneally); 79 mg\/kg (dog; administered intravenously).\n\nDisease, disorders, and pharmacology\nThe dopamine system plays a central role in several significant medical conditions, including Parkinson's disease, attention deficit hyperactivity disorder, Tourette syndrome, schizophrenia, bipolar disorder, and addiction. Aside from dopamine itself, there are many other important drugs that act on dopamine systems in various parts of the brain or body. Some are used for medical or recreational purposes, but neurochemists have also developed a variety of research drugs, some of which bind with high affinity to specific types of dopamine receptors and either agonize or antagonize their effects, and many that affect other aspects of dopamine physiology, including dopamine transporter inhibitors, VMAT inhibitors, and enzyme inhibitors.\n\nAging brain\nA number of studies have reported an age-related decline in dopamine synthesis and dopamine receptor density (i.e., the number of receptors) in the brain. This decline has been shown to occur in the striatum and extrastriatal regions. Decreases in the D1, D2, and D3 receptors are well documented. The reduction of dopamine with aging is thought to be responsible for many neurological symptoms that increase in frequency with age, such as decreased arm swing and increased rigidity. Changes in dopamine levels may also cause age-related changes in cognitive flexibility.\n\nMultiple sclerosis\nStudies reported that dopamine imbalance influences the fatigue in multiple sclerosis. In patients with multiple sclerosis, dopamine inhibits production of IL-17 and IFN-\u03b3 by peripheral blood mononuclear cells.\n\nParkinson's disease\nParkinson's disease is an age-related disorder characterized by movement disorders such as stiffness of the body, slowing of movement, and trembling of limbs when they are not in use. In advanced stages it progresses to dementia and eventually death. The main symptoms are caused by the loss of dopamine-secreting cells in the substantia nigra. These dopamine cells are especially vulnerable to damage, and a variety of insults, including encephalitis (as depicted in the book and movie Awakenings), repeated sports-related concussions, and some forms of chemical poisoning such as MPTP, can lead to substantial cell loss, producing a parkinsonian syndrome that is similar in its main features to Parkinson's disease. Most cases of Parkinson's disease, however, are idiopathic, meaning that the cause of cell death cannot be identified.\nThe most widely used treatment for parkinsonism is administration of L-DOPA, the metabolic precursor for dopamine. L-DOPA is converted to dopamine in the brain and various parts of the body by the enzyme DOPA decarboxylase. L-DOPA is used rather than dopamine itself because, unlike dopamine, it is capable of crossing the blood\u2013brain barrier. It is often co-administered with an enzyme inhibitor of peripheral decarboxylation such as carbidopa or benserazide, to reduce the amount converted to dopamine in the periphery and thereby increase the amount of L-DOPA that enters the brain. When L-DOPA is administered regularly over a long time period, a variety of unpleasant side effects such as dyskinesia often begin to appear; even so, it is considered the best available long-term treatment option for most cases of Parkinson's disease.\nL-DOPA treatment cannot restore the dopamine cells that have been lost, but it causes the remaining cells to produce more dopamine, thereby compensating for the loss to at least some degree. In advanced stages the treatment begins to fail because the cell loss is so severe that the remaining ones cannot produce enough dopamine regardless of L-DOPA levels. Other drugs that enhance dopamine function, such as bromocriptine and pergolide, are also sometimes used to treat Parkinsonism, but in most cases L-DOPA appears to give the best trade-off between positive effects and negative side-effects.\nDopaminergic medications that are used to treat Parkinson's disease are sometimes associated with the development of a dopamine dysregulation syndrome, which involves the overuse of dopaminergic medication and medication-induced compulsive engagement in natural rewards like gambling and sexual activity. The latter behaviors are similar to those observed in individuals with a behavioral addiction.\n\nDrug addiction and psychostimulants\nCocaine, substituted amphetamines (including methamphetamine), Adderall, methylphenidate (marketed as Ritalin or Concerta), and other psychostimulants exert their effects primarily or partly by increasing dopamine levels in the brain by a variety of mechanisms. Cocaine and methylphenidate are dopamine transporter blockers or reuptake inhibitors; they non-competitively inhibit dopamine reuptake, resulting in increased dopamine concentrations in the synaptic cleft.:\u200a54\u201358\u200a Like cocaine, substituted amphetamines and amphetamine also increase the concentration of dopamine in the synaptic cleft, but by different mechanisms.:\u200a147\u2013150\u200a\nThe effects of psychostimulants include increases in heart rate, body temperature, and sweating; improvements in alertness, attention, and endurance; increases in pleasure produced by rewarding events; but at higher doses agitation, anxiety, or even loss of contact with reality. Drugs in this group can have a high addiction potential, due to their activating effects on the dopamine-mediated reward system in the brain. However some can also be useful, at lower doses, for treating attention deficit hyperactivity disorder (ADHD) and narcolepsy. An important differentiating factor is the onset and duration of action. Cocaine can take effect in seconds if it is injected or inhaled in free base form; the effects last from 5 to 90 minutes. This rapid and brief action makes its effects easily perceived and consequently gives it high addiction potential. Methylphenidate taken in pill form, in contrast, can take two hours to reach peak levels in the bloodstream, and depending on formulation the effects can last for up to 12 hours. These longer acting formulations have the benefit of reducing the potential for abuse, and improving adherence for treatment by using more convenient dosage regimens.\n\nA variety of addictive drugs produce an increase in reward-related dopamine activity. Stimulants such as nicotine, cocaine and methamphetamine promote increased levels of dopamine which appear to be the primary factor in causing addiction. For other addictive drugs such as the opioid heroin, the increased levels of dopamine in the reward system may play only a minor role in addiction. When people addicted to stimulants go through withdrawal, they do not experience the physical suffering associated with alcohol withdrawal or withdrawal from opiates; instead they experience craving, an intense desire for the drug characterized by irritability, restlessness, and other arousal symptoms, brought about by psychological dependence.\nThe dopamine system plays a crucial role in several aspects of addiction. At the earliest stage, genetic differences that alter the expression of dopamine receptors in the brain can predict whether a person will find stimulants appealing or aversive. Consumption of stimulants produces increases in brain dopamine levels that last from minutes to hours. Finally, the chronic elevation in dopamine that comes with repetitive high-dose stimulant consumption triggers a wide-ranging set of structural changes in the brain that are responsible for the behavioral abnormalities which characterize an addiction. Treatment of stimulant addiction is very difficult, because even if consumption ceases, the craving that comes with psychological withdrawal does not. Even when the craving seems to be extinct, it may re-emerge when faced with stimuli that are associated with the drug, such as friends, locations and situations. Association networks in the brain are greatly interlinked.\n\nPsychosis and antipsychotic drugs\nPsychiatrists in the early 1950s discovered that a class of drugs known as typical antipsychotics (also known as major tranquilizers), were often effective at reducing the psychotic symptoms of schizophrenia. The introduction of the first widely used antipsychotic, chlorpromazine (Thorazine), in the 1950s, led to the release of many patients with schizophrenia from institutions in the years that followed. By the 1970s researchers understood that these typical antipsychotics worked as antagonists on the D2 receptors. This realization led to the so-called dopamine hypothesis of schizophrenia, which postulates that schizophrenia is largely caused by hyperactivity of brain dopamine systems. The dopamine hypothesis drew additional support from the observation that psychotic symptoms were often intensified by dopamine-enhancing stimulants such as methamphetamine, and that these drugs could also produce psychosis in healthy people if taken in large enough doses. In the following decades other atypical antipsychotics that had fewer serious side effects were developed. Many of these newer drugs do not act directly on dopamine receptors, but instead produce alterations in dopamine activity indirectly. These drugs were also used to treat other psychoses. Antipsychotic drugs have a broadly suppressive effect on most types of active behavior, and particularly reduce the delusional and agitated behavior characteristic of overt psychosis.\nLater observations, however, have caused the dopamine hypothesis to lose popularity, at least in its simple original form. For one thing, patients with schizophrenia do not typically show measurably increased levels of brain dopamine activity. Even so, many psychiatrists and neuroscientists continue to believe that schizophrenia involves some sort of dopamine system dysfunction. As the \"dopamine hypothesis\" has evolved over time, however, the sorts of dysfunctions it postulates have tended to become increasingly subtle and complex.\nPsychopharmacologist Stephen M. Stahl suggested in a review of 2018 that in many cases of psychosis, including schizophrenia, three interconnected networks based on dopamine, serotonin, and glutamate \u2013 each on its own or in various combinations \u2013 contributed to an overexcitation of dopamine D2 receptors in the ventral striatum.\n\nAttention deficit hyperactivity disorder\nAltered dopamine neurotransmission is implicated in attention deficit hyperactivity disorder (ADHD), a condition associated with impaired cognitive control, in turn leading to problems with regulating attention (attentional control), inhibiting behaviors (inhibitory control), and forgetting things or missing details (working memory), among other problems. There are genetic links between dopamine receptors, the dopamine transporter, and ADHD, in addition to links to other neurotransmitter receptors and transporters. The most important relationship between dopamine and ADHD involves the drugs that are used to treat ADHD. Some of the most effective therapeutic agents for ADHD are psychostimulants such as methylphenidate (Ritalin, Concerta) and amphetamine (Evekeo, Adderall, Dexedrine), drugs that increase both dopamine and norepinephrine levels in the brain. The clinical effects of these psychostimulants in treating ADHD are mediated through the indirect activation of dopamine and norepinephrine receptors, specifically dopamine receptor D1 and adrenoceptor \u03b12, in the prefrontal cortex.\n\nPain\nDopamine plays a role in pain processing in multiple levels of the central nervous system including the spinal cord, periaqueductal gray, thalamus, basal ganglia, and cingulate cortex. Decreased levels of dopamine have been associated with painful symptoms that frequently occur in Parkinson's disease. Abnormalities in dopaminergic neurotransmission also occur in several painful clinical conditions, including burning mouth syndrome, fibromyalgia, and restless legs syndrome.\n\nNausea\nNausea and vomiting are largely determined by activity in the area postrema in the medulla of the brainstem, in a region known as the chemoreceptor trigger zone. This area contains a large population of type D2 dopamine receptors. Consequently, drugs that activate D2 receptors have a high potential to cause nausea. This group includes some medications that are administered for Parkinson's disease, as well as other dopamine agonists such as apomorphine. In some cases, D2-receptor antagonists such as metoclopramide are useful as anti-nausea drugs.\n\nComparative biology and evolution\nMicroorganisms\nThere are no reports of dopamine in archaea, but it has been detected in some types of bacteria and in the protozoan called Tetrahymena. Perhaps more importantly, there are types of bacteria that contain homologs of all the enzymes that animals use to synthesize dopamine. It has been proposed that animals derived their dopamine-synthesizing machinery from bacteria, via horizontal gene transfer that may have occurred relatively late in evolutionary time, perhaps as a result of the symbiotic incorporation of bacteria into eukaryotic cells that gave rise to mitochondria.\n\nAnimals\nDopamine is used as a neurotransmitter in most multicellular animals. In sponges there is only a single report of the presence of dopamine, with no indication of its function; however, dopamine has been reported in the nervous systems of many other radially symmetric species, including the cnidarian jellyfish, hydra and some corals. This dates the emergence of dopamine as a neurotransmitter back to the earliest appearance of the nervous system, over 500 million years ago in the Cambrian Period. Dopamine functions as a neurotransmitter in vertebrates, echinoderms, arthropods, molluscs, and several types of worm.\nIn every type of animal that has been examined, dopamine has been seen to modify motor behavior. In the model organism, nematode Caenorhabditis elegans, it reduces locomotion and increases food-exploratory movements; in flatworms it produces \"screw-like\" movements; in leeches it inhibits swimming and promotes crawling. Across a wide range of vertebrates, dopamine has an \"activating\" effect on behavior-switching and response selection, comparable to its effect in mammals.\nDopamine has also consistently been shown to play a role in reward learning, in all animal groups. As in all vertebrates \u2013 invertebrates such as roundworms, flatworms, molluscs and common fruit flies can all be trained to repeat an action if it is consistently followed by an increase in dopamine levels. In fruit flies, distinct elements for reward learning suggest a modular structure to the insect reward processing system that broadly parallels that in the mammalian one. For example, dopamine regulates short- and long-term learning in monkeys; in fruit flies, different groups of dopamine neurons mediate reward signals for short- and long-term memories.\nIt had long been believed that arthropods were an exception to this with dopamine being seen as having an adverse effect. Reward was seen to be mediated instead by octopamine, a neurotransmitter closely related to norepinephrine. More recent studies, however, have shown that dopamine does play a part in reward learning in fruit flies. It has also been found that the rewarding effect of octopamine is due to its activating a set of dopaminergic neurons not previously accessed in the research.\n\nPlants\nMany plants, including a variety of food plants, synthesize dopamine to varying degrees. The highest concentrations have been observed in bananas\u2014the fruit pulp of red and yellow bananas contains dopamine at levels of 40 to 50 parts per million by weight. Potatoes, avocados, broccoli, and Brussels sprouts may also contain dopamine at levels of 1 part per million or more; oranges, tomatoes, spinach, beans, and other plants contain measurable concentrations less than 1 part per million. The dopamine in plants is synthesized from the amino acid tyrosine, by biochemical mechanisms similar to those that animals use. It can be metabolized in a variety of ways, producing melanin and a variety of alkaloids as byproducts. The functions of plant catecholamines have not been clearly established, but there is evidence that they play a role in the response to stressors such as bacterial infection, act as growth-promoting factors in some situations, and modify the way that sugars are metabolized. The receptors that mediate these actions have not yet been identified, nor have the intracellular mechanisms that they activate.\nDopamine consumed in food cannot act on the brain, because it cannot cross the blood\u2013brain barrier. However, there are also a variety of plants that contain L-DOPA, the metabolic precursor of dopamine. The highest concentrations are found in the leaves and bean pods of plants of the genus Mucuna, especially in Mucuna pruriens (velvet beans), which have been used as a source for L-DOPA as a drug. Another plant containing substantial amounts of L-DOPA is Vicia faba, the plant that produces fava beans (also known as \"broad beans\"). The level of L-DOPA in the beans, however, is much lower than in the pod shells and other parts of the plant. The seeds of Cassia and Bauhinia trees also contain substantial amounts of L-DOPA.\nIn a species of marine green algae Ulvaria obscura, a major component of some algal blooms, dopamine is present in very high concentrations, estimated at 4.4% of dry weight. There is evidence that this dopamine functions as an anti-herbivore defense, reducing consumption by snails and isopods.\n\nAs a precursor for melanin\nMelanins are a family of dark-pigmented substances found in a wide range of organisms. Chemically they are closely related to dopamine, and there is a type of melanin, known as dopamine-melanin, that can be synthesized by oxidation of dopamine via the enzyme tyrosinase. The melanin that darkens human skin is not of this type: it is synthesized by a pathway that uses L-DOPA as a precursor but not dopamine. However, there is substantial evidence that the neuromelanin that gives a dark color to the brain's substantia nigra is at least in part dopamine-melanin.\nDopamine-derived melanin probably appears in at least some other biological systems as well. Some of the dopamine in plants is likely to be used as a precursor for dopamine-melanin. The complex patterns that appear on butterfly wings, as well as black-and-white stripes on the bodies of insect larvae, are also thought to be caused by spatially structured accumulations of dopamine-melanin.\n\nHistory and development\nDopamine was first synthesized in 1910 by George Barger and James Ewens at Wellcome Laboratories in London, England and first identified in the human brain by Katharine Montagu in 1957. It was named dopamine because it is a monoamine whose precursor in the Barger-Ewens synthesis is 3,4-dihydroxyphenylalanine (levodopa or L-DOPA). Dopamine's function as a neurotransmitter was first recognized in 1958 by Arvid Carlsson and Nils-\u00c5ke Hillarp at the Laboratory for Chemical Pharmacology of the National Heart Institute of Sweden. Carlsson was awarded the 2000 Nobel Prize in Physiology or Medicine for showing that dopamine is not only a precursor of norepinephrine (noradrenaline) and epinephrine (adrenaline), but is also itself a neurotransmitter.\n\nPolydopamine\nResearch motivated by adhesive polyphenolic proteins in mussels led to the discovery in 2007 that a wide variety of materials, if placed in a solution of dopamine at slightly basic pH, will become coated with a layer of polymerized dopamine, often referred to as polydopamine. This polymerized dopamine forms by a spontaneous oxidation reaction, and is formally a type of melanin. Furthermore, dopamine self-polymerization can be used to modulate the mechanical properties of peptide-based gels. Synthesis of polydopamine usually involves reaction of dopamine hydrochloride with Tris as a base in water. The structure of polydopamine is unknown.\nPolydopamine coatings can form on objects ranging in size from nanoparticles to large surfaces. Polydopamine layers have chemical properties that have the potential to be extremely useful, and numerous studies have examined their possible applications. At the simplest level, they can be used for protection against damage by light, or to form capsules for drug delivery. At a more sophisticated level, their adhesive properties may make them useful as substrates for biosensors or other biologically active macromolecules.\n\nSee also\nDopamine fasting\nBreastfeeding and fertility\nEndorphin\nSerotonin\nOxytocin\nEnkephalin\n\nReferences\nExternal links\n The dictionary definition of Dopamine at Wiktionary\n Media related to Dopamine at Wikimedia Commons","66":"Capt. Edward Deforest Thalmann, USN (ret.) (April 3, 1945 \u2013 July 24, 2004) was an American hyperbaric medicine specialist who was principally responsible for developing the current United States Navy dive tables for mixed-gas diving, which are based on his eponymous Thalmann Algorithm (VVAL18). At the time of his death, Thalmann was serving as assistant medical director of the Divers Alert Network (DAN) and an assistant clinical professor in anesthesiology at Duke University's Center for Hyperbaric Medicine and Environmental Physiology.\n\nEducation\nThalmann graduated in 1962 from Sayreville War Memorial High School in Sayreville, New Jersey. He attended the Rensselaer Polytechnic Institute, graduating in 1966 with a bachelor of science degree. He attended medical school at Georgetown University in Washington, D.C. From 1970 to 1971, Thalmann was a surgical intern at the Royal Victoria Hospital in Montreal, Quebec. It was there that he met his future wife, a nursing student.\nWhile on active duty, from 1975 to 1977, Thalmann conducted a two-year postdoctoral fellowship under the guidance of Claes Lundgren and Hermann Rahn, at the State University of New York at Buffalo, studying the effects of immersion and breathing bag placement in rebreathers on underwater exercise.\n\nNaval career\nThalmann served as Chief Medical Officer on board the ballistic missile submarine USS Thomas Jefferson for a single deployment, from 1971 to 1972 before being posted as a research diving medical officer at the United States Navy Experimental Diving Unit (NEDU) at the Washington Navy Yard, where he was stationed until 1975.\nFollowing his post-doctoral fellowship in Buffalo, in 1977, Thalmann returned to NEDU, now located in Panama City, Florida, as Assistant Senior Medical Officer, where he began developing new dive tables and mixed-gas diving techniques. While at NEDU, Thalmann created a number of unique and innovative underwater exercise devices, still in use today, intended to assist in gauging the underwater endurance of divers using various gas mixtures while performing physically demanding tasks.\nIn 1985, Thalmann, at that time the Senior Medical Officer at NEDU, was selected for the NATO Undersea Medicine Personnel Exchange Program and assigned to the Royal Navy Institute of Naval Medicine, Alverstoke, United Kingdom. There he continued development of a new decompression table and worked on improving undersea thermal protection garments. Upon the conclusion of his exchange tour in 1987, Thalmann returned to Bethesda to serve as the commander of the Naval Medical Research Institute's diving medicine and physiology research division.\n\nCivilian career\nFollowing his retirement from the Navy in 1993, Thalmann stayed on at NMRI as a senior scientist in decompression research. In July 1994 took a position in Durham, North Carolina at Duke's Center for Hyperbaric Medicine and Environmental Physiology and later accepted a simultaneous position as the Assistant Medical Director of DAN in 1995.\nThalmann died on July 24, 2004, in Durham, due to congestive heart failure, at the age of 59. He was committed to the sea on August 31, 2004, with services conducted aboard USS Maryland, an Ohio-class submarine, off the coast of Kings Bay, Georgia at 30\u00b057\u203200\u2033N 79\u00b053\u203230\u2033W.\n\nContributions to hyperbaric medicine\nBased on scientific studies of gas exchange in human tissues, further informed by his supervision of hundreds of experimental dives, Thalmann developed his namesake mathematical algorithm to protect divers from decompression sickness. The Thalmann algorithm was the basis for a new set of decompression tables that provided more flexibility for diving time, depth, gas mixtures and pressures. The algorithm was also used for developing wearable dive computers to manage complex individual dives. Thalmann's research ultimately improved decompression safety for military divers, recreational divers, and even astronauts.\n\nAwards\nPublications\nRefereed journals\nNon-refereed journals and reports\nBook chapters\nReferences\nExternal links\n\nEd Thalmann Papers at Duke University Medical Center Archives","67":"Electronystagmography (ENG) is a diagnostic test to record involuntary movements of the eye caused by a condition known as nystagmus.  It can also be used to diagnose the cause of vertigo, dizziness or balance dysfunction by testing the vestibular system. Electronystagmography is used to assess voluntary and involuntary eye movements. It evaluates the cochlear nerve and the oculomotor nerve (CN III). The ENG can be used to determine the origin of various eye and ear disorders.\n\nTechnique and results\nElectrodes applied around the eyes record eye movements using the corneo-retinal potential. Some portions of the ENG measure a patient's ability to track a moving stimuli while others observe the presence of nystagmus. The vestibular system monitors the position and movements of the head to stabilize retinal images. This information is integrated with the visual system and spinal afferents in the brain stem to produce the vestibulo-ocular reflex (VOR). ENG provides an objective assessment of the oculomotor and vestibular systems. A newer standard for the recording is the use of infrared video systems which allow for a more detailed observation and analysis of these eye movements, called video nystagmography (VNG). A similar test is performed for testing vertigo by using the caloric reflex test, which can be induced by air or water of specific temperatures, typically \u00b1 7 degrees Celsius from body temperature.\nThe standard ENG test battery consists of three parts:\n\noculomotor evaluation\npositioning and positional testing\ncaloric stimulation of the vestibular system\nThe comparison of results obtained from various subtests of ENG assists in determining whether a disorder is central or peripheral. In peripheral vestibular disorders, the side of lesion can be inferred from the results of caloric stimulation and, to some degree, from positional findings.\nENG or VNG can be used to record nystagmus during oculomotor tests such as saccades, pursuit and gaze testing, optokinetics and also calorics (dithermal or monothermal). Abnormal oculomotor test results may indicate either systemic or central pathology as opposed to peripheral (vestibular) pathology.\nOptokinetics generally are used as a cross-check on abnormal responses to oculomotor tests. Both of these tests use a \"light bar\" involving a moving light (usually red) which the patient will track with the eyes.\nThe caloric irrigation is the only vestibular test which allows the clinician to test the vestibular organs individually; however, it only tests one of the three semicircular canals: the horizontal canal.\nWhile ENG is the most widely used clinical laboratory test to assess vestibular function, normal ENG test results do not necessarily mean that a patient has typical vestibular function. ENG abnormalities can be useful in the diagnosis and localization of the site of lesion; however, many abnormalities are nonlocalizing; therefore, the clinical history and otologic examination of the patient are vital in formulating a diagnosis and treatment plan for a patient presenting with dizziness or vertigo.\n\nReferences\nExternal links\nVestibular Testing - Timothy C. Hain, MD\neMedicine - Electronystagmography","68":"Electrocochleography (abbreviated ECochG or ECOG) is a technique of recording electrical potentials generated in the inner ear and auditory nerve in response to sound stimulation, using an electrode placed in the ear canal or tympanic membrane. The test is performed by an otologist or audiologist with specialized training, and is used for detection of elevated inner ear pressure (endolymphatic hydrops) or for the testing and monitoring of inner ear and auditory nerve function during surgery.\n\nClinical applications\nThe most common clinical applications of electrocochleography include:\n\nObjective identification and monitoring of M\u00e9ni\u00e8re's disease and endolymphatic hydrops (EH)\nIntraoperative monitoring of auditory system function during surgery on the brainstem or cerebellum\nEnhancement of Wave I of the auditory brainstem response, particularly in patients who are hard of hearing\nDiagnosis of auditory neuropathy\n\nCochlear physiology\nThe basilar membrane and the hair cells of the cochlea function as a sharply tuned frequency analyzer. Sound is transmitted to the inner ear via vibration of the tympanic membrane, leading to movement of the middle ear bones (malleus, incus, and stapes).  Movement of the stapes on the oval window generates a pressure wave in the perilymph within the cochlea, causing the basilar membrane to vibrate. Sounds of different frequencies vibrate different parts of the basilar membrane, and the point of maximal vibration amplitude depends on the sound frequency.\nAs the basilar membrane vibrates, the hair cells attached to this membrane are rhythmically pushed up against the tectorial membrane, bending the hair cell stereocilia. This opens mechanically gated ion channels on the hair cell, allowing influx of potassium (K+) and calcium (Ca2+) ions. The flow of ions generates an AC current through the hair cell surface, at the same frequency as the acoustic stimulus. This measurable AC voltage is called the cochlear microphonic (CM), which mimics the stimulus. The hair cells function as a transducer, converting the mechanical movement of the basilar membrane into electrical voltage, in a process requiring ATP from the stria vascularis as an energy source.\nThe depolarized hair cell releases neurotransmitters across a synapse to primary auditory neurons of the spiral ganglion. Upon reaching receptors on the postsynaptic spiral ganglion neurons, the neurotransmitters induce a postsynaptic potential or generator potential in the neuronal projections. When a certain threshold potential is reached, the spiral ganglion neuron fires an action potential, which enters the auditory processing pathway of the brain.\n\nCochlear potentials\nA resting endolymphatic potential of a normal cochlea is + 80 mV. There are at least 3 other potentials generated upon cochlear stimulation:\n\nCochlear microphonic (CM)\nSummating potential (SP)\nAction potential (AP)\nAs described above, the cochlear microphonic (CM) is an alternating current (AC) voltage that mirrors the waveform of the acoustic stimulus. It is dominated by the outer hair cells of the organ of Corti. The magnitude of the recording is dependent on the proximity of the recording electrodes to the hair cells. The CM is proportional to the displacement of the basilar membrane. A fourth potential, the auditory nerve neurophonic, is sometimes dissociated from the CM. The neurophonic represents the neural part (auditory nerve spikes) phased-locked to the stimulus and is similar to the Frequency following response.\nThe summating potential (SP), first described by Tasaki et al. in 1954, represents the direct current (DC) response of the hair cells as they move in conjunction with the basilar membrane, as well as the DC response from dendritic and axonal potentials of the auditory nerve. The SP is the stimulus-related potential of the cochlea. Although historically it has been the least studied, renewed interest has surfaced due to changes in the SP reported in cases of endolymphatic hydrops or M\u00e9ni\u00e8re's disease.\nThe auditory nerve action potential, also called the compound action potential (CAP), is the most widely studied component in ECochG. The AP represents the summed response of the synchronous firing of the nerve fibers. It also appears as an AC voltage. The first and largest wave (N1) is identical to wave I of auditory brainstem response (ABR). Following this is N2, which is identical to wave II of the ABR. The magnitude of the action potential reflects the number of fibers that are firing. The latency of the AP is measured as the time between the onset and the peak of the N1 wave.\nThe CAP is considered to have low sensitivity to changes in stimulus polarity, in contrast to the CM which follows the polarity of the stimulation. As a result, researchers often use the sum (or difference) of responses to stimuli of alternating polarity to dissociate the CAP from CM.\n\nProcedure and recording parameters\nECochG can be performed with either invasive or non-invasive electrodes. Invasive electrodes, such as transtympanic (TT) needles, give clearer, more robust electrical responses (with larger amplitudes) since the electrodes are very close to the voltage generators. The needle is placed on the promontory wall of the middle ear and the round window. Non-invasive, or extratympanic (ET), electrodes have the advantage of not causing pain or discomfort to the patient. Unlike with invasive electrodes, there is no need for sedation, anesthesia, or medical supervision. The responses, however, are smaller in magnitude.\nAuditory stimuli in the form of broadband clicks 100 microseconds in duration are used. The stimulus polarity can be rarefaction polarity, condensation polarity, or alternating polarity. Signals are recorded from a primary recording (non-inverted) electrode located in the ear canal, tympanic membrane, or promontory (depending on type of electrode used). Reference (inverting) electrodes can be placed on the contralateral earlobe, mastoid, or ear canal.\nThe signal is processed, including signal amplification (by as much as a factor 100000 for extratympanic electrode recordings), noise filtration, and signal averaging.  A band-pass filter from 10 Hz to 1.5 kHz is often used.\n\nInterpretation of results\nThe CM, SP, and AP are all used in the diagnosis of endolymphatic hydrops and M\u00e9ni\u00e8re's disease. In particular, abnormally high SP and a high SP:AP ratio are signs of M\u00e9ni\u00e8re's disease. An SP:AP ratio of 0.45 or greater is considered abnormal.\n\nHistory\nThe CM was first discovered in 1930 by Ernest Wever and Charles Bray in cats. Wever and Bray mistakenly concluded that this recording was generated by the auditory nerve. They named the discovery the \"Wever-Bray effect\". Hallowell Davis and A.J. Derbyshire from Harvard replicated the study and concluded that the waves were in fact cochlear origin and not from the auditory nerve.\nFromm et al. were the first investigators to employ the ECochG technique in humans by inserting a wire electrode through the tympanic membrane and recording the CM from the niche of the round window and cochlear promontory. Their first measurement of the CM in humans was in 1935. They also discovered the N1, N2, and N3 waves following the CM, but it was Tasaki who identified these waves as auditory nerve action potentials.\nFisch and Ruben were the first to record the compound action potentials from both the round window and the eighth cranial nerve (CN VIII) in cats and mice. Ruben was also the first person to use CM and AP clinically.\nThe summating potential, a stimulus-related hair cell potential, was first described by Tasaki and colleagues in 1954. Ernest J. Moore was the first investigator to record the CM from surface electrodes. In 1971, Moore conducted five experiments in which he recorded CM and AP from 38 human subjects using surface electrodes. The purpose of the experiment was to establish the validity of the responses and to develop an artifact-free earphone system. Unfortunately, bulk of his work was never published.\n\nSee also\nAuditory evoked potential\nCochlea\nEEG\nElectrophysiology\n\n\n== References ==","69":"Endolymph is the fluid contained in the membranous labyrinth of the inner ear. The major cation in endolymph is potassium, with the values of sodium and potassium concentration in the endolymph being 0.91 mM and 154 mM, respectively. It is also called Scarpa's fluid, after Antonio Scarpa.\n\nStructure\nThe inner ear has two parts: the bony labyrinth and the membranous labyrinth.  The membranous labyrinth is contained within the bony labyrinth, and within the membranous labyrinth is a fluid called endolymph.  Between the outer wall of the membranous labyrinth and the wall of the bony labyrinth is the location of perilymph.\n\nComposition\nPerilymph and endolymph have unique ionic compositions suited to their functions in regulating electrochemical impulses of hair cells. The electric potential of endolymph is ~80-90 mV more positive than perilymph due to a higher concentration of K compared to Na.\nThe main component of this unique extracellular fluid is potassium, which is secreted from the stria vascularis. The high potassium content of the endolymph means that potassium, not sodium, is carried as the de-polarizing electric current in the hair cells. This is known as the mechano-electric transduction (MET) current.\nEndolymph has a high positive potential (80\u2013120 mV in the cochlea), relative to other nearby fluids such as perilymph, due to its high concentration of positively charged ions.  It is mainly this electrical potential difference that allows potassium ions to flow into the hair cells during mechanical stimulation of the hair bundle. Because the hair cells are at a negative potential of about \u221250 mV, the potential difference from endolymph to hair cell is on the order of 150 mV, which is the largest electrical potential difference found in the body.\n\nFunction\nHearing: Cochlear duct: fluid waves in the endolymph of the cochlear duct stimulate the receptor cells, which in turn translate their movement into nerve impulses that the brain perceives as sound.\nBalance: Semicircular canals: angular acceleration of the endolymph in the semicircular canals stimulate the vestibular receptors of the endolymph.  The semicircular canals of both inner ears act in concert to coordinate balance.\n\nClinical significance\nDisruption of the endolymph due to jerky movements (like spinning around or driving over bumps while riding in a car) can cause motion sickness. A condition where the volume of the endolymph is greatly enlarged is called endolymphatic hydrops and has been linked to M\u00e9ni\u00e8re's disease.\n\nAdditional images\nSee also\nDark cell\nPerilymph\nStria vascularis\nOrgan of Corti\nM\u00e9ni\u00e8re's disease\n\nReferences\nExternal links\nhttps:\/\/web.archive.org\/web\/20051030092447\/http:\/\/oto.wustl.edu\/cochlea\/res1.htm Longitudinal Flow of Endolymph at wustl.edu","70":"A cardiac electrophysiology study (EP test or EP study) is a minimally invasive procedure using catheters introduced through a vein or artery to record electrical activity from within the heart. This electrical activity is recorded when the heart is in a normal rhythm (sinus rhythm) to assess the conduction system of the heart and to look for additional electrical connections (accessory pathways), and during any abnormal heart rhythms that can be induced. EP studies are used to investigate the cause, location of origin, and best treatment for various abnormal heart rhythms, and are often followed by a catheter ablation during the same procedure.\n\nPreparation\nIt is important for patients not to eat or drink for up to 12 hours before the procedure. This is to prevent vomiting, which can result in aspiration, and also cause severe bleeding from the insertion site of the catheter. Failure to follow this simple preparation may result in dangerous consequences. In general, small amounts of water can be consumed up to 2 hours before the exam. Patients should try to schedule the exam at a time when they will be having symptoms and will not need to drive for 2 to 3 days.\n\nProcedure\nAn EP study is typically performed in an EP lab or cath lab. These are specially equipped operating rooms that usually contain an X-ray machine capable of acquiring live X-ray video images (a fluoroscope), equipment to record electrical signals from the heart, a stimulator to electrically excite the heart and control the heart rate, and ablation equipment to destroy abnormal tissue. A 3D navigation system that tracks and records the catheter position and associated electrical signals may also be used.\nThe procedure may be performed awake under local anaesthetic, or under general anaesthetic. Monitoring equipment is attached including an automated blood pressure cuff and a pulse oximeter to measure the oxygen saturation of the blood. A peripheral venous cannula is generally inserted to allow medication to be given such as sedatives, anesthesia, or drugs.\nAn access site that will allow catheters to be passed to the heart via an artery or vein is shaved and cleaned, usually in the groin. The blood vessels used to reach the heart (the femoral or subclavian veins, and sometimes the femoral artery) are punctured before a guidewire and plastic sheath are inserted into the vessel using the Seldinger technique.\n\nEP Study and inducibility\nOnce the catheter is in and all preparations are complete elsewhere in the lab, the EP study begins. The X-ray machine will give the doctor a live view of the heart and the position of the electrodes. He will guide the (steerable) electrodes to the correct position inside the heart. The electrophysiologist begins by moving the electrodes along the conduction pathways and along the inner walls of the heart, measuring the electrical activity along the way.\n\nThe next step is pacing the heart, this means he\/she will speed up or slow down the heart by placing the electrode at certain points along the conductive pathways of the heart and control the depolarization rate of the heart. The doctor will pace each chamber of the heart one by one, looking for any abnormalities. Then the electrophysiologist tries to provoke arrhythmias and reproduce any conditions that have resulted in the patient's placement in the study. This is done by injecting electric current into the conductive pathways and into the endocardium at various places. Last, the electrophysiologist may administer various drugs (proarrhythmic agents) to induce arrhythmia (inducibility of VT\/VF). If the arrhythmia is reproduced by the drugs (inducible), the electrophysiologist will search out the source of the abnormal electrical activity. The entire procedure can take several hours.\n\nAblation\nIf at any step during the EP study the electrophysiologist finds the source of the abnormal electrical activity, they may try to ablate the cells that are misfiring. This is done using high-energy radio frequencies (similar to microwaves) to effectively heat up the abnormal cells, to form scar tissue.\nThis can be painful with pain felt in the heart itself, the neck and shoulder areas. A more recent method of ablation is cryoablation, which is considered less risky and less painful.\n\nRecovery\nWhen the necessary procedures are complete, the catheter is removed. Firm pressure is applied to the site to prevent bleeding. This may be done by hand or with a mechanical device. Other closure techniques include an internal suture and plug. If the femoral artery was used, the patient will probably be asked to lie flat for several hours (3 to 6) to prevent bleeding or the development of a hematoma. Trying to sit up or even lift the head is strongly discouraged until an adequate clot has formed. The patient will be moved to a recovery area where he\/she will be monitored.\nFor patients who had a catheterization at the femoral artery or vein (and even some of those with a radial insertion site), in general recovery is fairly quick, as the only damage is at the insertion site. The patient will probably feel fine within 8 to 12 hours after the procedure, but may feel a small pinch at the insertion site. After a short period of general rest, the patient may resume some minor activity such as gentle, short, slow walks after the first 24 hours. If stairs must be climbed, they should be taken one step at a time and very slowly. All vigorous activity must be postponed until approved by a physician.\nIt is also important to note that unless directed by a doctor, some patients should avoid taking blood thinners and foods that contain salicylates, such as cranberry-containing products until the clot has healed (1\u20132 weeks).\n\nComplications\nAs with any surgical procedure, cardiac catheterizations come with a generic list of possible complications. One of the complications that are sometimes reported involves some temporary nerve involvement. Sometimes a small amount of swelling occurs that can put pressure on nerves in the area of the incision. Venous thrombosis is the most common complication with an incidence ranging between 0.5 and 2.5%. There have been reports of patients feeling like they have hot fluid like blood or urine running down their leg for up to a month or two after the incision has healed. This usually passes with time, but patients should tell their doctor if they have these symptoms and if they last.\nMore severe but relatively rare complications include: damage or trauma to a blood vessel, which could require repair; infection from the skin puncture or from the catheter itself; cardiac perforation, causing blood to leak into the sac around the heart and compromising the heart's pumping action, requiring removal using a needle under the breast bone (pericardiocentesis); hematoma at the site(s) of the puncture(s); induction of a dangerous cardiac rhythm requiring an external shock(s); a clot may be dislodged, which may travel to a distant organ and impede blood flow or cause a stroke; myocardial infarction; unanticipated reactions to the medications used during the procedure; damage to the conduction system, requiring a permanent pacemaker; death.\n\nSee also\nElectrocardiogram (ECG or EKG)\nElectrical conduction system of the heart\nVentricular tachycardia\nArrhythmia\nAntiarrhythmic agents\n\n\n== References ==","71":"Epilepsy is a group of non-communicable neurological disorders characterized by recurrent epileptic seizures. An epileptic seizure is the clinical manifestation of an abnormal, excessive, and synchronized electrical discharge in the neurons. The occurrence of two or more unprovoked seizures defines epilepsy. The occurrence of just one seizure may warrant the definition (set out by the International League Against Epilepsy) in a more clinical usage where recurrence may be able to be prejudged. Epileptic seizures can vary from brief and nearly undetectable periods to long periods of vigorous shaking due to abnormal electrical activity in the brain. These episodes can result in physical injuries, either directly, such as broken bones, or through causing accidents. In epilepsy, seizures tend to recur and may have no detectable underlying cause. Isolated seizures that are provoked by a specific cause such as poisoning are not deemed to represent epilepsy. People with epilepsy may be treated differently in various areas of the world and experience varying degrees of social stigma due to the alarming nature of their symptoms.\nThe underlying mechanism of an epileptic seizure is excessive and abnormal neuronal activity in the cortex of the brain, which can be observed in the electroencephalogram (EEG) of an individual. The reason this occurs in most cases of epilepsy is unknown (cryptogenic); some cases occur as the result of brain injury, stroke, brain tumors, infections of the brain, or birth defects through a process known as epileptogenesis. Known genetic mutations are directly linked to a small proportion of cases. The diagnosis involves ruling out other conditions that might cause similar symptoms, such as fainting, and determining if another cause of seizures is present, such as alcohol withdrawal or electrolyte problems. This may be partly done by imaging the brain and performing blood tests. Epilepsy can often be confirmed with an EEG, but a normal reading does not rule out the condition.\nEpilepsy that occurs as a result of other issues may be preventable. Seizures are controllable with medication in about 69% of cases; inexpensive anti-seizure medications are often available. In those whose seizures do not respond to medication; surgery, neurostimulation or dietary changes may be considered. Not all cases of epilepsy are lifelong, and many people improve to the point that treatment is no longer needed.\nAs of 2021, about 51 million people have epilepsy. Nearly 80% of cases occur in the developing world. In 2021, it resulted in 140,000 deaths, an increase from 125,000 in 1990. Epilepsy is more common in children and older people. In the developed world, onset of new cases occurs most frequently in babies and the elderly. In the developing world, onset is more common at the extremes of age \u2013 in younger children and in older children and young adults due to differences in the frequency of the underlying causes. About 5\u201310% of people will have an unprovoked seizure by the age of 80. The chance of experiencing a second seizure within two years after the first is around 40%. In many areas of the world, those with epilepsy either have restrictions placed on their ability to drive or are not permitted to drive until they are free of seizures for a specific length of time. The word epilepsy is from Ancient Greek \u1f10\u03c0\u03b9\u03bb\u03b1\u03bc\u03b2\u03ac\u03bd\u03b5\u03b9\u03bd, 'to seize, possess, or afflict'.\n\nSigns and symptoms\nEpilepsy is characterized by a long-term risk of recurrent epileptic seizures. These seizures may present in several ways depending on the parts of the brain involved and the person's age.\n\nSeizures\nThe most common type (60%) of seizures are convulsive which involve involuntary muscle contractions. Of these, one-third begin as generalized seizures from the start, affecting both hemispheres of the brain and impairing consciousness. Two-thirds begin as focal seizures (which affect one hemisphere of the brain) which may progress to generalized seizures. The remaining 40% of seizures are non-convulsive. An example of this type is the absence seizure, which presents as a decreased level of consciousness and usually lasts about 10 seconds.\nCertain experiences, known as auras often precede focal seizures. The seizures can include sensory (visual, hearing, or smell), psychic, autonomic, and motor phenomena depending on which part of the brain is involved. Muscle jerks may start in a specific muscle group and spread to surrounding muscle groups in which case it is known as a Jacksonian march. Automatisms may occur, which are non-consciously generated activities and mostly simple repetitive movements like smacking the lips or more complex activities such as attempts to pick up something.\nThere are six main types of generalized seizures:\n\ntonic-clonic,\ntonic,\nclonic,\nmyoclonic,\nabsence, and\natonic seizures.\nThey all involve loss of consciousness and typically happen without warning.\nTonic-clonic seizures occur with a contraction of the limbs followed by their extension and arching of the back which lasts 10\u201330 seconds (the tonic phase). A cry may be heard due to contraction of the chest muscles, followed by a shaking of the limbs in unison (clonic phase). Tonic seizures produce constant contractions of the muscles. A person often turns blue as breathing is stopped. In clonic seizures there is shaking of the limbs in unison. After the shaking has stopped it may take 10\u201330 minutes for the person to return to normal; this period is called the \"postictal state\" or \"postictal phase.\" Loss of bowel or bladder control may occur during a seizure. People experiencing a seizure may bite their tongue, either the tip or on the sides; in tonic-clonic seizure, bites to the sides are more common. Tongue bites are also relatively common in psychogenic non-epileptic seizures. Psychogenic non-epileptic seizures are seizure like behavior without an associated synchronised electrical discharge on EEG and are considered a dissociative disorder.\nMyoclonic seizures involve very brief muscle spasms in either a few areas or all over. These sometimes cause the person to fall, which can cause injury. Absence seizures can be subtle with only a slight turn of the head or eye blinking with impaired consciousness; typically, the person does not fall over and returns to normal right after it ends. Atonic seizures involve losing muscle activity for greater than one second, typically occurring on both sides of the body. Rarer seizure types can cause involuntary unnatural laughter (gelastic), crying (dyscrastic), or more complex experiences such as d\u00e9j\u00e0 vu.\nAbout 6% of those with epilepsy have seizures that are often triggered by specific events and are known as reflex seizures. Those with reflex epilepsy have seizures that are only triggered by specific stimuli. Common triggers include flashing lights and sudden noises. In certain types of epilepsy, seizures happen more often during sleep, and in other types they occur almost only when sleeping. In 2017, the International League Against Epilepsy published new uniform guidelines for the classification of seizures as well as epilepsies along with their cause and comorbidities.\n\nSeizure clusters\nPatients with epilepsy may experience seizure clusters which may be broadly defined as an acute deterioration in seizure control. The prevalence of seizure clusters is uncertain given that studies have used different definitions to define them. However, estimates suggest that the prevalence may range from 5% to 50% of epilepsy patients. Refractory epilepsy patients who have a high seizure frequency are at the greatest risk for having seizure clusters. Seizure clusters are associated with increased healthcare use, worse quality of life, impaired psychosocial functioning, and possibly increased mortality. Benzodiazepines are used as an acute treatment for seizure clusters.\n\nPost-ictal\nAfter the active portion of a seizure (the ictal state) there is typically a period of recovery during which there is confusion, referred to as the postictal period, before a normal level of consciousness returns. It usually lasts 3 to 15 minutes but may last for hours. Other common symptoms include feeling tired, headache, difficulty speaking, and abnormal behavior. Psychosis after a seizure is relatively common, occurring in 6\u201310% of people. Often people do not remember what happened during this time. Localized weakness, known as Todd's paralysis, may also occur after a focal seizure. It would typically last for seconds to minutes but may rarely last for a day or two.\n\nPsychosocial\nEpilepsy can have adverse effects on social and psychological well-being. These effects may include social isolation, stigmatization, or disability. They may result in lower educational achievement and worse employment outcomes. Learning disabilities are common in those with the condition, and especially among children with epilepsy. The stigma of epilepsy can also affect the families of those with the disorder.\nCertain disorders occur more often in people with epilepsy, depending partly on the epilepsy syndrome present. These include depression, anxiety, obsessive\u2013compulsive disorder (OCD), and migraine. Attention deficit hyperactivity disorder (ADHD) affects three to five times more children with epilepsy than children without the condition. ADHD and epilepsy have significant consequences on a child's behavioral, learning, and social development. Epilepsy is also more common in children with autism.\nApproximately, one-in-three people with epilepsy have a lifetime history of a psychiatric disorder. There are believed to be multiple causes for this including pathophysiological changes related to the epilepsy itself as well as adverse experiences related to living with epilepsy (e.g., stigma, discrimination). In addition, it is thought that the relationship between epilepsy and psychiatric disorders is not unilateral but rather bidirectional. For example, patients with depression have an increased risk for developing new-onset epilepsy.\nThe presence of comorbid depression or anxiety in patients with epilepsy is associated with a poorer quality of life, increased mortality, increased healthcare use and a worse response to treatment (including surgical). Anxiety disorders and depression may explain more variability in quality of life than seizure type or frequency. There is evidence that both depression and anxiety disorders are underdiagnosed and undertreated in patients with epilepsy.\n\nCauses\nEpilepsy can have both genetic and acquired causes, with the interaction of these factors in many cases. Established acquired causes include serious brain trauma, stroke, tumours, and brain problems resulting from a previous infection. In about 60% of cases, the cause is unknown. Epilepsies caused by genetic, congenital, or developmental conditions are more common among younger people, while brain tumors and strokes are more likely in older people.\nSeizures may also occur as a consequence of other health problems; if they occur right around a specific cause, such as a stroke, head injury, toxic ingestion, or metabolic problem, they are known as acute symptomatic seizures and are in the broader classification of seizure-related disorders rather than epilepsy itself.\n\nGenetics\nGenetics is believed to be involved in the majority of cases, either directly or indirectly. Some epilepsies are due to a single gene defect (1\u20132%); most are due to the interaction of multiple genes and environmental factors. Each of the single gene defects is rare, with more than 200 in all described. Most genes involved affect ion channels, either directly or indirectly. These include genes for ion channels, enzymes, GABA, and G protein-coupled receptors.\nIn identical twins, if one is affected, there is a 50\u201360% chance that the other will also be affected. In non-identical twins, the risk is 15%. These risks are greater in those with generalized rather than focal seizures. If both twins are affected, most of the time they have the same epileptic syndrome (70\u201390%). Other close relatives of a person with epilepsy have a risk five times that of the general population. Between 1 and 10% of those with Down syndrome and 90% of those with Angelman syndrome have epilepsy.\n\nPhakomatoses\nPhakomatoses, also known as neurocutaneous disorders, are a group of multisystemic diseases that most prominently affect the skin and central nervous system. They are caused by defective development of the embryonic ectodermal tissue that is most often due to a single genetic mutation. The brain, as well as other neural tissue and the skin, are all derived from the ectoderm and thus defective development may result in epilepsy as well as other manifestations such as autism and intellectual disability. Some types of phakomatoses such as tuberous sclerosis complex and Sturge-Weber syndrome have a higher prevalence of epilepsy relative to others such as neurofibromatosis type 1.\nTuberous sclerosis complex is an autosomal dominant disorder that is caused by mutations in either the TSC1 or TSC2 gene and it affects approximately 1 in 6,000\u201310,000 live births. These mutations result in the upregulation of the mechanistic target of rapamycin (mTOR) pathway which leads to the growth of tumors in many organs including the brain, skin, heart, eyes and kidneys. In addition, abnormal mTOR activity is believed to alter neural excitability. The prevalence of epilepsy is estimated to be 80-90%. The majority of cases of epilepsy present within the first 3 years of life and are medically refractory. Relatively recent developments for the treatment of epilepsy in TSC patients include mTOR inhibitors, cannabidiol and vigabatrin. Epilepsy surgery is often pursued.\nSturge-Weber syndrome is caused by an activating somatic mutation in the GNAQ gene and it affects approximately 1 in 20,000\u201350,000 live births. The mutation results in vascular malformations affecting the brain, skin and eyes. The typical presentation includes a facial port-wine birthmark, ocular angiomas and cerebral vascular malformations which are most often unilateral but are bilateral in 15% of cases. The prevalence of epilepsy is 75-100% and is higher in those with bilateral involvement. Seizures typically occur within the first two years of life and are refractory in nearly half of cases. However, high rates of seizure freedom with surgery have been reported in as many as 83%.\nNeurofibromatosis type 1 is the most common phakomatoses and occurs in approximately 1 in 3,000 live births. It is caused by autosomal dominant mutations in the Neurofibromin 1 gene. Clinical manifestations are variable but may include hyperpigmented skin marks, hamartomas of the iris called Lisch nodules, neurofibromas, optic pathway gliomas and cognitive impairment. The prevalence of epilepsy is estimated to be 4\u20137%. Seizures are typically easier to control with anti-seizure medications relative to other phakomatoses but in some refractory cases surgery may need to be pursued.\n\nAcquired\nEpilepsy may occur as a result of several other conditions, including tumors, strokes, head trauma, previous infections of the central nervous system, genetic abnormalities, and as a result of brain damage around the time of birth. Of those with brain tumors, almost 30% have epilepsy, making them the cause of about 4% of cases. The risk is greatest for tumors in the temporal lobe and those that grow slowly. Other mass lesions such as cerebral cavernous malformations and arteriovenous malformations have risks as high as 40\u201360%. Of those who have had a stroke, 6\u201310% develop epilepsy. Risk factors for post-stroke epilepsy include stroke severity, cortical involvement, hemorrhage and early seizures. Between 6 and 20% of epilepsy is believed to be due to head trauma. Mild brain injury increases the risk about two-fold while severe brain injury increases the risk seven-fold. In those who have experienced a high-powered gunshot wound to the head, the risk is about 50%.\nSome evidence links epilepsy and celiac disease and non-celiac gluten sensitivity, while other evidence does not. There appears to be a specific syndrome that includes coeliac disease, epilepsy, and calcifications in the brain. A 2012 review estimates that between 1% and 6% of people with epilepsy have coeliac disease while 1% of the general population has the condition.\nThe risk of epilepsy following meningitis is less than 10%; it more commonly causes seizures during the infection itself. In herpes simplex encephalitis the risk of a seizure is around 50% with a high risk of epilepsy following (up to 25%). A form of an infection with the pork tapeworm (cysticercosis), in the brain, is known as neurocysticercosis, and is the cause of up to half of epilepsy cases in areas of the world where the parasite is common. Epilepsy may also occur after other brain infections such as cerebral malaria, toxoplasmosis, and toxocariasis. Chronic alcohol use increases the risk of epilepsy: those who drink six units of alcohol per day have a 2.5-fold increase in risk. Other risks include Alzheimer's disease, multiple sclerosis, and autoimmune encephalitis. Getting vaccinated does not increase the risk of epilepsy. Malnutrition is a risk factor seen mostly in the developing world, although it is unclear however if it is a direct cause or an association. People with cerebral palsy have an increased risk of epilepsy, with half of people with spastic quadriplegia and spastic hemiplegia having the condition.\n\nMechanism\nNormally brain electrical activity is non-synchronous, as large numbers of neurons do not normally fire at the same time, but rather fire in order as signals travel throughout the brain. Neuron activity is regulated by various factors both within the cell and the cellular environment. Factors within the neuron include the type, number and distribution of ion channels, changes to receptors and changes of gene expression. Factors around the neuron include ion concentrations, synaptic plasticity and regulation of transmitter breakdown by glial cells.\n\nEpilepsy\nThe exact mechanism of epilepsy is unknown, but a little is known about its cellular and network mechanisms. However, it is unknown under which circumstances the brain shifts into the activity of a seizure with its excessive synchronization.\nIn epilepsy, the resistance of excitatory neurons to fire during this period is decreased. This may occur due to changes in ion channels or inhibitory neurons not functioning properly. This then results in a specific area from which seizures may develop, known as a \"seizure focus\". Another mechanism of epilepsy may be the up-regulation of excitatory circuits or down-regulation of inhibitory circuits following an injury to the brain. These secondary epilepsies occur through processes known as epileptogenesis. Failure of the blood\u2013brain barrier may also be a causal mechanism as it would allow substances in the blood to enter the brain.\n\nSeizures\nThere is evidence that epileptic seizures are usually not a random event. Seizures are often brought on by factors (also known as triggers) such as stress, excessive alcohol use, flickering light, or a lack of sleep, among others. The term seizure threshold is used to indicate the amount of stimulus necessary to bring about a seizure; this threshold is lowered in epilepsy.\nIn epileptic seizures a group of neurons begin firing in an abnormal, excessive, and synchronized manner. This results in a wave of depolarization known as a paroxysmal depolarizing shift. Normally, after an excitatory neuron fires it becomes more resistant to firing for a period of time. This is due in part to the effect of inhibitory neurons, electrical changes within the excitatory neuron, and the negative effects of adenosine.\nFocal seizures begin in one area of the brain while generalized seizures begin in both hemispheres. Some types of seizures may change brain structure, while others appear to have little effect. Gliosis, neuronal loss, and atrophy of specific areas of the brain are linked to epilepsy but it is unclear if epilepsy causes these changes or if these changes result in epilepsy.\nThe seizures can be described on different scales, from the cellular level to the whole brain. These are several concomitant factor, which on different scale can \"drive\" the brain to pathological states and trigger a seizure.\n\nDiagnosis\nThe diagnosis of epilepsy is typically made based on observation of the seizure onset and the underlying cause. An electroencephalogram (EEG) to look for abnormal patterns of brain waves and neuroimaging (CT scan or MRI) to look at the structure of the brain are also usually part of the initial investigations. While figuring out a specific epileptic syndrome is often attempted, it is not always possible. Video and EEG monitoring may be useful in difficult cases.\n\nDefinition\nEpilepsy is a disorder of the brain defined by any of the following conditions:\n\nFurthermore, epilepsy is considered to be resolved for individuals who had an age-dependent epilepsy syndrome but are now past that age or those who have remained seizure-free for the last 10 years, with no seizure medicines for the last 5 years.\nThis 2014 definition of the International League Against Epilepsy (ILAE) is a clarification of the ILAE 2005 conceptual definition, according to which epilepsy is \"a disorder of the brain characterized by an enduring predisposition to generate epileptic seizures and by the neurobiologic, cognitive, psychological, and social consequences of this condition. The definition of epilepsy requires the occurrence of at least one epileptic seizure.\"\nIt is, therefore, possible to outgrow epilepsy or to undergo treatment that causes epilepsy to be resolved, but with no guarantee that it will not return. In the definition, epilepsy is now called a disease, rather than a disorder. This was a decision of the executive committee of the ILAE, taken because the word disorder, while perhaps having less stigma than does disease, also does not express the degree of seriousness that epilepsy deserves.\nThe definition is practical in nature and is designed for clinical use. In particular, it aims to clarify when an \"enduring predisposition\" according to the 2005 conceptual definition is present. Researchers, statistically minded epidemiologists, and other specialized groups may choose to use the older definition or a definition of their own devising. The ILAE considers doing so is perfectly allowable, so long as it is clear what definition is being used.\nThe ILAE definition for one seizure needs an understanding of projecting an enduring predisposition to the generation of epileptic seizures. WHO, for instance, chooses to just use the traditional definition of two unprovoked seizures.\n\nClassification\nIn contrast to the classification of seizures which focuses on what happens during a seizure, the classification of epilepsies focuses on the underlying causes. When a person is admitted to hospital after an epileptic seizure the diagnostic workup results preferably in the seizure itself being classified (e.g. tonic-clonic) and in the underlying disease being identified (e.g. hippocampal sclerosis). The name of the diagnosis finally made depends on the available diagnostic results and the applied definitions and classifications (of seizures and epilepsies) and its respective terminology.\nThe International League Against Epilepsy (ILAE) provided a classification of the epilepsies and epileptic syndromes in 1989 as follows:\n\nThis classification was widely accepted but has also been criticized mainly because the underlying causes of epilepsy (which are a major determinant of clinical course and prognosis) were not covered in detail. In 2010 the ILAE Commission for Classification of the Epilepsies addressed this issue and divided epilepsies into three categories (genetic, structural\/metabolic, unknown cause) which were refined in their 2011 recommendation into four categories and a number of subcategories reflecting recent technological and scientific advances.\n\n A revised, operational classification of seizure types has been introduced by the ILAE. It allows more clearly understood terms and clearly defines focal and generalized onset dichotomy, when possible, even without observing the seizures based on description by patient or observers. The essential changes in terminology are that \"partial\" is called \"focal\" with awareness used as a classifier for focal seizures -based on description focal seizures are now defined as behavioral arrest, automatisms, cognitive, autonomic, emotional or hyperkinetic variants while atonic, myoclonic, clonic, infantile spasms, and tonic seizures may be either focal or generalized based on their onset. Several terms that were not clear or consistent in the description were removed such as dyscognitive, psychic, simple, and complex partial, while \"secondarily generalized\" is replaced by a clearer term \"focal to bilateral tonic-clonic seizure\". New seizure types now believed to be generalized are eyelid myoclonia, myoclonic atonic, myoclonic absence, and myoclonic tonic-clonic. Sometimes it is possible to classify seizures as focal or generalized based on presenting features even though onset in not known. This system is based on the 1981 seizure classification modified in 2010 and principally is the same with an effort to improve the flexibility and clarity of use to understand seizure types better in keeping with current knowledge.\n\nSyndromes\nCases of epilepsy may be organized into epilepsy syndromes by the specific features that are present. These features include the age that seizure begin, the seizure types, EEG findings, among others. Identifying an epilepsy syndrome is useful as it helps determine the underlying causes as well as what anti-seizure medication should be tried.\nThe ability to categorize a case of epilepsy into a specific syndrome occurs more often with children since the onset of seizures is commonly early. Less serious examples are benign rolandic epilepsy (2.8 per 100,000), childhood absence epilepsy (0.8 per 100,000) and juvenile myoclonic epilepsy (0.7 per 100,000). Severe syndromes with diffuse brain dysfunction caused, at least partly, by some aspect of epilepsy, are also referred to as developmental and epileptic encephalopathies. These are associated with frequent seizures that are resistant to treatment and cognitive dysfunction, for instance Lennox\u2013Gastaut syndrome (1\u20132% of all persons with epilepsy), Dravet syndrome(1: 15000-40000 worldwide), and West syndrome(1\u20139: 100000). Genetics is believed to play an important role in epilepsies by a number of mechanisms. Simple and complex modes of inheritance have been identified for some of them. However, extensive screening have failed to identify many single gene variants of large effect. More recent exome and genome sequencing studies have begun to reveal a number of de novo gene mutations that are responsible for some epileptic encephalopathies, including CHD2 and SYNGAP1 and DNM1, GABBR2, FASN and RYR3.\nSyndromes in which causes are not clearly identified are difficult to match with categories of the current classification of epilepsy. Categorization for these cases was made somewhat arbitrarily. The idiopathic (unknown cause) category of the 2011 classification includes syndromes in which the general clinical features and\/or age specificity strongly point to a presumed genetic cause. Some childhood epilepsy syndromes are included in the unknown cause category in which the cause is presumed genetic, for instance benign rolandic epilepsy. Clinical syndromes in which epilepsy is not the main feature (e.g. Angelman syndrome) were categorized symptomatic but it was argued to include these within the category idiopathic. Classification of epilepsies and particularly of epilepsy syndromes will change with advances in research.\n\nTests\nAn electroencephalogram (EEG) can assist in showing brain activity suggestive of an increased risk of seizures. It is only recommended for those who are likely to have had an epileptic seizure on the basis of symptoms. In the diagnosis of epilepsy, electroencephalography may help distinguish the type of seizure or syndrome present. In children it is typically only needed after a second seizure unless specified by a specialist. It cannot be used to rule out the diagnosis and may be falsely positive in those without the condition. In certain situations it may be useful to perform the EEG while the affected individual is sleeping or sleep deprived.\nDiagnostic imaging by CT scan and MRI is recommended after a first non-febrile seizure to detect structural problems in and around the brain. MRI is generally a better imaging test except when bleeding is suspected, for which CT is more sensitive and more easily available. If someone attends the emergency room with a seizure but returns to normal quickly, imaging tests may be done at a later point. If a person has a previous diagnosis of epilepsy with previous imaging, repeating the imaging is usually not needed even if there are subsequent seizures.\nFor adults, the testing of electrolyte, blood glucose and calcium levels is important to rule out problems with these as causes. An electrocardiogram can rule out problems with the rhythm of the heart. A lumbar puncture may be useful to diagnose a central nervous system infection but is not routinely needed. In children additional tests may be required such as urine biochemistry and blood testing looking for metabolic disorders. Together with EEG and neuroimaging, genetic testing is becoming one of the most important diagnostic technique for epilepsy, as a diagnosis might be achieved in a relevant proportion of cases with severe epilepsies, both in children and adults. For those with negative genetic testing, in some it might be important to repeat or re-analyze previous genetic studies after 2\u20133 years.\nA high blood prolactin level within the first 20 minutes following a seizure may be useful to help confirm an epileptic seizure as opposed to psychogenic non-epileptic seizure. Serum prolactin level is less useful for detecting focal seizures. If it is normal an epileptic seizure is still possible and a serum prolactin does not separate epileptic seizures from syncope. It is not recommended as a routine part of the diagnosis of epilepsy.\n\nDifferential diagnosis\nDiagnosis of epilepsy can be difficult. A number of other conditions may present very similar signs and symptoms to seizures, including syncope, hyperventilation, migraines, narcolepsy, panic attacks and psychogenic non-epileptic seizures (PNES). In particular, syncope can be accompanied by a short episode of convulsions. Nocturnal frontal lobe epilepsy, often misdiagnosed as nightmares, was considered to be a parasomnia but later identified to be an epilepsy syndrome. Attacks of the movement disorder paroxysmal dyskinesia may be taken for epileptic seizures. The cause of a drop attack can be, among many others, an atonic seizure.\nChildren may have behaviors that are easily mistaken for epileptic seizures but are not. These include breath-holding spells, bedwetting, night terrors, tics and shudder attacks. Gastroesophageal reflux may cause arching of the back and twisting of the head to the side in infants, which may be mistaken for tonic-clonic seizures.\nMisdiagnosis is frequent (occurring in about 5 to 30% of cases). Different studies showed that in many cases seizure-like attacks in apparent treatment-resistant epilepsy have a cardiovascular cause. Approximately 20% of the people seen at epilepsy clinics have PNES and of those who have PNES about 10% also have epilepsy; separating the two based on the seizure episode alone without further testing is often difficult.\n\nPrevention\nWhile many cases are not preventable, efforts to reduce head injuries, provide good care around the time of birth, and reduce environmental parasites such as the pork tapeworm may be effective. Efforts in one part of Central America to decrease rates of pork tapeworm resulted in a 50% decrease in new cases of epilepsy. Yoga-based Nadi Shodhana Pranayama, also known as Alternate Nostril Breathing, may positively impact the nervous system and help manage seizure disorders. Regular exercise helps balance brain function by providing the body with oxygen and removing carbon dioxide and toxins from the blood.\n\nComplications\nEpilepsy can be dangerous when seizure occurs at certain times. The risk of drowning or being involved in a motor vehicle collision is higher. It is also found that people with epilepsy are more likely to have psychological problems. Other complications include aspiration pneumonia and difficulty learning.\n\nManagement\nEpilepsy is usually treated with daily medication once a second seizure has occurred, while medication may be started after the first seizure in those at high risk for subsequent seizures. Supporting people's self-management of their condition may be useful. In drug-resistant cases different management options may be considered, including special diets, the implantation of a neurostimulator, or neurosurgery.\n\nFirst aid\nRolling people with an active tonic-clonic seizure onto their side and into the recovery position helps prevent fluids from getting into the lungs. Putting fingers, a bite block or tongue depressor in the mouth is not recommended as it might make the person vomit or result in the rescuer being bitten. Efforts should be taken to prevent further self-injury. Spinal precautions are generally not needed.\nIf a seizure lasts longer than 5 minutes or if there are more than two seizures in 5 minutes without a return to a normal level of consciousness between them, it is considered a medical emergency known as status epilepticus. This may require medical help to keep the airway open and protected; a nasopharyngeal airway may be useful for this. At home the recommended initial medication for seizure of a long duration is midazolam placed in the nose or mouth. Diazepam may also be used rectally. In hospital, intravenous lorazepam is preferred.\nIf two doses of benzodiazepines are not effective, other medications such as phenytoin are recommended. Convulsive status epilepticus that does not respond to initial treatment typically requires admission to the intensive care unit and treatment with stronger agents such as midazolam infusion, ketamine, thiopentone or propofol. Most institutions have a preferred pathway or protocol to be used in a seizure emergency like status epilepticus. These protocols have been found to be effective in reducing time to delivery of treatment.\n\nMedications\nThe mainstay treatment of epilepsy is anticonvulsant medications, possibly for the person's entire life. The choice of anticonvulsant is based on seizure type, epilepsy syndrome, other medications used, other health problems, and the person's age and lifestyle. A single medication is recommended initially; if this is not effective, switching to a single other medication is recommended. Two medications at once is recommended only if a single medication does not work. In about half, the first agent is effective; a second single agent helps in about 13% and a third or two agents at the same time may help an additional 4%. About 30% of people continue to have seizures despite anticonvulsant treatment.\nThere are a number of medications available including phenytoin, carbamazepine and valproate. Evidence suggests that phenytoin, carbamazepine, and valproate may be equally effective in both focal and generalized seizures. Controlled release carbamazepine appears to work as well as immediate release carbamazepine, and may have fewer side effects.In the United Kingdom, carbamazepine or lamotrigine are recommended as first-line treatment for focal seizures, with levetiracetam and valproate as second-line due to issues of cost and side effects. Valproate is recommended first-line for generalized seizures with lamotrigine being second-line. In those with absence seizures, ethosuximide or valproate are recommended; valproate is particularly effective in myoclonic seizures and tonic or atonic seizures. If seizures are well-controlled on a particular treatment, it is not usually necessary to routinely check the medication levels in the blood.\nThe least expensive anticonvulsant is phenobarbital at around US$5 a year. The World Health Organization gives it a first-line recommendation in the developing world and it is commonly used there. Access, however, may be difficult as some countries label it as a controlled drug.\nAdverse effects from medications are reported in 10% to 90% of people, depending on how and from whom the data is collected. Most adverse effects are dose-related and mild. Some examples include mood changes, sleepiness, or an unsteadiness in gait. Certain medications have side effects that are not related to dose such as rashes, liver toxicity, or suppression of the bone marrow. Up to a quarter of people stop treatment due to adverse effects. Some medications are associated with birth defects when used in pregnancy. Many of the common used medications, such as valproate, phenytoin, carbamazepine, phenobarbital, and gabapentin have been reported to cause increased risk of birth defects, especially when used during the first trimester. Despite this, treatment is often continued once effective, because the risk of untreated epilepsy is believed to be greater than the risk of the medications. Among the antiepileptic medications, levetiracetam and lamotrigine seem to carry the lowest risk of causing birth defects.\nSlowly stopping medications may be reasonable in some people who do not have a seizure for two to four years; however, around a third of people have a recurrence, most often during the first six months. Stopping is possible in about 70% of children and 60% of adults. Measuring medication levels is not generally needed in those whose seizures are well controlled.\n\nSurgery\nEpilepsy surgery should be considered for any person with epilepsy who is medically refractory. Patients are evaluated on a case-by-case basis in centres that are familiar with and have expertise in epilepsy surgery. Results from a 2023 systematic review found that surgical interventions for children aged 1\u201336 months with drug-resistant epilepsy can lead to significant seizure reduction or freedom, especially when other treatments have failed. Epilepsy surgery may be an option for people with focal seizures that remain a problem despite other treatments. These other treatments include at least a trial of two or three medications. The goal of surgery has been total control of seizures. However, most physicians believe that even palliative surgery where the burden of seizures is reduced significantly can help in achieving developmental progress or reversal of developmental stagnation in children with drug-resistant epilepsy and this may be achieved in 60\u201370% of cases. Common procedures include cutting out the hippocampus via an anterior temporal lobe resection, removal of tumors, and removing parts of the neocortex. Some procedures such as a corpus callosotomy are attempted in an effort to decrease the number of seizures rather than cure the condition. Following surgery, medications may be slowly withdrawn in many cases.\n\nNeurostimulation\nNeurostimulation via neuro-cybernetic prosthesis implantation may be another option in those who are not candidates for surgery, providing chronic, pulsatile electrical stimulation of specific nerve or brain regions, alongside standard care. Three types have been used in those who do not respond to medications: vagus nerve stimulation (VNS), anterior thalamic stimulation, and closed-loop responsive stimulation (RNS).\n\nVagus nerve stimulation\nNon-pharmacological modulation of neurotransmitters via high-level VNS (h-VNS) may reduce seizure frequency in children and adults who do not respond to medical and\/or surgical therapy, when compared with low-level VNS (l-VNS). In a 2022 Cochrane review of four randomized controlled trials, with moderate certainty of evidence, people receiving h-VNS treatment were 73% more likely (13% more likely to 164% more likely) to experience a reduction in seizure frequency by at least 50% (the minimum threshold defined for individual clinical response). Potentially 249 (163 to 380) per 1000 people with drug-resistant epilepsy may achieve a 50% reduction in seizures following h-VNS, benefiting an additional 105 per 1000 people compared with l-VNS.\nThis outcome was limited by the number of studies available, and the quality of one trial in particular, wherein three people received l-VNS in error. A sensitivity analysis suggested that the best case scenario was that the likelihood of clinical response to h-VNS may be 91% (27% to 189%) higher than those receiving l-VNS. In the worst-case scenario, the likelihood of clinical response to h-VNS was still 61% higher (7% higher to 143% higher) than l-VNS.\nDespite the potential benefit for h-VNS treatment, the Cochrane review also found that the risk of several adverse-effects was greater than those receiving l-VNS. There was moderate certainty of evidence that voice alteration or hoarseness risk may be 2.17(1.49 to 3.17) fold higher than people receiving l-VNS. Dyspnoea risk was also 2.45 (1.07 to 5.60) times that of l-VNS recipients, although the low number of events and studies meant that the certainty of evidence was low. The risk of rebound-withdrawal symptoms, coughing, pain and paraesthesia was unclear.\n\nDiet\nThere is promising evidence that a ketogenic diet (high-fat, low-carbohydrate, adequate-protein) decreases the number of seizures and eliminates seizures in some; however, further research is necessary. A 2022 systematic review of the literature has found some evidence to support that a ketogenic diet or modified Atkins diet can be helpful in the treatment of epilepsy in some infants. These types of diets may be beneficial for children with drug-resistant epilepsy; the use for adults remains uncertain. The most commonly reported adverse effects were vomiting, constipation and diarrhoea. It is unclear why this diet works. In people with coeliac disease or non-celiac gluten sensitivity and occipital calcifications, a gluten-free diet may decrease the frequency of seizures.\n\nOther\nAvoidance therapy consists of minimizing or eliminating triggers. For example, those who are sensitive to light may have success with using a small television, avoiding video games, or wearing dark glasses. Operant-based biofeedback based on the EEG waves has some support in those who do not respond to medications. Psychological methods should not, however, be used to replace medications.\nExercise has been proposed as possibly useful for preventing seizures, with some data to support this claim. Some dogs, commonly referred to as seizure dogs, may help during or after a seizure. It is not clear if dogs have the ability to predict seizures before they occur.\nThere is moderate-quality evidence supporting the use of psychological interventions along with other treatments in epilepsy. This can improve quality of life, enhance emotional wellbeing, and reduce fatigue in adults and adolescents. Psychological interventions may also improve seizure control for some individuals by promoting self-management and adherence.\nAs an add-on therapy in those who are not well controlled with other medications, cannabidiol appears to be useful in some children. In 2018 the FDA approved this product for Lennox\u2013Gastaut syndrome and Dravet syndrome.\nThere are a few studies on the use of dexamethasone for the successful treatment of drug-resistant seizures in both adults and children.\n\nAlternative medicine\nAlternative medicine, including acupuncture, routine vitamins, and yoga, have no reliable evidence to support their use in epilepsy. Melatonin, as of 2016, is insufficiently supported by evidence. The trials were of poor methodological quality and it was not possible to draw any definitive conclusions.\nSeveral supplements (with varied reliabilities of evidence) have been reported to be helpful for drug-resistant epilepsy. These include high-dose Omega-3, berberine, Manuka honey, reishi and lion's mane mushrooms, curcumin, vitamin E, coenzyme Q-10, and resveratrol. The reason these can work (in theory) is that they reduce inflammation or oxidative stress, two of the major mechanism contributing to epilepsy.\n\nContraception and pregnancy\nWomen of child-bearing age, including those with epilepsy, are at risk of unintended pregnancies if they are not using an effective form of contraception. Women with epilepsy may experience a temporary increase in seizure frequency when they begin hormonal contraception.\nSome anti-seizure medications interact with enzymes in the liver and cause the drugs in hormonal contraception to be broken down more quickly. These enzyme inducing drugs make hormonal contraception less effective, and this is particularly hazardous if the anti-seizure medication is associated with birth defects. Potent enzyme-inducing anti-seizure medications include carbamazepine, eslicarbazepine acetate, oxcarbazepine, phenobarbital, phenytoin, primidone, and rufinamide. The drugs perampanel and topiramate can be enzyme-inducing at higher doses. Conversely, hormonal contraception can lower the amount of the anti-seizure medication lamotrigine circulating in the body, making it less effective. The failure rate of oral contraceptives, when used correctly, is 1%, but this increases to between 3\u20136% in women with epilepsy. Overall, intrauterine devices (IUDs) are preferred for women with epilepsy who are not intending to become pregnant.\nWomen with epilepsy, especially if they have other medical conditions, may have a slightly lower, but still high, chance of becoming pregnant. Women with infertility have about the same chance of success with in vitro fertilisation or other forms of assisted reproductive technology as women without epilepsy. There may be a higher risk of pregnancy loss.\nOnce pregnant, there are two main concerns related to pregnancy. The first concern is about the risk of seizures during pregnancy, and the second concern is that the anti-seizure medications may result in birth defects. Most women with epilepsy must continue treatment with anti-seizure drugs, and the treatment goal is to balance the need to prevent seizures with the need to prevent drug-induced birth defects.\nPregnancy does not seem to change seizure frequency very much. When seizures happen, however, they can cause some pregnancy complications, such as pre-term births or the babies being smaller than usual when they are born.  \nAll pregnancies have a risk of birth defects, e.g., due to smoking during pregnancy. In addition to this typical level of risk, some anti-seizure drugs significantly increase the risk of birth defects and intrauterine growth restriction, as well as developmental, neurocognitive, and behavioral disorders. Most women with epilepsy receive safe and effective treatment and have typical, healthy children. The highest risks are associated with specific anti-seizure drugs, such as valproic acid and carbamazepine, and with higher doses. Folic acid supplementation, such as through prenatal vitamins, reduced the risk. Planning pregnancies in advance gives women with epilepsy an opportunity to switch to a lower-risk treatment program and reduced drug doses. \nAlthough anti-seizure drugs can be found in breast milk, women with epilepsy can breastfeed their babies, and the benefits usually outweigh the risks.\n\nPrognosis\nEpilepsy cannot usually be cured, but medication can control seizures effectively in about 70% of cases. Of those with generalized seizures, more than 80% can be well controlled with medications while this is true in only 50% of people with focal seizures. One predictor of long-term outcome is the number of seizures that occur in the first six months. Other factors increasing the risk of a poor outcome include little response to the initial treatment, generalized seizures, a family history of epilepsy, psychiatric problems, and waves on the EEG representing generalized epileptiform activity. In the developing world, 75% of people are either untreated or not appropriately treated. In Africa, 90% do not get treatment. This is partly related to appropriate medications not being available or being too expensive.\n\nMortality\nPeople with epilepsy may have a higher risk of premature death compared to those without the condition. This risk is estimated to be between 1.6 and 4.1 times greater than that of the general population. The greatest increase in mortality from epilepsy is among the elderly. Those with epilepsy due to an unknown cause have a relatively low increase in risk.\nMortality is often related to the underlying cause of the seizures, status epilepticus, suicide, trauma, and sudden unexpected death in epilepsy (SUDEP). Death from status epilepticus is primarily due to an underlying problem rather than missing doses of medications. The risk of suicide is between two and six times higher in those with epilepsy; the cause of this is unclear. SUDEP appears to be partly related to the frequency of generalized tonic-clonic seizures and accounts for about 15% of epilepsy-related deaths; it is unclear how to decrease its risk.\nRisk factors for SUDEP include nocturnal generalized tonic-clonic seizures, seizures, sleeping alone and medically intractable epilepsy.\nIn the United Kingdom, it is estimated that 40\u201360% of deaths are possibly preventable. In the developing world, many deaths are due to untreated epilepsy leading to falls or status epilepticus.\n\nEpidemiology\nEpilepsy is one of the most common serious neurological disorders affecting about 50 million people as of 2021. It affects 1% of the population by age 20 and 3% of the population by age 75. It is more common in males than females with the overall difference being small. Most of those with the disorder (80%) are in low income populations or the developing world.\nThe estimated prevalence of active epilepsy (as of 2012) is in the range 3\u201310 per 1,000, with active epilepsy defined as someone with epilepsy who has had at least one unprovoked seizure in the last five years. Epilepsy begins each year in 40\u201370 per 100,000 in developed countries and 80\u2013140 per 100,000 in developing countries. Poverty is a risk and includes both being from a poor country and being poor relative to others within one's country. In the developed world epilepsy most commonly starts either in the young or in the old. In the developing world its onset is more common in older children and young adults due to the higher rates of trauma and infectious diseases. In developed countries the number of cases a year has decreased in children and increased among the elderly between the 1970s and 2003. This has been attributed partly to better survival following strokes in the elderly.\n\nHistory\nThe oldest medical records show that epilepsy has been affecting people at least since the beginning of recorded history. Throughout ancient history, the condition was thought to be of a spiritual cause. The world's oldest description of an epileptic seizure comes from a text in Akkadian (a language used in ancient Mesopotamia) and was written around 2000 BC. The person described in the text was diagnosed as being under the influence of a moon god, and underwent an exorcism. Epileptic seizures are listed in the Code of Hammurabi (c.\u20091790 BC) as reason for which a purchased slave may be returned for a refund, and the Edwin Smith Papyrus (c.\u20091700 BC) describes cases of individuals with epileptic convulsions.\nThe oldest known detailed record of the condition itself is in the Sakikku, a Babylonian cuneiform medical text from 1067\u20131046 BC. This text gives signs and symptoms, details treatment and likely outcomes, and describes many features of the different seizure types. As the Babylonians had no biomedical understanding of the nature of epilepsy, they attributed the seizures to possession by evil spirits and called for treating the condition through spiritual means. Around 900 BC, Punarvasu Atreya described epilepsy as loss of consciousness; this definition was carried forward into the Ayurvedic text of Charaka Samhita (c.\u2009400 BC).\nThe ancient Greeks had contradictory views of the condition. They thought of epilepsy as a form of spiritual possession, but also associated the condition with genius and the divine. One of the names they gave to it was the sacred disease (Ancient Greek: \u1f20 \u1f31\u03b5\u03c1\u1f70 \u03bd\u03cc\u03c3\u03bf\u03c2). Epilepsy appears within Greek mythology: it is associated with the Moon goddesses Selene and Artemis, who afflicted those who upset them. The Greeks thought that important figures such as Julius Caesar and Hercules had the condition. The notable exception to this divine and spiritual view was that of the school of Hippocrates. In the fifth century BC, Hippocrates rejected the idea that the condition was caused by spirits. In his landmark work On the Sacred Disease, he proposed that epilepsy was not divine in origin and instead was a medically treatable problem originating in the brain. He accused those of attributing a sacred cause to the condition of spreading ignorance through a belief in superstitious magic. Hippocrates proposed that heredity was important as a cause, described worse outcomes if the condition presents at an early age, and made note of the physical characteristics as well as the social shame associated with it. Instead of referring to it as the sacred disease, he used the term great disease, giving rise to the modern term grand mal, used for tonic\u2013clonic seizures. Despite his work detailing the physical origins of the condition, his view was not accepted at the time. Evil spirits continued to be blamed until at least the 17th century.\nIn Ancient Rome people did not eat or drink with the same pottery as that used by someone who was affected. People of the time would spit on their chest believing that this would keep the problem from affecting them. According to Apuleius and other ancient physicians, to detect epilepsy, it was common to light a piece of gagates, whose smoke would trigger the seizure. Occasionally a spinning potter's wheel was used, perhaps a reference to photosensitive epilepsy.\nIn most cultures, persons with epilepsy have been stigmatized, shunned, or even imprisoned. As late as in the second half of the 20th century, in Tanzania and other parts of Africa epilepsy was associated with possession by evil spirits, witchcraft, or poisoning and was believed by many to be contagious. In the Salp\u00eatri\u00e8re, the birthplace of modern neurology, Jean-Martin Charcot found people with epilepsy side by side with the mentally ill, those with chronic syphilis, and the criminally insane. In Ancient Rome, epilepsy was known as the morbus comitialis or 'disease of the assembly hall' and was seen as a curse from the gods. In northern Italy, epilepsy was traditionally known as Saint Valentine's malady. In at least the 1840s in the United States of America, epilepsy was known as the falling sickness or the falling fits, and was considered a form of medical insanity. Around the same time period, epilepsy was known in France as the haut-mal lit.\u2009'high evil', mal-de terre lit.\u2009'earthen sickness', mal de Saint Jean lit.\u2009'Saint John's sickness', mal des enfans lit.\u2009'child sickness', and mal-caduc lit.\u2009'falling sickness'. Patients of epilepsy in France were also known as tombeurs lit.\u2009'people who fall', due to the seizures and loss of consciousness in an epileptic episode.\nIn the mid-19th century, the first effective anti-seizure medication, bromide, was introduced. The first modern treatment, phenobarbital, was developed in 1912, with phenytoin coming into use in 1938.\n\nSociety and culture\nStigma\nSocial stigma is commonly experienced, around the world, by those with epilepsy. It can affect people economically, socially and culturally. In India and Europe, epilepsy may be used as justification to deny marriage. People in some areas still believe those with epilepsy to be cursed. In parts of Africa, such as Tanzania and Uganda, epilepsy is claimed to be associated with possession by evil spirits, witchcraft, or poisoning and is incorrectly believed by many to be contagious. Before 1971 in the United Kingdom, epilepsy was considered grounds for the annulment of marriage. The stigma may result in some people with epilepsy denying that they have ever had seizures.\n\nEconomics\nSeizures result in direct economic costs of about one billion dollars in the United States. Epilepsy resulted in economic costs in Europe of around 15.5 billion euros in 2004. In India epilepsy is estimated to result in costs of US$1.7 billion or 0.5% of the GDP. It is the cause of about 1% of emergency department visits (2% for emergency departments for children) in the United States.\n\nVehicles\nThose with epilepsy are at about twice the risk of being involved in a motor vehicular collision and thus in many areas of the world are not allowed to drive or only able to drive if certain conditions are met. Diagnostic delay has been suggested to be a cause of some potentially avoidable motor vehicle collisions since at least one study showed that most motor vehicle accidents occurred in those with undiagnosed non-motor seizures as opposed to those with motor seizures at epilepsy onset. In some places physicians are required by law to report if a person has had a seizure to the licensing body while in others the requirement is only that they encourage the person in question to report it himself. Countries that require physician reporting include Sweden, Austria, Denmark and Spain. Countries that require the individual to report include the UK and New Zealand, and physicians may report if they believe the individual has not already. In Canada, the United States and Australia the requirements around reporting vary by province or state. If seizures are well controlled most feel allowing driving is reasonable. The amount of time a person must be free from seizures before he can drive varies by country. Many countries require one to three years without seizures. In the United States the time needed without a seizure is determined by each state and is between three months and one year.\nThose with epilepsy or seizures are typically denied a pilot license.\n\nIn Canada if an individual has had no more than one seizure, they may be considered after five years for a limited license if all other testing is normal. Those with febrile seizures and drug related seizures may also be considered.\nIn the United States, the Federal Aviation Administration does not allow those with epilepsy to get a commercial pilot license. Rarely, exceptions can be made for persons who have had an isolated seizure or febrile seizures and have remained free of seizures into adulthood without medication.\nIn the United Kingdom, a full national private pilot license requires the same standards as a professional driver's license. This requires a period of ten years without seizures while off medications. Those who do not meet this requirement may acquire a restricted license if free from seizures for five years.\n\nSupport organizations\nThere are organizations that provide support for people and families affected by epilepsy. The Out of the Shadows campaign, a joint effort by the World Health Organization, the ILAE and the International Bureau for Epilepsy, provides help internationally. In the United States, the Epilepsy Foundation is a national organization that works to increase the acceptance of those with the disorder, their ability to function in society and to promote research for a cure. The Epilepsy Foundation, some hospitals, and some individuals also run support groups in the United States. In Australia, the Epilepsy Foundation provides support, delivers education and training and funds research for people living with epilepsy.\nInternational Epilepsy Day (World Epilepsy Day) began in 2015 and occurs on the second Monday in February.\nPurple Day, a different world-wide epilepsy awareness day for epilepsy, was initiated by a nine-year-old Canadian named Cassidy Megan in 2008, and is every year on 26 March.\n\nResearch\nSeizure prediction and modeling\nSeizure prediction refers to attempts to forecast epileptic seizures based on the EEG before they occur. As of 2011, no effective mechanism to predict seizures has been developed. Although no effective device that can predict seizures is available, the science behind seizure prediction and ability to deliver such a tool has made progress.\nKindling, where repeated exposures to events that could cause seizures eventually causes seizures more easily, has been used to create animal models of epilepsy.\nDifferent animal models of epilepsy have been characterized in rodents that recapitulate the EEG and behavioral concomitants of different forms of epilepsy, in particular the occurrence of recurrent spontaneous seizures. Because epileptic seizures of different kinds are observed naturally in some of these animals, strains of mice and rats have been selected to be used as genetic models of epilepsy. In particular, several lines of mice and rats display spike-and-wave discharges when EEG recorded and have been studied to understand absence epilepsy. Among these models, the strain of GAERS (Genetic Absence Epilepsy Rats from Strasbourg) was characterized in the 1980s and has helped to understand the mechanisms underlying childhood absence epilepsy.\nRat brain slices serve as a valuable model for assessing the potential of compounds in reducing epileptiform activity. By evaluating the frequency of epileptiform bursting in hippocampal networks, researchers can identify promising candidates for novel anti-seizure drugs.\nOne of the hypotheses present in the literature is based on inflammatory pathways. Studies supporting this mechanism revealed that inflammatory, glycolipid, and oxidative factors are higher in epilepsy patients, especially those with generalized epilepsy.\n\nPotential future therapies\nGene therapy is being studied in some types of epilepsy. Medications that alter immune function, such as intravenous immunoglobulins, may reduce the frequency of seizures when including in normal care as an add-on therapy; however, further research is required to determine whether these medications are very well tolerated in children and in adults with epilepsy. Noninvasive stereotactic radiosurgery is, as of 2012, being compared to standard surgery for certain types of epilepsy.\n\nOther animals\nEpilepsy occurs in a number of other animals including dogs and cats; it is in fact the most common brain disorder in dogs. It is typically treated with anticonvulsants such as levetiracetam, phenobarbital, or bromide in dogs and phenobarbital in cats. Imepitoin is also used in dogs. While generalized seizures in horses are fairly easy to diagnose, it may be more difficult in non-generalized seizures and EEGs may be useful.\n\nReferences\nAs of this edit, this article uses content from \"Epilepsy\", authored by https:\/\/en.wikipedia.org\/w\/index.php?title=Epilepsy&action=history, which is licensed in a way that permits reuse under the Creative Commons Attribution-ShareAlike 4.0 International License, but not under the GFDL. All relevant terms must be followed.\n\nFurther reading\nWilson JV, Reynolds EH (April 1990). \"Texts and documents. Translation and analysis of a cuneiform text forming part of a Babylonian treatise on epilepsy\". Medical History. 34 (2): 185\u2013198. doi:10.1017\/s0025727300050651. ISSN 0025-7273. PMC 1036070. PMID 2187129.\n\nExternal links\n\n\"Epilepsy Basics: An Overview for Behavioral Health Providers\". YouTube. Epilepsy Foundation. 30 May 2019. Archived from the original on 11 December 2021.\n\"What To Do If Someone Has A Seizure \u2013 First Aid Training \u2013 St John Ambulance\". YouTube. St John Ambulance. 1 February 2017. Archived from the original on 11 December 2021.\nEpilepsy at Curlie\nWorld Health Organization fact sheet","72":"The Epley maneuver or  repositioning maneuver is a maneuver used by medical professionals to treat one common cause of vertigo, benign paroxysmal positional vertigo (BPPV) of the posterior or anterior canals of the ear. The maneuver works by allowing free-floating particles, displaced otoconia, from the affected semicircular canal to be relocated by using gravity, back into the utricle, where they can no longer stimulate the cupula, therefore relieving the patient of bothersome vertigo. The maneuver was developed by the physician John M. Epley, and was first described in 1980.\nA version of the maneuver called the \"modified\" Epley does not include vibrations of the mastoid process originally indicated by Epley, as the vibration procedures have been proven ineffective. The modified procedure is now the one generally accepted as the Epley maneuver.\n\nEffectiveness\nAn Epley maneuver is a safe and effective treatment for BPPV, although the condition recurs in approximately one third of cases.\n\nSequence of positions\nThe following sequence of positions describes the Epley maneuver:\n\nThe patient begins in an upright sitting posture, with the legs fully extended and the head rotated 45 degrees toward the side in the same direction that gives a positive Dix\u2013Hallpike test.\nThen the patient is quickly lowered into a supine position (on the back), with the head held approximately in a 30-degree neck extension (Dix-Hallpike position), with the head remaining rotated to the side.\nThe clinician observes the patient's eyes for \u201cprimary stage\u201d nystagmus.\nThe patient remains in this position for approximately 1\u20132 minutes.\nThen the patient's head is rotated 90 degrees in the opposite direction, so that the opposite ear faces the floor, while maintaining 30 degrees of neck extension.\nThe patient remains in this position for approximately 1\u20132 minutes.\nKeeping the head and neck in a fixed position relative to the body, the patient rolls onto the shoulder, rotating the head another 90 degrees in the direction being faced. Now the patient is looking downward at a 45-degree angle.\nThe eyes should be observed immediately by the clinician for \u201csecondary stage\u201d nystagmus (this secondary stage nystagmus should be in the same direction as the primary stage nystagmus).\nThe patient remains in this position for approximately 1\u20132 minutes.\nFinally, the patient is slowly brought up to an upright sitting posture, while maintaining the 45-degree rotation of the head.\nThe patient holds a sitting position for up to 30 seconds.\nThese steps may be repeated twice, for a total of three times during a procedure. During every step of this procedure, the patient may experience some dizziness.\n\nPost-treatment phase\nFollowing the treatment, the clinician may provide the patient with a soft collar, often worn for the remainder of the day, as a cue to avoid any head positions that may once again displace the otoconia. The patient may be instructed to be cautious of bending over, lying backward, moving the head up and down, or tilting the head to either side. For the next two nights, patients should sleep in a semi-recumbent position. This means sleeping with the head halfway between being flat and being upright (at a 45-degree angle). This is most easily done by using a recliner chair or by using pillows arranged on a couch. The soft collar is removed occasionally. When doing so, the patient should be encouraged to perform horizontal movements of the head to maintain normal neck range of motion. It is important to instruct the patient that horizontal movement of the head should be performed to prevent stiff neck muscles.\nIt remains uncertain whether activity restrictions following the treatment improve the effectiveness of the canalith repositioning maneuver. However, study patients who were not provided with any activity restrictions, needed one or two additional treatment sessions to attain a successful outcome. The Epley maneuver appears to be a long-term, effective, and conservative treatment for BPPV that has a limited number of complications (nausea, vomiting, and residual vertigo) and is well tolerated by patients.\n\nBackground information\nThe goal of an Epley maneuver is to restore the equilibrium of the vestibular system, more specifically, to the semicircular canals, in order to treat the symptoms associated with BPPV. There is compelling evidence that free-floating otoconia, probably displaced from the otolithic membrane in the utricle are the main cause of this disequilibrium. Recent pathological findings also suggest that the displaced otoconia typically settle in the posterior semicircular canal in the cupula of the ampulla and render it sensitive to gravity. The cupula move in relation to acceleration of the head during rotary movements and signal to the brain via action potentials about which way the head is moving in relation to its surroundings. However, once a crystal becomes lodged in the cupula, it only takes slight head movements in combination with gravity, to create an action potential, which signals to the brain that the head is moving through space, when in reality, it is not, thus creating the experience of vertigo associated with BPPV.\nWhen a therapist is performing an Epley maneuver, the patient's head is rotated to 45 degrees in the direction of the affected side, in order to target the posterior semicircular canal of the affected side. When the patient is passively positioned from an upright seated posture down to a supine (lying on the back) position, this momentum helps to dislodge the otoconia (crystal) embedded in the cupula. Steps 3\u201310 in the above-mentioned procedure are intended to cause the newly dislodged crystal to be brought back to the utricle through the posterior semicircular canal so that it can be re-absorbed by the utricle.\nIn 1957, John M. Epley received his M.D. degree from the University of Oregon Medical School (now Oregon Health Sciences University). While a resident at Stanford Medical School he conducted original research on the first multichannel cochlear implant. He developed his BPPV technique in 1979. He died July 31, 2019.\n\nSee also\nBalance disorder\nDizzyFIX\n\nReferences\nExternal links\n\nVideo demonstration \u2013 \"Epley maneuver\" (no sound) (0:40 Flash Video)\nVideo demonstration \u2013 \"Epley's maneuver\" (with narration) (0:45 Flash Video)\nDemo of 2 min per position version \"Epley's maneuver\"\nClear read BVVP page first for terminology \"Modified Epley maneuver\"","73":"The sense of balance or equilibrioception is the perception of balance and spatial orientation. It helps prevent humans and nonhuman animals from falling over when standing or moving. Equilibrioception is the result of a number of sensory systems working together; the eyes (visual system), the inner ears (vestibular system), and the body's sense of where it is in space (proprioception) ideally need to be intact.\nThe vestibular system, the region of the inner ear where three semicircular canals converge, works with the visual system to keep objects in focus when the head is moving. This is called the vestibulo-ocular reflex (VOR). The balance system works with the visual and skeletal systems (the muscles and joints and their sensors) to maintain orientation or balance. Visual signals sent to the brain about the body's position in relation to its surroundings are processed by the brain and compared to information from the vestibular and skeletal systems.\n\nVestibular system\nIn the vestibular system, equilibrioception is determined by the level of a fluid called endolymph in the labyrinth, a complex set of tubing in the inner ear.\n\nDysfunction\nWhen the sense of balance is interrupted it causes dizziness, disorientation and nausea. Balance can be upset by M\u00e9ni\u00e8re's disease, superior canal dehiscence syndrome, an inner ear infection, by a bad common cold affecting the head or a number of other medical conditions including but not limited to vertigo. It can also be temporarily disturbed by quick or prolonged acceleration, for example, riding on a merry-go-round. Blows can also affect equilibrioreception, especially those to the side of the head or directly to the ear.\nMost astronauts find that their sense of balance is impaired when in orbit because they are in a constant state of weightlessness. This causes a form of motion sickness called space adaptation syndrome.\n\nSystem overview\nThis overview also explains acceleration as its processes are interconnected with balance.\n\nMechanical\nThere are five sensory organs innervated by the vestibular nerve; three semicircular canals (Horizontal SCC, Superior SCC, Posterior SCC) and two otolith organs (saccule and utricle). Each semicircular canal (SSC) is a thin tube that doubles in thickness briefly at a point called osseous ampullae. At their center-base, each contains an ampullary cupula. The cupula is a gelatin bulb connected to the stereocilia of hair cells, affected by the relative movement of the endolymph it is bathed in.\nSince the cupula is part of the bony labyrinth, it rotates along with actual head movement, and by itself without the endolymph, it cannot be stimulated and therefore, could not detect movement. Endolymph follows the rotation of the canal; however, due to inertia its movement initially lags behind that of the bony labyrinth. The delayed movement of the endolymph bends and activates the cupula. When the cupula bends, the connected stereocilia bend along with it, activating chemical reactions in the hair cells surrounding crista ampullaris and eventually create action potentials carried by the vestibular nerve signaling to the body that it has moved in space.\nAfter any extended rotation, the endolymph catches up to the canal and the cupula returns to its upright position and resets. When extended rotation ceases, however, endolymph continues, (due to inertia) which bends and activates the cupula once again to signal a change in movement.\nPilots doing long banked turns begin to feel upright (no longer turning) as endolymph matches canal rotation; once the pilot exits the turn the cupula is once again stimulated, causing the feeling of turning the other way, rather than flying straight and level.\nThe horizontal SCC handles head rotations about a vertical axis (e.g. looking side to side), the superior SCC handles head movement about a lateral axis (e.g. head to shoulder), and the posterior SCC handles head rotation about a rostral-caudal axis (e.g. nodding). SCC sends adaptive signals, unlike the two otolith organs, the saccule and utricle, whose signals do not adapt over time.\nA shift in the otolithic membrane that stimulates the cilia is considered the state of the body until the cilia are once again stimulated. For example, lying down stimulates cilia and standing up stimulates cilia, however, for the time spent lying the signal that you are lying remains active, even though the membrane resets.\nOtolithic organs have a thick, heavy gelatin membrane that, due to inertia (like endolymph), lags behind and continues ahead past the macula it overlays, bending and activating the contained cilia.\nUtricle responds to linear accelerations and head-tilts in the horizontal plane (head to shoulder), whereas saccule responds to linear accelerations and head-tilts in the vertical plane (up and down). Otolithic organs update the brain on the head-location when not moving; SCC update during movement.\nKinocilium are the longest stereocilia and are positioned (one per 40-70 regular cilia) at the end of the bundle. If stereocilia go towards kinocilium, depolarization occurs, causing more neurotransmitters, and more vestibular nerve firings, as compared to when stereocilia tilt away from kinocilium (hyperpolarization, less neurotransmitter, less firing).\n\nNeural\nFirst order vestibular nuclei (VN) project to lateral vestibular nucleus (IVN), medial vestibular nucleus (MVN), and superior vestibular nucleus (SVN).\nThe inferior cerebellar peduncle is the largest center through which balance information passes. It is the area of integration between proprioceptive, and vestibular inputs, to aid in unconscious maintenance of balance and posture.\nThe inferior olivary nucleus aids in complex motor tasks by encoding coordinating timing sensory information; this is decoded and acted upon in the cerebellum.\nThe cerebellar vermis has three main parts. The vestibulocerebellum regulates eye movements by the integration of visual info provided by the superior colliculus and balance information. The spinocerebellum integrates visual, auditory, proprioceptive, and balance information to act out body and limb movements. It receives input from the trigeminal nerve, dorsal column (of the spinal cord), midbrain, thalamus, reticular formation and vestibular nuclei (medulla) outputs. Lastly, the cerebrocerebellum plans, times, and initiates movement after evaluating sensory input from, primarily, motor cortex areas, via pons and cerebellar dentate nucleus. It outputs to the thalamus, motor cortex areas, and red nucleus.\nThe flocculonodular lobe is a cerebellar lobe that helps maintain body equilibrium by modifying muscle tone (the continuous and passive muscle contractions).\nMVN and IVN are in the medulla, LVN and SVN are smaller and in pons. SVN, MVN, and IVN ascend within the medial longitudinal fasciculus. LVN descend the spinal cord within the lateral vestibulospinal tract and ends at the sacrum. MVN also descend the spinal cord, within the medial vestibulospinal tract, ending at lumbar 1.\nThe thalamic reticular nucleus distributes information to various other thalamic nuclei, regulating the flow of information. It is speculatively able to stop signals, ending transmission of unimportant info. The thalamus relays info between pons (cerebellum link), motor cortices, and insula.\nThe insula is also heavily connected to motor cortices; the insula is likely where balance is likely brought into perception.\nThe oculomotor nuclear complex refers to fibers going to tegmentum (eye movement), red nucleus (gait (natural limb movement)), substantia nigra (reward), and cerebral peduncle (motor relay). Nucleus of Cajal are one of the named oculomotor nuclei, they are involved in eye movements and reflex gaze coordination.\nThe abducens nerve solely innervates the lateral rectus muscle of the eye, moving the eye with the trochlear nerve. The trochlear solely innervates the superior oblique muscle of the eye. Together, trochlear and abducens contract and relax to simultaneously direct the pupil towards an angle and depress the globe on the opposite side of the eye (e.g. looking down directs the pupil down and depresses (towards the brain) the top of the globe). The pupil is not only directed, but often rotated, by these muscles. (See visual system)\n\nThe thalamus and superior colliculus are connected via the lateral geniculate nucleus. The superior colliculus (SC) is the topographical map for balance and quick orienting movements with primarily visual inputs. SC integrates multiple senses.\n\nOther animals\nSome animals have better equilibrioception than humans; for example, a cat uses its inner ear and tail to walk on a thin fence.\nEquilibrioception in many marine animals is done with an entirely different organ, the statocyst, which detects the position of tiny calcareous stones to determine which way is \"up\".\n\nIn plants\nPlants could be said to exhibit a form of equilibrioception, in that when rotated from their normal attitude the stems grow in the direction that is upward (away from gravity) while their roots grow downward (in the direction of gravity). This phenomenon is known as gravitropism and it has been shown that, for example, poplar stems can detect reorientation and inclination.\n\nSee also\nProprioception\nVertigo\n\nReferences\n\n\n== External links ==","74":"Eustachian tube dysfunction (ETD) is a disorder where pressure abnormalities in the middle ear result in symptoms.\n\nSigns and symptoms\nSymptoms include aural fullness, ears popping, a feeling of pressure in the affected ear(s), a feeling that the affected ear(s) is clogged, crackling, ear pain, tinnitus, autophony, and muffled hearing.\n\nDiagnosis\nWhile Eustachian tube dysfunction can be hard to diagnose, due to the Eustachian tubes and the nasopharynx not being easily visible, usually a tympanometry is indicated, along with findings on an otoscopy. For cases of baro-challenge induced Eustachian tube dysfunction, diagnosis usually relies on the history of the patient and their reported symptoms, as otoscopy and tympanometry is sometimes normal at normal ambient pressure. Opening pressure has been proposed as a method for preoperative and intraoperative evaluation of any obstructive process within the Eustachian tube. As well, Valsalva CT scanning using advanced 64 slice  or higher machines has been proposed as a way of diagnosing and localizing anatomic obstruction within the Eustachian tube.\n\nTypes\nFour subtypes have been described:\n\nAnatomic obstruction within the proximal cartilaginous eustachian tube.\nDilatory Eustachian tube dysfunction: Functional, dynamic (muscle failure), or anatomical obstruction of the Eustachian tube\nBaro-challenge induced Eustachian tube dysfunction: Eustachian tube dysfunction which generally features a normal otoscopy and normal tympanometry\nPatulous Eustachian tube dysfunction\n\nCauses\nEustachian tube dysfunction can be caused by a number of factors. Some common causes include the flu, allergies, a cold, and sinus infections. In patients with chronic ear disease such as cholesteatoma and chronic discharge, studies showed that they have obstructive pathology at the ear side of the Eustachian tube. Given that proximity of that part of the Eustachian tube to the tympanic cavity, the site of frequent infections during childhood, it is logical to conclude that this segment of the tube experiences fibrosis and stenosis from recurrent infections.  This  is a possible explanation for the increased frequency of chronic ear disease in disadvantaged populations who lack access to medical care including antibiotics and tympanostomy tubes.\n\nTreatment\nFirst-line treatment options are generally aimed at treating the underlying cause and include attempting to \"pop\" the ears, usually via the Valsalva maneuver, the use of oral or topical decongestants, oral steroids, oral antihistamines, and topical nasal steroid sprays, such as Flonase.\nIf medical management fails, myringotomy, which is a surgical procedure in which an incision is made in the eardrum to drain pus from the middle ear or to relieve pressure caused by a large buildup of fluid, is indicated, and usually accompanied by the insertion of a tympanostomy tube.\nTentative evidence supports the use of balloon dilation of the Eustachian tube. In 2018, researchers published a prospective, multicenter, randomized, controlled trial demonstrating efficacy of this technique. Dilatation of the eustachian tube using balloon catheter has gained attention as a method of treating eustachian tube obstruction. There are two methods of performing this procedure depending on the route of the catheter introduction and the area of the Eustachian tube to be dilated.\n\n\n== References ==","75":"Ethanol (also called ethyl alcohol, grain alcohol, drinking alcohol, or simply alcohol) is an organic compound with the chemical formula CH3CH2OH. It is an alcohol, with its formula also written as C2H5OH, C2H6O or EtOH, where Et stands for ethyl. Ethanol is a volatile, flammable, colorless liquid with a characteristic wine-like odor and pungent taste. In nature, grape-sugar breaks up by the action of fermentation into alcohol or carbonic acid, without anything being added. As a psychoactive depressant, it is the active ingredient in alcoholic drinks, and the second most consumed drug globally behind caffeine.\nEthanol is naturally produced by the fermentation process of sugars by yeasts or via petrochemical processes such as ethylene hydration. Historically it was used as a general anesthetic, and has modern medical applications as an antiseptic, disinfectant, solvent for some medications, and antidote for methanol poisoning and ethylene glycol poisoning. It is used as a chemical solvent and in the synthesis of organic compounds, and as a fuel source for lamps, stoves, and internal combustion engines. Ethanol also can be dehydrated to make ethylene, an important chemical feedstock. As of 2023, world production of ethanol fuel was 29,590,000,000 US gallons (112.0 gigalitres), coming mostly from the U.S. (51%) and Brazil (26%).\n\nName\nEthanol is the systematic name defined by the International Union of Pure and Applied Chemistry for a compound consisting of an alkyl group with two carbon atoms (prefix \"eth-\"), having a single bond between them (infix \"-an-\") and an attached \u2212OH functional group (suffix \"-ol\").\nThe \"eth-\" prefix and the qualifier \"ethyl\" in \"ethyl alcohol\" originally came from the name \"ethyl\" assigned in 1834 to the group C2H5\u2212 by Justus Liebig. He coined the word from the German name Aether of the compound C2H5\u2212O\u2212C2H5 (commonly called \"ether\" in English, more specifically called \"diethyl ether\"). According to the Oxford English Dictionary, Ethyl is a contraction of the Ancient Greek \u03b1\u1f30\u03b8\u03ae\u03c1 (aith\u1e17r, \"upper air\") and the Greek word \u1f55\u03bb\u03b7 (h\u00fdl\u0113, \"wood, raw material\", hence \"matter, substance\").\nThe name ethanol was coined as a result of a resolution on naming alcohols and phenols that was adopted at the International Conference on Chemical Nomenclature that was held in April 1892 in Geneva, Switzerland.\nThe term alcohol now refers to a wider class of substances in chemistry nomenclature, but in common parlance it remains the name of ethanol. It is a medieval loan from Arabic al-ku\u1e25l, a powdered ore of antimony used since antiquity as a cosmetic, and retained that meaning in Middle Latin. The use of 'alcohol' for ethanol (in full, \"alcohol of wine\") is modern and was first recorded in 1753. Before the late 18th century the term \"alcohol\" generally referred to any sublimated substance.\n\nUses\nMedical\nAnesthetic\nEthanol is the oldest known sedative, used as an oral general anesthetic during surgery in ancient Mesopotamia and in medieval times. Mild intoxication starts at a blood alcohol concentration of 0.03-0.05 % and induces anesthetic coma at 0.4%. However, this use carried the high risk of deadly alcohol intoxication, pulmonary aspiration and vomit, which led to use of alternatives in antiquity, such as opium and cannabis, and later diethyl ether starting in the 1840s.\n\nAntiseptic\nEthanol is used as an antiseptic in medical wipes and hand sanitizer gels  for its bactericidal and anti-fungal effects. Ethanol kills microorganisms by dissolving their membrane lipid bilayer and denaturing their proteins, and is effective against most bacteria, fungi and viruses. However, it is ineffective against bacterial spores, which can be treated with hydrogen peroxide. \nA solution of 70% ethanol is more effective than pure ethanol because ethanol relies on water molecules for optimal antimicrobial activity. Absolute ethanol may inactivate microbes without destroying them because the alcohol is unable to fully permeate the microbe's membrane. Ethanol can also be used as a disinfectant and antiseptic by inducing cell dehydration through disruption of the osmotic balance across the cell membrane, causing  water to leave the cell, leading to cell death.\n\nAntidote\nEthanol may be administered as an antidote to ethylene glycol poisoning and methanol poisoning. It does so by acting as a competitive inhibitor against methanol and ethylene glycol for alcohol dehydrogenase. Though it has more side effects, ethanol is less expensive and more readily available than fomepizole in the role.\n\nMedicinal solvent\nEthanol is used to dissolve many water-insoluble medications and related compounds. Liquid preparations of pain medications, cough and cold medicines, and mouth washes, for example, may contain up to 25% ethanol and may need to be avoided in individuals with adverse reactions to ethanol such as alcohol-induced respiratory reactions. Ethanol is present mainly as an antimicrobial preservative in over 700 liquid preparations of medicine including acetaminophen, iron supplements, ranitidine, furosemide, mannitol, phenobarbital, trimethoprim\/sulfamethoxazole and over-the-counter cough medicine.\nSome medicinal solutions of ethanol are also known as tinctures.\n\nPharmacology\nIn mammals, ethanol is primarily metabolized in the liver and stomach by alcohol dehydrogenase (ADH) enzymes. These enzymes catalyze the oxidation of ethanol into acetaldehyde (ethanal):\n\nCH3CH2OH + NAD+ \u2192 CH3CHO + NADH + H+\nWhen present in significant concentrations, this metabolism of ethanol is additionally aided by the cytochrome P450 enzyme CYP2E1 in humans, while trace amounts are also metabolized by catalase.\nThe resulting intermediate, acetaldehyde, is a known carcinogen, and poses significantly greater toxicity in humans than ethanol itself. Many of the symptoms typically associated with alcohol intoxication\u2014as well as many of the health hazards typically associated with the long-term consumption of ethanol\u2014can be attributed to acetaldehyde toxicity in humans.\nThe subsequent oxidation of acetaldehyde into acetate is performed by aldehyde dehydrogenase (ALDH) enzymes. A mutation in the ALDH2 gene that encodes for an inactive or dysfunctional form of this enzyme affects roughly 50 % of east Asian populations, contributing to the characteristic alcohol flush reaction that can cause temporary reddening of the skin as well as a number of related, and often unpleasant, symptoms of acetaldehyde toxicity. This mutation is typically accompanied by another mutation in the alcohol dehydrogenase enzyme ADH1B in roughly 80 % of east Asians, which improves the catalytic efficiency of converting ethanol into acetaldehyde.\n\nRecreational\nAs a central nervous system depressant, ethanol is one of the most commonly consumed psychoactive drugs.\nDespite alcohol's psychoactive, addictive, and carcinogenic properties, it is readily available and legal for sale in most countries. There are laws regulating the sale, exportation\/importation, taxation, manufacturing, consumption, and possession of alcoholic beverages. The most common regulation is prohibition for minors.\n\nFuel\nEngine fuel\nThe largest single use of ethanol is as an engine fuel and fuel additive. Brazil in particular relies heavily upon the use of ethanol as an engine fuel, due in part to its role as one of the world's leading producers of ethanol. Gasoline sold in Brazil contains at least 25% anhydrous ethanol. Hydrous ethanol (about 95% ethanol and 5% water) can be used as fuel in more than 90% of new gasoline-fueled cars sold in the country.\nThe US and many other countries primarily use E10 (10% ethanol, sometimes known as gasohol) and E85 (85% ethanol) ethanol\/gasoline mixtures. Over time, it is believed that a material portion of the \u2248150-billion-US-gallon (570,000,000 m3) per year market for gasoline will begin to be replaced with fuel ethanol.\n\nAustralian law limits the use of pure ethanol from sugarcane waste to 10 % in automobiles. Older cars (and vintage cars designed to use a slower burning fuel) should have the engine valves upgraded or replaced.\nAccording to an industry advocacy group, ethanol as a fuel reduces harmful tailpipe emissions of carbon monoxide, particulate matter, oxides of nitrogen, and other ozone-forming pollutants. Argonne National Laboratory analyzed greenhouse gas emissions of many different engine and fuel combinations, and found that biodiesel\/petrodiesel blend (B20) showed a reduction of 8%, conventional E85 ethanol blend a reduction of 17% and cellulosic ethanol 64%, compared with pure gasoline. Ethanol has a much greater research octane number (RON) than gasoline, meaning it is less prone to pre-ignition, allowing for better ignition advance which means more torque, and efficiency in addition to the lower carbon emissions.\nEthanol combustion in an internal combustion engine yields many of the products of incomplete combustion produced by gasoline and significantly larger amounts of formaldehyde and related species such as acetaldehyde. This leads to a significantly larger photochemical reactivity and more ground level ozone. This data has been assembled into The Clean Fuels Report comparison of fuel emissions and show that ethanol exhaust generates 2.14 times as much ozone as gasoline exhaust. When this is added into the custom Localized Pollution Index of The Clean Fuels Report, the local pollution of ethanol (pollution that contributes to smog) is rated 1.7, where gasoline is 1.0 and higher numbers signify greater pollution. The California Air Resources Board formalized this issue in 2008 by recognizing control standards for formaldehydes as an emissions control group, much like the conventional NOx and reactive organic gases (ROGs).\nMore than 20% of Brazilian cars are able to use 100% ethanol as fuel, which includes ethanol-only engines and flex-fuel engines. Flex-fuel engines in Brazil are able to work with all ethanol, all gasoline or any mixture of both. In the United States, flex-fuel vehicles can run on 0% to 85% ethanol (15% gasoline) since higher ethanol blends are not yet allowed or efficient. Brazil supports this fleet of ethanol-burning automobiles with large national infrastructure that produces ethanol from domestically grown sugarcane.\nEthanol's high miscibility with water makes it unsuitable for shipping through modern pipelines like liquid hydrocarbons. Mechanics have seen increased cases of damage to small engines (in particular, the carburetor) and attribute the damage to the increased water retention by ethanol in fuel.\n\nRocket fuel\nEthanol was commonly used as fuel in early bipropellant rocket (liquid-propelled) vehicles, in conjunction with an oxidizer such as liquid oxygen. The German A-4 ballistic rocket of World War II (better known by its propaganda name V-2), which is credited as having begun the space age, used ethanol as the main constituent of B-Stoff. Under such nomenclature, the ethanol was mixed with 25% water to reduce the combustion chamber temperature. The V-2's design team helped develop U.S. rockets following World War II, including the ethanol-fueled Redstone rocket, which launched the first U.S. astronaut on suborbital spaceflight. Alcohols fell into general disuse as more energy-dense rocket fuels were developed, although ethanol is currently used in lightweight rocket-powered racing aircraft.\n\nFuel cells\nCommercial fuel cells operate on reformed natural gas, hydrogen or methanol. Ethanol is an attractive alternative due to its wide availability, low cost, high purity and low toxicity. There is a wide range of fuel cell concepts that have entered trials including direct-ethanol fuel cells, auto-thermal reforming systems and thermally integrated systems. The majority of work is being conducted at a research level although there are a number of organizations at the beginning of the commercialization of ethanol fuel cells.\n\nHousehold heating and cooking\nEthanol fireplaces can be used for home heating or for decoration. Ethanol can also be used as stove fuel for cooking.\n\nFeedstock\nEthanol is an important industrial ingredient. It has widespread use as a precursor for other organic compounds such as ethyl halides, ethyl esters, diethyl ether, acetic acid, and ethyl amines.\n\nSolvent\nEthanol is considered a universal solvent, as its molecular structure allows for the dissolving of both polar, hydrophilic and nonpolar, hydrophobic compounds. As ethanol also has a low boiling point, it is easy to remove from a solution that has been used to dissolve other compounds, making it a popular extracting agent for botanical oils. Cannabis oil extraction methods often use ethanol as an extraction solvent, and also as a post-processing solvent to remove oils, waxes, and chlorophyll from solution in a process known as winterization.\nEthanol is found in paints, tinctures, markers, and personal care products such as mouthwashes, perfumes and deodorants. Polysaccharides precipitate from aqueous solution in the presence of alcohol, and ethanol precipitation is used for this reason in the purification of DNA and RNA.\n\nLow-temperature liquid\nBecause of its low freezing point of \u2212114 \u00b0C (\u2212173 \u00b0F) and low toxicity, ethanol is sometimes used in laboratories (with dry ice or other coolants) as a cooling bath to keep vessels at temperatures below the freezing point of water. For the same reason, it is also used as the active fluid in alcohol thermometers.\n\nChemistry\nChemical formula\nEthanol is a 2-carbon alcohol. Its molecular formula is CH3CH2OH. The structure of the molecule of ethanol is CH3\u2212CH2\u2212OH (an ethyl group linked to a hydroxyl group), which indicates that the carbon of a methyl group (CH3\u2212) is attached to the carbon of a methylene group (\u2212CH2\u2013), which is attached to the oxygen of a hydroxyl group (\u2212OH). It is a constitutional isomer of dimethyl ether. Ethanol is sometimes abbreviated as EtOH, using the common organic chemistry notation of representing the ethyl group (C2H5\u2212) with Et.\n\nPhysical properties\nEthanol is a volatile, colorless liquid that has a slight odor. It burns with a smokeless blue flame that is not always visible in normal light. The physical properties of ethanol stem primarily from the presence of its hydroxyl group and the shortness of its carbon chain. Ethanol's hydroxyl group is able to participate in hydrogen bonding, rendering it more viscous and less volatile than less polar organic compounds of similar molecular weight, such as propane.\nEthanol's adiabatic flame temperature for combustion in air\tis 2082 \u00b0C or 3779 \u00b0F.\nEthanol is slightly more refractive than water, having a refractive index of 1.36242 (at \u03bb=589.3 nm and 18.35 \u00b0C or 65.03 \u00b0F). The triple point for ethanol is 150 \u00b1 20 K.\n\nSolvent properties\nEthanol is a versatile solvent, miscible with water and with many organic solvents, including acetic acid, acetone, benzene, carbon tetrachloride, chloroform, diethyl ether, ethylene glycol, glycerol, nitromethane, pyridine, and toluene. Its main use as a solvent is in making tincture of iodine, cough syrups, etc. It is also miscible with light aliphatic hydrocarbons, such as pentane and hexane, and with aliphatic chlorides such as trichloroethane and tetrachloroethylene.\nEthanol's miscibility with water contrasts with the immiscibility of longer-chain alcohols (five or more carbon atoms), whose water miscibility decreases sharply as the number of carbons increases. The miscibility of ethanol with alkanes is limited to alkanes up to undecane: mixtures with dodecane and higher alkanes show a miscibility gap below a certain temperature (about 13 \u00b0C for dodecane). The miscibility gap tends to get wider with higher alkanes, and the temperature for complete miscibility increases.\nEthanol-water mixtures have less volume than the sum of their individual components at the given fractions. Mixing equal volumes of ethanol and water results in only 1.92 volumes of mixture. Mixing ethanol and water is exothermic, with up to 777 J\/mol being released at 298 K.\nMixtures of ethanol and water form an azeotrope at about 89 mole-% ethanol and 11 mole-% water or a mixture of 95.6% ethanol by mass (or about 97% alcohol by volume) at normal pressure, which boils at 351 K (78 \u00b0C). This azeotropic composition is strongly temperature- and pressure-dependent and vanishes at temperatures below 303 K.\n\nHydrogen bonding causes pure ethanol to be hygroscopic to the extent that it readily absorbs water from the air. The polar nature of the hydroxyl group causes ethanol to dissolve many ionic compounds, notably sodium and potassium hydroxides, magnesium chloride, calcium chloride, ammonium chloride, ammonium bromide, and sodium bromide. Sodium and potassium chlorides are slightly soluble in ethanol. Because the ethanol molecule also has a nonpolar end, it will also dissolve nonpolar substances, including most essential oils and numerous flavoring, coloring, and medicinal agents.\nThe addition of even a few percent of ethanol to water sharply reduces the surface tension of water. This property partially explains the \"tears of wine\" phenomenon. When wine is swirled in a glass, ethanol evaporates quickly from the thin film of wine on the wall of the glass. As the wine's ethanol content decreases, its surface tension increases and the thin film \"beads up\" and runs down the glass in channels rather than as a smooth sheet.\n\nFlammability\nAn ethanol\u2013water solution will catch fire if heated above a temperature called its flash point and an ignition source is then applied to it. For 20% alcohol by mass (about 25% by volume), this will occur at about 25 \u00b0C (77 \u00b0F). The flash point of pure ethanol is 13 \u00b0C (55 \u00b0F), but may be influenced very slightly by atmospheric composition such as pressure and humidity. Ethanol mixtures can ignite below average room temperature. Ethanol is considered a flammable liquid (Class 3 Hazardous Material) in concentrations above 2.35% by mass (3.0% by volume; 6 proof).\n\nDishes using burning alcohol for culinary effects are called flamb\u00e9.\n\nNatural occurrence\nEthanol is a byproduct of the metabolic process of yeast. As such, ethanol will be present in any yeast habitat. Ethanol can commonly be found in overripe fruit. Ethanol produced by symbiotic yeast can be found in bertam palm blossoms. Although some animal species, such as the pentailed treeshrew, exhibit ethanol-seeking behaviors, most show no interest or avoidance of food sources containing ethanol. Ethanol is also produced during the germination of many plants as a result of natural anaerobiosis. \nEthanol has been detected in outer space, forming an icy coating around dust grains in interstellar clouds.\nMinute quantity amounts (average 196 ppb) of endogenous ethanol and acetaldehyde were found in the exhaled breath of healthy volunteers. Auto-brewery syndrome, also known as gut fermentation syndrome, is a rare medical condition in which intoxicating quantities of ethanol are produced through endogenous fermentation within the digestive system.\n\nProduction\nEthanol is produced both as a petrochemical, through the hydration of ethylene and, via biological processes, by fermenting sugars with yeast. Which process is more economical depends on prevailing prices of petroleum and grain feed stocks.\n\nSources\nWorld production of ethanol in 2006 was 51 gigalitres (1.3\u00d71010 US gal), with 69% of the world supply coming from Brazil and the U.S. Brazilian ethanol is produced from sugarcane, which has relatively high yields (830% more fuel than the fossil fuels used to produce it) compared to some other energy crops. Sugarcane not only has a greater concentration of sucrose than corn (by about 30%), but is also much easier to extract. The bagasse generated by the process is not discarded, but burned by power plants to produce electricity. Bagasse burning accounts for around 9% of the electricity produced in Brazil.\nIn the 1970s most industrial ethanol in the U.S. was made as a petrochemical, but in the 1980s the U.S. introduced subsidies for corn-based ethanol. According to the Renewable Fuels Association, as of 30 October 2007, 131 grain ethanol bio-refineries in the U.S. have the capacity to produce 7\u00d710^9 US gal (26,000,000 m3) of ethanol per year. An additional 72 construction projects underway (in the U.S.) can add 6.4 billion US gallons (24,000,000 m3) of new capacity in the next 18 months.\nIn India ethanol is made from sugarcane. Sweet sorghum is another potential source of ethanol, and is suitable for growing in dryland conditions. The International Crops Research Institute for the Semi-Arid Tropics is investigating the possibility of growing sorghum as a source of fuel, food, and animal feed in arid parts of Asia and Africa. Sweet sorghum has one-third the water requirement of sugarcane over the same time period. It also requires about 22% less water than corn. The world's first sweet sorghum ethanol distillery began commercial production in 2007 in Andhra Pradesh, India.\n\nHydration\nEthanol can be produced from petrochemical feed stocks, primarily by the acid-catalyzed hydration of ethylene. It is often referred to as synthetic ethanol.\n\nC2H4 + H2O  \u2192  C2H5OH\nThe catalyst is most commonly phosphoric acid, adsorbed onto a porous support such as silica gel or diatomaceous earth. This catalyst was first used for large-scale ethanol production by the Shell Oil Company in 1947. The reaction is carried out in the presence of high pressure steam at 300 \u00b0C (572 \u00b0F) where a 5:3 ethylene to steam ratio is maintained. This process was used on an industrial scale by Union Carbide Corporation and others. It is no longer practiced in the US as fermentation ethanol produced from corn is more economical.\nIn an older process, first practiced on the industrial scale in 1930 by Union Carbide but now almost entirely obsolete, ethylene was hydrated indirectly by reacting it with concentrated sulfuric acid to produce ethyl sulfate, which was hydrolyzed to yield ethanol and regenerate the sulfuric acid:\n\nC2H4 + H2SO4  \u2192  C2H5HSO4\nC2H5HSO4 + H2O  \u2192  H2SO4 + C2H5OH\n\nFrom carbon dioxide\nEthanol has been produced in the laboratory by converting carbon dioxide via biological and electrochemical reactions.\n\nFermentation\nEthanol in alcoholic beverages and fuel is produced by fermentation. Certain species of yeast (e.g., Saccharomyces cerevisiae) metabolize sugar (namely polysaccharides), producing ethanol and carbon dioxide. The chemical equations below summarize the conversion:\n\nFermentation is the process of culturing yeast under favorable thermal conditions to produce alcohol. This process is carried out at around 35\u201340 \u00b0C (95\u2013104 \u00b0F). Toxicity of ethanol to yeast limits the ethanol concentration obtainable by brewing; higher concentrations, therefore, are obtained by fortification or distillation. The most ethanol-tolerant yeast strains can survive up to approximately 18% ethanol by volume.\nTo produce ethanol from starchy materials such as cereals, the starch must first be converted into sugars. In brewing beer, this has traditionally been accomplished by allowing the grain to germinate, or malt, which produces the enzyme amylase. When the malted grain is mashed, the amylase converts the remaining starches into sugars.\n\nCellulose\nSugars for ethanol fermentation can be obtained from cellulose. Deployment of this technology could turn a number of cellulose-containing agricultural by-products, such as corncobs, straw, and sawdust, into renewable energy resources. Other agricultural residues such as sugarcane bagasse and energy crops such as switchgrass may also be fermentable sugar sources.\n\nTesting\nBreweries and biofuel plants employ two methods for measuring ethanol concentration. Infrared ethanol sensors measure the vibrational frequency of dissolved ethanol using the C\u2212H band at 2900 cm\u22121. This method uses a relatively inexpensive solid-state sensor that compares the C\u2212H band with a reference band to calculate the ethanol content. The calculation makes use of the Beer\u2013Lambert law. Alternatively, by measuring the density of the starting material and the density of the product, using a hydrometer, the change in specific gravity during fermentation indicates the alcohol content. This inexpensive and indirect method has a long history in the beer brewing industry.\n\nPurification\nDistillation\nEthylene hydration or brewing produces an ethanol\u2013water mixture. For most industrial and fuel uses, the ethanol must be purified. Fractional distillation at atmospheric pressure can concentrate ethanol to 95.6% by weight (89.5 mole%). This mixture is an azeotrope with a boiling point of 78.1 \u00b0C (172.6 \u00b0F), and cannot be further purified by distillation. Addition of an entraining agent, such as benzene, cyclohexane, or heptane, allows a new ternary azeotrope comprising the ethanol, water, and the entraining agent to be formed. This lower-boiling ternary azeotrope is removed preferentially, leading to water-free ethanol.\n\nMolecular sieves and desiccants\nApart from distillation, ethanol may be dried by addition of a desiccant, such as molecular sieves, cellulose, or cornmeal. The desiccants can be dried and reused. Molecular sieves can be used to selectively absorb the water from the 95.6% ethanol solution. Molecular sieves of pore-size 3 \u00c5ngstrom, a type of zeolite, effectively sequester water molecules while excluding ethanol molecules. Heating the wet sieves drives out the water, allowing regeneration of their desiccant capability.\n\nMembranes and reverse osmosis\nMembranes can also be used to separate ethanol and water. Membrane-based separations are not subject to the limitations of the water-ethanol azeotrope because the separations are not based on vapor-liquid equilibria. Membranes are often used in the so-called hybrid membrane distillation process. This process uses a pre-concentration distillation column as the first separating step. The further separation is then accomplished with a membrane operated either in vapor permeation or pervaporation mode. Vapor permeation uses a vapor membrane feed and pervaporation uses a liquid membrane feed.\n\nOther techniques\nA variety of other techniques have been discussed, including the following:\n\nSalting using potassium carbonate to exploit its insolubility will cause a phase separation with ethanol and water. This offers a very small potassium carbonate impurity to the alcohol that can be removed by distillation. This method is very useful in purification of ethanol by distillation, as ethanol forms an azeotrope with water.\nDirect electrochemical reduction of carbon dioxide to ethanol under ambient conditions using copper nanoparticles on a carbon nanospike film as the catalyst;\nExtraction of ethanol from grain mash by supercritical carbon dioxide;\nPervaporation;\nFractional freezing is also used to concentrate fermented alcoholic solutions, such as traditionally made Applejack (beverage);\nPressure swing adsorption.\n\nGrades of ethanol\nDenatured alcohol\nPure ethanol and alcoholic beverages are heavily taxed as psychoactive drugs, but ethanol has many uses that do not involve its consumption. To relieve the tax burden on these uses, most jurisdictions waive the tax when an agent has been added to the ethanol to render it unfit to drink. These include bittering agents such as denatonium benzoate and toxins such as methanol, naphtha, and pyridine. Products of this kind are called denatured alcohol.\n\nAbsolute alcohol\nAbsolute or anhydrous alcohol refers to ethanol with a low water content. There are various grades with maximum water contents ranging from 1% to a few parts per million (ppm). If azeotropic distillation is used to remove water, it will contain trace amounts of the material separation agent (e.g. benzene). Absolute alcohol is not intended for human consumption. Absolute ethanol is used as a solvent for laboratory and industrial applications, where water will react with other chemicals, and as fuel alcohol. Spectroscopic ethanol is an absolute ethanol with a low absorbance in ultraviolet and visible light, fit for use as a solvent in ultraviolet-visible spectroscopy.\nPure ethanol is classed as 200 proof in the US, equivalent to 175 degrees proof in the UK system.\n\nRectified spirits\nRectified spirit, an azeotropic composition of 96% ethanol containing 4% water, is used instead of anhydrous ethanol for various purposes. Spirits of wine are about 94% ethanol (188 proof). The impurities are different from those in 95% (190 proof) laboratory ethanol.\n\nReactions\nEthanol is classified as a primary alcohol, meaning that the carbon that its hydroxyl group attaches to has at least two hydrogen atoms attached to it as well. Many ethanol reactions occur at its hydroxyl group.\n\nEster formation\nIn the presence of acid catalysts, ethanol reacts with carboxylic acids to produce ethyl esters and water:\n\nRCOOH + HOCH2CH3 \u2192 RCOOCH2CH3 + H2O\nThis reaction, which is conducted on large scale industrially, requires the removal of the water from the reaction mixture as it is formed. Esters react in the presence of an acid or base to give back the alcohol and a salt. This reaction is known as saponification because it is used in the preparation of soap. Ethanol can also form esters with inorganic acids. Diethyl sulfate and triethyl phosphate are prepared by treating ethanol with sulfur trioxide and phosphorus pentoxide respectively. Diethyl sulfate is a useful ethylating agent in organic synthesis. Ethyl nitrite, prepared from the reaction of ethanol with sodium nitrite and sulfuric acid, was formerly used as a diuretic.\n\nDehydration\nIn the presence of acid catalysts, alcohols can be converted to alkenes such as ethanol to ethylene. Typically solid acids such as alumina are used.\n\nCH3CH2OH \u2192 H2C=CH2 + H2O\nSince water is removed from the same molecule, the reaction is known as intramolecular dehydration. Intramolecular dehydration of an alcohol requires a high temperature and the presence of an acid catalyst such as sulfuric acid.\nEthylene produced from sugar-derived ethanol (primarily in Brazil) competes with ethylene produced from petrochemical feedstocks such as naphtha and ethane.\nAt a lower temperature than that of intramolecular dehydration, intermolecular alcohol dehydration may occur producing a symmetrical ether. This is a condensation reaction. In the following example, diethyl ether is produced from ethanol:\n\n2 CH3CH2OH \u2192 CH3CH2OCH2CH3 + H2O\n\nCombustion\nComplete combustion of ethanol forms carbon dioxide and water:\n\nC2H5OH (l) + 3 O2 (g) \u2192 2 CO2 (g) + 3 H2O (l); \u2212\u0394cH = 1371 kJ\/mol = 29.8 kJ\/g = 327 kcal\/mol = 7.1 kcal\/g\nC2H5OH (l) + 3 O2 (g) \u2192 2 CO2 (g) + 3 H2O (g); \u2212\u0394cH = 1236 kJ\/mol = 26.8 kJ\/g = 295.4 kcal\/mol = 6.41 kcal\/g\nSpecific heat = 2.44 kJ\/(kg\u00b7K)\n\nAcid-base chemistry\nEthanol is a neutral molecule and the pH of a solution of ethanol in water is nearly 7.00. Ethanol can be quantitatively converted to its conjugate base, the ethoxide ion (CH3CH2O\u2212), by reaction with an alkali metal such as sodium:\n\n2 CH3CH2OH + 2 Na \u2192 2 CH3CH2ONa + H2\nor a very strong base such as sodium hydride:\n\nCH3CH2OH + NaH \u2192 CH3CH2ONa + H2\nThe acidities of water and ethanol are nearly the same, as indicated by their pKa of 15.7 and 16 respectively. Thus, sodium ethoxide and sodium hydroxide exist in an equilibrium that is closely balanced:\n\nCH3CH2OH + NaOH \u21cc CH3CH2ONa + H2O\n\nHalogenation\nEthanol is not used industrially as a precursor to ethyl halides, but the reactions are illustrative. Ethanol reacts with hydrogen halides to produce ethyl halides such as ethyl chloride and ethyl bromide via an SN2 reaction:\n\nCH3CH2OH + HCl \u2192 CH3CH2Cl + H2O\nHCl requires a catalyst such as zinc chloride.\nHBr requires refluxing with a sulfuric acid catalyst. Ethyl halides can, in principle, also be produced by treating ethanol with more specialized halogenating agents, such as thionyl chloride or phosphorus tribromide.\n\nCH3CH2OH + SOCl2 \u2192 CH3CH2Cl + SO2 + HCl\nUpon treatment with halogens in the presence of base, ethanol gives the corresponding haloform (CHX3, where X = Cl, Br, I). This conversion is called the haloform reaction.\nAn intermediate in the reaction with chlorine is the aldehyde called chloral, which forms chloral hydrate upon reaction with water:\n\n4 Cl2 + CH3CH2OH \u2192 CCl3CHO + 5 HCl\nCCl3CHO + H2O \u2192 CCl3C(OH)2H\n\nOxidation\nEthanol can be oxidized to acetaldehyde and further oxidized to acetic acid, depending on the reagents and conditions. This oxidation is of no importance industrially, but in the human body, these oxidation reactions are catalyzed by the enzyme liver alcohol dehydrogenase. The oxidation product of ethanol, acetic acid, is a nutrient for humans, being a precursor to acetyl CoA, where the acetyl group can be spent as energy or used for biosynthesis.\n\nMetabolism\nEthanol is similar to macronutrients such as proteins, fats, and carbohydrates in that it provides calories. When consumed and metabolized, it contributes 7 kilocalories per gram via ethanol metabolism.\n\nSafety\nEthanol is very flammable and should not be used around an open flame. \nPure ethanol will irritate the skin and eyes. Nausea, vomiting, and intoxication are symptoms of ingestion. Long-term use by ingestion can result in serious liver damage. Atmospheric concentrations above one part per thousand are above the European Union occupational exposure limits.\n\nHistory\nThe fermentation of sugar into ethanol is one of the earliest biotechnologies employed by humans. Ethanol has historically been identified variously as spirit of wine or ardent spirits, and as aqua vitae or aqua vita. The intoxicating effects of its consumption have been known since ancient times. Ethanol has been used by humans since prehistory as the intoxicating ingredient of alcoholic beverages. Dried residue on 9,000-year-old pottery found in China suggests that Neolithic people consumed alcoholic beverages.\nThe inflammable nature of the exhalations of wine was already known to ancient natural philosophers such as Aristotle (384\u2013322 BCE), Theophrastus (c.\u2009371\u2013287 BCE), and Pliny the Elder (23\/24\u201379 CE). However, this did not immediately lead to the isolation of ethanol, despite the development of more advanced distillation techniques in second- and third-century Roman Egypt. An important recognition, first found in one of the writings attributed to J\u0101bir ibn \u1e24ayy\u0101n (ninth century CE), was that by adding salt to boiling wine, which increases the wine's relative volatility, the flammability of the resulting vapors may be enhanced. The distillation of wine is attested in Arabic works attributed to al-Kind\u012b (c.\u2009801\u2013873 CE) and to al-F\u0101r\u0101b\u012b (c.\u2009872\u2013950), and in the 28th book of al-Zahr\u0101w\u012b's (Latin: Abulcasis, 936\u20131013) Kit\u0101b al-Ta\u1e63r\u012bf (later translated into Latin as Liber servatoris). In the twelfth century, recipes for the production of aqua ardens (\"burning water\", i.e., ethanol) by distilling wine with salt started to appear in a number of Latin works, and by the end of the thirteenth century it had become a widely known substance among Western European chemists.\nThe works of Taddeo Alderotti (1223\u20131296) describe a method for concentrating ethanol involving repeated fractional distillation through a water-cooled still, by which an ethanol purity of 90% could be obtained. The medicinal properties of ethanol were studied by Arnald of Villanova (1240\u20131311 CE) and John of Rupescissa (c.\u20091310\u20131366), the latter of whom regarded it as a life-preserving substance able to prevent all diseases (the aqua vitae or \"water of life\", also called by John the quintessence of wine).\nIn China, archaeological evidence indicates that the true distillation of alcohol began during the Jin (1115\u20131234) or Southern Song (1127\u20131279) dynasties. A still has been found at an archaeological site in Qinglong, Hebei, dating to the 12th century. In India, the true distillation of alcohol was introduced from the Middle East, and was in wide use in the Delhi Sultanate by the 14th century.\nIn 1796, German-Russian chemist Johann Tobias Lowitz obtained pure ethanol by mixing partially purified ethanol (the alcohol-water azeotrope) with an excess of anhydrous alkali and then distilling the mixture over low heat. French chemist Antoine Lavoisier described ethanol as a compound of carbon, hydrogen, and oxygen, and in 1807 Nicolas-Th\u00e9odore de Saussure determined ethanol's chemical formula. Fifty years later, Archibald Scott Couper published the structural formula of ethanol,  one of the first structural formulas determined.\nEthanol was first prepared synthetically in 1825 by Michael Faraday. He found that sulfuric acid could absorb large volumes of coal gas. He gave the resulting solution to Henry Hennell, a British chemist, who found in 1826 that it contained \"sulphovinic acid\" (ethyl hydrogen sulfate). In 1828, Hennell and the French chemist Georges-Simon Serullas independently discovered that sulphovinic acid could be decomposed into ethanol. Thus, in 1825 Faraday had unwittingly discovered that ethanol could be produced from ethylene (a component of coal gas) by acid-catalyzed hydration, a process similar to current industrial ethanol synthesis.\nEthanol was used as lamp fuel in the U.S. as early as 1840, but a tax levied on industrial alcohol during the Civil War made this use uneconomical. The tax was repealed in 1906. Use as an automotive fuel dates back to 1908, with the Ford Model T able to run on petrol (gasoline) or ethanol. It fuels some spirit lamps.\nEthanol intended for industrial use is often produced from ethylene. Ethanol has widespread use as a solvent of substances intended for human contact or consumption, including scents, flavorings, colorings, and medicines. In chemistry, it is both a solvent and a feedstock for the synthesis of other products. It has a long history as a fuel for heat and light, and more recently as a fuel for internal combustion engines.\n\nSee also\nReferences\nFurther reading\nExternal links\n\nAlcohol (Ethanol) at The Periodic Table of Videos (University of Nottingham)\nInternational Labour Organization ethanol safety information\nNational Pollutant Inventory \u2013 Ethanol Fact Sheet\nCDC \u2013 NIOSH Pocket Guide to Chemical Hazards \u2013 Ethyl Alcohol\nNational Institute of Standards and Technology chemical data on ethanol\nChicago Board of Trade news and market data on ethanol futures\nCalculation of vapor pressure, liquid density, dynamic liquid viscosity, surface tension of ethanol\nEthanol History A look into the history of ethanol\nChemSub Online: Ethyl alcohol\nIndustrial ethanol production process flow diagram using ethylene and sulphuric acid","76":"GABA (gamma Aminobutyric acid, \u03b3-Aminobutyric acid) is the chief inhibitory neurotransmitter in the developmentally mature mammalian central nervous system. Its principal role is reducing neuronal excitability throughout the nervous system.\nGABA is sold as a dietary supplement in many countries. It has been traditionally thought that exogenous GABA (i.e., taken as a supplement) does not cross the blood\u2013brain barrier, but data obtained from more recent research (2010s) in rats describes the notion as being unclear.\nThe carboxylate form of GABA is \u03b3-aminobutyrate.\n\nFunction\nNeurotransmitter\nTwo general classes of GABA receptor are known:\n\nGABAA in which the receptor is part of a ligand-gated ion channel complex\nGABAB metabotropic receptors, which are G protein-coupled receptors that open or close ion channels via intermediaries (G proteins)\n\nNeurons that produce GABA as their output are called GABAergic neurons, and have chiefly inhibitory action at receptors in the adult vertebrate. Medium spiny cells are a typical example of inhibitory central nervous system GABAergic cells. In contrast, GABA exhibits both excitatory and inhibitory actions in insects, mediating muscle activation at synapses between nerves and muscle cells, and also the stimulation of certain glands. In mammals, some GABAergic neurons, such as chandelier cells, are also able to excite their glutamatergic counterparts. In addition to fast-acting phasic inhibition, small amounts of extracellular GABA can induce slow timescale tonic inhibition on neurons.   \nGABAA receptors are ligand-activated chloride channels: when activated by GABA, they allow the flow of chloride ions across the membrane of the cell. Whether this chloride flow is depolarizing (makes the voltage across the cell's membrane less negative), shunting (has no effect on the cell's membrane potential), or inhibitory\/hyperpolarizing (makes the cell's membrane more negative) depends on the direction of the flow of chloride. When net chloride flows out of the cell, GABA is depolarising; when chloride flows into the cell, GABA is inhibitory or hyperpolarizing. When the net flow of chloride is close to zero, the action of GABA is shunting. Shunting inhibition has no direct effect on the membrane potential of the cell; however, it reduces the effect of any coincident synaptic input by reducing the electrical resistance of the cell's membrane. Shunting inhibition can \"override\" the excitatory effect of depolarising GABA, resulting in overall inhibition even if the membrane potential becomes less negative. It was thought that a developmental switch in the molecular machinery controlling the concentration of chloride inside the cell changes the functional role of GABA between neonatal and adult stages. As the brain develops into adulthood, GABA's role changes from excitatory to inhibitory.\n\nBrain development\nGABA is an inhibitory transmitter in the mature brain; its actions were thought to be primarily excitatory in the developing brain. The gradient of chloride was reported to be reversed in immature neurons, with its reversal potential higher than the resting membrane potential of the cell; activation of a GABA-A receptor thus leads to efflux of Cl\u2212 ions from the cell (that is, a depolarizing current). The differential gradient of chloride in immature neurons was shown to be primarily due to the higher concentration of NKCC1 co-transporters relative to KCC2 co-transporters in immature cells. GABAergic interneurons mature faster in the hippocampus and the GABA machinery appears earlier than glutamatergic transmission. Thus, GABA is considered the major excitatory neurotransmitter in many regions of the brain before the maturation of glutamatergic synapses.\nIn the developmental stages preceding the formation of synaptic contacts, GABA is synthesized by neurons and acts both as an autocrine (acting on the same cell) and paracrine (acting on nearby cells) signalling mediator. The ganglionic eminences also contribute greatly to building up the GABAergic cortical cell population.\nGABA regulates the proliferation of neural progenitor cells, the migration and differentiation the elongation of neurites and the formation of synapses.\nGABA also regulates the growth of embryonic and neural stem cells. GABA can influence the development of neural progenitor cells via brain-derived neurotrophic factor (BDNF) expression. GABA activates the GABAA receptor, causing cell cycle arrest in the S-phase, limiting growth.\n\nBeyond the nervous system\nBesides the nervous system, GABA is also produced at relatively high levels in the insulin-producing beta cells (\u03b2-cells) of the pancreas. The \u03b2-cells secrete GABA along with insulin and the GABA binds to GABA receptors on the neighboring islet alpha cells (\u03b1-cells) and inhibits them from secreting glucagon (which would counteract insulin's effects).\nGABA can promote the replication and survival of \u03b2-cells and also promote the conversion of \u03b1-cells to \u03b2-cells, which may lead to new treatments for diabetes.\nAlongside GABAergic mechanisms, GABA has also been detected in other peripheral tissues including intestines, stomach, fallopian tubes, uterus, ovaries, testicles, kidneys, urinary bladder, the lungs and liver, albeit at much lower levels than in neurons or \u03b2-cells.\nExperiments on mice have shown that hypothyroidism induced by fluoride poisoning can be halted by administering GABA. The test also found that the thyroid recovered naturally without further assistance after the fluoride had been expelled by the GABA.\nImmune cells express receptors for GABA and administration of GABA can suppress inflammatory immune responses and promote \"regulatory\" immune responses, such that GABA administration has been shown to inhibit autoimmune diseases in several animal models.\nIn 2018, GABA has shown to regulate secretion of a greater number of cytokines. In plasma of T1D patients, levels of 26 cytokines are increased and of those, 16 are inhibited by GABA in the cell assays.\nIn 2007, an excitatory GABAergic system was described in the airway epithelium. The system is activated by exposure to allergens and may participate in the mechanisms of asthma. GABAergic systems have also been found in the testis and in the eye lens.\n\nStructure and conformation\nGABA is found mostly as a zwitterion (i.e., with the carboxyl group deprotonated and the amino group protonated). Its conformation depends on its environment. In the gas phase, a highly folded conformation is strongly favored due to the electrostatic attraction between the two functional groups. The stabilization is about 50 kcal\/mol, according to quantum chemistry calculations. In the solid state, an extended conformation is found, with a trans conformation at the amino end and a gauche conformation at the carboxyl end. This is due to the packing interactions with the neighboring molecules. In solution, five different conformations, some folded and some extended, are found as a result of solvation effects. The conformational flexibility of GABA is important for its biological function, as it has been found to bind to different receptors with different conformations. Many GABA analogues with pharmaceutical applications have more rigid structures in order to control the binding better.\n\nHistory\nIn 1883, GABA was first synthesized, and it was first known only as a plant and microbe metabolic product.\nIn 1950, GABA was discovered as an integral part of the mammalian central nervous system.\nIn 1959, it was shown that at an inhibitory synapse on crayfish muscle fibers GABA acts like stimulation of the inhibitory nerve.  Both inhibition by nerve stimulation and by applied GABA are blocked by picrotoxin.\n\nBiosynthesis\nGABA is primarily synthesized from glutamate via the enzyme glutamate decarboxylase (GAD) with pyridoxal phosphate (the active form of vitamin B6) as a cofactor. This process converts glutamate (the principal excitatory neurotransmitter) into GABA (the principal inhibitory neurotransmitter).\nGABA can also be synthesized from putrescine by diamine oxidase and aldehyde dehydrogenase.\nHistorically it was thought that exogenous GABA did not penetrate the blood\u2013brain barrier, but more current research describes the notion as being unclear pending further research.\n\nMetabolism\nGABA transaminase enzymes catalyze the conversion of 4-aminobutanoic acid (GABA) and 2-oxoglutarate (\u03b1-ketoglutarate) into succinic semialdehyde and glutamate. Succinic semialdehyde is then oxidized into succinic acid by succinic semialdehyde dehydrogenase and as such enters the citric acid cycle as a usable source of energy.\n\nPharmacology\nDrugs that act as allosteric modulators of GABA receptors (known as GABA analogues or GABAergic drugs), or increase the available amount of GABA, typically have relaxing, anti-anxiety, and anti-convulsive effects (with equivalent efficacy to lamotrigine based on studies of mice). Many of the substances below are known to cause anterograde amnesia and retrograde amnesia.\nIn general, GABA does not cross the blood\u2013brain barrier, although certain areas of the brain that have no effective blood\u2013brain barrier, such as the periventricular nucleus, can be reached by drugs such as systemically injected GABA. At least one study suggests that orally administered GABA increases the amount of human growth hormone (HGH). GABA directly injected to the brain has been reported to have both stimulatory and inhibitory effects on the production of growth hormone, depending on the physiology of the individual. Consequently, considering the potential biphasic effects of GABA on growth hormone production, as well as other safety concerns, its usage is not recommended during pregnancy and lactation.\nGABA enhances the catabolism of serotonin into N-acetylserotonin (the precursor of melatonin) in rats. It is thus suspected that GABA is involved in the synthesis of melatonin and thus might exert regulatory effects on sleep and reproductive functions.\n\nChemistry\nAlthough in chemical terms, GABA is an amino acid (as it has both a primary amine and a carboxylic acid functional group), it is rarely referred to as such in the professional, scientific, or medical community. By convention the term \"amino acid\", when used without a qualifier, refers specifically to an alpha amino acid. GABA is not an alpha amino acid, meaning the amino group is not attached to the alpha carbon. Nor is it incorporated into proteins as are many alpha-amino acids.\n\nGABAergic drugs\nGABAA receptor ligands are shown in the following table\n\nGABAergic pro-drugs include chloral hydrate, which is metabolised to trichloroethanol, which then acts via the GABAA receptor.\nThe plant kava contains GABAergic compounds, including kavain, dihydrokavain, methysticin, dihydromethysticin and yangonin.\n\nOther GABAergic modulators include:\n\nGABAB receptor ligands.\nAgonists: baclofen, propofol, GHB, phenibut.\nAntagonists: phaclofen, saclofen.\nGABA reuptake inhibitors: deramciclane, hyperforin, tiagabine.\nGABA transaminase inhibitors: gabaculine, phenelzine, valproate, vigabatrin, lemon balm (Melissa officinalis).\nGABA analogues: pregabalin, gabapentin, picamilon, progabide\n\nIn plants\nGABA is also found in plants. It is the most abundant amino acid in the apoplast of tomatoes. Evidence also suggests a role in cell signalling in plants.\n\nSee also\n3-Aminoisobutyric acid\n4-aminobutyrate transaminase (GABA-transaminase) deficiency\nGABA analogue\nGABA receptor\nGABA tea\nGiant depolarizing potential\nSpastic diplegia, a GABA deficiency neuromuscular neuropathology\nSpasticity\nSuccinic semialdehyde dehydrogenase deficiency\nTaurine\n\nNotes\nReferences\nExternal links\n\nSmart TG, Stephenson FA (2019). \"A half century of \u03b3-aminobutyric acid\". Brain Neurosci Adv. 3: 2398212819858249. doi:10.1177\/2398212819858249. PMC 7058221. PMID 32166183.\nParviz M, Vogel K, Gibson KM, Pearl PL (2014-11-25). \"Disorders of GABA metabolism: SSADH and GABA-transaminase deficiencies\". Journal of Pediatric Epilepsy. 3 (4): 217\u2013227. doi:10.3233\/PEP-14097. PMC 4256671. PMID 25485164. Clinical disorders known to affect inherited GABA metabolism\nGamma-aminobutyric acid MS Spectrum\nScholarpedia article on GABA\nList of GABA neurons on NeuroLex.org\nEffects of Oral Gamma-Aminobutyric Acid (GABA) Administration on Stress and Sleep in Humans: A Systematic Review","77":"The fear of falling (FOF), also referred to as basophobia (or basiphobia), is a natural fear and is typical of most humans and mammals, in varying degrees of extremity.  It differs from acrophobia (the fear of heights), although the two fears are closely related. The fear of falling encompasses the anxieties accompanying the sensation and the possibly dangerous effects of falling, as opposed to the heights themselves.  Those who have little fear of falling may be said to have a head for heights.  Basophobia is sometimes associated with astasia-abasia, the fear of walking\/standing erect.\n\nIn humans\nInfants\nStudies done by psychologists Eleanor J. Gibson and Richard D. Walk have further explained the nature of this fear.  One of their more famous studies is the \"visual cliff\".  Below is their description of the cliff: \n\n\u2026a board laid across a large sheet of heavy glass which is supported a foot or more above the floor. On one side of the board a sheet of patterned material is placed flush against the undersurface of the glass, giving the glass the appearance as well as the substance of solidity. On the other side a sheet of the same material is laid upon the floor; this side of the board thus becomes the visual cliff.\nThirty-six infants were tested in their experiments, ranging from six to fourteen months.   Gibson and Walk found that when placed on the board, 27 of the infants would crawl on the shallow side when called by their mothers; only three ventured off the \"edge\" of the cliff.  Many infants would crawl away from their mothers who were calling from the deep end, and some would cry because they couldn\u2019t reach their mothers without crossing an apparent chasm.  Some would pat the glass on the deep end, but even with this assurance would not crawl on the glass.  These results, although unable to prove that this fear is innate, indicate that most human infants have well developed depth perception and are able to make the connection between depth and the danger that accompanies falling.\nIn May 1998, Behaviour Research and Therapy published a longitudinal survey by psychologists Richie Poulton, Simon Davies, Ross G. Menzies, John D. Langley, and Phil A. Silva of subjects sampled from the Dunedin Multidisciplinary Health and Development Study who had been injured in a fall between the ages of 5 and 9, compared them to children who had no similar injury, and found that at age 18, acrophobia was present in only 2 percent of the subjects who had an injurious fall but was present among 7 percent of subjects who had no injurious fall (with the same sample finding that typical basophobia was 7 times less common in subjects at age 18 who had injurious falls as children than subjects that did not). Psychiatrists Isaac Marks and Randolph M. Nesse and evolutionary biologist George C. Williams have noted that people with systematically deficient responses to various adaptive phobias (e.g. basophobia, ophidiophobia, arachnophobia) are more temperamentally careless and more likely to receive unintentional injuries that are potentially fatal and have proposed that such deficient phobia should be classified as \"hypophobia\" due to its selfish genetic consequences.\n\nElderly persons\nFor a long time, the fear of falling was merely believed to be a result of the psychological trauma of a fall, also called \"post-fall syndrome\". This syndrome was first mentioned in 1982 by Murphy and Isaacs,  who noticed that after a fall, ambulatory persons developed intense fear and walking disorders. Fear of falling has been identified as one of the key symptoms of this syndrome. Since that time, FOF has gained recognition as a specific health problem among older adults. However, FOF was also commonly found among elderly persons who had not yet experienced a fall.\nPrevalence of FOF appears to increase with age and to be higher in women. Age remains significant in multiple logistic regression analyses. The results of different studies have reported gender as a somewhat significant risk factor for fear of falling. Other risk factors of fear of falling in the elderly include dizziness, self-rated health status, depression, and problems with gait and balance.\n\nIn animals\nStudies of nonhuman subjects support the theory that falling is an inborn fear.  Gibson and Walk performed identical experiments with chicks, turtles, rats, kids, lambs, kittens, and puppies.  The results were similar to those of the human infants, although each animal behaved a little differently according to the characteristics of its species.\nThe chicks were tested less than 24 hours after hatching.  It suggested that depth perception develops quickly in chickens, as the chicks never made the \"mistake\" of walking off the \"deep\" side of the cliff.  The kids and lambs were also tested as soon as they could stand on their own.  During the experiment, no goat or lamb ever stepped onto the glass of the deep side.  When placed there, the animals displayed typical behavior by going into a posture of defense, with their front legs rigid and their back legs limp.  In this state of immobility, the animals were pushed forward across the glass until their head and field of vision crossed the solid edge on the opposite side of the cliff; the goats and lambs would then relax and proceed to spring forward upon its surface.  Based on the results of the animals tested, the danger and fear of falling is instilled in animals at a very young age.\n\nFactors that influence the fear of falling\nPostural control\nThe postural control system has two functions:  to ensure that balance is maintained by bracing the body against gravity, and to fix the orientation and position of the features that serve as a frame of reference for perception and action with respect to the external world. Postural control relies on multisensory processing and motor responses that seem to be automatic and occur without awareness.  Studies have shown that people afraid of heights or falling have poor postural control, especially in the absence of strong visual cues.  These individuals rely heavily on vision to regulate their posture and balance.   When faced with high or unstable ground, the vestibular system in these individuals senses the instability and attempts to correct it by increasing postural sway to reactivate visual balance feedback (postural sway refers to the phenomenon of constant displacement and correction of the position of the center of gravity within the base of support). This often fails, however, resulting in a feeling of increased instability and anxiety, which is often interpreted as fear.\n\nHeight vertigo\nClosely related to postural control is the sensation of vertigo: a warning signal created by a loss of postural control when the distance between the observer and visible stationary objects becomes too large, and caused by a dysfunction of the vestibular system in the inner ear.  In short, it is the feeling of motion when one is actually stationary. Symptoms of vertigo include dizziness, nausea, vomiting, shortness of breath, and the inability to walk or stand.  Some individuals are more reliant on visual cues to control posture than others.  Vestibular sensations can arise when unsound information is detected along the sensory channels (this happens even to those with normal vestibular function), and feelings of vertigo can result in people with postural control issues.\n\nSpace and motion discomfort\nStudies have shown that people with acrophobia and\/or an extreme fear of falling have higher scores of SMD, or space and motion discomfort.  These are physical symptoms elicited by visual or kinesthetic information that is inadequate for normal spatial orientation.  Space and motion discomfort arises when conflicting information is detected among visual, kinesthetic, and vestibular sensory channels.  Evidence has supported the claim that patients with anxiety and SMD rely more heavily on visual cues for postural changes.\n\nFalling in dreams\nAccording to Sigmund Freud's The Interpretation of Dreams, falling dreams fall under the category of \"typical dreams\", meaning the \"dreams which almost everyone has dreamt alike and which we are accustomed to assume must have the same meaning for everyone\". In the fairly recent study, \"The Typical Dreams of Canadian University Students\", common dreams were investigated by administering a Typical Dreams Questionnaire (TDQ).  The results confirmed that typical dreams are consistent over time, region, and gender, and a few themes can be considered almost universal: falling (73.8% prevalence), flying or soaring in the air (48.3%) and swimming (34.3%).  In 1967, Saul and Curtis published a paper entitled \"Dream Form and Strength of Impulse in Dreams of Falling and Other Dreams of Descent\".  According to Saul and Curtis, dreams of falling can have various meanings, such as the sensation of falling asleep, the symbolization of a real risk of falling from bed, the repetition of traumatic experiences of falling or sensations of falling from parents\u2019 arms in childhood, birth and delivery, ambition or the renouncement of responsibility, or life experiences such as flying in an airplane.  They quote another author, Gutheil (1951), who suggests a range of possible meanings subsumed under the general idea of loss of (mental) equilibrium.  These include loss of temper, loss of self-control, yielding, decline of the accepted moral standard or loss of consciousness.  Studies performed in recent years on the dream patterns of a group of 685 students attending secondary schools in Milan have concluded that, in dreams, fear is more frequently associated with falling, while happiness is connected with flying, and surprise with suspension and vertical movement (climbing, descent, ladder) content.\n\nMedia treatment\nIn the Alfred Hitchcock film Vertigo, the hero, played by James Stewart, has to resign from the police force after an incident which causes him to develop both acrophobia and vertigo. Early on in the film he faints while climbing a stepladder. There are numerous references throughout the film to fear of heights and falling.\n\nSee also\nAccident-proneness\nAcrophobia \u2013 Extreme fear of heights\nFear of flying\nHead for heights \u2013 Having no acrophobia or vertigo\nList of phobias \u2013 List of fear of different things or objects\nVertigo \u2013 Type of dizziness where a person has the sensation of moving or surrounding objects moving\n\n\n== References ==","78":"Gentamicin is an aminoglycoside antibiotic used to treat several types of bacterial infections. This may include bone infections, endocarditis, pelvic inflammatory disease, meningitis, pneumonia, urinary tract infections, and sepsis among others. It is not effective for gonorrhea or chlamydia infections. It can be given intravenously, by intramuscular injection, or topically. Topical formulations may be used in burns or for infections of the outside of the eye. It is often only used for two days until bacterial cultures determine what specific antibiotics the infection is sensitive to. The dose required should be monitored by blood testing.\nGentamicin can cause inner ear problems and kidney problems. The inner ear problems can include problems with balance and hearing loss. These problems may be permanent. If used during pregnancy, it can cause harm to the developing fetus. However, it appears to be safe for use during breastfeeding. Gentamicin is a type of aminoglycoside  and works by disrupting the ability of the bacteria to make proteins, which typically kills the bacteria.\nGentamicin is naturally produced by the bacterium Micromonospora purpurea, was patented in 1962, approved for medical use in 1964. The antibiotic is collected from the culture of the Micromonospora by perforating the cell wall of the bacterium. Current research is underway to understand the biosynthesis of this antibiotic in an attempt to increase expression and force secretion of gentamicin for higher titer. Gentamicin is on the World Health Organization's List of Essential Medicines. The World Health Organization classifies gentamicin as critically important for human medicine. It is available as a generic medication.\n\nMedical uses\nGentamicin is active against a wide range of bacterial infections, mostly Gram-negative bacteria including Pseudomonas, Proteus, Escherichia coli, Klebsiella pneumoniae, Enterobacter aerogenes, Serratia, and the Gram-positive Staphylococcus. Gentamicin is used in the treatment of respiratory tract infections, urinary tract infections, blood, bone and soft tissue infections of these susceptible bacteria.\nThere is insufficient evidence to support gentamicin as the first line treatment of Neisseria gonorrhoeae infection. Gentamicin is not used for Neisseria meningitidis or Legionella pneumophila bacterial infections (because of the risk of the person going into shock from lipid A endotoxin found in certain Gram-negative organisms). Gentamicin is also useful against Yersinia pestis (responsible for plague), its relatives, and Francisella tularensis (the organism responsible for tularemia often seen in hunters and trappers).\nSome Enterobacteriaceae, Pseudomonas spp., Enterococcus spp., Staphylococcus aureus and other Staphylococcus spp. have varying degrees of resistance to gentamicin.\n\nSpecial populations\nPregnancy and breastfeeding\nGentamicin is not recommended in pregnancy unless the benefits outweigh the risks for the mother. Gentamicin can cross the placenta and several reports of irreversible bilateral congenital deafness in children have been seen. Intramuscular injection of gentamicin in mothers can cause muscle weakness in the newborn.\nThe safety and efficacy for gentamicin in nursing mothers has not been established. Detectable gentamicin levels are found in human breast milk and in nursing babies.\n\nElderly\nIn the elderly, renal function should be assessed before beginning therapy as well as during treatment due to a decline in glomerular filtration rate. Gentamicin levels in the body can remain higher for a longer period of time in this population. Gentamicin should be used cautiously in persons with renal, auditory, vestibular, or neuromuscular dysfunction.\n\nChildren\nGentamicin may not be appropriate to use in children, including babies. Studies have shown higher serum levels and a longer half-life in this population. Kidney function should be checked periodically during therapy. Long-term effects of treatment can include hearing loss and balance problems. Hypocalcemia, hypokalemia, and muscle weakness have been reported when used by injection.\n\nContraindications\nGentamicin should not be used if a person has a history of hypersensitivity, such as anaphylaxis, or other serious toxic reaction to gentamicin or any other aminoglycosides. Greater care is required in people with myasthenia gravis and other neuromuscular disorders as there is a risk of worsening weakness. Gentamicin should also be avoided when prescribing empirical antibiotics in the setting of possible infant botulism (Ampicillin with Gentamicin is commonly used as empiric therapy in infants) also due to worsening of neuromuscular function.\n\nAdverse effects\nAdverse effects of gentamicin can range from less severe reactions, such as nausea and vomiting, to more severe reactions including:\n\nLow blood cell counts\nAllergic reactions\nNeuromuscular problems\nNerve damage (neuropathy)\nKidney damage (nephrotoxicity)\nEar disorders (ototoxicity)\nNephrotoxicity and ototoxicity are thought to be dose related with higher doses causing greater chance of toxicity. These two toxicities may have delayed presentation, sometimes not appearing until after completing treatment.\n\nKidney damage\nKidney damage is a problem in 10\u201325% of people who receive aminoglycosides, and gentamicin is one of the most nephrotoxic drugs of this class. Oftentimes, acute nephrotoxicity is reversible, but it may be fatal. The risk of nephrotoxicity can be affected by the dose, frequency, duration of therapy, and concurrent use of certain medications, such as NSAIDs, diuretics, cisplatin, ciclosporin, cephalosporins, amphotericin, iodide contrast media, and vancomycin.\nFactors that increase risk of nephrotoxicity include:\n\nIncreased age\nReduced renal function\nPregnancy\nHypothyroidism\nHepatic dysfunction\nVolume depletion\nMetabolic acidosis\nSodium depletion\nKidney dysfunction is monitored by measuring creatinine in the blood, electrolyte levels, urine output, presence of protein in the urine,  and concentrations of other chemicals, such as urea, in the blood.\n\nInner ear\nAbout 11% of the population who receives aminoglycosides experience damage to their inner ear. The common symptoms of inner ear damage include tinnitus, hearing loss, vertigo, trouble with coordination, and dizziness. Chronic use of gentamicin can affect two areas of the ears. First, damage of the inner ear hair cells can result in irreversible hearing loss. Second, damage to the inner ear vestibular apparatus can lead to balance problems. To reduce the risk of ototoxicity during treatment, it is recommended to stay hydrated.\nFactors that increase the risk of inner ear damage include:\n\nIncreased age\nHigh blood uric acid levels\nKidney dysfunction\nLiver dysfunction\nHigher doses\nLong courses of therapy\nAlso taking strong diuretics (e.g., furosemide)\n\nPharmacology\nMechanism of action\nGentamicin is a bactericidal antibiotic that works by binding the 30S subunit of the bacterial ribosome, negatively impacting protein synthesis. The primary mechanism of action is generally accepted to work through ablating the ability of the ribosome to discriminate on proper transfer RNA and messenger RNA interactions. Typically, if an incorrect tRNA pairs with an mRNA codon at the aminoacyl site of the ribosome, adenosines 1492 and 1493 are excluded from the interaction and retract, signaling the ribosome to reject the aminoacylated tRNA::Elongation Factor Thermo-Unstable complex. However, when gentamicin binds at helix 44 of the 16S rRNA, it forces the adenosines to maintain the position they take when there is a correct, or cognate, match between aa-tRNA and mRNA. This leads to the acceptance of incorrect aa-tRNAs, causing the ribosome to synthesize proteins with wrong amino acids placed throughout (roughly every 1 in 500). The non-functional, mistranslated proteins misfold and aggregate, eventually leading to death of the bacterium. Moreover,  it has been observed that gentamicin can cause a substantial slowdown in the overall elongation rate of peptide chains in live bacterial cells, independent of the misincorporation of amino acids. This finding indicates that gentamicin not only induces errors in protein synthesis but also broadly hampers the efficiency of the translation process itself.  An additional mechanism has been proposed based on crystal structures of gentamicin in a secondary binding site at helix 69 of the 23S rRNA, which interacts with helix 44 and proteins that recognize stop codons. At this secondary site, gentamicin is believed to preclude interactions of the ribosome with ribosome recycling factors, causing the two subunits of the ribosome to stay complexed even after translation completes, creating a pool of inactive ribosomes that can no longer re-initiate and translate new proteins.\n\nChemistry\nStructure\nSince gentamicin is derived from the species Micromonospora, the backbone for this antibiotic is the aminocyclitol 2-deoxystreptamine. This six carbon ring is substituted at the carbon positions 4 and 6 by the amino sugar molecules cyclic purpurosamine and garosamine, respectively. The gentamicin complex, is differentiated into five major components (C1, C1a, C2, C2a, C2b) and multiple minor components by substitution at the 6' carbon of the purpurosamine unit indicated in the image to the right by R1 and R2. The R1 and R2 can have the follow substitutions for some of the species in the gentamicin complex.\n\nGentamicins consist of three hexosamines: gentosamine\/garosamine, 2-deoxystreptamine, and purpurosamine (see illustrations, from left to right).\n\nKanamycins and tobramycin exhibit similar structures. Sisomicin is 4,5-dehydrogentamicin-C1a.\n\nComponents\nGentamicin is composed of a number of related gentamicin components and fractions which have varying degrees of antimicrobial potency. The main components of gentamicin include members of the gentamicin C complex: gentamicin C1, gentamicin C1a, and gentamicin C2 which compose approximately 80% of gentamicin and have been found to have the highest antibacterial activity. Gentamicin A, B, X, and a few others make up the remaining 20% of gentamicin and have lower antibiotic activity than the gentamicin C complex. The exact composition of a given sample or lot of gentamicin is not well defined, and the level of gentamicin C components or other components in gentamicin may differ from lot-to-lot depending on the gentamicin manufacturer or manufacturing process. Because of this lot-to-lot variability, it can be difficult to study various properties of gentamicin including pharmacokinetics and microorganism susceptibility if there is an unknown combination of chemically related but different compounds.\n\nBiosynthesis\nThe complete biosynthesis of gentamicin is not entirely elucidated. The genes controlling the biosynthesis of gentamicin are of particular interest due to the difficulty in obtaining the antibiotic after production. Since gentamicin is collected at the cell surface and the cell surface must be perforated somehow to obtain the antibiotic. Many propose the amount of gentamicin collected after production could increase if the genes are identified and re-directed to secrete the antibiotic instead of collecting gentamicin at the cell surface. Literature also agrees with the gentamicin biosynthesis pathway starting with D-Glucose-6-phosphate being dephopsphorylated, transaminated, dehydrogenated and finally glycosylated with D-glucosamine to generate paromamine inside Micromonospora echinospora. The addition of D-xylose leads to the first intermediate of the gentamicin C complex pathway, gentamicin A2. Gentamicin A2 is C-methylated and epimerized into gentamicin X2, the first branch point of this biosynthesis pathway\nWhen X2 is acted on by the cobalamin-dependent radical S-adenosyl-L-methionine enzyme GenK, the carbon position 6' is methylated to form the pharmacologically active intermediate G418 G418 then undergoes dehydrogenation and amination at the C6' position  by the dehydrogenase gene, GenQ, to generate the pharmacologically active JI-20B, although another intermediate, 6'-dehydro-6'oxo-G418 (6'DOG) is proposed to be in-between this step and for which the gene GenB1 is proposed as the aminating gene. JI-20B is dehydroxylated and epimerized to first component of the gentamicin C complex, gentamicin C2a which then undergoes an epimerization by GenB2 and then a N-methylation by an unconfirmed gene to form the final product in this branch point, gentamicin C1.\nWhen X2 bypasses GenK and is directly dehydrogenated and aminated by the GenQ enzyme, the other pharmacologically relevant intermediate JI-20A is formed. Although, there has been identification of an intermediate for this step, 6'-dehydro-6'-oxo-gentamicin X2 (6'-DOX), for which the enzyme GenB1 is purposed as the aminating enzyme. JI-20A is then dehydroxylated into the first component of the gentamicin C complex for this branch, gentamicin C1a via a catalytic reaction with GenB4. C1a then undergoes an N-methylation by an unconfirmed enzyme to form the final component, gentamicin C2b.\n\nFermentation\nGentamicin is only synthesized via submerged fermentation and inorganic sources of nutrients have been found to reduce production. Traditional fermentation used yeast beef broth, but there has been research into optimizing the growth medium for producing gentamicin C complex due to the C complex currently being the only pharmaceutically relevant component. The main components of the growth medium are carbon sources, mainly sugars, but several studies found increased gentamicin production by adding vegetable and fish oils and decreased gentamicin production with the addition of glucose, xylose and several carboxylic acids. Tryptone and various forms of yeast and yeast derivatives are traditionally used as the nitrogen source in the growth medium, but several amino acids, soybean meal, corn steep liquor, ammonium sulfate, and ammonium chloride have proven to be beneficial additives. Phosphate ions, metal ions (cobalt and a few others at low concentration), various vitamins (mostly B vitamins), purine and pyrimidine bases are also supplemented into the growth medium to increase gentamicin production, but the margin of increase is dependent on the species of Micromonospora and the other components in the growth medium. With all of these aforementioned additives, pH and aeration are key determining factors for the amount of gentamicin produced. A range of pH from 6.8 to 7.5 is used for gentamicin biosynthesis and the aeration is determined by independent experimentation reliant on type of growth medium and species of Micromonospora.\n\nHistory\nGentamicin is produced by the fermentation of Micromonospora purpurea. It was discovered in 1963 by Weinstein, Wagman et al. at Schering Corporation in Bloomfield, N.J. while working with source material (soil samples) provided by Rico Woyciesjes. When M. purpurea grows in culture it is a vivid purple colour similar to the colour of the dye Gentian Violet and hence this was why Gentamicin took then name it did. Subsequently, it was purified and the structures of its three components were determined by Cooper, et al., also at the Schering Corporation. It was initially used as a topical treatment for burns at burn units in Atlanta and San Antonio and was introduced into IV usage in 1971. It remains a mainstay for use in sepsis.\nIt is synthesized by Micromonospora, a genus of Gram-positive bacteria widely present in the environment (water and soil). According to the American Medical Association Committee on Generic Names, antibiotics not produced by Streptomyces should not use y in the ending of the name, and to highlight their specific biological origins, gentamicin and other related antibiotics produced by this genus (verdamicin, mutamicin, sisomicin, netilmicin, and retymicin) have their spellings ending in ~micin and not in ~mycin.\n\nResearch\nGentamicin is also used in molecular biology research as an antibacterial agent in tissue and cell culture, to prevent contamination of sterile cultures. Gentamicin is one of the few heat-stable antibiotics that remain active even after autoclaving, which makes it particularly useful in the preparation of some microbiological growth media.\n\nReferences\nFurther reading\nExternal links\n\"Gentamicin\". Drug Information Portal. U.S. National Library of Medicine.","79":"Gradenigo's syndrome, also called Gradenigo-Lannois syndrome, is a complication of otitis media and mastoiditis involving the apex of the petrous temporal bone.  It was first described by Giuseppe Gradenigo in 1904.\n\nSymptoms\nComponents of the syndrome include:\n\nretroorbital pain due to pain in the area supplied by the ophthalmic branch of the trigeminal nerve (fifth cranial nerve),\nabducens nerve palsy (sixth cranial nerve)\notitis media\nOther symptoms can include photophobia, excessive lacrimation, fever, and reduced corneal sensitivity. The syndrome is classically caused by the spread of an infection into the petrous apex of the temporal bone.\n\nDiagnosis\nThe constellation of symptoms was first described as a consequence of severe, advanced ear infection which has spread to a central portion of the temporal bone of the skull. This type of presentation was common prior to development of antibiotic treatments, and is now a rare complication.\nIn persons with longstanding ear infection and typical symptoms, medical imaging such as CT or MRI of the head may show changes that confirm disease involvement of the petrous apex of temporal bone.\n\nTreatment\nThe medical treatment is done with antibiotics: ceftriaxone plus metronidazole (which covers anaerobic bacteria). Depending on the duration of the infection, the severity, and which complications have arisen, it may also be necessary to perform surgery. Due to critical structures that block surgical access, it is not possible to completely remove the petrous apex. The focus is therefore on providing adequate drainage of the affected air cells.\n\nEponym\nIt is named after Count Giuseppe Gradenigo, an Italian Otolaryngologist, and Maurice Lannois.\n\nReferences\n\n\n== External links ==","80":"To have a head for heights  means that one has no acrophobia (irrational fear of heights), and is also not particularly prone to fear of falling or suffering from vertigo (the spinning sensation that can be triggered, for example, by looking down from a high place).\nA head for heights is frequently cited as a requirement when mountain hiking or climbing for a particular route, as well as when paragliding and hang-gliding. It is needed for certain jobs, such as for wind turbine technicians, chimney sweeps, roofers, steeplejacks and window cleaners.\nMohawk ironworkers have worked for generations erecting New York City skyscrapers,\nbut the idea that all Mohawk people have an innate skill for doing so is a myth.\nUnlike acrophobia, a natural fear of falling is normal. When one finds oneself in an exposed place at a great height, one normally feels one\u2019s own posture as unstable. A normal fear of falling can generate feelings of anxiety, as well as autonomic symptoms like outbreaks of sweat. In someone with acrophobia, however, the fear of falling arises in situations that would not affect most people, and the fear can be much stronger, out of proportion to the situation.\n\nCauses of fear of falling\nThere are logical biological causes of fear of falling. Firstly, there is the innate so-called \"cliff edge phenomenon\", whereby even toddlers, as well as many animals, avoid large drops, even without having previously had a bad experience.\n\nVertigo is a separate condition, caused by a destabilization of the posture. When it occurs at height, it is the result of too large a distance from the eyes to the nearest visible solid object and is referred to as \"distance vertigo\" or \"height vertigo\". In order to see the object, the head starts to sway imperceptibly, and \"location reflex\" then causes the body to sway with it. At the same time, the body stabilizes its sense of position using the periphery of the retina, but if the person looks down they lose this stabilizing factor. This physiological swaying is normally counteracted by the body's vestibular system and proprioceptive sense. However, for example, if the person's sense of balance has previously been damaged, the risk of falling is increased.\n\nReferences\nLiterature\nMartin Roos: Wenn die H\u00f6he zur H\u00f6lle wird. In: DAV Panorama 1\/2008, ISSN 1437-5923\nPepi St\u00fcckl\/Georg Sojer: Bergsteigen: Lehrbuch f\u00fcr alle Spielarten des Bergsteigens, Bruckmann, Munich, 1996, ISBN 3-7654-2859-0","81":"Hearing, or auditory perception, is the ability to perceive sounds through an organ, such as an ear, by detecting vibrations as periodic changes in the pressure of a surrounding medium. The academic field concerned with hearing is auditory science.\n\nSound may be heard through solid, liquid, or gaseous matter. It is one of the traditional five senses. Partial or total inability to hear is called hearing loss.\nIn humans and other vertebrates, hearing is performed primarily by the auditory system: mechanical waves, known as vibrations, are detected by the ear and transduced into nerve impulses that are perceived by the brain (primarily in the temporal lobe). Like touch, audition requires sensitivity to the movement of molecules in the world outside the organism. Both hearing and touch are types of mechanosensation.\n\nHearing mechanism\nThere are three main components of the human auditory system: the outer ear, the middle ear, and the inner ear.\n\nOuter ear\nThe outer ear includes the pinna, the visible part of the ear, as well as the ear canal, which terminates at the eardrum, also called the tympanic membrane. The pinna serves to focus sound waves through the ear canal toward the eardrum. Because of the asymmetrical character of the outer ear of most mammals, sound is filtered differently on its way into the ear depending on the location of its origin. This gives these animals the ability to localize sound vertically. The eardrum is an airtight membrane, and when sound waves arrive there, they cause it to vibrate following the waveform of the sound. Cerumen (ear wax) is produced by ceruminous and sebaceous glands in the skin of the human ear canal, protecting the ear canal and tympanic membrane from physical damage and microbial invasion.\n\nMiddle ear\nThe middle ear consists of a small air-filled chamber that is located medial to the eardrum. Within this chamber are the three smallest bones in the body, known collectively as the ossicles which include the malleus, incus, and stapes (also known as the hammer, anvil, and stirrup, respectively). They aid in the transmission of the vibrations from the eardrum into the inner ear, the cochlea. The purpose of the middle ear ossicles is to overcome the impedance mismatch between air waves and cochlear waves, by providing impedance matching.\nAlso located in the middle ear are the stapedius muscle and tensor tympani muscle, which protect the hearing mechanism through a stiffening reflex. The stapes transmits sound waves to the inner ear through the oval window, a flexible membrane separating the air-filled middle ear from the fluid-filled inner ear. The round window, another flexible membrane, allows for the smooth displacement of the inner ear fluid caused by the entering sound waves.\n\nInner ear\nThe inner ear consists of the cochlea, which is a spiral-shaped, fluid-filled tube. It is divided lengthwise by the organ of Corti, which is the main organ of mechanical to neural transduction. Inside the organ of Corti is the basilar membrane, a structure that vibrates when waves from the middle ear propagate through the cochlear fluid \u2013 endolymph. The basilar membrane is tonotopic, so that each frequency has a characteristic place of resonance along it. Characteristic frequencies are high at the basal entrance to the cochlea, and low at the apex. Basilar membrane motion causes depolarization of the hair cells, specialized auditory receptors located within the organ of Corti. While the hair cells do not produce action potentials themselves, they release neurotransmitter at synapses with the fibers of the auditory nerve, which does produce action potentials. In this way, the patterns of oscillations on the basilar membrane are converted to spatiotemporal patterns of firings which transmit information about the sound to the brainstem.\n\nNeuronal\nThe sound information from the cochlea travels via the auditory nerve to the cochlear nucleus in the brainstem. From there, the signals are projected to the inferior colliculus in the midbrain tectum. The inferior colliculus integrates auditory input with limited input from other parts of the brain and is involved in subconscious reflexes such as the auditory startle response.\nThe inferior colliculus in turn projects to the medial geniculate nucleus, a part of the thalamus where sound information is relayed to the primary auditory cortex in the temporal lobe. Sound is believed to first become consciously experienced at the primary auditory cortex. Around the primary auditory cortex lies Wernickes area, a cortical area involved in interpreting sounds that is necessary to understand spoken words.\nDisturbances (such as stroke or trauma) at any of these levels can cause hearing problems, especially if the disturbance is bilateral. In some instances it can also lead to auditory hallucinations or more complex difficulties in perceiving sound.\n\nHearing tests\nHearing can be measured by behavioral tests using an audiometer. Electrophysiological tests of hearing can provide accurate measurements of hearing thresholds even in unconscious subjects. Such tests include auditory brainstem evoked potentials (ABR), otoacoustic emissions (OAE) and electrocochleography (ECochG). Technical advances in these tests have allowed hearing screening for infants to become widespread.\nHearing can be measured by mobile applications which includes audiological hearing test function or hearing aid application. These applications allow the user to measure hearing thresholds at different frequencies (audiogram). Despite possible errors in measurements, hearing loss can be detected.\n\nHearing loss\nThere are several different types of hearing loss: conductive hearing loss, sensorineural hearing loss and mixed types.\nThere are defined degrees of hearing loss:\n\nMild hearing loss - People with mild hearing loss have difficulties keeping up with conversations, especially in noisy surroundings. The most quiet sounds that people with mild hearing loss can hear with their better ear are between 25 and 40 dB HL.\nModerate hearing loss - People with moderate hearing loss have difficulty keeping up with conversations when they are not using a hearing aid. On average, the most quiet sounds heard by people with moderate hearing loss with their better ear are between 40 and 70 dB HL.\nSevere hearing loss - People with severe hearing loss depend on powerful hearing aid. However, they often rely on lip-reading even when they are using hearing aids. The most quiet sounds heard by people with severe hearing loss with their better ear are between 70 and 95 dB HL.\nProfound hearing loss - People with profound hearing loss are very hard of hearing and they mostly rely on lip-reading and sign language. The most quiet sounds heard by people with profound hearing loss with their better ear are from 95 dB HL or more.\n\nCauses\nHeredity\nCongenital conditions\nPresbycusis\nAcquired\nNoise-induced hearing loss\nOtotoxic drugs and chemicals\nInfection\n\nPrevention\nHearing protection is the use of devices designed to prevent noise-induced hearing loss (NIHL), a type of post-lingual hearing impairment. The various means used to prevent hearing loss generally focus on reducing the levels of noise to which people are exposed. One way this is done is through environmental modifications such as acoustic quieting, which may be achieved with as basic a measure as lining a room with curtains, or as complex a measure as employing an anechoic chamber, which absorbs nearly all sound. Another means is the use of devices such as earplugs, which are inserted into the ear canal to block noise, or earmuffs, objects designed to cover a person's ears entirely.\n\nManagement\nThe loss of hearing, when it is caused by neural loss, cannot presently be cured. Instead, its effects can be mitigated by the use of audioprosthetic devices, i.e. hearing assistive devices such as hearing aids and cochlear implants. In a clinical setting, this management is offered by otologists and audiologists.\n\nRelation to health\nHearing loss is associated with Alzheimer's disease and dementia with a greater degree of hearing loss tied to a higher risk. There is also an association between type 2 diabetes and hearing loss.\n\nHearing underwater\nHearing threshold and the ability to localize sound sources are reduced underwater in humans, but not in aquatic animals, including whales, seals, and fish which have ears adapted to process water-borne sound.\n\nIn vertebrates\nNot all sounds are normally audible to all animals. Each species has a range of normal hearing for both amplitude and frequency. Many animals use sound to communicate with each other, and hearing in these species is particularly important for survival and reproduction. In species that use sound as a primary means of communication, hearing is typically most acute for the range of pitches produced in calls and speech.\n\nFrequency range\nFrequencies capable of being heard by humans are called audio or sonic. The range is typically considered to be between 20 Hz and 20,000 Hz. Frequencies higher than audio are referred to as ultrasonic, while frequencies below audio are referred to as infrasonic. Some bats use ultrasound for echolocation while in flight. Dogs are able to hear ultrasound, which is the principle of 'silent' dog whistles. Snakes sense infrasound through their jaws, and baleen whales, giraffes, dolphins and elephants use it for communication. Some fish have the ability to hear more sensitively due to a well-developed, bony connection between the ear and their swim bladder. This \"aid to the deaf\" for fishes appears in some species such as carp and herring.\n\nTime discrimination\nHuman perception of audio signal time separation has been measured to less than 10 microseconds (10\u03bcs). This does not mean that frequencies above 100 kHz are audible, but that time discrimination is not directly coupled with frequency range. Georg Von B\u00e9k\u00e9sy in 1929 identifying sound source directions suggested humans can resolve timing differences of 10\u03bcs or less. In 1976 Jan Nordmark's research indicated inter-aural resolution better than 2\u03bcs. Milind Kuncher's 2007 research resolved time misalignment to under 10\u03bcs.\n\nIn birds\nIn invertebrates\nEven though they do not have ears, invertebrates have developed other structures and systems to decode vibrations traveling through the air, or \u201csound\u201d. Charles Henry Turner was the first scientist to formally show this phenomenon through rigorously controlled experiments in ants. Turner ruled out the detection of ground vibration and suggested that other insects likely have auditory systems as well.\nMany insects detect sound through the way air vibrations deflect hairs along their body. Some insects have even developed specialized hairs tuned to detecting particular frequencies, such as certain caterpillar species that have evolved hair with properties such that it resonates most with the sound of buzzing wasps, thus warning them of the presence of natural enemies.\nSome insects possess a tympanal organ. These are \"eardrums\", that cover air filled chambers on the legs. Similar to the hearing process with vertebrates, the eardrums react to sonar waves. Receptors that are placed on the inside translate the oscillation into electric signals and send them to the brain. Several groups of flying insects that are preyed upon by echolocating bats can perceive the ultrasound emissions this way and reflexively practice ultrasound avoidance.\n\nSee also\nBasics\n\nGeneral\n\nDisorders\n\nTest and measurement\n\nReferences\nFurther reading\nLopez-Poveda, Enrique A.; Palmer, A. R. (Alan R.); Meddis, Ray. (2010). The neurophysiological bases of auditory perception. New York: Springer. ISBN 978-1-4419-5685-9. OCLC 471801201.\n\nExternal links\nGlobal Audiology- International Society of Audiology\nWorld Health Organization, Deafness and Hearing Loss\n Media related to Hearing at Wikimedia Commons\n The dictionary definition of hearing at Wiktionary\n Quotations related to Hearing at Wikiquote\nOpen University - OpenLearn - Article about hearing Archived 2018-10-15 at the Wayback Machine\nPreventing Occupational Noise-Induced Hearing Loss, NIOSH","82":"Hearing loss is a partial or total inability to hear. Hearing loss may be present at birth or acquired at any time afterwards.  Hearing loss may occur in one or both ears. In children, hearing problems can affect the ability to acquire spoken language, and in adults it can create difficulties with social interaction and at work. Hearing loss can be temporary or permanent. Hearing loss related to age usually affects both ears and is due to cochlear hair cell loss. In some people, particularly older people, hearing loss can result in loneliness.\n\nHearing loss may be caused by a number of factors, including: genetics, ageing, exposure to noise, some infections, birth complications, trauma to the ear, and certain medications or toxins. A common condition that results in hearing loss is chronic ear infections. Certain infections during pregnancy, such as cytomegalovirus, syphilis and rubella, may also cause hearing loss in the child. Hearing loss is diagnosed when hearing testing finds that a person is unable to hear 25 decibels in at least one ear. Testing for poor hearing is recommended for all newborns. Hearing loss can be categorized as mild (25 to 40 dB), moderate (41 to 55 dB), moderate-severe (56 to 70 dB), severe (71 to 90 dB), or profound (greater than 90 dB). There are three main types of hearing loss: conductive hearing loss, sensorineural hearing loss, and mixed hearing loss.\nAbout half of hearing loss globally is preventable through public health measures. Such practices include immunization, proper care around pregnancy, avoiding loud noise, and avoiding certain medications. The World Health Organization recommends that young people limit exposure to loud sounds and the use of personal audio players to an hour a day in an effort to limit exposure to noise. Early identification and support are particularly important in children. For many, hearing aids, sign language, cochlear implants and subtitles are useful. Lip reading is another useful skill some develop. Access to hearing aids, however, is limited in many areas of the world.\nAs of 2013 hearing loss affects about 1.1 billion people to some degree. It causes disability in about 466 million people (5% of the global population), and moderate to severe disability in 124 million people. Of those with moderate to severe disability 108 million live in low and middle income countries. Of those with hearing loss, it began during childhood for 65 million. Those who use sign language and are members of Deaf culture may see themselves as having a difference rather than a disability. Many members of Deaf culture oppose attempts to cure deafness and some within this community view cochlear implants with concern as they have the potential to eliminate their culture.\n\nDefinition\nHearing loss is defined as diminished acuity to sounds which would otherwise be heard normally. The terms hearing impaired or hard of hearing are usually reserved for people who have relative inability to hear sound in the speech frequencies. Hearing loss occurs when sound waves enter the ears and damage the sensitive tissues The severity of hearing loss is categorized according to the increase in intensity of sound above the usual level required for the listener to detect it.\nDeafness is defined as a degree of loss such that a person is unable to understand speech, even in the presence of amplification. In profound deafness, even the highest intensity sounds produced by an audiometer (an instrument used to measure hearing by producing pure tone sounds through a range of frequencies) may not be detected. In total deafness, no sounds at all, regardless of amplification or method of production, can be heard.\nSpeech perception is another aspect of hearing which involves the perceived clarity of a word rather than the intensity of sound made by the word. In humans, this is usually measured with speech discrimination tests, which measure not only the ability to detect sound, but also the ability to understand speech. There are very rare types of hearing loss that affect speech discrimination alone.  One example is auditory neuropathy, a variety of hearing loss in which the outer hair cells of the cochlea are intact and functioning, but sound information is not faithfully transmitted by the auditory nerve to the brain.\nUse of the terms \"hearing impaired\", \"deaf-mute\", or \"deaf and dumb\" to describe deaf and hard of hearing people is discouraged by many in the deaf community as well as advocacy organizations, as they are offensive to many deaf and hard of hearing people.\n\nHearing standards\nHuman hearing extends in frequency from 20 to 20,000 Hz, and in intensity from 0 dB to 120 dB HL or more. 0 dB does not represent absence of sound, but rather the softest sound an average unimpaired human ear can hear; some people can hear down to \u22125 or even \u221210 dB.  Sound is generally uncomfortably loud above 90 dB and 115 dB represents the threshold of pain. The ear does not hear all frequencies equally well: hearing sensitivity peaks around 3,000 Hz.  There are many qualities of human hearing besides frequency range and intensity that cannot easily be measured quantitatively. However, for many practical purposes, normal hearing is defined by a frequency versus intensity graph, or audiogram, charting sensitivity thresholds of hearing at defined frequencies.  Because of the cumulative impact of age and exposure to noise and other acoustic insults, 'typical' hearing may not be normal.\n\nSigns and symptoms\ndifficulty using the telephone\nloss of sound localization\ndifficulty understanding speech, especially of children and women whose voices are of a higher frequency.\ndifficulty understanding speech in the presence of background noise (cocktail party effect)\nsounds or speech sounding dull, muffled or attenuated\nneed for increased volume on television, radio, music and other audio sources\nHearing loss is sensory, but may have accompanying symptoms:\n\npain or pressure in the ears\na blocked feeling\nThere may also be accompanying secondary symptoms:\n\nhyperacusis, heightened sensitivity with accompanying auditory pain to certain intensities and frequencies of sound, sometimes defined as \"auditory recruitment\"\ntinnitus, ringing, buzzing, hissing or other sounds in the ear when no external sound is present\nvertigo and disequilibrium\ntympanophonia, also known as autophonia, abnormal hearing of one's own voice and respiratory sounds, usually as a result of a patulous (a constantly open) eustachian tube or dehiscent superior semicircular canals\ndisturbances of facial movement (indicating a possible tumour or stroke) or in persons with Bell's palsy\n\nComplications\nHearing loss is associated with Alzheimer's disease and dementia. The risk increases with the hearing loss degree. There are several hypotheses including cognitive resources being redistributed to hearing and social isolation from hearing loss having a negative effect. According to preliminary data, hearing aid usage can slow down the decline in cognitive functions.\nHearing loss is responsible for causing thalamocortical dysrthymia in the brain which is a cause for several neurological disorders including tinnitus and visual snow syndrome.\n\nCognitive decline\nHearing loss is an increasing concern especially in aging populations. The prevalence of hearing loss increases about two-fold for each decade increase in age after age 40. While the secular trend might decrease individual level risk of developing hearing loss, the prevalence of hearing loss is expected to rise due to the aging population in the US. Another concern about aging process is cognitive decline, which may progress to mild cognitive impairment and eventually dementia. The association between hearing loss and cognitive decline has been studied in various research settings. Despite the variability in study design and protocols, the majority of these studies have found consistent association between age-related hearing loss and cognitive decline, cognitive impairment, and dementia. The association between age-related hearing loss and Alzheimer's disease was found to be nonsignificant, and this finding supports the hypothesis that hearing loss is associated with dementia independent of Alzheimer pathology. There are several hypotheses about the underlying causal mechanism for age-related hearing loss and cognitive decline. One hypothesis is that this association can be explained by common etiology or shared neurobiological pathology with decline in other physiological system. Another possible cognitive mechanism emphasize on individual's cognitive load. As people developing hearing loss in the process of aging, the cognitive load demanded by auditory perception increases, which may lead to change in brain structure and eventually to dementia. One other hypothesis suggests that the association between hearing loss and cognitive decline is mediated through various psychosocial factors, such as decrease in social contact and increase in social isolation. Findings on the association between hearing loss and dementia have significant public health implication, since about 9% of dementia cases are associated with hearing loss.\n\nFalls\nFalls have important health implications, especially for an aging population where they can lead to significant morbidity and mortality. Elderly people are particularly vulnerable to the consequences of injuries caused by falls, since older individuals typically have greater bone fragility and poorer protective reflexes. Fall-related injury can also lead to burdens on the financial and health care systems. In literature, age-related hearing loss is found to be significantly associated with incident falls. There is also a potential dose-response relationship between hearing loss and falls\u2014greater severity of hearing loss is associated with increased difficulties in postural control and increased prevalence of falls. The underlying causal link between the association of hearing loss and falls is yet to be elucidated. There are several hypotheses that indicate that there may be a common process between decline in auditory system and increase in incident falls, driven by physiological, cognitive, and behavioral factors. This evidence suggests that treating hearing loss has potential to increase health-related quality of life in older adults.\n\nDepression\nDepression is one of the leading causes of morbidity and mortality worldwide. In older adults, the suicide rate is higher than it is for younger adults, and more suicide cases are attributable to depression. Different studies have been done to investigate potential risk factors that can give rise to depression in later life. Some chronic diseases are found to be significantly associated with risk of developing depression, such as coronary heart disease, pulmonary disease, vision loss and hearing loss. Hearing loss can attribute to decrease in health-related quality of life, increase in social isolation and decline in social engagement, which are all risk factors for increased risk of developing depression symptoms.\n\nSpoken language ability\nPost-lingual deafness is hearing loss that is sustained after the acquisition of language, which can occur due to disease, trauma, or as a side-effect of a medicine. Typically, hearing loss is gradual and often detected by family and friends of affected individuals long before the patients themselves will acknowledge the disability. Post-lingual deafness is far more common than pre-lingual deafness. Those who lose their hearing later in life, such as in late adolescence or adulthood, face their own challenges, living with the adaptations that allow them to live independently.\nPrelingual deafness is profound hearing loss that is sustained before the acquisition of language, which can occur due to a congenital condition or through hearing loss before birth or in early infancy. Prelingual deafness impairs an individual's ability to acquire a spoken language in children, but deaf children can acquire spoken language through support from cochlear implants (sometimes combined with hearing aids). Non-signing (hearing) parents of deaf babies (90\u201395% of cases) usually go with oral approach without the support of sign language, as these families lack previous experience with sign language and cannot competently provide it to their children without learning it themselves. This may in some cases (late implantation or not sufficient benefit from cochlear implants) bring the risk of language deprivation for the deaf baby because the deaf baby would not have a sign language if the child is unable to acquire spoken language successfully. The 5\u201310% of cases of deaf babies born into signing families have the potential of age-appropriate development of language due to early exposure to a sign language by sign-competent parents, thus they have the potential to meet language milestones, in sign language in lieu of spoken language.\n\nCauses\nHearing loss has multiple causes, including ageing, genetics, perinatal problems and acquired causes like noise and disease.  For some kinds of hearing loss the cause may be classified as of unknown cause.\nThere is a progressive loss of ability to hear high frequencies with aging known as presbycusis. For men, this can start as early as 25 and women at 30. Although genetically variable, it is a normal concomitant of ageing and is distinct from hearing losses caused by noise exposure, toxins or disease agents. Common conditions that can increase the risk of hearing loss in elderly people are high blood pressure, diabetes (hearing loss in diabetes), or the use of certain medications harmful to the ear. While everyone loses hearing with age, the amount and type of hearing loss is variable.\nNoise-induced hearing loss (NIHL), also known as acoustic trauma, typically manifests as elevated hearing thresholds (i.e. less sensitivity or muting). Noise exposure is the cause of approximately half of all cases of hearing loss, causing some degree of problems in 5% of the population globally. The majority of hearing loss is not due to age, but due to noise exposure. Various governmental, industry and standards organizations set noise standards. Many people are unaware of the presence of environmental sound at damaging levels, or of the level at which sound becomes harmful. Common sources of damaging noise levels include car stereos, children's toys, motor vehicles, crowds, lawn and maintenance equipment, power tools, gun use, musical instruments, and even hair dryers. Noise damage is cumulative; all sources of damage must be considered to assess risk. In the US, 12.5% of children aged 6\u201319 years have permanent hearing damage from excessive noise exposure.  The World Health Organization estimates that half of those between 12 and 35 are at risk from using personal audio devices that are too loud. Hearing loss in adolescents may be caused by loud noise from toys, music by headphones, and concerts or events.\nHearing loss can be inherited. Around 75\u201380% of all these cases are inherited by recessive genes, 20\u201325% are inherited by dominant genes, 1\u20132% are inherited by X-linked patterns, and fewer than 1% are inherited by mitochondrial inheritance.  Syndromic deafness occurs when there are other signs or medical problems aside from deafness in an individual, such as Usher syndrome, Stickler syndrome, Waardenburg syndrome, Alport's syndrome, and neurofibromatosis type 2. Nonsyndromic deafness occurs when there are no other signs or medical problems associated with the deafness in an individual.\nFetal alcohol spectrum disorders are reported to cause hearing loss in up to 64% of infants born to alcoholic mothers, from the ototoxic effect on the developing fetus plus malnutrition during pregnancy from the excess alcohol intake. Premature birth can be associated with sensorineural hearing loss because of an increased risk of hypoxia, hyperbilirubinaemia, ototoxic medication and infection as well as noise exposure in the neonatal units. Also, hearing loss in premature babies is often discovered far later than a similar hearing loss would be in a full-term baby because normally babies are given a hearing test within 48 hours of birth, but doctors must wait until the premature baby is medically stable before testing hearing, which can be months after birth. The risk of hearing loss is greatest for those weighing less than 1500 g at birth.\nDisorders responsible for hearing loss include auditory neuropathy, Down syndrome, Charcot\u2013Marie\u2013Tooth disease variant 1E, autoimmune disease, multiple sclerosis, meningitis, cholesteatoma, otosclerosis, perilymph fistula, M\u00e9ni\u00e8re's disease, recurring ear infections, strokes, superior semicircular canal dehiscence, Pierre Robin, Treacher-Collins, Usher Syndrome, Pendred Syndrome, and Turner syndrome, syphilis, vestibular schwannoma, and viral infections such as measles, mumps, congenital rubella (also called German measles) syndrome, several varieties of herpes viruses, HIV\/AIDS, and West Nile virus.\nSome medications may reversibly or irreversibly affect hearing. These medications are considered ototoxic. This includes loop diuretics such as furosemide and bumetanide, non-steroidal anti-inflammatory drugs (NSAIDs) both over-the-counter (aspirin, ibuprofen, naproxen) as well as prescription (celecoxib, diclofenac, etc.), paracetamol, quinine, and macrolide antibiotics. Others may cause permanent hearing loss. The most important group is the aminoglycosides (main member gentamicin) and platinum based chemotherapeutics such as cisplatin and carboplatin.\nIn addition to medications, hearing loss can also result from specific chemicals in the environment: metals, such as lead; solvents, such as toluene (found in crude oil, gasoline and automobile exhaust, for example); and asphyxiants. Combined with noise, these ototoxic chemicals have an additive effect on a person's hearing loss. Hearing loss due to chemicals starts in the high frequency range and is irreversible. It damages the cochlea with lesions and degrades central portions of the auditory system. For some ototoxic chemical exposures, particularly styrene, the risk of hearing loss can be higher than being exposed to noise alone. The effects is greatest when the combined exposure include impulse noise. A 2018 informational bulletin by the US Occupational Safety and Health Administration (OSHA) and the National Institute for Occupational Safety and Health (NIOSH) introduces the issue, provides examples of ototoxic chemicals, lists the industries and occupations at risk and provides prevention information.\nThere can be damage either to the ear, whether the external or middle ear, to the cochlea, or to the brain centers that process the aural information conveyed by the ears.  Damage to the middle ear may include fracture and discontinuity of the ossicular chain.  Damage to the inner ear (cochlea) may be caused by temporal bone fracture.  People who sustain head injury are especially vulnerable to hearing loss or tinnitus, either temporary or permanent.\n\nPathophysiology\nSound waves reach the outer ear and are conducted down the ear canal to the eardrum, causing it to vibrate. The vibrations are transferred by the 3 tiny ear bones of the middle ear to the fluid in the inner ear. The fluid moves hair cells (stereocilia), and their movement generates nerve impulses which are then taken to the brain by the cochlear nerve.  The auditory nerve takes the impulses to the brainstem, which sends the impulses to the midbrain. Finally, the signal goes to the auditory cortex of the temporal lobe to be interpreted as sound.\nHearing loss is most commonly caused by long-term exposure to loud noises, from recreation or from work, that damage the hair cells, which do not grow back on their own.\nOlder people may lose their hearing from long exposure to noise, changes in the inner ear, changes in the middle ear, or from changes along the nerves from the ear to the brain.\n\nDiagnosis\nIdentification of a hearing loss is usually conducted by a general practitioner medical doctor, otolaryngologist, certified and licensed audiologist, school or industrial audiometrist, or other audiometric technician. Diagnosis of the cause of a hearing loss is carried out by a specialist physician (audiovestibular physician) or otorhinolaryngologist.\nHearing loss is generally measured by playing generated or recorded sounds, and determining whether the person can hear them. Hearing sensitivity varies according to the frequency of sounds. To take this into account, hearing sensitivity can be measured for a range of frequencies and plotted on an audiogram. Other method for quantifying hearing loss is a hearing test using a mobile application or hearing aid application, which includes a hearing test. Hearing diagnosis using mobile application is similar to the audiometry procedure. Audiograms, obtained using mobile applications, can be used to adjust hearing aid applications. Another method for quantifying hearing loss is a speech-in-noise test. which gives an indication of how well one can understand speech in a noisy environment. Otoacoustic emissions test is an objective hearing test that may be administered to toddlers and children too young to cooperate in a conventional hearing test. Auditory brainstem response testing is an electrophysiological test used to test for hearing deficits caused by pathology within the ear, the cochlear nerve and also within the brainstem.\nA case history (usually a written form, with questionnaire) can provide valuable information about the context of the hearing loss, and indicate what kind of diagnostic procedures to employ. Examinations include otoscopy, tympanometry, and differential testing with the Weber, Rinne, Bing and Schwabach tests. In case of infection or inflammation, blood or other body fluids may be submitted for laboratory analysis.  MRI and CT scans can be useful to identify the pathology of many causes of hearing loss.\nHearing loss is categorized by severity, type, and configuration. Furthermore, a hearing loss may exist in only one ear (unilateral) or in both ears (bilateral).  Hearing loss can be temporary or permanent, sudden or progressive. The severity of a hearing loss is ranked according to ranges of nominal thresholds in which a sound must be so it can be detected by an individual. It is measured in decibels of hearing loss, or dB HL. There are three main types of hearing loss: conductive hearing loss, sensorineural hearing loss, and mixed hearing loss. An additional problem which is increasingly recognised is auditory processing disorder which is not a hearing loss as such but a difficulty perceiving sound. The shape of an audiogram shows the relative configuration of the hearing loss, such as a Carhart notch for otosclerosis, 'noise' notch for noise-induced damage, high frequency rolloff for presbycusis, or a flat audiogram for conductive hearing loss. In conjunction with speech audiometry, it may indicate central auditory processing disorder, or the presence of a schwannoma or other tumor.\nPeople with unilateral hearing loss or single-sided deafness (SSD) have difficulty in hearing conversation on their impaired side, localizing sound, and understanding speech in the presence of background noise.  One reason for the hearing problems these patients often experience is due to the head shadow effect.\nIdiopathic sudden hearing loss is a condition where a person as an immediate decrease in the sensitivity of their sensorineural hearing that does not have a known cause. This type of loss is usually only on one side (unilateral) and the severity of the loss varies. A common threshold of a \"loss of at least 30 dB in three connected frequencies within 72 hours\" is sometimes used, however there is no universal definition or international consensus for diagnosing idiopathic sudden hearing loss.\n\nPrevention\nIt is estimated that half of cases of hearing loss are preventable. About 60% of hearing loss in children under the age of 15 can be avoided. There are a number of effective preventative strategies, including: immunization against rubella to prevent congenital rubella syndrome, immunization against H. influenza and S. pneumoniae to reduce cases of meningitis, and avoiding or protecting against excessive noise exposure. The World Health Organization also recommends immunization against measles, mumps, and meningitis, efforts to prevent premature birth, and avoidance of certain medication as prevention. World Hearing Day is a yearly event to promote actions to prevent hearing damage.\nAvoiding exposure to loud noise can help prevent noise-induced hearing loss. 18% of adults exposed to loud noise at work for five years or more report hearing loss in both ears as compared to 5.5% of adults who were not exposed to loud noise at work. Different programs exist for specific populations such as school-age children, adolescents and workers.  But the HPD (without individual selection, training and fit testing) does not significantly reduce the risk of hearing loss. The use of antioxidants is being studied for the prevention of noise-induced hearing loss, particularly for scenarios in which noise exposure cannot be reduced, such as during military operations.\n\nWorkplace noise regulation\nNoise is widely recognized as an occupational hazard. In the United States, the National Institute for Occupational Safety and Health (NIOSH) and the Occupational Safety and Health Administration (OSHA) work together to provide standards and enforcement on workplace noise levels. The hierarchy of hazard controls demonstrates the different levels of controls to reduce or eliminate exposure to noise and prevent hearing loss, including engineering controls and personal protective equipment (PPE). Other programs and initiative have been created to prevent hearing loss in the workplace. For example, the Safe-in-Sound Award was created to recognize organizations that can demonstrate results of successful noise control and other interventions. Additionally, the Buy Quiet program was created to encourage employers to purchase quieter machinery and tools. By purchasing less noisy power tools like those found on the NIOSH Power Tools Database and limiting exposure to ototoxic chemicals, great strides can be made in preventing hearing loss.\nCompanies can also provide personal hearing protector devices tailored to both the worker and type of employment. Some hearing protectors universally block out all noise, and some allow for certain noises to be heard. Workers are more likely to wear hearing protector devices when they are properly fitted.\nOften interventions to prevent noise-induced hearing loss have many components. A 2017 Cochrane review found that stricter legislation might reduce noise levels.  Providing workers with information on their sound exposure levels was not shown to decrease exposure to noise. Ear protection, if used correctly, can reduce noise to safer levels, but often, providing them is not sufficient to prevent hearing loss. Engineering noise out and other solutions such as proper maintenance of equipment can lead to noise reduction, but further field studies on resulting noise exposures following such interventions are needed. Other possible solutions include improved enforcement of existing legislation and better implementation of well-designed prevention programmes, which have not yet been proven conclusively to be effective. The conclusion of the Cochrane Review was that further research could modify what is now regarding the effectiveness of the evaluated interventions.\nThe Institute for Occupational Safety and Health of the German Social Accident Insurance has created a hearing impairment calculator based on the ISO 1999 model for studying threshold shift in relatively homogeneous groups of people, such as workers with the same type of job.  The ISO 1999 model estimates how much hearing impairment in a group can be ascribed to age and noise exposure.  The result is calculated via an algebraic equation that uses the A-weighted sound exposure level, how many years the people were exposed to this noise, how old the people are, and their sex. The model's estimations are only useful for people without hearing loss due to non-job related exposure and can be used for prevention activities.\n\nScreening\nThe United States Preventive Services Task Force recommends neonatal hearing screening for all newborns, as the first three years of life are believed to be the most important for language development. Universal neonatal hearing screenings have now been widely implemented across the U.S., with rates of newborn screening increasing from less than 3% in the early 1990s to 98% in 2009. Newborns whose screening reveals a high index of suspicion of hearing loss are referred for additional diagnostic testing with the goal of providing early intervention and access to language.\nThe American Academy of Pediatrics advises that children should have their hearing tested several times throughout their schooling:\n\nWhen they enter school\nAt ages 6, 8, and 10\nAt least once during middle school\nAt least once during high school\nWhile the American College of Physicians indicated that there is not enough evidence to determine the utility of screening in adults over 50 years old who do not have any symptoms, the American Language, Speech Pathology and Hearing Association recommends that adults should be screened at least every decade through age 50 and at three-year intervals thereafter, to minimize the detrimental effects of the untreated condition on quality of life. For the same reason, the US Office of Disease Prevention and Health Promotion included as one of Healthy People 2020 objectives: to increase the proportion of persons who have had a hearing examination.\n\nManagement\nManagement depends on the specific cause if known as well as the extent, type and configuration of the hearing loss. Sudden hearing loss due to an underlying nerve problem may be treated with corticosteroids.\nMost hearing loss, that result from age and noise, is progressive and irreversible, and there are currently no approved or recommended treatments.  A few specific kinds of hearing loss are amenable to surgical treatment.  In other cases, treatment is addressed to underlying pathologies, but any hearing loss incurred may be permanent. Some management options include hearing aids, cochlear implants, middle ear implants, assistive technology, and closed captioning; in movie theaters, a Hearing Impaired (HI) audio track may be available via headphones to better hear dialog.\nThis choice depends on the level of hearing loss, type of hearing loss, and personal preference. Hearing aid applications are one of the options for hearing loss management. For people with bilateral hearing loss, it is not clear if bilateral hearing aids (hearing aids in both ears) are better than a unilateral hearing aid (hearing aid in one ear).\n\nIdiopathic sudden hearing loss\nFor people with idiopathic sudden hearing loss, different treatment approaches have been suggested that are usually based on the suspected cause of the sudden hearing loss. Treatment approaches may include corticosteroid medications, rheological drugs, vasodilators, anesthetics, and other medications chosen based on the suspected underlying pathology that caused the sudden hearing loss. The evidence supporting most treatment options for idiopathic sudden hearing loss is very weak and adverse effects of these different medications is a consideration when deciding on a treatment approach.\n\nEpidemiology\nGlobally, hearing loss affects about 10% of the population to some degree. It caused moderate to severe disability in 124.2 million people as of 2004 (107.9 million of whom are in low and middle income countries). Of these 65 million acquired the condition during childhood. At birth ~3 per 1000 in developed countries and more than 6 per 1000 in developing countries have hearing problems.\nHearing loss increases with age. In those between 20 and 35 rates of hearing loss are 3% while in those 44 to 55 it is 11% and in those 65 to 85 it is 43%.\nA 2017 report by the World Health Organization estimated the costs of unaddressed hearing loss and the cost-effectiveness of interventions, for the health-care sector, for the education sector and as broad societal costs.  Globally, the annual cost of unaddressed hearing loss was estimated to be in the range of $750\u2013790 billion international dollars.\nThe International Organization for Standardization (ISO) developed the ISO 1999 standards for the estimation of hearing thresholds and noise-induced hearing impairment. They used data from two noise and hearing study databases, one presented by Burns and Robinson (Hearing and Noise in Industry, Her Majesty's Stationery Office,  London, 1970) and by Passchier-Vermeer (1968). As race are some of the factors that can affect the expected distribution of pure-tone hearing thresholds several other national or regional datasets exist, from Sweden,  Norway, South Korea, the United States and Spain.\nIn the United States hearing is one of the health outcomes measure by the National Health and Nutrition Examination Survey (NHANES), a survey research program conducted by the National Center for Health Statistics. It examines health and nutritional status of adults and children in the United States.  Data from the United States in 2011\u20132012 found that rates of hearing loss has declined among adults aged 20 to 69 years, when compared with the results from an earlier time period (1999\u20132004). It also found that adult hearing loss is associated with increasing age, sex, ethnicity, educational level, and noise exposure.  Nearly one in four adults had audiometric results suggesting noise-induced hearing loss. Almost one in four adults who reported excellent or good hearing had a similar pattern (5.5% on both sides and 18% on one side). Among people who reported exposure to loud noise at work, almost one third had such changes.\n\nSocial and cultural aspects\nPeople with extreme hearing loss may communicate through sign languages. Sign languages convey meaning through manual communication and body language instead of acoustically conveyed sound patterns. This involves the simultaneous combination of hand shapes, orientation and movement of the hands, arms or body, and facial expressions to express a speaker's thoughts. \"Sign languages are based on the idea that vision is the most useful tool a deaf person has to communicate and receive information\".\nDeaf culture refers to a tight-knit cultural group of people whose primary language is signed, and who practice social and cultural norms which are distinct from those of the surrounding hearing community. This community does not automatically include all those who are clinically or legally deaf, nor does it exclude every hearing person. According to Baker and Padden, it includes any person or persons who \"identifies him\/herself as a member of the Deaf community, and other members accept that person as a part of the community,\" an example being children of deaf adults with normal hearing ability.  It includes the set of social beliefs, behaviors, art, literary traditions, history, values, and shared institutions of communities that are influenced by deafness and which use sign languages as the main means of communication.  Members of the Deaf community tend to view deafness as a difference in human experience rather than a disability or disease.  When used as a cultural label especially within the culture, the word deaf is often written with a capital D and referred to as \"big D Deaf\" in speech and sign. When used as a label for the audiological condition, it is written with a lower case d.\nThere also multiple educational institutions for both deaf and Deaf people, that usually use sign language as the main language of instruction. Famous institutions include Gallaudet University and the National Technical Institute for the Deaf in the US, and the National University Corporation of Tsukuba University of Technology in Japan.\n\nResearch\nStem cell transplant and gene therapy\nA 2005 study achieved successful regrowth of cochlea cells in guinea pigs. However, the regrowth of cochlear hair cells does not imply the restoration of hearing sensitivity, as the sensory cells may or may not make connections with neurons that carry the signals from hair cells to the brain. A 2008 study has shown that gene therapy targeting Atoh1 can cause hair cell growth and attract neuronal processes in embryonic mice. Some hope that a similar treatment will one day ameliorate hearing loss in humans.\nRecent research, reported in 2012 achieved growth of cochlear nerve cells resulting in hearing improvements in gerbils, using stem cells. Also reported in 2013 was regrowth of hair cells in deaf adult mice using a drug intervention resulting in hearing improvement. The Hearing Health Foundation in the US has embarked on a project called the Hearing Restoration Project. Also Action on Hearing Loss in the UK is also aiming to restore hearing.\nResearchers reported in 2015 that genetically deaf mice which were treated with TMC1 gene therapy recovered some of their hearing. In 2017, additional studies were performed to treat Usher syndrome and here, a recombinant adeno-associated virus seemed to outperform the older vectors.\n\nAudition\nBesides research studies seeking to improve hearing, such as the ones listed above, research studies on the deaf have also been carried out in order to understand more about audition. Pijil and Shwarz (2005) conducted their study on the deaf who lost their hearing later in life and, hence, used cochlear implants to hear. They discovered further evidence for rate coding of pitch, a system that codes for information for frequencies by the rate that neurons fire in the auditory system, especially for lower frequencies as they are coded by the frequencies that neurons fire from the basilar membrane in a synchronous manner. Their results showed that the subjects could identify different pitches that were proportional to the frequency stimulated by a single electrode. The lower frequencies were detected when the basilar membrane was stimulated, providing even further evidence for rate coding.\n\nSee also\nAudiology\nDeaf hearing\nH.870\nSafe listening\nWorld Hearing Day\n\nReferences\nExternal links\n\nWHO fact sheet on deafness and hearing loss\nHearing loss at Curlie\nNational Institute for the Prevention of Deafness and other Communication Disorders\nGlobal Costs of unaddressed hearing loss and cost-effectiveness of interventions World Health Organization. 2017. Internet archive on 12 May 2020.\nOccupational Noise and Hearing Loss Prevention U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, National Institute for Occupational Safety and Health. (6 February 2018).\nGlobal Audiology, International Society of Audiology\nInternational Ototoxicity Management Group","83":"A hearing test provides an evaluation of the sensitivity of a person's sense of hearing and is most often performed by an audiologist using an audiometer.  An audiometer is used to determine a person's hearing sensitivity at different frequencies. There are other hearing tests as well, e.g., Weber test and Rinne test.\n\nEar examination\nPrior to the hearing test, the ears of the patient are usually examined with an otoscope to make sure they are free of wax, that the eardrum is intact, the ears are not infected, and the middle ear is free of fluid (indicating middle ear infection).\n\nPure tone audiometry\nThe standard and most common type of hearing test is pure tone audiometry, which measures the air and bone conduction thresholds for each ear in a set of 8 standard frequencies from 250Hz to 8000Hz. The test is conducted in a sound booth using either a pair of foam inserts or supraural headphones connected to an external audiometer.  The result of the test is an audiogram diagram which plots a person's hearing sensitivity at the tested frequencies. On an audiogram an \"x\" plot represents the softest threshold heard at each specific frequency in the left ear, and an \"o\" plot represents the softest threshold heard at each specific frequency in the right ear. There is also a high frequency version of the test which tests frequencies over 8000Hz to 16000Hz which may be employed in special circumstances.\n\nIn-situ audiometry using mobile applications\nThe availability of stereo headphones and smartphones or tablets equipped with sound reproduction systems led to the appearance of new audiologic diagnostic methods which help people identify their degree of hearing loss without assistance. For users of these mobile devices, there are a number of applications available with a function for audiometric hearing testing. There are also hearing aid applications with a built-in hearing test for making hearing aid adjustments.\nIn the process of hearing test with specialized applications, initial hearing thresholds of perception of tone signals on different frequencies (audiogram) are identified.\nHearing thresholds, like with traditional audiometry, and with a special application, are determined on a standard set of frequencies from 125 Hz to 8 kHz. Also, an application can be integrated with a function for testing the relevance of perception of separate sounds and figures of intelligibility in various acoustic conditions\nTechnically, the hearing test application consists of the following blocks:\n\nprogram module-generator of tone signals of the required frequency;\ngraphic interface options (for fixing the user's reaction to exceeding tone perception threshold);\ninterpreter of test results (text and graphics);\ndatabase with the results of previous examinations and hearing age norms parameters\nHearing test results obtained through the application will be in error as compared to the results of hearing test conducted by an audiologist because of the following reasons:\n\nthe use of specialized calibrated equipment;\nsound-proofing of the room where hearing test is held;\nheterogeneity of parameters of sound-recording systems in smartphones and tablets, and also headphones or headsets;\nnoise masking effect of tone signals\nAdvantages of the audiometry conducted with a specialized application or hearing aid application include availability and possibility to do the hearing test without assistance.\nDespite possible errors in the results of diagnostics, the undoubted advantages of hearing testing with a special application or hearing aid application include the ability to do the hearing test without assistance and the availability of hearing testing.\nScientists suggest that the hearing test using a mobile application can be used to identify hearing pathologies and also for hearing screening tests.\n\nWeber and Rinne\nA complete hearing evaluation involves several other tests as well. In order to determine what kind of hearing loss is present, a bone conduction hearing test is administered.  In this test, a vibrating tuning fork is placed behind the ear, on the mastoid process. When the patient can no longer feel\/hear the vibration, the tuning fork is held in front of the ear; the patient should once more be able to hear a ringing sound. If they cannot, there is conductive hearing loss in that ear. Additionally, the tuning fork is placed on the forehead. The patient is then asked if the sound is localised in the centre of the head or whether it is louder in either ear. If there is conductive hearing loss, it is likely to be louder in the affected ear; if there is sensorineural hearing loss, it will be quieter in the affected ear. This test helps the audiologist determine whether the hearing loss is conductive (caused by problems in the outer or middle ear) or sensorineural (caused by problems in the cochlea, the sensory organ of hearing) or neural - caused by a problem in the auditory nerve or auditory pathways\/cortex of the brain.\n\nHearing in Noise\nThe Hearing in Noise Test (HINT) measures a person's ability to hear speech in quiet and in noise.  In the test, the patient is required to repeat sentences both in a quiet environment and with competing noise being presented from different directions.  More specifically, there are four conditions: (1) sentences with no competing noise, (2) sentences with competing noise presented directly in front of the patient, (3) noise presented at 90\u00b0 to the right of the patient, and (4) noise presented at 90\u00b0 to the left of the patient.  The test measures signal-to-noise ratio for the different conditions which corresponds to how loud the sentences needed to be played above the noise so that the patient can repeat them correctly 50% of the time.\n\nWords-in-Noise Test\nThe Words-in-Noise Test (WIN) uses monosyllabic words presented at seven different signal-to-noise ratios with masking noise - typically speech spectrum noise.   The WIN test will yield a score for a person's ability to understand speech in a noisy background.  Unlike a pure-tone audiogram, the WIN test may provide a more functional test of a person's hearing in a situation that is likely to occur.\n\nModified Rhyme Test\nThe Modified Rhyme Test (MRT) is defined in the American National Standard ANSI S3.2 Methods for Measuring the Intelligibility of Speech Over Communication Systems.   The method consists of 50 sets of six monosyllabic words that differ in initial or final consonant (e.g. not, tot, got, pot, hot, lot or ray, raze, rate, rave, rake, race).  The listener is typically presented with one of the words in the couplet preceded by a phrase, \"You will mark the word ___\".  The six words that rhyme are presented to the listener to select what they believe to be the correct answer. The MRT has been extensively used by the US Air Force to test the performance of different communication systems, which often include a noise interference component.  If a condition achieves a score of 80% correct responses or better, then that is often an acceptable performance level.\n\nOther\nThe audiologist or hearing instrument specialist may also conduct speech tests, wherein the patient repeats the words he or she hears.\nIn addition, a test called a tympanogram is generally done. In this test, a small probe is placed in the ear and the air pressure in the ear canal is varied.  This test tells the audiologist how well the eardrum and other structures in the middle ear are working.  The ear canal volume indicates whether a perforation in the eardrum (tympanic membrane) may be present.  The middle ear pressure indicates whether any fluid is present in the middle ear space (also called \"glue ear\" or \"otitis media with effusion\").  Compliance measurement indicates how well the eardrum and ossicles (the three ear bones) are moving.\nThe last test the audiologist may perform is an acoustic reflex test. In this test a probe is placed in the ear and a loud tone, greater than 70 dBSPL, is produced.  The test measures the reflexive contraction of the stapedius muscle, which is important in protecting the ear from loud noises, such as a person's own speech which may be 90 dBSPL at the eardrum. This test can be used to give information about the vestibular and facial nerves and indicate if a lesion may be present.\n\nReferences\nExternal links\nOnline Hearing Exam\nHearing Test Ireland","84":"Histamine is an organic nitrogenous compound involved in local immune responses communication, as well as regulating physiological functions in the gut and acting as a neurotransmitter for the brain, spinal cord, and uterus. Discovered in 1910, histamine has been considered a local hormone (autocoid) because it's produced without involvement of the classic endocrine glands; however, in recent years, histamine has been recognized as a central neurotransmitter. Histamine is involved in the inflammatory response and has a central role as a mediator of itching. As part of an immune response to foreign pathogens, histamine is produced by basophils and by mast cells found in nearby connective tissues.  Histamine increases the permeability of the capillaries to white blood cells and some proteins, to allow them to engage pathogens in the infected tissues. It consists of an imidazole ring attached to an ethylamine chain; under physiological conditions, the amino group of the side-chain is protonated.\n\nProperties\nHistamine base, obtained as a mineral oil mull, melts at 83\u201384 \u00b0C. Hydrochloride and phosphorus salts form white hygroscopic crystals and are easily dissolved in water or ethanol, but not in ether. In aqueous solution, the imidazole ring of histamine exists in two tautomeric forms, identified by which of the two nitrogen atoms is protonated. The nitrogen farther away from the side chain is the 'tele' nitrogen and is denoted by a lowercase tau sign and the nitrogen closer to the side chain is the 'pros' nitrogen and is denoted by the pi sign. The tele tautomer, N\u03c4-H-histamine, is preferred in solution as compared to the pros tautomer, N\u03c0-H-histamine.\n\nHistamine has two basic centres, namely the aliphatic amino group and whichever nitrogen atom of the imidazole ring does not already have a proton. Under physiological conditions, the aliphatic amino group (having a pKa around 9.4) will be protonated, whereas the second nitrogen of the imidazole ring (pKa \u2248 5.8) will not be protonated.\nThus, histamine is normally protonated to a singly charged cation. Since human blood is slightly basic (with a normal pH range of 7.35 to 7.45) therefore the predominant form of histamine present in human blood is monoprotic at the aliphatic nitrogen. Histamine is a monoamine neurotransmitter.\n\nSynthesis and metabolism\nHistamine is derived from the decarboxylation of the amino acid histidine, a reaction catalyzed by the enzyme L-histidine decarboxylase. It is a hydrophilic vasoactive amine.\n\nOnce formed, histamine is either stored or rapidly inactivated by its primary degradative enzymes, histamine-N-methyltransferase or diamine oxidase. In the central nervous system, histamine released into the synapses is primarily broken down by histamine-N-methyltransferase, while in other tissues both enzymes may play a role. Several other enzymes, including MAO-B and ALDH2, further process the immediate metabolites of histamine for excretion or recycling.\nBacteria also are capable of producing histamine using histidine decarboxylase enzymes unrelated to those found in animals.  A non-infectious form of foodborne disease, scombroid poisoning, is due to histamine production by bacteria in spoiled food, particularly fish.  Fermented foods and beverages naturally contain small quantities of histamine due to a similar conversion performed by fermenting bacteria or yeasts.  Sake contains histamine in the 20\u201340 mg\/L range; wines contain it in the 2\u201310 mg\/L range.\n\nStorage and release\nMost histamine in the body is generated in granules in mast cells and in white blood cells (leukocytes) called basophils. Mast cells are especially numerous at sites of potential injury \u2013 the nose, mouth, and feet, internal body surfaces, and blood vessels.  Non-mast cell histamine is found in several tissues, including the hypothalamus region of the brain, where it functions as a neurotransmitter.  Another important site of histamine storage and release is the enterochromaffin-like (ECL) cell of the stomach.\nThe most important pathophysiologic mechanism of mast cell and basophil histamine release is immunologic.  These cells, if sensitized by IgE antibodies attached to their membranes, degranulate when exposed to the appropriate antigen.  Certain amines and alkaloids, including such drugs as morphine, and curare alkaloids, can displace histamine in granules and cause its release. Antibiotics like polymyxin are also found to stimulate histamine release.\nHistamine release occurs when allergens bind to mast-cell-bound IgE antibodies. Reduction of IgE overproduction may lower the likelihood of allergens finding sufficient free IgE to trigger a mast-cell-release of histamine.\n\nDegradation\nHistamine is released by mast cells as an immune response and is later degraded primarily by two enzymes: diamine oxidase (DAO), coded by AOC1 genes, and histamine-N-methyltransferase (HNMT), coded by the HNMT gene. The presence of single nucleotide polymorphisms (SNPs) at these genes are associated with a wide variety of disorders, from ulcerative colitis to autism spectrum disorder (ASD). Histamine degradation is crucial to the prevention of allergic reactions to otherwise harmless substances.\nDAO is typically expressed in epithelial cells at the tip of the villus of the small intestine mucosa. Reduced DAO activity is associated with gastrointestinal disorders and widespread food intolerances. This is due to an increase in histamine absorption through enterocytes, which increases histamine concentration in the bloodstream. One study found that migraine patients with gluten sensitivity were positively correlated with having lower DAO serum levels. Low DAO activity can have more severe consequences as mutations in the ABP1 alleles of the AOC1 gene have been associated with ulcerative colitis. Heterozygous or homozygous recessive genotypes at the rs2052129, rs2268999, rs10156191 and rs1049742 alleles increased the risk for reduced DAO activity. People with genotypes for reduced DAO activity can avoid foods high in histamine, such as alcohol, fermented foods, and aged foods, to attenuate any allergic reactions. Additionally, they should be aware whether any probiotics they are taking contain any histamine-producing strains and consult with their doctor to receive proper support .\nHNMT is expressed in the central nervous system, where deficiencies have been shown to lead to aggressive behavior and abnormal sleep-wake cycles in mice. Since brain histamine as a neurotransmitter regulates a number of neurophysiological functions, emphasis has been placed on the development of drugs to target histamine regulation. Yoshikawa et al. explores how the C314T, A939G, G179A, and T632C polymorphisms all impact HNMT enzymatic activity and the pathogenesis of various neurological disorders. These mutations can have either a positive or negative impact. Some patients with ADHD have been shown to exhibit exacerbated symptoms in response to food additives and preservatives, due in part to histamine release. In a double-blind placebo-controlled crossover trial, children with ADHD who responded with aggravated symptoms after consuming a challenge beverage were more likely to have HNMT polymorphisms at T939C and Thr105Ile. Histamine's role in neuroinflammation and cognition has made it a target of study for many neurological disorders, including autism spectrum disorder (ASD). De novo deletions in the HNMT gene have also been associated with ASD.\nMast cells serve an important immunological role by defending the body from antigens and maintaining homeostasis in the gut microbiome. They act as an alarm to trigger inflammatory responses by the immune system. Their presence in the digestive system enables them to serve as an early barrier to pathogens entering the body. People who suffer from widespread sensitivities and allergic reactions may have mast cell activation syndrome (MCAS), in which excessive amounts of histamine are released from mast cells, and cannot be properly degraded. The abnormal release of histamine can be caused by either dysfunctional internal signals from defective mast cells or by the development of clonal mast cell populations through mutations occurring in the tyrosine kinase Kit. In such cases, the body may not be able to produce sufficient degradative enzymes to properly eliminate the excess histamine. Since MCAS is symptomatically characterized as such a broad disorder, it is difficult to diagnose and can be mislabeled as a variety of diseases, including irritable bowel syndrome and fibromyalgia.\nHistamine is often explored as a potential cause for diseases related to hyper-responsiveness of the immune system. In patients with asthma, abnormal histamine receptor activation in the lungs is associated with bronchospasm, airway obstruction, and production of excess mucus. Mutations in histamine degradation are more common in patients with a combination of asthma and allergen hypersensitivity than in those with just asthma. The HNMT-464 TT and HNMT-1639 TT polymorphisms are significantly more common among children with allergic asthma, the latter of which is overrepresented in African-American children.\n\nMechanism of action\nIn humans, histamine exerts its effects primarily by binding to G protein-coupled histamine receptors, designated H1 through H4. As of 2015, histamine is believed to activate ligand-gated chloride channels in the brain and intestinal epithelium.\n\nRoles in the body\nAlthough histamine is small compared to other biological molecules (containing only 17 atoms), it plays an important role in the body. It is known to be involved in 23 different physiological functions. Histamine is known to be involved in many physiological functions because of its chemical properties that allow it to be versatile in binding. It is Coulombic (able to carry a charge), conformational, and flexible. This allows it to interact and bind more easily.\n\nVasodilation and fall in blood pressure\nIt has been known for more than one hundred years that an intravenous injection of histamine causes a fall in the blood pressure. The underlying mechanism concerns both vascular hyperpermeability and vasodilation. Histamine binding to endothelial cells causes them to contract, thus increasing vascular leak. It also stimulates synthesis and release of various vascular smooth muscle cell relaxants, such as nitric oxide, endothelium-derived hyperpolarizing factors and other compounds, resulting in blood vessel dilation. These two mechanisms play a key role in the pathophysiology of anaphylaxis.\n\nEffects on nasal mucous membrane\nIncreased vascular permeability causes fluid to escape from capillaries into the tissues, which leads to the classic symptoms of an allergic reaction: a runny nose and watery eyes. Allergens can bind to IgE-loaded mast cells in the nasal cavity's mucous membranes. This can lead to three clinical responses:\n\nsneezing due to histamine-associated sensory neural stimulation\nhyper-secretion from glandular tissue\nnasal congestion due to vascular engorgement associated with vasodilation and increased capillary permeability\n\nSleep-wake regulation\nHistamine is a neurotransmitter that is released from histaminergic neurons which project out of the mammalian hypothalamus. The cell bodies of these neurons are located in a portion of the posterior hypothalamus known as the tuberomammillary nucleus (TMN). The histamine neurons in this region comprise the brain's histamine system, which projects widely throughout the brain and includes axonal projections to the cortex, medial forebrain bundle, other hypothalamic nuclei, medial septum, the nucleus of the diagonal band, ventral tegmental area, amygdala, striatum, substantia nigra, hippocampus, thalamus and elsewhere. The histamine neurons in the TMN are involved in regulating the sleep-wake cycle and promote arousal when activated. The neural firing rate of histamine neurons in the TMN is strongly positively correlated with an individual's state of arousal. These neurons fire rapidly during periods of wakefulness, fire more slowly during periods of relaxation\/tiredness, and stop firing altogether during REM and NREM (non-REM) sleep.\nFirst-generation H1 antihistamines (i.e., antagonists of histamine receptor H1) are capable of crossing the blood\u2013brain barrier and produce drowsiness by antagonizing histamine H1 receptors in the tuberomammillary nucleus. The newer class of second-generation H1 antihistamines do not readily permeate the blood\u2013brain barrier and thus are less likely to cause sedation, although individual reactions, concomitant medications and dosage may increase the likelihood of a sedating effect. In contrast, histamine H3 receptor antagonists increase wakefulness. Similar to the sedative effect of first-generation H1 antihistamines, an inability to maintain vigilance can occur from the inhibition of histamine biosynthesis or the loss (i.e., degeneration or destruction) of histamine-releasing neurons in the TMN.\n\nGastric acid release\nEnterochromaffin-like cells in the stomach release histamine, stimulating parietal cells via H2 receptors. This triggers carbon dioxide and water uptake from the blood, converted to carbonic acid by carbonic anhydrase. The acid dissociates into hydrogen and bicarbonate ions within the parietal cell. Bicarbonate returns to the bloodstream, while hydrogen is pumped into the stomach lumen. Histamine release ceases as stomach pH decreases. Antagonist molecules, such as ranitidine or famotidine, block the H2 receptor and prevent histamine from binding, causing decreased hydrogen ion secretion.\n\nProtective effects\nWhile histamine has stimulatory effects upon neurons, it also has  suppressive ones that protect against the  susceptibility to convulsion, drug sensitization, denervation supersensitivity, ischemic lesions and stress. It has also been suggested that histamine controls the mechanisms by which memories and learning are forgotten.\n\nErection and sexual function\nLoss of libido and erectile dysfunction can occur during treatment with histamine H2 receptor antagonists such as cimetidine, ranitidine, and risperidone. The injection of histamine into the corpus cavernosum in males with psychogenic impotence produces full or partial erections in 74% of them. It has been suggested that H2 antagonists may cause sexual dysfunction by reducing the functional binding of testosterone to its androgen receptors.\n\nSchizophrenia\nMetabolites of histamine are increased in the cerebrospinal fluid of people with schizophrenia, while the efficiency of H1 receptor binding sites is decreased. Many atypical antipsychotic medications have the effect of increasing histamine production, because histamine levels seem to be imbalanced in people with that disorder.\n\nMultiple sclerosis\nHistamine therapy for treatment of multiple sclerosis is currently being studied. The different H receptors have been known to have different effects on the treatment of this disease. The H1 and H4 receptors, in one study, have been shown to be counterproductive in the treatment of MS. The H1 and H4 receptors are thought to increase permeability in the blood-brain barrier, thus increasing infiltration of unwanted cells in the central nervous system. This can cause inflammation, and MS symptom worsening. The H2 and H3 receptors are thought to be helpful when treating MS patients. Histamine has been shown to help with T-cell differentiation. This is important because in MS, the body's immune system attacks its own myelin sheaths on nerve cells (which causes loss of signaling function and eventual nerve degeneration). By helping T cells to differentiate, the T cells will be less likely to attack the body's own cells, and instead, attack invaders.\n\nDisorders\nAs an integral part of the immune system, histamine may be involved in immune system disorders and allergies. Mastocytosis is a rare disease in which there is a proliferation of mast cells that produce excess histamine.\nHistamine intolerance is a presumed set of adverse reactions (such as flush, itching, rhinitis, etc.) to ingested histamine in food. The mainstream theory accepts that there may exist adverse reactions to ingested histamine, but does not recognize histamine intolerance as a separate condition that can be diagnosed.\nThe role of histamine in health and disease is an area of ongoing research. For example, histamine is researched in its potential link with migraine episodes, when there is a noted elevation in the plasma concentrations of both histamine and calcitonin gene-related peptide (CGRP). These two substances are potent vasodilators, and have been demonstrated to mutually stimulate each other's release within the trigeminovascular system, a mechanism that could potentially instigate the onset of migraines. In patients with a deficiency in histamine degradation due to variants in the AOC1 gene that encodes diamine oxidase enzyme, a diet high in histamine has been observed to trigger migraines, that suggests a potential functional relationship between exogenous histamine and CGRP, which could be instrumental in understanding the genesis of diet-induced migraines, so that the role of histamine, particularly in relation to CGRP, is a promising area of research for elucidating the mechanisms underlying migraine development and aggravation, especially relevant in the context of dietary triggers and genetic predispositions related to histamine metabolism.\n\nMeasurement\nHistamine, a biogenic amine, involves many physiological functions, including the immune response, gastric acid secretion, and neuromodulation. However, its rapid metabolism makes it challenging to measure histamine levels directly in plasma. \nAs a solution for the rapid metabolism of histamine, the measurement of histamine and its metabolites, particularly the 1,4-methyl-imidazolacetic acid, in a 24-hour urine sample, provides an efficient alternative to histamine measurement because the values of these metabolites remain elevated for a much longer period than the histamine itself.\nCommercial laboratories provide a 24-hour urine sample test for 1,4-methyl-imidazolacetic acid, the metabolite of histamine. This test is a valuable tool in assessing the metabolism of histamine in the body, as direct measurement of histamine in the serum has low diagnostic value due to the specificities of histamine metabolism.\nThe urine test involves collecting all urine produced in a 24-hour period, which is then analyzed for the presence of 1,4-methyl-imidazolacetic acid. This comprehensive approach ensures a more accurate reflection of histamine metabolism over an extended period; as such, the 1,4-methyl-imidazolacetic acid urine test offered by commercial labs is currently the most reliable method to determine the rate of histamine metabolism, which may be helpful for the health care practitioners to assess individual\u2019s health status, such as to diagnose interstitial cystitis.\n\nHistory\nThe properties of histamine, then called \u03b2-imidazolylethylamine, were first described in 1910 by the British scientists Henry H. Dale and P.P. Laidlaw. By 1913 the name histamine was in use, using combining forms of histo- + amine, yielding \"tissue amine\".\n\"H substance\" or \"substance H\" are occasionally used in medical literature for histamine or a hypothetical histamine-like diffusible substance released in allergic reactions of skin and in the responses of tissue to inflammation.\n\nSee also\nAnaphylaxis\nDiamine oxidase\nHistamine N-methyltransferase\nHay fever (allergic rhinitis)\nHistamine intolerance\nHistamine receptor antagonist\nScombroid food poisoning\nPhotic sneeze reflex\n\nReferences\nExternal links\nHistamine MS Spectrum\nHistamine bound to proteins in the PDB","85":"Stroke (also known as a cerebrovascular accident (CVA) or brain attack) is a medical condition in which poor blood flow to the brain causes cell death. There are two main types of stroke: \n\nischemic, due to lack of blood flow, and\nhemorrhagic, due to bleeding.\nBoth cause parts of the brain to stop functioning properly.\nSigns and symptoms of stroke may include an inability to move or feel on one side of the body, problems understanding or speaking, dizziness, or loss of vision to one side. Signs and symptoms often appear soon after the stroke has occurred. If symptoms last less than one or two hours, the stroke is a transient ischemic attack (TIA), also called a mini-stroke. Hemorrhagic stroke may also be associated with a severe headache. The symptoms of stroke can be permanent. Long-term complications may include pneumonia and loss of bladder control.\nThe biggest risk factor for stroke is high blood pressure. Other risk factors include high blood cholesterol, tobacco smoking, obesity, diabetes mellitus, a previous TIA, end-stage kidney disease, and atrial fibrillation. Ischemic stroke is typically caused by blockage of a blood vessel, though there are also less common causes. Hemorrhagic stroke is caused by either bleeding directly into the brain or into the space between the brain's membranes. Bleeding may occur due to a ruptured brain aneurysm. Diagnosis is typically based on a physical exam and supported by medical imaging such as a CT scan or MRI scan. A CT scan can rule out bleeding, but may not necessarily rule out ischemia, which early on typically does not show up on a CT scan. Other tests such as an electrocardiogram (ECG) and blood tests are done to determine risk factors and rule out other possible causes. Low blood sugar may cause similar symptoms.\nPrevention includes decreasing risk factors, surgery to open up the arteries to the brain in those with problematic carotid narrowing, and warfarin in people with atrial fibrillation. Aspirin or statins may be recommended by physicians for prevention. Stroke is a medical emergency. Ischemic strokes, if detected within three to four-and-a-half hours, may be treatable with medication that can break down the clot, while hemorrhagic strokes sometimes benefit from surgery. Treatment to attempt recovery of lost function is called stroke rehabilitation, and ideally takes place in a stroke unit; however, these are not available in much of the world.\nIn 2023, 15 million people worldwide had a stroke. In 2021, stroke was the third biggest cause of death, responsible for approximately 10% of total deaths. In 2015, there were about 42.4 million people who had previously had stroke and were still alive. Between 1990 and 2010 the annual incidence of stroke decreased by approximately 10% in the developed world, but increased by 10% in the developing world. In 2015, stroke was the second most frequent cause of death after coronary artery disease, accounting for 6.3 million deaths (11% of the total). About 3.0 million deaths resulted from ischemic stroke while 3.3 million deaths resulted from hemorrhagic stroke. About half of people who have had stroke live less than one year. Overall, two thirds of cases of stroke occurred in those over 65 years old.\n\nClassification\nStroke can be classified into two major categories: ischemic and hemorrhagic. Ischemic stroke is caused by interruption of the blood supply to the brain, while hemorrhagic stroke results from the rupture of a blood vessel or an abnormal vascular structure. \nAbout 87% of stroke is ischemic, with the rest being hemorrhagic. Bleeding can develop inside areas of ischemia, a condition known as \"hemorrhagic transformation.\" It is unknown how many cases of hemorrhagic stroke actually start as ischemic stroke.\n\nDefinition\nIn the 1970s the World Health Organization defined \"stroke\" as a \"neurological deficit of cerebrovascular cause that persists beyond 24 hours or is interrupted by death within 24 hours\", although the word \"stroke\" is centuries old. This definition was supposed to reflect the reversibility of tissue damage and was devised for the purpose, with the time frame of 24 hours being chosen arbitrarily. The 24-hour limit divides stroke from transient ischemic attack, which is a related syndrome of stroke symptoms that resolve completely within 24 hours. With the availability of treatments that can reduce stroke severity when given early, many now prefer alternative terminology, such as \"brain attack\" and \"acute ischemic cerebrovascular syndrome\" (modeled after heart attack and acute coronary syndrome, respectively), to reflect the urgency of stroke symptoms and the need to act swiftly.\n\nIschemic\nDuring ischemic stroke, blood supply to part of the brain is decreased, leading to dysfunction of the brain tissue in that area. There are four reasons why this might happen:\n\nThrombosis (obstruction of a blood vessel by a blood clot forming locally)\nEmbolism (obstruction due to an embolus from elsewhere in the body),\nSystemic hypoperfusion (general decrease in blood supply, e.g., in shock)\nCerebral venous sinus thrombosis.\nStroke without an obvious explanation is termed cryptogenic stroke (idiopathic); this constitutes 30\u201340% of all cases of ischemic stroke.\nThere are classification systems for acute ischemic stroke. The Oxford Community Stroke Project classification (OCSP, also known as the Bamford or Oxford classification) relies primarily on the initial symptoms; based on the extent of the symptoms, the stroke episode is classified as total anterior circulation infarct (TACI), partial anterior circulation infarct (PACI), lacunar infarct (LACI) or posterior circulation infarct (POCI). These four entities predict the extent of the stroke, the area of the brain that is affected, the underlying cause, and the prognosis. \nThe TOAST (Trial of Org 10172 in Acute Stroke Treatment) classification is based on clinical symptoms as well as results of further investigations; on this basis, stroke is classified as being due to \n(1) thrombosis or embolism due to atherosclerosis of a large artery, \n(2) an embolism originating in the heart, \n(3) complete blockage of a small blood vessel, \n(4) other determined cause, \n(5) undetermined cause (two possible causes, no cause identified, or incomplete investigation). \nUsers of stimulants such as cocaine and methamphetamine are at a high risk for ischemic stroke.\n\nHemorrhagic\nThere are two main types of hemorrhagic stroke:\n\nIntracerebral hemorrhage, which is bleeding within the brain itself (when an artery in the brain bursts, flooding the surrounding tissue with blood), due to either intraparenchymal hemorrhage (bleeding within the brain tissue) or intraventricular hemorrhage (bleeding within the brain's ventricular system).\nSubarachnoid hemorrhage, which is bleeding that occurs outside of the brain tissue but still within the skull, and precisely between the arachnoid mater and pia mater (the delicate innermost layer of the three layers of the meninges that surround the brain).\nThe above two main types of hemorrhagic stroke are also two different forms of intracranial hemorrhage, which is the accumulation of blood anywhere within the cranial vault; but the other forms of intracranial hemorrhage, such as epidural hematoma (bleeding between the skull and the dura mater, which is the thick outermost layer of the meninges that surround the brain) and subdural hematoma (bleeding in the subdural space), are not considered \"hemorrhagic stroke\".\nHemorrhagic stroke may occur on the background of alterations to the blood vessels in the brain, such as cerebral amyloid angiopathy, cerebral arteriovenous malformation and an intracranial aneurysm, which can cause intraparenchymal or subarachnoid hemorrhage.\nIn addition to neurological impairment, hemorrhagic stroke usually causes specific symptoms (for instance, subarachnoid hemorrhage classically causes a severe headache known as a thunderclap headache) or reveal evidence of a previous head injury.\n\nSigns and symptoms\nStroke symptoms typically start suddenly, over seconds to minutes, and in most cases do not progress further. The symptoms depend on the area of the brain affected. The more extensive the area of the brain affected, the more functions that are likely to be lost. Some forms of stroke can cause additional symptoms. For example, in intracranial hemorrhage, the affected area may compress other structures. Most forms of stroke are not associated with a headache, apart from subarachnoid hemorrhage and cerebral venous thrombosis and occasionally intracerebral hemorrhage.\n\nEarly recognition\nSystems have been proposed to increase recognition of stroke. Sudden-onset face weakness, arm drift (i.e., if a person, when asked to raise both arms, involuntarily lets one arm drift downward) and abnormal speech are the findings most likely to lead to the correct identification of a case of stroke, increasing the likelihood by 5.5 when at least one of these is present. Similarly, when all three of these are absent, the likelihood of stroke is decreased (\u2013 likelihood ratio of 0.39). While these findings are not perfect for diagnosing stroke, the fact that they can be evaluated relatively rapidly and easily make them very valuable in the acute setting.\nA mnemonic to remember the warning signs of stroke is FAST (facial droop, arm weakness, speech difficulty, and time to call emergency services), as advocated by the Department of Health (United Kingdom) and the Stroke Association, the American Stroke Association, and the National Stroke Association (US). FAST is less reliable in the recognition of posterior circulation stroke. The revised mnemonic BE FAST, which adds balance (sudden trouble keeping balance while walking or standing) and eyesight (new onset of blurry or double vision or sudden, painless loss of sight)  to the assessment, has been proposed to address this shortcoming and improve early detection of stroke even further. Other scales for prehospital detection of stroke include the Los Angeles Prehospital Stroke Screen (LAPSS) and the Cincinnati Prehospital Stroke Scale (CPSS), on which the FAST method was based. Use of these scales is recommended by professional guidelines.\nFor people referred to the emergency room, early recognition of stroke is deemed important as this can expedite diagnostic tests and treatments. A scoring system called ROSIER (recognition of stroke in the emergency room) is recommended for this purpose; it is based on features from the medical history and physical examination.\n\nAssociated symptoms\nLoss of consciousness, headache, and vomiting usually occur more often in hemorrhagic stroke than in thrombosis because of the increased intracranial pressure from the leaking blood compressing the brain.\nIf symptoms are maximal at onset, the cause is more likely to be a subarachnoid hemorrhage or an embolic stroke.\n\nSubtypes\nIf the area of the brain affected includes one of the three prominent central nervous system pathways\u2014the spinothalamic tract, corticospinal tract, and the dorsal column\u2013medial lemniscus pathway, symptoms may include:\n\nhemiplegia and muscle weakness of the face\nnumbness\nreduction in sensory or vibratory sensation\ninitial flaccidity (reduced muscle tone), replaced by spasticity (increased muscle tone), excessive reflexes, and obligatory synergies.\nIn most cases, the symptoms affect only one side of the body (unilateral). The defect in the brain is usually on the opposite side of the body. However, since these pathways also travel in the spinal cord and any lesion there can also produce these symptoms, the presence of any one of these symptoms does not necessarily indicate stroke. In addition to the above central nervous system pathways, the brainstem gives rise to most of the twelve cranial nerves. A brainstem stroke affecting the brainstem and brain, therefore, can produce symptoms relating to deficits in these cranial nerves:\n\naltered smell, taste, hearing, or vision (total or partial)\ndrooping of eyelid (ptosis) and weakness of ocular muscles\ndecreased reflexes: gag, swallow, pupil reactivity to light\ndecreased sensation and muscle weakness of the face\nbalance problems and nystagmus\naltered breathing and heart rate\nweakness in sternocleidomastoid muscle with inability to turn head to one side\nweakness in tongue (inability to stick out the tongue or move it from side to side)\nIf the cerebral cortex is involved, the central nervous system pathways can again be affected, but can also produce the following symptoms:\n\naphasia (difficulty with verbal expression, auditory comprehension, reading and writing; Broca's or Wernicke's area typically involved)\ndysarthria (motor speech disorder resulting from neurological injury)\napraxia (altered voluntary movements)\nvisual field defect\nmemory deficits (involvement of temporal lobe)\nhemineglect (involvement of parietal lobe)\ndisorganized thinking, confusion, hypersexual gestures (with involvement of frontal lobe)\nlack of insight of his or her, usually stroke-related, disability\nIf the cerebellum is involved, ataxia might be present and this includes:\n\naltered walking gait\naltered movement coordination\nvertigo and or disequilibrium\n\nPreceding signs and symptoms\nIn the days before a stroke (generally in the previous 7 days, even the previous one), a considerable proportion of patients have a \"sentinel headache\": a severe and unusual headache that indicates a problem. Its appearance makes it advisable to seek medical review and to consider prevention against stroke.\n\nCauses\nThrombotic stroke\nIn thrombotic stroke, a thrombus (blood clot) usually forms around atherosclerotic plaques. Since blockage of the artery is gradual, onset of symptomatic thrombotic stroke is slower than that of hemorrhagic stroke. A thrombus itself (even if it does not completely block the blood vessel) can lead to an embolic stroke (see below) if the thrombus breaks off and travels in the bloodstream, at which point it is called an embolus. Two types of thrombosis can cause stroke:\n\nLarge vessel disease involves the common and internal carotid arteries, the vertebral artery, and the Circle of Willis. Diseases that may form thrombi in the large vessels include (in descending incidence): atherosclerosis, vasoconstriction (tightening of the artery), aortic, carotid or vertebral artery dissection, inflammatory diseases of the blood vessel wall (Takayasu arteritis, giant cell arteritis, vasculitis), noninflammatory vasculopathy, Moyamoya disease and fibromuscular dysplasia. Strokes caused by artery dissections are in the strictest sense not always caused by a 'defined disease state', such events can occur in very young people and can be caused by physical injury such as hyperextension of the neck area or often by other forms of trauma.\nSmall vessel disease involves the smaller arteries inside the brain: branches of the circle of Willis, middle cerebral artery, stem, and arteries arising from the distal vertebral and basilar artery. Diseases that may form thrombi in the small vessels include (in descending incidence): lipohyalinosis (build-up of fatty hyaline matter in the blood vessel as a result of high blood pressure and aging) and fibrinoid degeneration (stroke involving these vessels is known as a lacunar stroke) and microatheroma (small atherosclerotic plaques).\nAnemia causes increase blood flow in the blood circulatory system. This causes the endothelial cells of the blood vessels to express adhesion factors which encourages the clotting of blood and formation of thrombus. Sickle-cell anemia, which can cause blood cells to clump up and block blood vessels, can also lead to stroke. Stroke is the second leading cause of death in people under 20 with sickle-cell anemia. Air pollution may also increase stroke risk.\n\nEmbolic stroke\nAn embolic stroke refers to an arterial embolism (a blockage of an artery) by an embolus, a traveling particle or debris in the arterial bloodstream originating from elsewhere. An embolus is most frequently a thrombus, but it can also be a number of other substances including fat (e.g., from bone marrow in a broken bone), air, cancer cells or clumps of bacteria (usually from infectious endocarditis).\nBecause an embolus arises from elsewhere, local therapy solves the problem only temporarily. Thus, the source of the embolus must be identified. Because the embolic blockage is sudden in onset, symptoms are usually maximal at the start. Also, symptoms may be transient as the embolus is partially resorbed and moves to a different location or dissipates altogether.\nEmboli most commonly arise from the heart (especially in atrial fibrillation) but may originate from elsewhere in the arterial tree. In paradoxical embolism, a deep vein thrombosis embolizes through an atrial or ventricular septal defect in the heart into the brain.\nCauses of stroke related to the heart can be distinguished between high- and low-risk:\n\nHigh risk: atrial fibrillation and paroxysmal atrial fibrillation, rheumatic disease of the mitral or aortic valve disease, artificial heart valves, known cardiac thrombus of the atrium or ventricle, sick sinus syndrome, sustained atrial flutter, recent myocardial infarction, chronic myocardial infarction together with ejection fraction <28 percent, symptomatic congestive heart failure with ejection fraction <30 percent, dilated cardiomyopathy, Libman-Sacks endocarditis, Marantic endocarditis, infective endocarditis, papillary fibroelastoma, left atrial myxoma, and coronary artery bypass graft (CABG) surgery.\nLow risk\/potential: calcification of the annulus (ring) of the mitral valve, patent foramen ovale (PFO), atrial septal aneurysm, atrial septal aneurysm with patent foramen ovale, left ventricular aneurysm without thrombus, isolated left atrial \"smoke\" on echocardiography (no mitral stenosis or atrial fibrillation), and complex atheroma in the ascending aorta or proximal arch\nAmong those who have a complete blockage of one of the carotid arteries, the risk of stroke on that side is about one percent per year.\nA special form of embolic stroke is the embolic stroke of undetermined source (ESUS). This subset of cryptogenic stroke is defined as a non-lacunar brain infarct without proximal arterial stenosis or cardioembolic sources. About one out of six cases of ischemic stroke could be classified as ESUS.\n\nCerebral hypoperfusion\nCerebral hypoperfusion is the reduction of blood flow to all parts of the brain. The reduction could be to a particular part of the brain depending on the cause. It is most commonly due to heart failure from cardiac arrest or arrhythmias, or from reduced cardiac output as a result of myocardial infarction, pulmonary embolism, pericardial effusion, or bleeding. Hypoxemia (low blood oxygen content) may precipitate the hypoperfusion. Because the reduction in blood flow is global, all parts of the brain may be affected, especially vulnerable \"watershed\" areas\u2014border zone regions supplied by the major cerebral arteries. A watershed stroke refers to the condition when the blood supply to these areas is compromised. Blood flow to these areas does not necessarily stop, but instead it may lessen to the point where brain damage can occur.\n\nVenous thrombosis\nCerebral venous sinus thrombosis leads to stroke due to locally increased venous pressure, which exceeds the pressure generated by the arteries. Infarcts are more likely to undergo hemorrhagic transformation (leaking of blood into the damaged area) than other types of ischemic stroke.\n\nIntracerebral hemorrhage\nIt generally occurs in small arteries or arterioles and is commonly due to hypertension, intracranial vascular malformations (including cavernous angiomas or arteriovenous malformations), cerebral amyloid angiopathy, or infarcts into which secondary hemorrhage has occurred. Other potential causes are trauma, bleeding disorders, amyloid angiopathy, illicit drug use (e.g., amphetamines or cocaine). The hematoma enlarges until pressure from surrounding tissue limits its growth, or until it decompresses by emptying into the ventricular system, CSF or the pial surface. A third of intracerebral bleed is into the brain's ventricles. ICH has a mortality rate of 44 percent after 30 days, higher than ischemic stroke or subarachnoid hemorrhage (which technically may also be classified as a type of stroke).\n\nOther\nOther causes may include spasm of an artery. This may occur due to cocaine. Cancer is also another well recognized potential cause of stroke. Although, malignancy in general can increase the risk of stroke, certain types of cancer such as pancreatic, lung and gastric are typically associated with a higher thromboembolism risk. The mechanism with which cancer increases stroke risk is thought to be secondary to an acquired hypercoagulability.\n\nSilent stroke\nSilent stroke is stroke that does not have any outward symptoms, and people are typically unaware they had experienced stroke. Despite not causing identifiable symptoms, silent stroke still damages the brain and places the person at increased risk for both transient ischemic attack and major stroke in the future. Conversely, those who have had major stroke are also at risk of having silent stroke. In a broad study in 1998, more than 11 million people were estimated to have experienced stroke in the United States. Approximately 770,000 of these were symptomatic and 11 million were first-ever silent MRI infarcts or hemorrhages. Silent stroke typically causes lesions which are detected via the use of neuroimaging such as MRI. Silent stroke is estimated to occur at five times the rate of symptomatic stroke. The risk of silent stroke increases with age, but they may also affect younger adults and children, especially those with acute anemia.\n\nPathophysiology\nIschemic\nIschemic stroke occurs because of a loss of blood supply to part of the brain, initiating the ischemic cascade. Atherosclerosis may disrupt the blood supply by narrowing the lumen of blood vessels leading to a reduction of blood flow by causing the formation of blood clots within the vessel or by releasing showers of small emboli through the disintegration of atherosclerotic plaques. Embolic infarction occurs when emboli formed elsewhere in the circulatory system, typically in the heart as a consequence of atrial fibrillation, or in the carotid arteries, break off, enter the cerebral circulation, then lodge in and block brain blood vessels. Since blood vessels in the brain are now blocked, the brain becomes low in energy, and thus it resorts to using anaerobic metabolism within the region of brain tissue affected by ischemia. Anaerobic metabolism produces less adenosine triphosphate (ATP) but releases a by-product called lactic acid. Lactic acid is an irritant which could potentially destroy cells since it is an acid and disrupts the normal acid-base balance in the brain. The ischemia area is referred to as the \"ischemic penumbra\". After the initial ischemic event the penumbra transitions from a tissue remodeling characterized by damage to a remodeling characterized by repair.\n\nAs oxygen or glucose becomes depleted in ischemic brain tissue, the production of high energy phosphate compounds such as adenosine triphosphate (ATP) fails, leading to failure of energy-dependent processes (such as ion pumping) necessary for tissue cell survival. This sets off a series of interrelated events that result in cellular injury and death. A major cause of neuronal injury is the release of the excitatory neurotransmitter glutamate. The concentration of glutamate outside the cells of the nervous system is normally kept low by so-called uptake carriers, which are powered by the concentration gradients of ions (mainly Na+) across the cell membrane. However, stroke cuts off the supply of oxygen and glucose which powers the ion pumps maintaining these gradients. As a result, the transmembrane ion gradients run down, and glutamate transporters reverse their direction, releasing glutamate into the extracellular space. Glutamate acts on receptors in nerve cells (especially NMDA receptors), producing an influx of calcium which activates enzymes that digest the cells' proteins, lipids, and nuclear material. Calcium influx can also lead to the failure of mitochondria, which can lead further toward energy depletion and may trigger cell death due to programmed cell death.\nIschemia also induces production of oxygen free radicals and other reactive oxygen species. These react with and damage a number of cellular and extracellular elements. Damage to the blood vessel lining or endothelium may occur. These processes are the same for any type of ischemic tissue and are referred to collectively as the ischemic cascade. However, brain tissue is especially vulnerable to ischemia since it has little respiratory reserve and is completely dependent on aerobic metabolism, unlike most other organs.\n\nCollateral flow\nThe brain can compensate inadequate blood flow in a single artery by the collateral system. This system relies on the efficient connection between the carotid and vertebral arteries through the circle of Willis and, to a lesser extent, the major arteries supplying the cerebral hemispheres. However, variations in the circle of Willis, caliber of collateral vessels, and acquired arterial lesions such as atherosclerosis can disrupt this compensatory mechanism, increasing the risk of brain ischemia resulting from artery blockage.\nThe extent of damage depends on the duration and severity of the ischemia. If ischemia persists for more than 5 minutes with perfusion below 5% of normal, some neurons will die. However, if ischemia is mild, the damage will occur slowly and may take up to 6 hours to completely destroy the brain tissue. In case of severe ischemia lasting more than 15 to 30 minutes, all of the affected tissue will die, leading to infarction. The rate of damage is affected by temperature, with hyperthermia accelerating damage and hypothermia slowing it down and  other factors. Prompt restoration of blood flow to ischemic tissues can reduce or reverse injury, especially if the tissues are not yet irreversibly damaged. This is particularly important for the moderately ischemic areas (penumbras) surrounding areas of severe ischemia, which may still be salvageable due to collateral flow.\n\nHemorrhagic\nHemorrhagic stroke is classified based on their underlying pathology. Some causes of hemorrhagic stroke are hypertensive hemorrhage, ruptured aneurysm, ruptured AV fistula, transformation of prior ischemic infarction, and drug-induced bleeding. They result in tissue injury by causing compression of tissue from an expanding hematoma or hematomas. In addition, the pressure may lead to a loss of blood supply to affected tissue with resulting infarction, and the blood released by brain hemorrhage appears to have direct toxic effects on brain tissue and vasculature. Inflammation contributes to the secondary brain injury after hemorrhage.\n\nDiagnosis\nStroke is diagnosed through several techniques: a neurological examination (such as the NIHSS), CT scans (most often without contrast enhancements) or MRI scans, Doppler ultrasound, and arteriography. The diagnosis of stroke itself is clinical, with assistance from the imaging techniques. Imaging techniques also assist in determining the subtypes and cause of stroke. There is yet no commonly used blood test for the stroke diagnosis itself, though blood tests may be of help in finding out the likely cause of stroke. In deceased people, an autopsy of stroke may help establishing the time between stroke onset and death.\n\nPhysical examination\nA physical examination, including taking a medical history of the symptoms and a neurological status, helps giving an evaluation of the location and severity of stroke. It can give a standard score on e.g., the NIH stroke scale.\n\nImaging\nFor diagnosing ischemic (blockage) stroke in the emergency setting:\n\nCT scans (without contrast enhancements)\nsensitivity= 16% (less than 10% within first 3 hours of symptom onset)\nspecificity= 96%\nMRI scan\nsensitivity= 83%\nspecificity= 98%\nFor diagnosing hemorrhagic stroke in the emergency setting:\n\nCT scans (without contrast enhancements)\nsensitivity= 89%\nspecificity= 100%\nMRI scan\nsensitivity= 81%\nspecificity= 100%\nFor detecting chronic hemorrhages, an MRI scan is more sensitive.\nFor the assessment of stable stroke, nuclear medicine scans such as single-photon emission computed tomography (SPECT) and positron emission tomography\u2013computed tomography (PET\/CT) may be helpful. SPECT documents cerebral blood flow, whereas PET with an FDG isotope shows cerebral glucose metabolism.\nCT scans may not detect ischemic stroke, especially if it is small, of recent onset, or in the brainstem or cerebellum areas (posterior circulation infarct). MRI is better at detecting a posterior circulation infarct with diffusion-weighted imaging. A CT scan is used more to rule out certain stroke mimics and detect bleeding. The presence of leptomeningeal collateral circulation in the brain is associated with better clinical outcomes after recanalization treatment. Cerebrovascular reserve capacity is another factor that affects stroke outcome \u2013 it is the amount of increase in cerebral blood flow after a purposeful stimulation of blood flow by the physician, such as by giving inhaled carbon dioxide or intravenous acetazolamide. The increase in blood flow can be measured by PET scan or transcranial doppler sonography. However, in people with obstruction of the internal carotid artery of one side, the presence of leptomeningeal collateral circulation is associated with reduced cerebral reserve capacity.\n\nUnderlying cause\nWhen stroke has been diagnosed, other studies may be performed to determine the underlying cause. With the treatment and diagnosis options available, it is of particular importance to determine whether there is a peripheral source of emboli. Test selection may vary since the cause of stroke varies with age, comorbidity and the clinical presentation. The following are commonly used techniques:\n\nan ultrasound\/doppler study of the carotid arteries (to detect carotid stenosis) or dissection of the precerebral arteries;\nan electrocardiogram (ECG) and echocardiogram (to identify arrhythmias and resultant clots in the heart which may spread to the brain vessels through the bloodstream);\na Holter monitor study to identify intermittent abnormal heart rhythms;\nan angiogram of the cerebral vasculature (if a bleed is thought to have originated from an aneurysm or arteriovenous malformation);\nblood tests to determine if blood cholesterol is high, if there is an abnormal tendency to bleed, and if some rarer processes such as homocystinuria might be involved.\nFor hemorrhagic stroke, a CT or MRI scan with intravascular contrast may be able to identify abnormalities in the brain arteries (such as aneurysms) or other sources of bleeding, and structural MRI if this shows no cause. If this too does not identify an underlying reason for the bleeding, invasive cerebral angiography could be performed but this requires access to the bloodstream with an intravascular catheter and can cause further stroke as well as complications at the insertion site and this investigation is therefore reserved for specific situations. If there are symptoms suggesting that the hemorrhage might have occurred as a result of venous thrombosis, CT or MRI venography can be used to examine the cerebral veins.\n\nMisdiagnosis\nAmong people with ischemic stroke, misdiagnosis occurs 2 to 26% of the time. A \"stroke chameleon\" (SC) is stroke which is diagnosed as something else.\nPeople not having stroke may also be misdiagnosed with the condition. Giving thrombolytics (clot-busting) in such cases causes intracerebral bleeding 1 to 2% of the time, which is less than that of people with stroke. This unnecessary treatment adds to health care costs. Even so, the AHA\/ASA guidelines state that starting intravenous tPA in possible mimics is preferred to delaying treatment for additional testing.\nWomen, African-Americans, Hispanic-Americans, Asian and Pacific Islanders are more often misdiagnosed for a condition other than stroke when in fact having stroke. In addition, adults under 44 years of age are seven times more likely to have stroke missed than are adults over 75 years of age. This is especially the case for younger people with posterior circulation infarcts. Some medical centers have used hyperacute MRI in experimental studies for people initially thought to have a low likelihood of stroke, and in some of these people, stroke has been found which were then treated with thrombolytic medication.\n\nPrevention\nGiven the disease burden of stroke, prevention is an important public health concern. Primary prevention is less effective than secondary prevention (as judged by the number needed to treat to prevent one stroke per year). Recent guidelines detail the evidence for primary prevention in stroke. About the use of aspirin as a preventive medication for stroke, in healthy people aspirin does not appear beneficial and thus is not recommended,  but in people with high cardiovascular risk, or those who have had a myocardial infarction, it provides some protection against a first stroke. In those who have previously had stroke, treatment with medications such as aspirin, clopidogrel, and dipyridamole may be beneficial. The U.S. Preventive Services Task Force (USPSTF) recommends against screening for carotid artery stenosis in those without symptoms.\n\nRisk factors\nThe most important modifiable risk factors for stroke are high blood pressure and atrial fibrillation, although the size of the effect is small; 833 people have to be treated for 1 year to prevent one stroke. Other modifiable risk factors include high blood cholesterol levels, diabetes mellitus, end-stage kidney disease, cigarette smoking (active and passive), heavy alcohol use, drug use, lack of physical activity, obesity, processed red meat consumption, and unhealthy diet. Smoking just one cigarette per day increases the risk more than 30%. Alcohol use could predispose to ischemic stroke, as well as intracerebral and subarachnoid hemorrhage via multiple mechanisms (for example, via hypertension, atrial fibrillation, rebound thrombocytosis and platelet aggregation and clotting disturbances). Drugs, most commonly amphetamines and cocaine, can induce stroke through damage to the blood vessels in the brain and acute hypertension. Migraine with aura doubles a person's risk for ischemic stroke. Untreated, celiac disease regardless of the presence of symptoms can be an underlying cause of stroke, both in children and adults. According to a 2021 WHO study, working 55+ hours a week raises the risk of stroke by 35% and the risk of dying from heart conditions by 17%, when compared to a 35-40-hour week.\nHigh levels of physical activity reduce the risk of stroke by about 26%. There is a lack of high quality studies looking at promotional efforts to improve lifestyle factors. Nonetheless, given the large body of circumstantial evidence, best medical management for stroke includes advice on diet, exercise, smoking and alcohol use. Medication is the most common method of stroke prevention; carotid endarterectomy can be a useful surgical method of preventing stroke.\n\nBlood pressure\nHigh blood pressure accounts for 35\u201350% of stroke risk. Blood pressure reduction of 10 mmHg systolic or 5 mmHg diastolic reduces the risk of stroke by ~40%. Lowering blood pressure has been conclusively shown to prevent both ischemic and hemorrhagic stroke. It is equally important in secondary prevention. Even people older than 80 years and those with isolated systolic hypertension benefit from antihypertensive therapy. The available evidence does not show large differences in stroke prevention between antihypertensive drugs\u2014therefore, other factors such as protection against other forms of cardiovascular disease and cost should be considered. The routine use of beta-blockers following stroke or TIA has not been shown to result in benefits.\n\nBlood lipids\nHigh cholesterol levels have been inconsistently associated with (ischemic) stroke. Statins have been shown to reduce the risk of stroke by about 15%. Since earlier meta-analyses of other lipid-lowering drugs did not show a decreased risk, statins might exert their effect through mechanisms other than their lipid-lowering effects.\n\nDiabetes mellitus\nDiabetes mellitus increases the risk of stroke by 2 to 3 times. While intensive blood sugar control has been shown to reduce small blood vessel complications such as kidney damage and damage to the retina of the eye it has not been shown to reduce large blood vessel complications such as stroke.\n\nAnticoagulant drugs\nOral anticoagulants such as warfarin have been the mainstay of stroke prevention for over 50 years. However, several studies have shown that aspirin and other antiplatelets are highly effective in secondary prevention after stroke or transient ischemic attack. Low doses of aspirin (for example 75\u2013150 mg) are as effective as high doses but have fewer side effects; the lowest effective dose remains unknown. Thienopyridines (clopidogrel, ticlopidine) might be slightly more effective than aspirin and have a decreased risk of gastrointestinal bleeding but are more expensive. Both aspirin and clopidogrel may be useful in the first few weeks after a minor stroke or high-risk TIA. Clopidogrel has less side effects than ticlopidine. Dipyridamole can be added to aspirin therapy to provide a small additional benefit, even though headache is a common side effect. Low-dose aspirin is also effective for stroke prevention after having a myocardial infarction.\nThose with atrial fibrillation have a 5% a year risk of stroke, and those with valvular atrial fibrillation have an even higher risk. Depending on the stroke risk, anticoagulation with medications such as warfarin or aspirin is useful for prevention with various levels of comparative effectiveness depending on the type of treatment used. \nOral anticoagulants, especially Xa (apixaban) and thrombin (dabigatran) inhibitors, have been shown to be superior to warfarin in stroke reduction and have a lower or similar bleeding risk in patients with atrial fibrillation. Except in people with atrial fibrillation, oral anticoagulants are not advised for stroke prevention\u2014any benefit is offset by bleeding risk.\nIn primary prevention, however, antiplatelet drugs did not reduce the risk of ischemic stroke but increased the risk of major bleeding. Further studies are needed to investigate a possible protective effect of aspirin against ischemic stroke in women.\n\nSurgery\nCarotid endarterectomy or carotid angioplasty can be used to remove atherosclerotic narrowing of the carotid artery. There is evidence supporting this procedure in selected cases. Endarterectomy for a significant stenosis has been shown to be useful in preventing further stroke in those who have already had the condition. Carotid artery stenting has not been shown to be equally useful. People are selected for surgery based on age, gender, degree of stenosis, time since symptoms and the person's preferences. Surgery is most efficient when not delayed too long\u2014the risk of recurrent stroke in a person who has a 50% or greater stenosis is up to 20% after 5 years, but endarterectomy reduces this risk to around 5%. The number of procedures needed to cure one person was 5 for early surgery (within two weeks after the initial stroke), but 125 if delayed longer than 12 weeks.\nScreening for carotid artery narrowing has not been shown to be a useful test in the general population. Studies of surgical intervention for carotid artery stenosis without symptoms have shown only a small decrease in the risk of stroke. To be beneficial, the complication rate of the surgery should be kept below 4%. Even then, for 100 surgeries, 5 people will benefit by avoiding stroke, 3 will develop stroke despite surgery, 3 will develop stroke or die due to the surgery itself, and 89 will remain stroke-free but would also have done so without intervention.\n\nDiet\nNutrition, specifically the Mediterranean-style diet, has the potential to decrease the risk of having a stroke by more than half. It does not appear that lowering levels of homocysteine with folic acid affects the risk of stroke.\n\nWomen\nA number of specific recommendations have been made for women including taking aspirin after the 11th week of pregnancy if there is a history of previous chronic high blood pressure and taking blood pressure medications during pregnancy if the blood pressure is greater than 150 mmHg systolic or greater than 100 mmHg diastolic. In those who have previously had preeclampsia, other risk factors should be treated more aggressively.\n\nPrevious stroke or TIA\nKeeping blood pressure below 140\/90 mmHg is recommended. Anticoagulation can prevent recurrent ischemic stroke. Among people with nonvalvular atrial fibrillation, anticoagulation can reduce stroke by 60% while antiplatelet agents can reduce stroke by 20%. However, a recent meta-analysis suggests harm from anticoagulation started early after an embolic stroke. Stroke prevention treatment for atrial fibrillation is determined according to the CHA2DS2\u2013VASc score. The most widely used anticoagulant to prevent thromboembolic stroke in people with nonvalvular atrial fibrillation is the oral agent warfarin while a number of newer agents including dabigatran are alternatives which do not require prothrombin time monitoring.\nAnticoagulants, when used following stroke, should not be stopped for dental procedures.\nIf studies show carotid artery stenosis, and the person has a degree of residual function on the affected side, carotid endarterectomy (surgical removal of the stenosis) may decrease the risk of recurrence if performed rapidly after stroke.\n\nManagement\nStroke, whether ischemic or hemorrhagic, is an emergency that warrants immediate medical attention. The specific treatment will depend on the type of stroke, the time elapsed since the onset of symptoms, and the underlying cause or presence of comorbidities.\n\nIschemic stroke\nAspirin reduces the overall risk of recurrence by 13% with greater benefit early on. Definitive therapy within the first few hours is aimed at removing the blockage by breaking the clot down (thrombolysis), or by removing it mechanically (thrombectomy). The philosophical premise underlying the importance of rapid stroke intervention was summed up as Time is Brain! in the early 1990s. Years later, that same idea, that rapid cerebral blood flow restoration results in fewer brain cells dying, has been proved and quantified.\nTight blood sugar control in the first few hours does not improve outcomes and may cause harm. High blood pressure is also not typically lowered as this has not been found to be helpful. Cerebrolysin, a mixture of pig brain-derived neurotrophic factors used widely to treat acute ischemic stroke in China, Eastern Europe, Russia, post-Soviet countries, and other Asian countries, does not improve outcomes or prevent death and may increase the risk of severe adverse events. There is also no evidence that cerebrolysin\u2010like peptide mixtures which are extracted from cattle brain is helpful in treating acute ischemic stroke.\n\nThrombolysis\nThrombolysis, such as with recombinant tissue plasminogen activator (rtPA), in acute ischemic stroke, when given within three hours of symptom onset, results in an overall benefit of 10% with respect to living without disability. It does not, however, improve chances of survival. Benefit is greater the earlier it is used. Between three and four and a half hours the effects are less clear. The AHA\/ASA recommend it for certain people in this time frame. A 2014 review found a 5% increase in the number of people living without disability at three to six months; however, there was a 2% increased risk of death in the short term. After four and a half hours thrombolysis worsens outcomes. These benefits or lack of benefits occurred regardless of the age of the person treated. There is no reliable way to determine who will have an intracranial bleed post-treatment versus who will not. In those with findings of savable tissue on medical imaging between 4.5 hours and 9 hours or who wake up with stroke, alteplase results in some benefit.\nIts use is endorsed by the American Heart Association, the American College of Emergency Physicians and the American Academy of Neurology as the recommended treatment for acute stroke within three hours of onset of symptoms as long as there are no other contraindications (such as abnormal lab values, high blood pressure, or recent surgery). This position for tPA is based upon the findings of two studies by one group of investigators which showed that tPA improves the chances for a good neurological outcome. When administered within the first three hours thrombolysis improves functional outcome without affecting mortality. 6.4% of people with large stroke developed substantial brain bleeding as a complication from being given tPA thus part of the reason for increased short term mortality. The American Academy of Emergency Medicine had previously stated that objective evidence regarding the applicability of tPA for acute ischemic stroke was insufficient. In 2013 the American College of Emergency Medicine refuted this position, acknowledging the body of evidence for the use of tPA in ischemic stroke; but debate continues. Intra-arterial fibrinolysis, where a catheter is passed up an artery into the brain and the medication is injected at the site of thrombosis, has been found to improve outcomes in people with acute ischemic stroke.\n\nEndovascular treatment\nMechanical removal of the blood clot causing the ischemic stroke, called mechanical thrombectomy, is a potential treatment for occlusion of a large artery, such as the middle cerebral artery. In 2015, one review demonstrated the safety and efficacy of this procedure if performed within 12 hours of the onset of symptoms. It did not change the risk of death but did reduce disability compared to the use of intravenous thrombolysis, which is generally used in people evaluated for mechanical thrombectomy. Certain cases may benefit from thrombectomy up to 24 hours after the onset of symptoms.\n\nCraniectomy\nStroke affecting large portions of the brain can cause significant brain swelling with secondary brain injury in surrounding tissue. This phenomenon is mainly encountered in stroke affecting brain tissue dependent upon the middle cerebral artery for blood supply and is also called \"malignant cerebral infarction\" because it carries a dismal prognosis. Relief of the pressure may be attempted with medication, but some require hemicraniectomy, the temporary surgical removal of the skull on one side of the head. This decreases the risk of death, although some people \u2013 who would otherwise have died \u2013 survive with disability.\n\nHemorrhagic stroke\nPeople with intracerebral hemorrhage require supportive care, including blood pressure control if required. People are monitored for changes in the level of consciousness, and their blood sugar and oxygenation are kept at optimum levels. Anticoagulants and antithrombotics can make bleeding worse and are generally discontinued (and reversed if possible). A proportion may benefit from neurosurgical intervention to remove the blood and treat the underlying cause, but this depends on the location and the size of the hemorrhage as well as patient-related factors, and ongoing research is being conducted into the question as to which people with intracerebral hemorrhage may benefit.\nIn subarachnoid hemorrhage, early treatment for underlying cerebral aneurysms may reduce the risk of further hemorrhages. Depending on the site of the aneurysm this may be by surgery that involves opening the skull or endovascularly (through the blood vessels).\n\nStroke unit\nIdeally, people who have had stroke are admitted to a \"stroke unit\", a ward or dedicated area in a hospital staffed by nurses and therapists with experience in stroke treatment. It has been shown that people admitted to stroke units have a higher chance of surviving than those admitted elsewhere in hospital, even if they are being cared for by doctors without experience in stroke. Nursing care is fundamental in maintaining skin care, feeding, hydration, positioning, and monitoring vital signs such as temperature, pulse, and blood pressure.\n\nRehabilitation\nStroke rehabilitation is the process by which those with disabling stroke undergo treatment to help them return to normal life as much as possible by regaining and relearning the skills of everyday living. It also aims to help the survivor understand and adapt to difficulties, prevent secondary complications, and educate family members to play a supporting role. Stroke rehabilitation should begin almost immediately with a multidisciplinary approach. The rehabilitation team may involve physicians trained in rehabilitation medicine, neurologists, clinical pharmacists, nursing staff, physiotherapists, occupational therapists, speech-language pathologists, and orthotists. Some teams may also include psychologists and social workers, since at least one-third of affected people manifests post stroke depression. Validated instruments such as the Barthel scale may be used to assess the likelihood of a person who has had stroke being able to manage at home with or without support subsequent to discharge from a hospital.\nStroke rehabilitation should be started as quickly as possible and can last anywhere from a few days to over a year. Most return of function is seen in the first few months, and then improvement falls off with the \"window\" considered officially by U.S. state rehabilitation units and others to be closed after six months, with little chance of further improvement. However, some people have reported that they continue to improve for years, regaining and strengthening abilities like writing, walking, running, and talking. Daily rehabilitation exercises should continue to be part of the daily routine for people who have had stroke. Complete recovery is unusual but not impossible and most people will improve to some extent: proper diet and exercise are known to help the brain to recover.\n\nSpatial neglect\nThe body of evidence is uncertain on the efficacy of cognitive rehabilitation for reducing the disabling effects of neglect and increasing independence remains unproven. However, there is limited evidence that cognitive rehabilitation may have an immediate beneficial effect on tests of neglect. Overall, no rehabilitation approach can be supported by evidence for spatial neglect.\n\nAutomobile driving\nThe body of evidence is uncertain whether the use of rehabilitation can improve on-road driving skills following stroke. There is limited evidence that training on a driving simulator will improve performance on recognizing road signs after training. The findings are based on low-quality evidence as further research is needed involving large numbers of participants.\n\nYoga\nBased on low quality evidence, it is uncertain whether yoga has a significant benefit for stroke rehabilitation on measures of quality of life, balance, strength, endurance, pain, and disability scores. Yoga may reduce anxiety and could be included as part of patient-centred stroke rehabilitation. Further research is needed assessing the benefits and safety of yoga in stroke rehabilitation.\n\nAction observation physical therapy for upper limbs\nLow-quality evidence suggests that action observation (a type of physiotherapy that is meant to improve neural plasticity through the mirror-neuronal system) may be of some benefit and has no significant adverse effects, however this benefit may not be clinically significant and further research is suggested.\n\nCognitive rehabilitation for attention deficits\nThe body of scientific evidence is uncertain on the effectiveness of cognitive rehabilitation for attention deficits in patients following stroke. While there may be an immediate effect after treatment on attention, the findings are based on low to moderate quality and small number of studies. Further research is needed to assess whether the effect can be sustained in day-to-day tasks requiring attention.\n\nMotor imagery for gait rehabilitation\nThe latest evidence supports the short-term benefits of motor imagery (MI) on walking speed in individuals who have had stroke, in comparison to other therapies. MI does not improve motor function after stroke and does not seem to cause significant adverse events. The findings are based on low-quality evidence as further research is needed to estimate the effect of MI on walking endurance and the dependence on personal assistance.\n\nPhysical and occupational therapy\nPhysical and occupational therapy have overlapping areas of expertise; however, physical therapy focuses on joint range of motion and strength by performing exercises and relearning functional tasks such as bed mobility, transferring, walking and other gross motor functions. Physiotherapists can also work with people who have had stroke to improve awareness and use of the hemiplegic side. Rehabilitation involves working on the ability to produce strong movements or the ability to perform tasks using normal patterns. Emphasis is often concentrated on functional tasks and people's goals. One example physiotherapists employ to promote motor learning involves constraint-induced movement therapy. Through continuous practice the person relearns to use and adapt the hemiplegic limb during functional activities to create lasting permanent changes. Physical therapy is effective for recovery of function and mobility after stroke. Occupational therapy is involved in training to help relearn everyday activities known as the activities of daily living (ADLs) such as eating, drinking, dressing, bathing, cooking, reading and writing, and toileting. Approaches to helping people with urinary incontinence include physical therapy, cognitive therapy, and specialized interventions with experienced medical professionals, however, it is not clear how effective these approaches are at improving urinary incontinence following stroke.\nTreatment of spasticity related to stroke often involves early mobilizations, commonly performed by a physiotherapist, combined with elongation of spastic muscles and sustained stretching through different positions. Gaining initial improvement in range of motion is often achieved through rhythmic rotational patterns associated with the affected limb. After full range has been achieved by the therapist, the limb should be positioned in the lengthened positions to prevent against further contractures, skin breakdown, and disuse of the limb with the use of splints or other tools to stabilize the joint. Cold ice wraps or ice packs may briefly relieve spasticity by temporarily reducing neural firing rates. Electrical stimulation to the antagonist muscles or vibrations has also been used with some success. Physical therapy is sometimes suggested for people who experience sexual dysfunction following stroke.\n\nInterventions for age-related visual problems in patients with stroke\nWith the prevalence of vision problems increasing with age in stroke patients, the overall effect of interventions for age-related visual problems is uncertain. It is also not sure whether people with stroke respond differently from the general population when treating eye problems. Further research in this area is needed as the body of evidence is very low quality.\n\nSpeech and language therapy\nSpeech and language therapy is appropriate for people with the speech production disorders: dysarthria and apraxia of speech, aphasia, cognitive-communication impairments, and problems with swallowing. \nSpeech and language therapy for aphasia following stroke improves functional communication, reading, writing and expressive language. Speech and language therapy that is higher intensity, higher dose or provided over a long duration of time leads to significantly better functional communication but people might be more likely to drop out of high intensity treatment (up to 15 hours per week). A total of 20-50 hours of speech and language therapy is necessary for the best recovery. The most improvement happens when 2-5 hours of therapy is provided each week over 4-5 days. Recovery is further improved when besides the therapy people practice tasks at home. Speech and language therapy is also effective if it is delivered online through video or by a family member who has been trained by a professional therapist.\nRecovery with therapy for aphasia is also dependent on the recency of stroke and the age of the person. Receiving therapy within a month after the stroke leads to the greatest improvements. 3 or 6 months after the stroke more therapy will be needed but symptoms can still be improved. People with aphasia who are younger than 55 years are the most likely to improve but people older than 75 years can still get better with therapy.\nPeople who have had stroke may have particular problems, such as dysphagia, which can cause swallowed material to pass into the lungs and cause aspiration pneumonia. The condition may improve with time, but in the interim, a nasogastric tube may be inserted, enabling liquid food to be given directly into the stomach. If swallowing is still deemed unsafe, then a percutaneous endoscopic gastrostomy (PEG) tube is passed and this can remain indefinitely. Swallowing therapy has mixed results as of 2018.\n\nDevices\nOften, assistive technology such as wheelchairs, walkers and canes may be beneficial. Many mobility problems can be improved by the use of ankle foot orthoses.\n\nPhysical fitness\nStroke can also reduce people's general fitness. Reduced fitness can reduce capacity for rehabilitation as well as general health. Physical exercises as part of a rehabilitation program following stroke appear safe. Cardiorespiratory fitness training that involves walking in rehabilitation can improve speed, tolerance and independence during walking, and may improve balance. There are inadequate long-term data about the effects of exercise and training on death, dependence and disability after stroke. The future areas of research may concentrate on the optimal exercise prescription and long-term health benefits of exercise. The effect of physical training on cognition also may be studied further.\nThe ability to walk independently in their community, indoors or outdoors, is important following stroke. Although no negative effects have been reported, it is unclear if outcomes can improve with these walking programs when compared to usual treatment.\n\nOther therapy methods\nSome current and future therapy methods include the use of virtual reality and video games for rehabilitation. These forms of rehabilitation offer potential for motivating people to perform specific therapy tasks that many other forms do not. While virtual reality and interactive video gaming are not more effective than conventional therapy for improving upper limb function, when used in conjunction with usual care these approaches may improve upper limb function and ADL function. There are inadequate data on the effect of virtual reality and interactive video gaming on gait speed, balance, participation and quality of life. Many clinics and hospitals are adopting the use of these off-the-shelf devices for exercise, social interaction, and rehabilitation because they are affordable, accessible and can be used within the clinic and home.\nMirror therapy is associated with improved motor function of the upper extremity in people who have had stroke.\nOther non-invasive rehabilitation methods used to augment physical therapy of motor function in people recovering from stroke include transcranial magnetic stimulation and transcranial direct-current stimulation. and robotic therapies. Constraint\u2010induced movement therapy (CIMT), mental practice, mirror therapy, interventions for sensory impairment, virtual reality and a relatively high dose of repetitive task practice may be effective in improving upper limb function. However, further primary research, specifically of CIMT, mental practice, mirror therapy and virtual reality is needed.\n\nOrthotics\nClinical studies confirm the importance of orthoses in stroke rehabilitation. The orthosis supports the therapeutic applications and also helps to mobilize the patient at an early stage. With the help of an orthosis, physiological standing and walking can be learned again, and late health consequences caused by a wrong gait pattern can be prevented. A treatment with an orthosis can therefore be used to support the therapy.\n\nSelf-management\nStroke can affect the ability to live independently and with quality. Self-management programs are a special training that educates stroke survivors about stroke and its consequences, helps them acquire skills to cope with their challenges, and helps them set and meet their own goals during their recovery process. These programs are tailored to the target audience, and led by someone trained and expert in stroke and its consequences (most commonly professionals, but also stroke survivors and peers). A 2016 review reported that these programs improve the quality of life after stroke, without negative effects. People with stroke felt more empowered, happy and satisfied with life after participating in this training.\n\nPrognosis\nDisability affects 75% of stroke survivors enough to decrease their ability to work.\nStroke can affect people physically, mentally, emotionally, or a combination of the three. The results of stroke vary widely depending on size and location of the lesion.\n\nPhysical effects\nSome of the physical disabilities that can result from stroke include muscle weakness, numbness, pressure sores, pneumonia, incontinence, apraxia (inability to perform learned movements), difficulties carrying out daily activities, appetite loss, speech loss, vision loss and pain. If the stroke is severe enough, or in a certain location such as parts of the brainstem, coma or death can result. Up to 10% of people following stroke develop seizures, most commonly in the week subsequent to the event; the severity of the stroke increases the likelihood of a seizure. An estimated 15% of people experience urinary incontinence for more than a year following stroke. 50% of people have a decline in sexual function (sexual dysfunction) following stroke.\n\nEmotional and mental effects\nEmotional and mental dysfunctions correspond to areas in the brain that have been damaged. Emotional problems following stroke can be due to direct damage to emotional centers in the brain or from frustration and difficulty adapting to new limitations. Post-stroke emotional difficulties include anxiety, panic attacks, flat affect (failure to express emotions), mania, apathy and psychosis. Other difficulties may include a decreased ability to communicate emotions through facial expression, body language and voice.\nDisruption in self-identity, relationships with others, and emotional well-being can lead to social consequences after stroke due to the lack of ability to communicate. Many people who experience communication impairments after stroke find it more difficult to cope with the social issues rather than physical impairments. Broader aspects of care must address the emotional impact speech impairment has on those who experience difficulties with speech after stroke. Those who experience a stroke are at risk of paralysis, which could result in a self-disturbed body image, which may also lead to other social issues.\n30 to 50% of stroke survivors develop post-stroke depression, which is characterized by lethargy, irritability, sleep disturbances, lowered self-esteem and withdrawal. Depression can reduce motivation and worsen outcome, but can be treated with social and family support, psychotherapy and, in severe cases, antidepressants. Psychotherapy sessions may have a small effect on improving mood and preventing depression after stroke. Antidepressant medications may be useful for treating depression after stroke but are associated with central nervous system and gastrointestinal adverse events.\nEmotional lability, another consequence of stroke, causes the person to switch quickly between emotional highs and lows and to express emotions inappropriately, for instance with an excess of laughing or crying with little or no provocation. While these expressions of emotion usually correspond to the person's actual emotions, a more severe form of emotional lability causes the affected person to laugh and cry pathologically, without regard to context or emotion. Some people show the opposite of what they feel, for example crying when they are happy. Emotional lability occurs in about 20% of those who have had stroke. Those with a right hemisphere stroke are more likely to have empathy problems which can make communication harder.\nCognitive deficits resulting from stroke include perceptual disorders, aphasia, dementia, and problems with attention and memory. Stroke survivors may be unaware of their own disabilities, a condition called anosognosia. In a condition called hemispatial neglect, the affected person is unable to attend to anything on the side of space opposite to the damaged hemisphere. Cognitive and psychological outcome after stroke can be affected by the age at which the stroke happened, pre-stroke baseline intellectual functioning, psychiatric history and whether there is pre-existing brain pathology.\n\nEpidemiology\nStroke was the second most frequent cause of death worldwide in 2011, accounting for 6.2 million deaths (~11% of the total). Approximately 17 million people had stroke in 2010 and 33 million people have previously had stroke and were still alive. Between 1990 and 2010 the incidence of stroke decreased by approximately 10% in the developed world and increased by 10% in the developing world. Overall, two-thirds of stroke occurred in those over 65 years old. South Asians are at particularly high risk of stroke, accounting for 40% of global stroke deaths. Incidence of ischemic stroke is ten times more frequent than haemorrhagic stroke.\nIt is ranked after heart disease and before cancer. In the United States stroke is a leading cause of disability, and recently declined from the third leading to the fourth leading cause of death. Geographic disparities in stroke incidence have been observed, including the existence of a \"stroke belt\" in the southeastern United States, but causes of these disparities have not been explained.\nThe risk of stroke increases exponentially from 30 years of age, and the cause varies by age. Advanced age is one of the most significant stroke risk factors. 95% of stroke occurs in people age 45 and older, and two-thirds of stroke occurs in those over the age of 65.\nA person's risk of dying if he or she does have stroke also increases with age. However, stroke can occur at any age, including in childhood.\nFamily members may have a genetic tendency for stroke or share a lifestyle that contributes to stroke. Higher levels of Von Willebrand factor are more common amongst people who have had ischemic stroke for the first time. The results of this study found that the only significant genetic factor was the person's blood type. Having stroke in the past greatly increases one's risk of future stroke.\nMen are 25% more likely to develop stroke than women, yet 60% of deaths from stroke occur in women. Since women live longer, they are older on average when they have stroke and thus more often killed. Some risk factors for stroke apply only to women. Primary among these are pregnancy, childbirth, menopause, and the treatment thereof (HRT).\n\nHistory\nEpisodes of stroke and familial stroke have been reported from the 2nd millennium BC onward in ancient Mesopotamia and Persia. Hippocrates (460 to 370 BC) was first to describe the phenomenon of sudden paralysis that is often associated with ischemia. Apoplexy, from the Greek word meaning \"struck down with violence\", first appeared in Hippocratic writings to describe this phenomenon.\nThe word stroke was used as a synonym for apoplectic seizure as early as 1599, and is a fairly literal translation of the Greek term. The term apoplectic stroke is an archaic, nonspecific term, for a cerebrovascular accident accompanied by haemorrhage or haemorrhagic stroke. Martin Luther was described as having an apoplectic stroke that deprived him of his speech shortly before his death in 1546.\nIn 1658, in his Apoplexia, Johann Jacob Wepfer (1620\u20131695) identified the cause of hemorrhagic stroke when he suggested that people who had died of apoplexy had bleeding in their brains.\nWepfer also identified the main arteries supplying the brain, the vertebral and carotid arteries, and identified the cause of a type of ischemic stroke known as a cerebral infarction when he suggested that apoplexy might be caused by a blockage to those vessels. Rudolf Virchow first described the mechanism of thromboembolism as a major factor.\nThe term cerebrovascular accident was introduced in 1927, reflecting a \"growing awareness and acceptance of vascular theories and (...) recognition of the consequences of a sudden disruption in the vascular supply of the brain\". Its use is now discouraged by a number of neurology textbooks, reasoning that the connotation of fortuitousness carried by the word accident insufficiently highlights the modifiability of the underlying risk factors. Cerebrovascular insult may be used interchangeably.\nThe term brain attack was introduced for use to underline the acute nature of stroke according to the American Stroke Association, which has used the term since 1990, and is used colloquially to refer to both ischemic as well as hemorrhagic stroke.\n\nResearch\nAs of 2017, angioplasty and stents were under preliminary clinical research to determine the possible therapeutic advantages of these procedures in comparison to therapy with statins, antithrombotics, or antihypertensive drugs.\n\nSee also\nReferences\nFurther reading\nExternal links\n\nStroke at Curlie\nDRAGON Score for Post-Thrombolysis Archived 2020-10-27 at the Wayback Machine\nTHRIVE score for stroke outcome Archived 2016-09-13 at the Wayback Machine\nNational Institute of Neurological Disorders and Stroke","86":"Histopathology (compound of three Greek words: \u1f31\u03c3\u03c4\u03cc\u03c2 histos 'tissue', \u03c0\u03ac\u03b8\u03bf\u03c2 pathos 'suffering', and -\u03bb\u03bf\u03b3\u03af\u03b1 -logia 'study of') is the microscopic examination of tissue in order to study the manifestations of disease. Specifically, in clinical medicine, histopathology refers to the examination of a biopsy or surgical specimen by a pathologist, after the specimen has been processed and histological sections have been placed onto glass slides. In contrast, cytopathology examines free cells or tissue micro-fragments (as \"cell blocks\n\").\n\nCollection of tissues\nHistopathological examination of tissues starts with surgery, biopsy, or autopsy. The tissue is removed from the body or plant, and then, often following expert dissection in the fresh state, placed in a fixative which stabilizes the tissues to prevent decay. The most common fixative is 10% neutral buffered formalin (corresponding to 3.7% w\/v formaldehyde in neutral buffered water, such as phosphate buffered saline).\n\nPreparation for histology\nThe tissue is then prepared for viewing under a microscope using either chemical fixation or frozen section.\nIf a large sample is provided e.g. from a surgical procedure then a pathologist looks at the tissue sample and selects the part most likely to yield a useful and accurate diagnosis - this part is removed for examination in a process commonly known as grossing or cut up. Larger samples are cut to correctly situate their anatomical structures in the cassette. Certain specimens (especially biopsies) can undergo agar pre-embedding to assure correct tissue orientation in cassette & then in the block & then on the diagnostic microscopy slide. This is then placed into a plastic cassette for most of the rest of the process.\n\nChemical fixation\nIn addition to formalin, other chemical fixatives have been used. But, with the advent of immunohistochemistry (IHC) staining and diagnostic molecular pathology testing on these specimen samples, formalin has become the standard chemical fixative in human diagnostic histopathology. Fixation times for very small specimens are shorter, and standards exist in human diagnostic histopathology.\n\nProcessing\nWater is removed from the sample in successive stages by the use of increasing concentrations of alcohol. Xylene is used in the last dehydration phase instead of alcohol - this is because the wax used in the next stage is soluble in xylene where it is not in alcohol allowing wax to permeate (infiltrate) the specimen. This process is generally automated and done overnight. The wax infiltrated specimen is then transferred to an individual specimen embedding (usually metal) container. Finally, molten wax is introduced around the specimen in the container and cooled to solidification so as to embed it in the wax block.  This process is needed to provide a properly oriented sample sturdy enough for obtaining a thin microtome section(s) for the slide.\nOnce the wax embedded block is finished, sections will be cut from it and usually placed to float on a water bath surface which spreads the section out. This is usually done by hand and is a skilled job (histotechnologist) with the lab personnel making choices about which parts of the specimen microtome wax ribbon to place on slides. A number of slides will usually be prepared from different levels throughout the block. After this the thin section mounted slide is stained and a protective cover slip is mounted on it. For common stains, an automatic process is normally used; but rarely used stains are often done by hand.\n\nFrozen section processing\nAn initial evaluation of a suspected lymphoma is to make a \"touch prep\" wherein a glass slide is lightly pressed against excised lymphoid tissue, and subsequently stained (usually H&E stain) for evaluation under light microscopy.\nThe second method of histology processing is called frozen section processing. This is a highly technical scientific method performed by a trained histoscientist. In this method, the tissue is frozen and sliced thinly using a microtome mounted in a below-freezing refrigeration device called the cryostat.  The thin frozen sections are mounted on a glass slide, fixed immediately & briefly in liquid fixative, and stained using the similar staining techniques as traditional wax embedded sections.  The advantages of this method is rapid processing time, less equipment requirement, and less need for ventilation in the laboratory.  The disadvantage is the poor quality of the final slide.  It is used in intra-operative pathology for determinations that might help in choosing the next step in surgery during that surgical session (for example, to preliminarily determine clearness of the resection margin of a tumor during surgery).\n\nStaining of processed histology slides\nThis can be done to slides processed by the chemical fixation or frozen section slides.  To see the tissue under a microscope, the sections are stained with one or more pigments. The aim of staining is to reveal cellular components; counterstains are used to provide contrast.\nThe most commonly used stain in histology is a combination of hematoxylin and eosin (often abbreviated H&E). Hematoxylin is used to stain nuclei blue, while eosin stains the cytoplasm and the extracellular connective tissue matrix of most cells pink. There are hundreds of various other techniques which have been used to selectively stain cells. Other compounds used to color tissue sections include safranin, Oil Red O, congo red, silver salts and artificial dyes. Histochemistry refers to the science of using chemical reactions between laboratory chemicals and components within tissue. A commonly performed histochemical technique is the Perls' Prussian blue reaction, used to demonstrate iron deposits in diseases like Hemochromatosis.\nRecently, antibodies have been used to stain particular proteins, lipids and carbohydrates. Called immunohistochemistry, this technique has greatly increased the ability to specifically identify categories of cells under a microscope. Other advanced techniques include in situ hybridization to identify specific DNA or RNA molecules. These antibody staining methods often require the use of frozen section histology. These procedures above are also carried out in the laboratory under scrutiny and precision by a trained specialist medical laboratory scientist (a histoscientist). Digital cameras are increasingly used to capture histopathological images.\n\nInterpretation\nThe histological slides are examined under a microscope by a pathologist, a medically qualified specialist who has completed a recognised training program. This medical diagnosis is formulated as a pathology report describing the histological findings and the opinion of the pathologist. In the case of cancer, this represents the tissue diagnosis required for most treatment protocols.  In the removal of cancer, the pathologist will indicate whether the surgical margin is cleared, or is involved (residual cancer is left behind).  This is done using either the bread loafing or CCPDMA method of processing. Microscopic visual artifacts can potentially cause misdiagnosis of samples.\nFollowing are examples of general features of suspicious findings that can be appreciated from low to high magnification on histopathology:\n\nArchitectural patterns\nMajor histopathologic architectural patterns include:\n\nNuclear patterns\nMajor nuclear patterns include:\n\nIn myocardial infarction\nAfter a myocardial infarction (heart attack), no histopathology is seen the first ~30 minutes. The only possible sign the first 4 hours is waviness of fibres at border. Later, however, a coagulation necrosis is initiated, with edema and hemorrhage. After 12 hours, there can be seen karyopyknosis and hypereosinophilia of myocytes with contraction band necrosis in margins, as well as beginning of neutrophil infiltration. At 1 \u2013 3 days there is continued coagulation necrosis with loss of nuclei and striations and an increased infiltration of neutrophils to interstitium. Until the end of the first week after infarction there is beginning of disintegration of dead muscle fibres, necrosis of neutrophils and beginning of macrophage removal of dead cells at border, which increases the succeeding days. After a week there is also beginning of granulation tissue formation at margins, which matures during the following month, and gets increased collagen deposition and decreased cellularity until the myocardial scarring is fully mature at approximately 2 months after infarction.\n\nSee also\nAnatomical pathology\nMolecular pathology\nFrozen section procedure\nMedical technologist\nLaser capture microdissection\nList of pathologists\n\nReferences\nExternal links\n\nVirtual Histology Course - University of Zurich (German, English version in preparation)\nHistopathology of the uterine cervix - digital atlas (IARC Screening Group)\nHistopathology Virtual Slidebox - University of Iowa","87":"Scopolamine, also known as hyoscine, or Devil's Breath, is a natural or synthetically produced tropane alkaloid and anticholinergic drug that is used as a medication to treat motion sickness and postoperative nausea and vomiting. It is also sometimes used before surgery to decrease saliva. When used by injection, effects begin after about 20 minutes and last for up to 8 hours. It may also be used orally and as a transdermal patch since it has been long known to have transdermal bioavailability.\nScopolamine is in the antimuscarinic family of drugs and works by blocking some of the effects of acetylcholine within the nervous system.\nScopolamine was first written about in 1881 and started to be used for anesthesia around 1900. Scopolamine is also the main active component produced by certain plants of the nightshade family, which historically have been used as psychoactive drugs, known as deliriants, due to their antimuscarinic-induced hallucinogenic effects in higher doses. In these contexts, its mind-altering effects have been utilized for recreational and occult purposes. The name \"scopolamine\" is derived from one type of nightshade known as Scopolia, while the name \"hyoscine\" is derived from another type known as Hyoscyamus niger, or black henbane. It is on the World Health Organization's List of Essential Medicines.\n\nMedical uses\nScopolamine has a number of formal uses in modern medicine where it is used in its isolated form and in low doses to treat:\n\nPostoperative nausea and vomiting.\nMotion sickness, including sea sickness, leading to its use by scuba divers (where it is often applied as a transdermal patch behind the ear)\nGastrointestinal spasms\nRenal or biliary spasms\nAid in gastrointestinal radiology and endoscopy\nIrritable bowel syndrome\nClozapine-induced drooling\nBowel colic\nEye inflammation\nIt is sometimes used as a premedication, especially to reduce respiratory tract secretions in surgery, most commonly by injection. Common side effects include sleepiness, blurred vision, dilated pupils, and dry mouth. It is not recommended in people with angle-closure glaucoma or bowel obstruction. Whether its use during pregnancy is safe remains unclear, and use during breastfeeding is still cautioned by health professionals and manufacturers of the drug.\n\nBreastfeeding\nScopolamine enters breast milk by secretion. Although no human studies exist to document the safety of scopolamine while nursing, the manufacturer recommends that caution be taken if scopolamine is administered to a breastfeeding woman.\n\nAdverse effects\nAdverse effect incidence:\nUncommon (0.1\u20131% incidence) adverse effects include:\n\nDry mouth\nAnhidrosis (reduced ability to sweat to cool off)\nTachycardia (usually occurs at higher doses and is succeeded by bradycardia)\nBradycardia\nUrticaria (hives)\nPruritus (itching)\nRare (<0.1% incidence) adverse effects include:\n\nConstipation\nUrinary retention\nHallucinations\nAgitation\nConfusion\nRestlessness\nSeizures\nUnknown frequency adverse effects include:\n\nAnaphylactic shock or reactions\nDyspnea (shortness of breath)\nRash\nErythema\nOther hypersensitivity reactions\nBlurred vision\nMydriasis (dilated pupils)\nDrowsiness\nDizziness\nSomnolence\nDeath\n\nOverdose\nPhysostigmine, a cholinergic drug that readily crosses the blood\u2013brain barrier, has been used as an antidote to treat the central nervous system depression symptoms of a scopolamine overdose. Other than this supportive treatment, gastric lavage and induced emesis (vomiting) are usually recommended as treatments for oral overdoses. The symptoms of overdose include:\n\nTachycardia\nArrhythmia\nBlurred vision\nPhotophobia\nUrinary retention\nDrowsiness or paradoxical reaction, which can present with hallucinations\nCheyne\u2013Stokes respiration\nDry mouth\nSkin reddening\nInhibition of gastrointestinal motility\n\nRoute of administration\nScopolamine can be taken by mouth, subcutaneously, in the eye, and intravenously, as well as via a transdermal patch.\n\nPharmacology\nPharmacodynamics\nThe pharmacological effects of scopolamine are mediated through the drug's competitive antagonism of the peripheral and central muscarinic acetylcholine receptors. Scopolamine acts as a nonspecific muscarinic antagonist at all four (M1, M2, M3, and M4) receptor sites.\nIn doses higher than intended for medicinal use; the hallucinogenic alteration of consciousness, as well as the deliriousness in particular are tied to the compound's activity at the M1 muscarinic receptor. M1 receptors are located primarily in the central nervous system and are involved in perception, attention and cognitive functioning. Delirium is only associated with the antagonism of postsynaptic M1 receptors and currently other receptor subtypes have not been implicated. \nPeripheral muscarinic receptors are part of the autonomic nervous system. M2 receptors are located in the brain and heart, M3 receptors are in salivary glands and M4 receptors are in the brain and lungs. Due to the drug's inhibition of various signal transduction pathways, the decrease in acetylcholine signaling is what leads to many of the cognitive deficits, mental impairments and delirium associated with psychoactive doses. Medicinal effects appear to mostly be tied to activation of the peripheral receptors and only from marginal decreases in acetylcholine signaling.\nAlthough often broadly referred to as simply being 'anticholinergic', antimuscarinic would be more specific and accurate terminology to use for scopolamine, as, for example, it is not known to block nicotinic receptors.\n\nPharmacokinetics\nScopolamine undergoes first-pass metabolism and about 2.6% is excreted unchanged in urine. It has a bioavailability of 20-40%, reaches peak plasma concentration in about 45 minutes, and in healthy subjects has an average half-life of 5 hours (observed range 2 - 10 hours). Scopolamine is primarily metabolized by the CYP3A4 enzyme, and Grapefruit juice decreases metabolism of scopolamine, consequently increasing plasma concentration.\n\nChemistry\nBiosynthesis in plants\nScopolamine is among the secondary metabolites of plants from Solanaceae (nightshade) family of plants, such as henbane (Hyoscyamus niger), jimson weed (Datura), angel's trumpet (Brugmansia), deadly nightshade (Belladonna), mandrake (Mandragora officinarum), and corkwood (Duboisia).\n\nThe biosynthesis of scopolamine begins with the decarboxylation of L-ornithine to putrescine by ornithine decarboxylase. Putrescine is methylated to N-methylputrescine by putrescine N-methyltransferase.\nA putrescine oxidase that specifically recognizes methylated putrescine catalyzes the deamination of this compound to 4-methylaminobutanal, which then undergoes a spontaneous ring formation to N-methyl-pyrrolium cation. In the next step, the pyrrolium cation condenses with acetoacetic acid yielding hygrine. No enzymatic activity could be demonstrated to catalyze this reaction. Hygrine further rearranges to tropinone.\nSubsequently, tropinone reductase I converts tropinone to tropine, which condenses with phenylalanine-derived phenyllactate to littorine. A cytochrome P450 classified as Cyp80F1 oxidizes and rearranges littorine to hyoscyamine aldehyde. In the final step, hyoscyamine undergoes epoxidation catalyzed by 6beta-hydroxyhyoscyamine epoxidase yielding scopolamine.\n\nHistory\nPlants naturally containing scopolamine such as Atropa belladonna (deadly nightshade), Brugmansia (angels trumpet), Datura (Jimson weed), Hyoscyamus niger, Mandragora officinarum, Scopolia carniolica, Latua and Duboisia myoporoides have been known about and used for various purposes in both the New and Old Worlds since ancient times. Being one of the earlier alkaloids isolated from plant sources, scopolamine has been in use in its purified forms, such as various salts, including hydrochloride, hydrobromide, hydroiodide, and sulfate, since its official isolation by the German scientist Albert Ladenburg in 1880, and as various preparations from its plant-based form since antiquity and perhaps prehistoric times. \nIn 1899, a Dr. Schneiderlin recommended the use of scopolamine and morphine for surgical anaesthesia, and it started to be used sporadically for that purpose. The use of this combination in obstetric anesthesiology (childbirth) was first proposed by Richard von Steinbuchel in 1902 and was picked up and further developed by Carl Gauss in Freiburg, Germany, starting in 1903. The method, which was based on a drug synergy between both scopolamine and morphine came to be known as D\u00e4mmerschlaf (\"twilight sleep\") or the \"Freiburg method\". It spread rather slowly, and different clinics experimented with different dosages and ingredients. In 1915, the Canadian Medical Association Journal reported, \"the method [was] really still in a state of development\". It remained widely used in the US until the 1960s, when growing chemophobia and a desire for more natural childbirth led to its abandonment.\n\nSociety and culture\nNames\nHyoscine hydrobromide is the international nonproprietary name, and scopolamine hydrobromide is the United States Adopted Name. Other names include levo-duboisine, devil's breath, and burundanga.\n\nAustralian bush medicine\nA bush medicine developed by Aboriginal peoples of the eastern states of Australia from the soft corkwood tree (Duboisia myoporoides) was used by the Allies in World War II to stop soldiers from getting seasick when they sailed across the English Channel on their way to France during the Invasion of Normandy. Later, the same substance was found to be usable in the production of scopolamine and hyoscyamine, which are used in eye surgery, and a multimillion-dollar industry was built in Queensland based on this substance.\n\nRecreational and religious use\nWhile it has been occasionally used recreationally for its hallucinogenic properties, the experiences are often unpleasant, mentally and physically. It is also physically dangerous and officially classified as a deliriant drug, so repeated recreational use is rare. In June 2008, more than 20 people were hospitalized with psychosis in Norway after ingesting counterfeit rohypnol tablets containing scopolamine. In January 2018, 9 individuals were hospitalized in Perth, Western Australia, after reportedly ingesting scopolamine. \nThe alkaloid scopolamine, when taken recreationally for its psychoactive effect is usually taken in the form of preparations from plants of the genera Datura or Brugmansia, often by adolescents or young adults in order to achieve hallucinations and an altered state of consciousness induced by muscarinic antagonism. In circumstances such as these, the intoxication is usually built on a synergistic, but even more toxic mixture of the additional alkaloids in the plants which includes atropine and hyoscyamine.\nHistorically, the various plants that produce scopolamine have been used psychoactively for spiritual and magical purposes, particularly by witches in western culture and indigenous groups throughout the Americas such as Native American tribes like the Chumash. When entheogenic preparations of these plants were used, scopolamine was considered to be the main psychoactive compound and was largely responsible for the hallucinogenic effects, particularly when the preparation was made into a topical ointment, most notably flying ointment. \nScopolamine is reported to be the only active alkaloid within these plants that can effectively be absorbed through the skin to cause effects. Different recipes for these ointments were explored in European witchcraft at least as far back as the Early Modern period and included multiple ingredients to help with the transdermal absorption of scopolamine, such as animal fat, as well as other possible ingredients to counteract its noxious and dysphoric effects.\nIn the Bible, there are multiple mentions of Mandrake, a psychoactive and hallucinogenic plant root that contains scopolamine. It was associated with fertility and (sexual) desire where it was yearned for by Rachel, who apparently was \"barren\" (infertile) but trying to conceive.\n\nInterrogation\nThe effects of scopolamine were studied for use as a truth serum in interrogations in the early 20th century, but because of the side effects, investigations were dropped. In 2009, the Czechoslovak state security secret police were proven to have used scopolamine at least three times to obtain confessions from alleged antistate dissidents.\n\nUse in crime\nIngestion of scopolamine can render a victim unconscious for 24 hours or more. In large doses, it can cause respiratory failure and death. The most common seems to be recorded in Colombia, where unofficial estimates put the number of annual scopolamine incidents at approximately 50,000. A travel advisory published by the U.S. Overseas Security Advisory Council (OSAC) in 2012 stated: One common and particularly dangerous method that criminals use in order to rob a victim is through the use of drugs. Scopolamine is most often administered in liquid or powder form in foods and beverages. The majority of these incidents occur in night clubs and bars, and usually men, perceived to be wealthy, are targeted by young, attractive women. It is recommended that, to avoid becoming a victim of scopolamine, a person should never accept food or beverages offered by strangers or new acquaintances, nor leave food or beverages unattended in their presence. Victims of scopolamine or other drugs should seek immediate medical attention.\nBetween 1998 and 2004, 13% of emergency-room admissions for \"poisoning with criminal intentions\" in a clinic of Bogot\u00e1 have been attributed to scopolamine, and 44% to benzodiazepines. Most commonly, the person has been poisoned by a robber who gave the victim a scopolamine-laced beverage, in the hope that the victim would become unconscious or unable to effectively resist the robbery.\nBeside robberies, it is also allegedly involved in express kidnappings and sexual assault. In 2008, the Hospital Cl\u00ednic in Barcelona introduced a protocol to help medical workers identify cases. In February 2015, Madrid hospitals adopted a similar working document. Hospital Cl\u00ednic has found little scientific evidence to support this use and relies on the victims' stories to reach any conclusion. \nAlthough poisoning by scopolamine appears quite often in the media as an aid for raping, kidnapping, killing, or robbery, the effects of this drug and the way it is applied by criminals (transdermal injection, on playing cards and papers, etc.) are often exaggerated, especially skin exposure, as the dose that can be absorbed by the skin is too low to have any effect. Scopolamine transdermal patches must be used for hours to days. \nThere are certain other aspects of the usage of scopolamine in crimes. Powdered scopolamine is referred to as \"devil's breath\". In popular media and television, it is portrayed as a method to brainwash or control people into being defrauded by their attackers. There is debate whether these claims are true.\n\nResearch\nScopolamine is used as a research tool to study memory encoding. Initially, in human trials, relatively low doses of the muscarinic receptor antagonist scopolamine were found to induce temporary cognitive defects. Since then, scopolamine has become a standard drug for experimentally inducing cognitive defects in animals. Results in primates suggest that acetylcholine is involved in the encoding of new information into long-term memory. Scopolamine has been shown to exert a greater impairment on episodic memory, event-related potentials, memory retention and free recall compared to diphenhydramine (an anticholinergic and antihistamine).\nScopolamine produces detrimental effects on short-term memory, memory acquisition, learning, visual recognition memory, visuospatial praxis, visuospatial memory, visuoperceptual function, verbal recall, and psychomotor speed. It does not seem to impair recognition and memory retrieval, though. Acetylcholine projections in hippocampal neurons, which are vital in mediating long-term potentiation, are inhibited by scopolamine. Scopolamine inhibits cholinergic-mediated glutamate release in hippocampal neurons, which assist in depolarization, potentiation of action potential, and synaptic suppression. Scopolamine's effects on acetylcholine and glutamate release in the hippocampus favor retrieval-dominant cognitive functioning. Scopolamine has been used to model the defects in cholinergic function for models of Alzheimer's, dementia, fragile X syndrome, and Down syndrome.\nScopolamine has been identified as a psychoplastogen, which refers to a compound capable of promoting rapid and sustained neuroplasticity in a single dose. It has been, and continues to be investigated as a rapid-onset antidepressant, with a number of small studies finding positive results, particularly in female subjects.\nNASA agreed to develop a nasal administration method. With a precise dosage, the NASA spray formulation has been shown to work faster and more reliably than the oral form to treat motion sickness.\nAlthough a fair amount of research has been applied to scopolamine in the field of medicine, its hallucinogenic (psychoactive) effects as well as the psychoactive effects of other antimuscarinic deliriants haven't been extensively researched or as well understood compared to other types of hallucinogens such as psychedelic and dissociative compounds, despite the alkaloid's long history of usage in mind-altering plant preparations.\n\nReferences\nExternal links\n Media related to Scopolamine at Wikimedia Commons","88":"Hyperacusis is an increased sensitivity to sound and a low tolerance for environmental noise. Definitions of hyperacusis can vary significantly; it often revolves around damage to or dysfunction of the stapes bone, stapedius muscle or tensor tympani (eardrum).  It is often categorized into four subtypes: loudness, pain (also called noxacusis), annoyance, and fear.  It can be a highly debilitating hearing disorder.\nThere are a variety of causes and risk factors, with the most common being exposure to loud noise. It is often coincident with tinnitus. Proposed mechanisms in the literature involve dysfunction in the brain, inner ear, or middle ear.\nLittle is known about the prevalence of hyperacusis, in part due to the degree of variation in the term's definition. Reported prevalence estimates in the literature vary widely, and further research is needed to obtain strong epidemiological data. While there are no exact numbers, several people have died by suicide due to the severe consequences of the disease.\n\nSigns and symptoms\nHyperacusis symptoms can include an increased perception of the loudness of sounds (loudness hyperacusis), pain (noxacusis\/pain hyperacusis\/sound-induced otalgia), annoyance, and\/or fear in response to sounds by which most people are unaffected. It may affect one or both ears. The majority of patients experience bilateral symptoms but often have one ear that is more affected than the other. Annoyance hyperacusis is often considered synonymous with misophonia. Fear hyperacusis is often considered synonymous with phonophobia. Many researchers more narrowly define hyperacusis to only include loudness hyperacusis and pain hyperacusis.\nHyperacusis can also be accompanied by tinnitus. The latter is more common and there are important differences between their involved mechanisms.\nHyperacusis can result in anxiety and stress. Avoidant behavior is often a response to prevent the effects of hyperacusis and this can include avoiding social situations.\n\nLoudness hyperacusis\nLoudness hyperacusis is characterized by an increased perception of the loudness of sounds. It is often associated with certain volumes and\/or frequencies. It can occur in children and adults, and can be either \"short-term\" in a duration of weeks to less than a year before recovery, or, less commonly, \"long-term,\" spanning years and in some cases becoming permanent. Sensitivity is often different between ears.\n\nNoxacusis\nIn some instances, hyperacusis is accompanied by pain, which is known as noxacusis. Noxacusis is characterized by pain resulting from sounds, often initiated at certain volumes or frequencies. Pain can be immediate or delayed, and it sometimes persists for an extended period of time following exposure. Pain can be acute or chronic, and is often described as stabbing, burning, throbbing, or aching. In healthy listeners, pain from sound is not typically experienced until the volume exceeds approximately 120 decibels.\nIndividuals experiencing noxacusis report less improvement over time and fewer benefits from sound therapy compared to individuals with loudness hyperacusis.\n\nLoudness discomfort level\nThe threshold of sound at which discomfort is initially experienced; measured in decibels (dB).\n\nSetback\nA setback is a temporary exacerbation of symptoms, a worsening of the perception of loudness or pain from sound, often due to a particular noise exposure. Setback prevention is an important focus among those affected. Efforts to avoid setbacks commonly include using hearing protection and avoiding loud noises. Pain hyperacusis patients experience setbacks more frequently than patients with loudness hyperacusis.\n\nAssociated conditions\nSome conditions that are associated with hyperacusis include:\n\nCauses and risk factors\nThe most common cause of hyperacusis is overexposure to excessively high decibel (sound pressure) levels, which can cause acoustic trauma. An acoustic shock, which can lead to symptoms such as hyperacusis and ear pain, can also occur after exposure to an unexpected moderately loud to loud noise, even if this does not necessarily result in permanent cochlear damage.\nSome affected people acquire hyperacusis suddenly as a result of taking ototoxic drugs (which can damage the cells responsible for hearing), Lyme disease, M\u00e9ni\u00e8re's disease, head injury, or surgery. Others are born with sound sensitivity or develop superior canal dehiscence syndrome. Bell's palsy can trigger hyperacusis if the associated flaccid paralysis affects the tensor tympani, and stapedius, two small muscles of the middle ear. Paralysis of the stapedius muscle prevents its function in dampening the oscillations of the ossicles, causing sound to be abnormally loud on the affected side. Age may also be a significant factor, with younger patients exhibiting more severe hyperacusis.\nRecently, it has been discovered that individuals with one copy of the GJB2 (Cx26) genetic mutation exhibit hearing that is more sensitive than average, akin to hyperacusis. These individuals appear to be at greater risk for damage from noise.\nSome psychoactive drugs such as LSD, methaqualone, benzodiazepines, or phencyclidine can cause hyperacusis. An antibiotic, ciprofloxacin, has also been seen to be a cause, known as ciprofloxacin-related hyperacusis. Benzodiazepine withdrawal syndrome is also a possible cause.\n\nEpidemiology\nPrevalence estimates for hyperacusis vary widely in the literature, and further epidemiological data is needed. No gender differences have yet been established among hyperacusis patients. Hyperacusis appears to be more severe in younger patients.\n\nPossible mechanisms\nLoudness hyperacusis\nAs one possible mechanism, adaptation processes in the auditory brain that influence the dynamic range of neural responses are assumed to be distorted by irregular input from the inner ear. This is mainly caused by hearing loss related damage in the inner ear. The mechanism behind hyperacusis is not currently known, but it is suspected to be caused by damage to the inner ear and cochlea.\n\nNoxacusis\nInner ear theory\nType II afferent fibers of the cochlear nerve are not responsible for hearing like the type I afferent fibers. They are thought to be cochlear pain neurons. Gain of function of these type II afferent fibers may be caused by a flood of ATP after hair cell damage. Now sensitized, they react to the small amount of ATP released during the normal process of hearing. This may result in pain.\n\nMiddle ear theory\nNore\u00f1a et al. (2018) propose a model that may account for sound-induced pain and a constellation of other symptoms often experienced after an acoustic shock, acoustic trauma, and potentially other mechanisms of auditory damage. Symptoms may include a sense of fullness in the ear, tinnitus, and dizziness.\nThe model details how symptoms may be initiated by tensor tympani muscle damage or overload due to acoustic shock or trauma. Hypercontraction or hyperactivity of the muscle may cause an \"ATP energy crisis.\" The muscle is then forced to create energy without sufficient oxygen, which results in the release of lactic acid into the middle ear space. This acidity can activate pain-sensing neurons. Muscle relaxation requires energy in the form of ATP. In the setting of low ATP, it is more difficult for the muscle to relax, which causes the cycle to continue. Via a cascade of events, the activated pain neurons can cause neurogenic inflammation, which may lead to additional pain. In this way, a \"vicious circle\" is created.\nPain from sound sometimes radiates to the face, scalp, and neck. This may be due to the trigeminocervical complex in the brainstem, which integrates input from and output to various regions of the head and neck, including the middle ear. Of note, the tensor tympani muscle is innervated by the trigeminal nerve. The model also explains how whiplash injuries, temporomandibular joint dysfunction, and other conditions affecting the head and neck regions may influence the function of the tensor tympani muscle and contribute to ear symptoms such as pain hyperacusis.\n\nDiagnosis\nThe basic diagnostic test is similar to a normal audiogram. The difference is that, in addition to the hearing threshold at each test frequency, the lowest uncomfortable sound level is also measured. This level is called loudness discomfort level (LDL) or uncomfortable loudness level (ULL). In patients with hyperacusis this level is often considerably lower than in normal subjects, and usually across most parts of the auditory spectrum. However, there is not a consensus regarding what constitutes a normal LDL. The relationship between LDL's and self-reported ability to tolerate sounds in everyday life in unclear.\nIn addition to self-report questionnaires, audiologists may employ a variety of other techniques to evaluate auditory function in patients experiencing noise sensitivity. When conducting testing that involves the presentation of sounds, which may cause the patient discomfort or pain, it is vital to inform the patient of the volume and duration of sounds to be presented prior to testing. Care should be taken to begin with sounds of low volume, and volume should be increased gradually. The audiologist and patient should both be prepared to stop testing at any time, depending on the patient's symptoms.\n\nManagement and treatment\nThere are currently no evidence-based guidelines regarding the treatment of patients with hyperacusis. The majority of audiologists report insufficient formal education in this area, likely due in part to the current lack of consensus in the literature regarding definitions and treatment of hyperacusis.\n\nClomipramine\nThe tricyclic anti-depressant clomipramine (brand name Anafranil) has been anecdotally useful for many people with hyperacusis. Both loudness hyperacusis and noxacusis have been successfully treated with this drug.  A dosage of up to 200 mg daily for a sustained period of six months may be needed to cure hyperacusis.  A possible mechanism of action of this drug is that clomipramine reduces reactions of the autonomic nervous system to sounds.\n\nAvoidance and hearing protection\nSetback prevention and reduction of pain symptoms are high priorities among those with hyperacusis and noxacusis, which is often managed through a combination of controlling the environment so as to avoid loud sounds, soundproofing, and wearing hearing protection, such as earplugs and safety earmuffs. Preliminary research has shown that individuals with pain hyperacusis can experience an exacerbation of their symptoms when not adequately protecting themselves against loud sounds.\nThere are diametrically opposing views on avoiding overuse of hearing protection and silence. Some audiologists may advise against using hearing protection in normal sound environments, claiming it can cause or worsen hyperacusis. This is based on a study in healthy volunteers and not individuals with preexisting loudness or pain hyperacusis.\n\nSound therapy\nSound therapy is sometimes recommended for those with hyperacusis, though its application among those with pain (noxacusis) should be used with caution. Tinnitus retraining therapy, a treatment originally used to treat tinnitus, uses broadband noise to treat hyperacusis. Pink noise can also be used to treat hyperacusis. By listening to broadband noise at soft levels for a disciplined period of time each day, some patients can rebuild (i.e., re-establish) their tolerances to sound. More research is needed on the efficacy of sound therapy techniques when hyperacusis is the primary complaint, rather than a secondary symptom, indicating that \"no strong conclusions can be made\" about its efficacy at this time. Importantly, individuals with pain hyperacusis are more likely than individuals with loudness hyperacusis to report worsening of their condition after the use of sound therapy.\n\nCognitive behavioral therapy\nAnother possible treatment is cognitive behavioral therapy (CBT), which may also be combined with sound therapy. However, randomized controlled trials with active control groups are still needed to establish the effectiveness of CBT for hyperacusis and the usefulness of CBT for noxacusis is not yet demonstrated in the scientific literature.\n\nSurgery\nStudies have shown improved loudness discomfort levels in patients with hyperacusis after round and oval window reinforcement.\nA case of chronic ear pain associated with hyperacusis after exposure to loud noise at a concert was successfully treated with tympanic neurectomy.\n\nTreatment for suicidal thoughts\nSuicidal ideations are a risk factor in hyperacusis patients. Hashir et al. (2019) interviewed 292 patients and found that 15.75% had expressed suicidal ideations in the previous two weeks of the study. They recommend screening for these issues.\n\nSociety and culture\nMusician Jason DiEmilio, who recorded under the name Azusa Plane, had hyperacusis and ultimately went on to die by suicide due in part to his sensitivity to noise.\nDietrich Hectors, a hyperacusis sufferer in Belgium, took his life in 2009, after years of struggling with catastrophic hyperacusis and tinnitus. A local government in Belgium made a short film based on his life to warn others of the dangers of hyperacusis and tinnitus.\nMusician Stephin Merritt has monaural hyperacusis in his left ear, which influences the instrumentation of his band, The Magnetic Fields, leads him to wear earplugs during performances and to cover his affected ear during audience applause.\nVladimir Lenin, the Russian communist revolutionary, politician, and political theorist, was reported seriously ill by the latter half of 1921, having hyperacusis and symptoms such as regular headache and insomnia.\nMusician Peter Silberman of The Antlers had hyperacusis and tinnitus which put his musical career on hold, until the conditions reduced down to a \"manageable level\".\nRacing driver Wolfgang Reip developed severe hyperacusis after several noise traumas during his racing career.\nLudwig van Beethoven possibly suffered from hyperacusis alongside his hearing loss. In a letter, he wrote: \"As soon as anybody shouts, I can't bear it. Heaven alone knows what is to become of me.\"\nBritish writer Linda Stratmann suffers from hyperacusis and reports noises \"cut through her like a scalpel.\"\n\nSee also\nHealth effects of noise pollution\nTinnitus\nMisophonia\nOtoacoustic emission\nSafe listening\nSensory processing sensitivity\nRecruitment (medicine)\nWorld Hearing Day\nHearing Health Foundation\n\nReferences\nExternal links\nHyperacusis Research, Ltd.\nHyperacusis Central Website\nHyperacusis Central YouTube\n\n\n== Further reading ==","89":"Hyperventilation syndrome (HVS), also known as chronic hyperventilation syndrome (CHVS), dysfunctional breathing hyperventilation syndrome, cryptotetany, spasmophilia, latent tetany, and central neuronal hyper excitability syndrome (NHS), is a respiratory disorder, psychologically or physiologically based, involving breathing too deeply or too rapidly (hyperventilation). HVS may present with chest pain and a tingling sensation in the fingertips and around the mouth (paresthesia), in some cases resulting in the hands 'locking up' or cramping (carpopedal spasm). HVS may accompany a panic attack.\nPeople with HVS may feel that they cannot get enough air. In reality, they have about the same oxygenation in the arterial blood (normal values are about 98% for hemoglobin saturation) and too little carbon dioxide (hypocapnia) in their blood and other tissues. While oxygen is abundant in the bloodstream, HVS reduces effective delivery of that oxygen to vital organs due to low-CO2-induced vasoconstriction and the suppressed Bohr effect.\nThe hyperventilation is self-promulgating as rapid or deep breathing causes carbon dioxide levels to fall below healthy levels, and respiratory alkalosis (high blood pH) develops. This makes the symptoms worse, which causes the person to breathe even faster, which then, further exacerbates the problem.\nThe respiratory alkalosis leads to changes in the way the nervous system fires and leads to the paresthesia, dizziness, and perceptual changes that often accompany this condition. Other mechanisms may also be at work, and some people are physiologically more susceptible to this phenomenon than others.\nThe mechanism for hyperventilation causing Paresthesia, lightheadedness, and fainting is: hyperventilation causes increased blood pH (see Respiratory alkalosis for this mechanism), which causes a decrease in free ionized calcium (Hypocalcaemia), which causes paresthesia and symptoms related to hypocalcaemia.\n\nCauses\nHyperventilation syndrome is believed to be caused by psychological factors. It is one cause of hyperventilation with others including infection, blood loss, heart attack, hypocapnia or alkalosis due to chemical imbalances, decreased cerebral blood flow, and increased nerve sensitivity.\nIn one study, one third of patients with HVS had \"subtle but definite lung disease\" that prompted them to breathe too frequently or too deeply.\nA study, found that 77% of patients with empty nose syndrome have hyperventilation syndrome. Empty nose syndrome can appear in people having done nose surgery like cauterization, turbinectomy, turbinoplasty, etc.\nMany people with panic disorder or agoraphobia will experience HVS. However, most people with HVS do not have these disorders.\n\nDiagnosis\nHyperventilation syndrome is a remarkably common cause of dizziness complaints. About 25% of patients who complain about dizziness are diagnosed with HVS.\nA 1985 study, Efficacy of Nijmegen Questionnaire in recognition of the hyperventilation syndrome, stated: \"It is concluded that the questionnaire is suitable as a screening instrument for early detection of HVS, and also as an aid in diagnosis and therapy planning.\"\n\nTreatment\nOne review of research, published in 2013, concluded \"The results of this systematic review are unable to inform clinical practice, based on the inclusion of only 1 small poorly reported RCT [randomised controlled trial] ... Therefore, no recommendations for clinical practice can be made.\"\nWhile traditional intervention for an acute episode has been to have the patient breathe into a paper bag, causing rebreathing and restoration of CO\u2082 levels, this is not advised. The same benefits can be obtained more safely from deliberately slowing down the breathing rate by counting or looking at the second hand on a watch. This is sometimes referred to as \"7-11 breathing\", because a gentle inhalation is stretched out to take 7 seconds (or counts), and the exhalation is slowed to take 11 seconds. This in-\/exhalation ratio can be safely decreased to 4-12 or even 4-20 and more, as the O\u2082 content of the blood will easily sustain normal cell function for several minutes at rest when normal blood acidity has been restored.\nIt has also been suggested that breathing therapies such as the Buteyko Breathing method may be effective in reducing the symptoms and recurrence of the syndrome.\nBenzodiazepines can be prescribed to reduce stress that provokes hyperventilation syndrome. Selective serotonin reuptake inhibitors (SSRIs) can reduce the severity and frequency of hyperventilation episodes.\n\nHistory\nThe original traditional treatment of breathing into a paper bag to control psychologically based hyperventilation syndrome (which is now almost universally known and often shown in movies and TV dramas) was invented by New York City physician (later radiologist), Alexander Winter, M.D. [1908-1978], based on his experiences in the U.S. Army Medical Corps during World War II and published in the Journal of the American Medical Association in 1951. Because other medical conditions can be confused with hyperventilation, namely asthma and heart attacks, most medical studies advise against using a paper bag since these conditions worsen when CO2 levels increase.\n\nReferences\n\n\n== External links ==","90":"Hyperbaric medicine is medical treatment in which an increase in barometric pressure over ambient pressure is employed increasing the partial pressures of all gases present in the ambient atmosphere. The immediate effects include reducing the size of gas embolisms and raising the partial pressures of all gases present according to Henry's law.\nCurrently, there are two types of hyperbaric medicine depending on the gases compressed, hyperbaric air and hyperbaric oxygen.\nHyperbaric air (HBA), consists of compressed atmospheric air (79% nitrogen, 21% oxygen, and minor gases) and is FDA-approved for acute mountain sickness. The hyperbaric air environment is created by placing the patient in a portable hyperbaric air chamber and inflating that chamber up to 7.35 psi gauge (1.5 atmospheres absolute) using a foot-operated or electric air pump. Although the mechanisms of hyperbaric air are poorly understood it is thought that it relieves hypoxemia caused by the decreased partial pressure of oxygen resulting from high altitude by increasing the partial pressure of air (including oxygen and nitrogen) simulating a descent in altitude.  \nHyperbaric oxygen therapy (HBOT), the medical use of greater than 99% oxygen at an ambient pressure higher than atmospheric pressure, and therapeutic recompression for decompression illness, intended to reduce the injurious effects of systemic gas bubbles by physically reducing their size and providing improved conditions for elimination of bubbles and excess dissolved gas.\nThe equipment required for hyperbaric oxygen treatment consists of a pressure vessel for human occupancy, which may be of rigid or flexible construction, and a means of a controlled atmosphere supply. Operation is performed to a predetermined schedule by trained personnel who monitor the patient and may adjust the schedule as required. HBOT found early use in the treatment of decompression sickness, and has also shown great effectiveness in treating conditions such as gas gangrene and carbon monoxide poisoning. More recent research has examined the possibility that it may also have value for other conditions such as cerebral palsy and multiple sclerosis, but no significant evidence has been found.\nA pressure vessel for human occupancy (PVHO) is an enclosure that is intended to be occupied by one or more persons at a pressure which differs from ambient by at least 2 pounds per square inch (0.14 bar). All chambers used in the US made for hyperbaric medicine fall under the jurisdiction of the Federal Food and Drug Agency (FDA). The FDA requires hyperbaric chambers to comply with the American Society of Mechanical Engineers PVHO Codes and the National Fire Protection Association Standard 99, Health Care Facilities Code.  Similar conditions apply in most other countries.\nHyperbaric medicine poses some inherent hazards that are mitigated by FDA-compliant equipment and trained personnel.  Serious injury can occur at pressures as low as 2 psig (13.8 kPa) if a person in the PVHO is rapidly decompressed. If oxygen is used in the hyperbaric therapy, this can increase the fire hazard. This is why the FDA requires hyperbaric chambers to meet ASME PVHO and NFPA 99 standards or the local equivalent. All chambers that meet FDA standards must have an ASME data plate, and people seeking hyperbaric treatment should check to ensure the equipment and facilities are to proper standards.\nTherapeutic recompression is usually also provided in a hyperbaric chamber. It is the definitive treatment for decompression sickness and may also be used to treat arterial gas embolism caused by pulmonary barotrauma of ascent. In emergencies divers may sometimes be treated by in-water recompression (when a chamber is not available) if suitable diving equipment (to reasonably secure the airway) is available.\nA number of hyperbaric treatment schedules have been published over the years for both therapeutic recompression and hyperbaric oxygen therapy for other conditions. Some of these use breathing gases other than air or pure oxygen, when the partial pressure of oxygen must be limited but the pressure required is relatively high. Nitrox and Heliox treatment schedules are available for these cases. Treatment gas may be the ambient chamber gas, or delivered via a built-in breathing system.\n\nScope\nHyperbaric medicine includes hyperbaric oxygen treatment, which is the medical use of oxygen at greater than atmospheric pressure to increase the availability of oxygen in the body; and therapeutic recompression, which involves increasing the ambient pressure on a person, usually a diver, to treat decompression sickness or an air embolism by reducing the volume and more rapidly eliminating bubbles that have formed within the body.\n\nMedical uses\nIn the United States the Undersea and Hyperbaric Medical Society, known as UHMS, lists approvals for reimbursement for certain diagnoses in hospitals and clinics. The following indications have approved (for reimbursement) uses of hyperbaric oxygen therapy as defined by the UHMS Hyperbaric Oxygen Therapy Committee:\n\nAir or gas embolism;\nCarbon monoxide poisoning;\nCarbon monoxide poisoning complicated by cyanide poisoning;\nCentral retinal artery occlusion;\nClostridal myositis and myonecrosis (gas gangrene);\nCrush injury, compartment syndrome, and other acute traumatic ischemias;\nDecompression sickness;\nEnhancement of healing in selected problem wounds;\nDiabetically derived illness, such as short-term relief of diabetic foot, diabetic retinopathy, diabetic nephropathy;\nExceptional blood loss (anemia);\nIdiopathic sudden sensorineural hearing loss;\nIntracranial abscess;\nMucormycosis, especially rhinocerebral disease in the setting of diabetes mellitus;\nNecrotizing soft tissue infections (necrotizing fasciitis);\nOsteomyelitis (refractory);\nDelayed radiation injury (soft tissue and bony necrosis);\nSkin grafts and flaps (compromised);\nThermal burns.\nThere is no reliable evidence to support its use in autism, cancer, diabetes, HIV\/AIDS, Alzheimer's disease, asthma, Bell's palsy, cerebral palsy, depression, heart disease, migraines, multiple sclerosis, Parkinson's disease, spinal cord injury, sports injuries, or stroke. Furthermore, there is evidence that potential side effects of hyperbaric medicine pose an unjustified risk in such cases. A Cochrane review published in 2016 reviewed a small set of clinical trials attempting to treat autism spectrum disorders with hyperbaric oxygen therapy. They noted a small sample size and large \"confidence intervals\" did not provide much evidence. No links between improvements in social abilities or cognitive function were noted. There are also ethical issues with further trials, as  the eardrum can be damaged during hyperbaric therapy. Despite the lack of evidence, in 2015, the number of people utilizing this therapy has continued to rise.\nThere is also insufficient evidence to support its use in acute traumatic or surgical wounds.\n\nHearing issues\nThere is limited evidence that hyperbaric oxygen therapy improves hearing in patients with sudden sensorineural hearing loss who present within two weeks of hearing loss. There is some indication that HBOT might improve tinnitus presenting in the same time frame.\n\nChronic ulcers\nHBOT in diabetic foot ulcers increased the rate of early ulcer healing but does not appear to provide any benefit in wound healing at long-term follow-up. In particular, there was no difference in major amputation rate. For venous, arterial and pressure ulcers, no evidence was apparent that HBOT provides a long-term improvement over standard treatment.\n\nRadiation injury\nThere is some evidence that HBOT is effective for late radiation tissue injury of bone and soft tissues of the head and neck. Some people with radiation injuries of the head, neck or bowel show an improvement in quality of life. Importantly, no such effect has been found in neurological tissues. The use of HBOT may be justified to selected patients and tissues, but further research is required to establish the best people to treat and timing of any HBO therapy.\n\nNeuro-rehabilitation\nAs of 2012, there was no sufficient evidence to support using hyperbaric oxygen therapy to treat people who have traumatic brain injuries. In acute stroke, HBOT does not show benefit. Small clinical trials, however, have shown benefits from HBOT for stroke survivors between 6 months to 3 years after the acute phase.\nHBOT in multiple sclerosis has not shown benefit and routine use is not recommended.\nA 2007 review of HBOT in cerebral palsy found no difference compared to the control group. Neuropsychological tests also showed no difference between HBOT and room air and based on caregiver report, those who received room air had significantly better mobility and social functioning. Children receiving HBOT were reported to experience seizures and the need for tympanostomy tubes to equalize ear pressure, though the incidence was not clear.\n\nCancer\nIn alternative medicine, hyperbaric medicine has been promoted as a treatment for cancer. However, a 2011 study by the American Cancer Society reported no evidence it is effective for this purpose. A 2012 review article in the journal, Targeted Oncology, reports that \"there is no evidence indicating that HBO neither acts as a stimulator of tumor growth nor as an enhancer of recurrence. On the other hand, there is evidence that implies that HBO might have tumor-inhibitory effects in certain cancer subtypes, and we thus strongly believe that we need to expand our knowledge on the effect and the mechanisms behind tumor oxygenation.\"\n\nMigraines\nLow-quality evidence suggests that hyperbaric oxygen therapy may reduce the pain associated with an acute migraine headache in some cases. It is not known which people would benefit from this treatment, and there is no evidence that hyperbaric medicine can prevent future migraines. More research is necessary to confirm the effectiveness of hyperbaric oxygen therapy for treating migraines.\n\nRespiratory distress\nPatients who are having extreme difficulty breathing \u2013 acute respiratory distress syndrome \u2013 are commonly given oxygen and there have been limited trials of hyperbaric equipment in such cases. Examples include treatment of the Spanish flu and COVID-19.\n\nContraindications\nThe toxicology of the treatment has been reviewed by Ustundag et al. and its risk management is discussed by Christian R. Mortensen, in light of the fact that most hyperbaric facilities are managed by departments of anaesthesiology and some of their patients are critically ill.\nAn absolute contraindication to hyperbaric oxygen therapy is untreated pneumothorax. The reason is concern that it can progress to tension pneumothorax, especially during the decompression phase of therapy, although treatment on oxygen-based tables may avoid that progression. The COPD patient with a large bleb represents a relative contraindication for similar reasons. Also, the treatment may raise the issue of occupational health and safety (OHS), for chamber inside attendants, who should not be compressed if they are unable to equalise ears and sinuses.\nThe following are relative contraindications \u2013 meaning that special consideration must be made by specialist physicians before HBO treatments begin:\n\nCardiac disease\nCOPD with air trapping \u2013 can lead to pneumothorax during treatment.\nUpper respiratory infections \u2013 These conditions can make it difficult for the patient to equalise their ears or sinuses, which can result in what is termed ear or sinus squeeze.\nHigh fevers \u2013 In most cases the fever should be lowered before HBO treatment begins. Fevers may predispose to convulsions.\nEmphysema with CO2 retention \u2013 This condition can lead to pneumothorax during HBO treatment due to rupture of an emphysematous bulla during decompression. This risk can be evaluated by x-ray.\nHistory of thoracic (chest) surgery \u2013 This is rarely a problem and usually not considered a contraindication. However, there is concern that air may be trapped in lesions that were created by surgical scarring. These conditions need to be evaluated prior to considering HBO therapy.\nMalignant disease: Cancers thrive in blood-rich environments but may be suppressed by high oxygen levels. HBO treatment of individuals who have cancer presents a problem, since HBO both increases blood flow via angiogenesis and also raises oxygen levels. Taking an anti-angiogenic supplement may provide a solution. A study by Feldemier, et al. and NIH funded study on Stem Cells by Thom, et al., indicate that HBO is actually beneficial in producing stem\/progenitor cells and the malignant process is not accelerated.\nMiddle ear barotrauma is always a consideration in treating both children and adults in a hyperbaric environment because of the necessity to equalise pressure in the ears.\nPregnancy is not a relative contraindication to hyperbaric oxygen treatments, although it may be for underwater diving. In cases where a pregnant woman has carbon monoxide poisoning there is evidence that lower pressure (2.0 ATA) HBOT treatments are not harmful to the fetus, and that the risk involved is outweighed by the greater risk of the untreated effects of CO on the fetus (neurologic abnormalities or death.) In pregnant patients, HBO therapy has been shown to be safe for the fetus when given at appropriate levels and \"doses\" (durations). In fact, pregnancy lowers the threshold for HBO treatment of carbon monoxide-exposed patients. This is due to the high affinity of fetal hemoglobin for CO.\n\nTherapeutic principles\nThe therapeutic consequences of HBOT and recompression result from multiple effects.\n\nClinical pressure (2.0\u20133.0 Bar)\nThe increased overall pressure is of therapeutic value in the treatment of decompression sickness and air embolism as it provides a physical means of reducing the volume of inert gas bubbles within the body; Exposure to this increased pressure is maintained for a period long enough to ensure that most of the bubble gas is dissolved back into the tissues, removed by perfusion and eliminated in the lungs.\nThe improved concentration gradient for inert gas elimination (oxygen window) by using a high partial pressure of oxygen increases the rate of inert gas elimination in the treatment of decompression sickness.\nFor many other conditions, the therapeutic principle of HBOT lies in its ability to drastically increase partial pressure of oxygen in the tissues of the body. The oxygen partial pressures achievable using HBOT are much higher than those achievable while breathing pure oxygen under normobaric conditions (i.e. at normal atmospheric pressure). This effect is achieved by an increase in the oxygen transport capacity of the blood. At normal atmospheric pressure, oxygen transport is limited by the oxygen binding capacity of hemoglobin in red blood cells and very little oxygen is transported by blood plasma. Because the hemoglobin of the red blood cells is almost saturated with oxygen at atmospheric pressure, this route of transport cannot be exploited any further. Oxygen transport by plasma, however, is significantly increased using HBOT because of the higher solubility of oxygen as pressure increases.\n\nProangiogenic stem progenitor cell mobilization\nA study suggests that exposure to hyperbaric oxygen (HBOT) might also mobilize stem\/progenitor cells from the bone marrow by a nitric oxide-dependent mechanism.\n\nLow pressure hyperoxia, stem progenitor cell mobilization and inflammatory cytokine expression\nA more recent study suggests that stem cell mobilization, similar to that seen in the Thom study, is also invoked at relative normo-baric pressure with a significantly smaller increase in oxygen concentration. This study also found a significant decrease in the expression of the systemic inflammatory cytokine TNF-\u03b1 in venous blood. These results suggest that hyperbaria may not be required to invoke the transcriptional responses seen at higher partial pressures of oxygen and that the effect is due solely to oxygen.\n\nHyperbaric chambers\nConstruction\nThe traditional type of hyperbaric chamber used for therapeutic recompression and HBOT is a rigid shelled pressure vessel. Such chambers can be run at absolute pressures typically about 6 bars (87 psi), 600,000 Pa or more in special cases. Navies, professional diving organizations, hospitals, and dedicated recompression facilities typically operate these. They range in size from semi-portable, one-patient units to room-sized units that can treat eight or more patients. The larger units may be rated for lower pressures if they are not primarily intended for treatment of diving injuries.\nA rigid chamber may consist of:\n\na pressure vessel designed to a code such as ASME Boiler and Pressure Vessel Code\nviewports to allow the medical personnel to visually monitor the occupants, and can be used for hand signalling as an auxiliary emergency communications method. The major components are the window (transparent acrylic), the window seat (holds the acrylic window), and retaining ring. Interior lighting can be provided by mounting lights outside the viewports. Viewports are a feature specific to PVHOs due to the need to see the people inside and evaluate their health.  Other materials have been attempted, but they consistently fail to maintain their seal or have cracks which would progress rapidly to catastrphophic failure. Acrylic is more likely to have small cracks the operators can see and have time to take mitigation steps instead of failing catastrophically. Counterfeit chambers often do not use acrylic windows.\none or more human entry hatches \u2013 small and circular or wheel-in type hatches for patients on gurneys;\nthe entry lock that allows human entry \u2013 a separate chamber with two hatches, one to the outside and one to the main chamber, which can be independently pressurized to allow patients to enter or exit the main chamber while it is still pressurized;\na low volume medical or service airlock for medicines, instruments, and food;\ntransparent ports or closed-circuit television that allows technicians and medical staff outside the chamber to monitor the patient inside the chamber;\nan intercom system allowing two-way communication;\nan optional carbon dioxide scrubber \u2013 consisting of a fan that passes the gas inside the chamber through a soda lime canister;\na control panel outside the chamber to open and close valves that control air flow to and from the chamber, and regulate oxygen to hoods or masks;\nan over-pressure relief valve;\na built-in breathing system (BIBS) to supply and exhaust treatment gas;\na fire suppression system.\nFlexible monoplace chambers are available ranging from collapsible flexible aramid fiber-reinforced chambers which can be disassembled for transport via truck or SUV, with a maximum working pressure of 2 bar above ambient complete with BIBS allowing full oxygen treatment schedules. to portable, air inflated \"soft\" chambers that can operate at between 0.3 and 0.5 bars (4.4 and 7.3 psi) above atmospheric pressure with no supplemental oxygen, and longitudinal zipper closure.\n\nOxygen supply\nIn the larger multiplace chambers, patients inside the chamber breathe from either \"oxygen hoods\" \u2013 flexible, transparent soft plastic hoods with a seal around the neck similar to a space suit helmet \u2013 or tightly fitting oxygen masks, which supply pure oxygen and may be designed to directly exhaust the exhaled gas from the chamber. During treatment patients breathe 100% oxygen most of the time to maximise the effectiveness of their treatment, but have periodic \"air breaks\" during which they breathe chamber air (21% oxygen) to reduce the risk of oxygen toxicity. The exhaled treatment gas must be removed from the chamber to prevent the buildup of oxygen, which could present a fire risk. Attendants may also breathe oxygen some of the time to reduce their risk of decompression sickness when they leave the chamber. The pressure inside the chamber is increased by opening valves allowing high-pressure air to enter from storage cylinders, which are filled by an air compressor. Chamber air oxygen content is kept between 19% and 23% to control fire risk (US Navy maximum 25%). If the chamber does not have a scrubber system to remove carbon dioxide from the chamber gas, the chamber must be isobarically ventilated to keep the CO2 within acceptable limits.\nA soft chamber may be pressurized directly from a compressor. or from storage cylinders. \nSmaller \"monoplace\" chambers can only accommodate the patient, and no medical staff can enter. The chamber may be pressurised with pure oxygen or compressed air. If pure oxygen is used, no oxygen breathing mask or helmet is needed, but the cost of using pure oxygen is much higher than that of using compressed air. If compressed air is used, then an oxygen mask or hood is needed as in a multiplace chamber. Most monoplace chambers can be fitted with a demand breathing system for air breaks. In low pressure soft chambers, treatment schedules may not require air breaks, because the risk of oxygen toxicity is low due to the lower oxygen partial pressures used (usually 1.3 ATA), and short duration of treatment.\nFor alert, cooperative patients, air breaks provided by mask are more effective than changing the chamber gas because they provide a quicker gas change and a more reliable gas composition both during the break and treatment periods.\n\nTreatments\nInitially, HBOT was developed as a treatment for diving disorders involving bubbles of gas in the tissues, such as decompression sickness and gas embolism, It is still considered the definitive treatment for these conditions. The chamber treats decompression sickness and gas embolism by increasing pressure, reducing the size of the gas bubbles and improving the transport of blood to downstream tissues. After elimination of bubbles, the pressure is gradually reduced back to atmospheric levels. Hyperbaric chambers are also used for animals.\nAs of September 2023, a number of hyperbaric chambers in the US are turning divers with decompression sickness away, and only treating more profitable scheduled cases. The number of hyperbaric medical facilities in the US is estimated at about 1500, of which 67 are treating diving accidents, according to Divers Alert Network. Many facilities only provide hyperbaric treatment for wound care for economic reasons. Emergency hyperbaric services are more expensive to train and staff, and liability is increased.\n\nProtocol\nEmergency HBOT for decompression illness follows treatment schedules laid out in treatment tables. Most cases employ a recompression to 2.8 bars (41 psi) absolute, the equivalent of 18 metres (60 ft) of water, for 4.5 to 5.5 hours with the casualty breathing pure oxygen, but taking air breaks every 20 minutes to reduce oxygen toxicity. For extremely serious cases resulting from very deep dives, the treatment may require a chamber capable of a maximum pressure of 8 bars (120 psi), the equivalent of 70 metres (230 ft) of water, and the ability to supply heliox as a breathing gas.\nU.S. Navy treatment charts are used in Canada and the United States to determine the duration, pressure, and breathing gas of the therapy. The most frequently used tables are Table 5 and Table 6. In the UK the Royal Navy 62 and 67 tables are used.\nThe Undersea and Hyperbaric Medical Society (UHMS) publishes a report that compiles the latest research findings and contains information regarding the recommended duration and pressure of the longer-term conditions.\n\nHome and out-patient clinic treatment\nThere are several sizes of portable chambers, which are used for home treatment. These are usually referred to as \"mild personal hyperbaric chambers\", which is a reference to the lower pressure (compared to hard chambers) of soft-sided chambers. The American Medical Association is opposed to home use or any other use of hyperbaric chambers if it is not \"in facilities with appropriately trained staff including physician supervision and prescription and only when the intervention has scientific support or rationale\" due demonstrated hazard \nIn the US, these \"mild personal hyperbaric chambers\" are categorized by the FDA as CLASS II medical devices and requires a prescription in order to purchase one or take treatments. As with any hyperbaric chamber, the FDA require compliance with ASME and NFPA standards.  The most common option (but not approved by FDA) some patients choose is to acquire an oxygen concentrator which typically delivers 85\u201396% oxygen as the breathing gas.\nOxygen is never fed directly into soft chambers but is rather introduced via a line and mask directly to the patient. FDA approved oxygen concentrators for human consumption in confined areas used for HBOT are regularly monitored for purity (\u00b11%) and flow (10 to 15 liters per minute outflow pressure). An audible alarm will sound if the purity ever drops below 80%. Personal hyperbaric chambers use 120 volt or 220 volt outlets. The FDA warns against the use of oxygen concentrators or oxygen tanks with chambers that does not meet ASME and FDA standards, regardless of if the concentrators are FDA approved.\n\nPossible complications and concerns\nThere are risks associated with HBOT, similar to some diving disorders. Pressure changes can cause a \"squeeze\" or barotrauma in the tissues surrounding trapped air inside the body, such as the lungs, behind the eardrum, inside paranasal sinuses, or trapped underneath dental fillings. Breathing high-pressure oxygen may cause oxygen toxicity. Temporarily blurred vision can be caused by swelling of the lens, which usually resolves in two to four weeks.\nThere are reports that cataracts may progress following HBOT, and rarely, may develop de novo, but this may be unrecognized and under reported. The cause is not fully explained, but evidence suggests that lifetime exposure of the lens to high partial pressure oxygen may be a major factor. Oxidative damage to lens proteins is thought to be responsible. This may be an end-stage of the relatively well documented myopic shift detected in most hyperbaric patients after a course of multiple treatments.\n\nEffects of pressure\nPatients inside the chamber may notice discomfort inside their ears as a pressure difference develops between their middle ear and the chamber atmosphere. This can be relieved by ear clearing using the Valsalva maneuver or other techniques. Continued increase of pressure without equalizing may cause ear drums to rupture, resulting in severe pain. As the pressure in the chamber increases further, the air may become warm.\nTo reduce the pressure, a valve is opened to allow air out of the chamber. As the pressure falls, the patient's ears may \"squeak\" as the pressure inside the ear equalizes with the chamber. The temperature in the chamber will fall. The speed of pressurization and de-pressurization can be adjusted to each patient's needs.\n\nSide effects\nOxygen toxicity is a limitation on both maximum partial pressure of oxygen, and on length of each treatment.\nHBOT can accelerate the development of cataracts over multiple repetitive treatments, and can cause temporary relative myopia over the shorter term.\n\nRegulation and legality\nThe use of hyperbaric chambers for medical and therapeutic procedures is generally regulated. Authorities have warned of potential risks to patients receiving treatment in unlicensed facilities, notably in Israel, Canada, and the United States. In Italy, the use of hyperbaric chambers for therapy was severely restricted to limited medical settings after a serious fire which killed ten patients in 1997.\nIn some jurisdictions, the use and availability of HBOT is further restricted at the subnational level. In the U.S. state of North Carolina, several cities including Durham, Raleigh and Charlotte have ordered operators of mild hyperbaric oxygen therapy to close to protect public safety due to a risk of fire.\nUnlicensed and fraudulent operators have been subject to prosecution. In Australia, Oxymed Australia Pty Ltd and director Malcolm Hooper were ordered to pay AUS $3 million in fines after advertising hyperbaric therapy against the country's Therapeutic Goods Act. In Canada, certain soft-shelled hyperbaric chambers were removed from the market for a potential risk to patients.\n\nCosts\nHBOT is recognized by Medicare in the United States as a reimbursable treatment for 14 UHMS \"approved\" conditions. A 1-hour HBOT session may cost between $300 and higher in private clinics, and over $2,000 in hospitals. U.S. physicians (M.D. or D.O.) may lawfully prescribe HBOT for \"off-label\" conditions such as stroke, and migraine. Such patients are treated in outpatient clinics. In the United Kingdom most chambers are financed by the National Health Service, although some, such as those run by Multiple Sclerosis Therapy Centres, are non-profit. In Australia, HBOT is not covered by Medicare as a treatment for multiple sclerosis. China and Russia treat more than 80 maladies, conditions and trauma with HBOT.\n\nPersonnel\nHyperbaric medical practitioner - a specialist in hyperbaric medicine\nDiving medical practitioner \u2013 a specialist in diving medicine\nChamber operator \u2013 a person competent to operate  a hyperbaric chamber\nHyperbaric nurse \u2013 a nurse responsible for administering hyperbaric oxygen therapy to patients and supervising them throughout the treatment.\nDiving medical technician \u2013 member of a dive team who is trained in advanced first aid.\nChamber attendant \u2013 a person trained in basic first aid who is medically fit to dive in a chamber, usually a member of a diving team allocated to looking after the diver being treated.\n\nResearch\nAspects under research include radiation-induced hemorrhagic cystitis; and inflammatory bowel disease, rejuvenation.\nSome research found evidence that HBOT improves local tumor control, mortality, and local tumor recurrence for cancers of the head and neck.\nSome research also found evidence of an increase in stem progenitor cells and a decrease in inflammation.\n\nNeurological\nTentative evidence shows a possible benefit in cerebrovascular diseases. Rats subjected to HBOT after some time following the acute phase of experimentally-induced stroke showed reduced inflammation, increased brain-derived neurotrophic factor, and evidence of neurogenesis. Another rat study showed improved neurofunctional recovery as well as neurogenesis following the late-chronic phase of experimentally-induced stroke.\nThe clinical experience and results so far published has promoted the use of HBOT therapy in patients with cerebrovascular injury and focal cerebrovascular injuries. However, the power of clinical research is limited because of the shortage of randomized controlled trials.\n\nRadiation wounds\nA 2010 review of studies of HBOT applied to wounds from radiation therapy reported that, while most studies suggest a beneficial effect, more experimental and clinical research is needed to validate its clinical use.\n\nHistory\nHyperbaric air\nJunod built a chamber in France in 1834 to treat pulmonary conditions at pressures between 2 and 4 atmospheres absolute.\nDuring the following century \"pneumatic centres\" were established in Europe and the USA which used hyperbaric air to treat a variety of conditions.\nOrval J Cunningham, a professor of anesthesia at the University of Kansas in the early 1900s observed that people with circulatory disorders did better at sea level than at altitude and this formed the basis for his use of hyperbaric air. In 1918, he successfully treated patients with the Spanish flu with hyperbaric air. In 1930 the American Medical Association forced him to stop hyperbaric treatment, since he did not provide acceptable evidence that the treatments were effective.\n\nHyperbaric oxygen\nThe English scientist Joseph Priestley discovered oxygen in 1775. Shortly after its discovery, there were reports of toxic effects of hyperbaric oxygen on the central nervous system and lungs, which delayed therapeutic applications until 1937, when Behnke and Shaw first used it in the treatment of decompression sickness.\nIn 1955 and 1956 Churchill-Davidson, in the UK, used hyperbaric oxygen to enhance the radiosensitivity of tumours, while Ite Boerema, at the University of Amsterdam, successfully used it in cardiac surgery.\nIn 1961 Willem Hendrik Brummelkamp et al. published on the use of hyperbaric oxygen in the treatment of clostridial gas gangrene.\nIn 1962 Smith and Sharp reported successful treatment of carbon monoxide poisoning with hyperbaric oxygen.\nThe Undersea Medical Society (now Undersea and Hyperbaric Medical Society) formed a Committee on Hyperbaric Oxygenation which has become recognized as the authority on indications for hyperbaric oxygen treatment.\n\nIncidents\nFires inside a hyperbaric chamber are extremely dangerous. A review article published in 1997 found 77 human fatalities in 35 different hyperbaric chamber fires that occurred from 1923 to 1996. Further studies indicate while the treatment is often considered safe, the use of hyperbaric equipment comes with risks to the operating personnel when improperly used. Proper equipment maintenance and safety procedures for the use  of pressure equipmrnt is mandatory.\n\n1997: Ten patients and a nurse were killed in Milan, Italy after a fire broke out inside a hyperbaric oxygen chamber.\n2009: A grandmother and her four year old grandson died after a hyperbaric chamber caught fire and exploded in Florida. The boy was receiving treatment in the chamber for cerebral palsy and had traveled from Italy where the treatment is outlawed to undergo the procedure.\n2012: A hyperbaric oxygen chamber exploded in Florida, killing a woman and a thoroughbred horse who was receiving treatment. The explosion occurred after the horse kicked out at the chamber, creating sparks which ignited a fire.\n2015: A dog was killed in Georgia when the chamber it was receiving treatment in caught fire and exploded. The dog was being treated for arthritis.\n2016: A fire killed four people who were receiving treatment inside a hyperbaric chamber at Mintohardjo Navy Hospital in Jakarta, Indonesia. The fire was reportedly caused by an electrical short circuit. After the fire broke out, operators used a sprinkler system and an emergency shut off system to rescue the victims, but live-saving efforts were prevented as the machine became engulfed in flames.\n2016: A man in Victoria, Australia died in a hyperbaric chamber of undisclosed causes while receiving treatment. The practitioners overseeing his care were found responsible for failing to ensure the patient's safety leading to his death. They were later fined AU$716,750.\n\nSee also\nUndersea and Hyperbaric Medical Society \u2013 US based organisation for research and education in hyperbaric physiology and medicine.\nSouth Pacific Underwater Medicine Society \u2013 Publisher for diving and hyperbaric medicine and physiology\nDecompression chamber \u2013 Any pressure vessel for huma occupancy used to decompress a person\nHyperbaric treatment schedules \u2013 Planned hyperbaric exposure using a specified breathing gas as medical treatment\nTransdermal continuous oxygen therapy \u2013 Wound closure technique using external oxygen exposure\n\nReferences\nFurther reading\nExternal links\nHyperbaric Oxygen Therapy from eMedicine\nDuke University Medical Center Archives contains collections of multiple individuals who worked with hyperbaric medicine\nDunning, Brian (November 19, 2019). \"Skeptoid #702: Hyperbaric Oxygen Therapy\". Skeptoid.","91":"ICD-10 is the 10th revision of the International Classification of Diseases (ICD), a medical classification list by the World Health Organization (WHO). It contains codes for diseases, signs and symptoms, abnormal findings, complaints, social circumstances, and external causes of injury or diseases. Work on ICD-10 began in 1983, became endorsed by the Forty-third World Health Assembly in 1990, and was first used by member states in 1994. It was replaced by ICD-11 on January 1, 2022.\nWhile WHO manages and publishes the base version of the ICD, several member states have modified it to better suit their needs. In the base classification, the code set allows for more than 14,000 different codes and permits the tracking of many new diagnoses compared to the preceding ICD-9. Through the use of optional sub-classifications, ICD-10 allows for specificity regarding the cause, manifestation, location, severity, and type of injury or disease. The adapted versions may differ in a number of ways, and some national editions have expanded the code set even further; with some going so far as to add procedure codes. ICD-10-CM, for example, has over 70,000 codes.\nThe WHO provides detailed information regarding the ICD via its website \u2013 including an ICD-10 online browser and ICD training materials. The online training includes a support forum, a self-learning tool and user guide.\n\nChapters\nThe following table lists the chapter number (using Roman numerals), the code range of each chapter, and the chapter's title from the international version of the ICD-10.\n\nNational adoptions\nApproximately 27 countries use ICD-10 for reimbursement and resource allocation in their health system, and some have made modifications to ICD to better accommodate its utility. The unchanged international version of ICD-10 is used in 117 countries for performing cause of death reporting and statistics.\nThe national versions may differ from the base classification in the level of detail, incomplete adoption of a category, or the addition of procedure codes.\n\nSwitzerland\nIn Switzerland, the German Modification (ICD-10-GM) is used for coding diagnoses. The Federal Statistical Office (FSO) of Switzerland publishes the ICD-10-GM in French and Italian every two years.\n\nAustralia\nIntroduced in 1998, ICD-10 Australian Modification (ICD-10-AM) was developed by the National Centre for Classification in Health at the University of Sydney. It is currently maintained by the Australian Consortium for Classification Development.\nICD-10-AM has also been adopted by New Zealand, Ireland, Saudi Arabia and several other countries.\n\nBrazil\nBrazil introduced ICD-10 in 1996.\n\nGreece\nGreece introduced ICD-10 on December 23, 2023. The Greek DRG (Gr-DRG) system is based on the Greek modification of the International Statistical Classification of Diseases and Related Health Problems, based on the 2013 German Amendment of ICD-10 (ICD-10-GM), and a systematic catalog of codes of medical procedures called Greek Medical Procedure Classification (GMPC), based on corresponding international procedural classification.\n\nCanada\nCanada began using ICD-10 for mortality reporting in 2000. A six-year, phased implementation of ICD-10-CA for morbidity reporting began in 2001. It was staggered across Canada's ten provinces, with Quebec the last to make the switch.\nICD-10-CA is available in both English- and French-language versions.\n\nChina\nChina adopted ICD-10 in 2002.\n\nCzech Republic\nThe Czech Republic adopted ICD-10 in 1994, one year after its official release by WHO. Revisions to the international edition are adopted continuously. The official Czech translation of ICD-10 2016 10th Revision was published in 2018.\n\nDenmark\nICD-10 was first introduced into the psychiatric health service system on 1 January 1994.\n\nEstonia\nEstonia adopted ICD-10 from January 1, 1997, via a ministerial degree. However, chapter V \"Mental and behavioural disorders\" had already been in use from January 1, 1994, also via a ministerial degree.\n\nFrance\nFrance introduced a clinical addendum to ICD-10 in 1997. See also website of the ATIH.\n\nGermany\nGermany's ICD-10 German Modification (ICD-10-GM) is based on ICD-10-AM. ICD-10-GM was developed between 2003 and 2004, by the German Institute for Medical Documentation and Information.\n\nHungary\nHungary introduced the use of ICD-10 from January 1, 1996, via a ministerial decree.\n\nKorea\nA Korean modification has existed since 2008.\n\nNetherlands\nThe Dutch translation of ICD-10 is ICD10-nl, which was created by the WHO-FIC Network in 1994. There is an online dictionary.\n\nRussia\nThe Ministry of Healthcare of the Russian Federation ordered in 1997 to transfer all health organizations to ICD-10.\n\nSouth Africa\nICD-10 was implemented in July 2005 under the auspice of the National ICD-10 Implementation Task Team which is a joint task team between the National Department of Health and the Council for Medical Schemes.\n\nSweden\nThe current Swedish translation of ICD-10 was created in 1997.\n\nThailand\nThe ICD-10-TM (Thai Modification) is a Thai language version based on the 2016 ICD-10. An unusual feature of the index of ICD-10-TM is that it is bilingual, containing both Thai and English trails.\n\nUnited Kingdom\nICD-10 was first mandated for use in the UK in 1995. In 2010 the UK Government made a commitment to update the UK version of ICD-10 every three years. On 1 April 2016, following a year's delay, ICD-10 5th Edition replaced the 4th Edition as the mandated diagnostic classification within the UK, and remains the current version for use within the UK.\n\nUnited States\nFor disease reporting, the US utilizes its own national variant of ICD-10 called the ICD-10 Clinical Modification (ICD-10-CM). A procedural classification called ICD-10 Procedure Coding System (ICD-10-PCS) has also been developed for capturing inpatient procedures. The ICD-10-CM and ICD-10-PCS were developed by the Centers for Medicare and Medicaid Services (CMS) and the National Center for Health Statistics (NCHS). There are over 70,000 ICD-10-PCS procedure codes and over 69,000 ICD-10-CM diagnosis codes, compared to about 3,800 procedure codes and roughly 14,000 diagnosis codes found in the previous ICD-9-CM.\nThere was much controversy when the transition from the ICD-9-CM to the ICD-10-CM was first announced in the US. Many providers were concerned about the vast number of codes being added, the complexity of the new coding system, and the costs associated with the transition. The Centers for Medicare and Medicaid Services (CMS) weighed these concerns against the benefits of having more accurate data collection, clearer documentation of diagnoses and procedures, and more accurate claims processing. CMS decided the financial and public health cost associated with continuing to use the ICD-9-CM was too high and mandated the switch to ICD-10-CM.\nThe deadline for the United States to begin using ICD-10-CM for diagnosis coding and Procedure Coding System ICD-10-PCS for inpatient hospital procedure coding was set at October 1, 2015, a year later than the previous 2014 deadline. Before the 2014 deadline, the previous deadline had been a year before that on October 1, 2013. All HIPAA \"covered entities\" were required to make the change; a pre-requisite to ICD-10-CM is the adoption of EDI Version 5010 by January 1, 2012. Enforcement of 5010 transition by the Centers for Medicare & Medicaid Services (CMS), however, was postponed by CMS until March 31, 2012, with the federal agency citing numerous factors, including slow software upgrades. The implementation of ICD-10-CM has been subject to previous delays. In January 2009, the date was pushed back to October 1, 2013, rather than an earlier proposal of October 1, 2011.\nTwo common complaints in the United States about the ICD-10-CM are 1) the long list of potentially relevant codes for a given condition (such as rheumatoid arthritis) which can be confusing and reduce efficiency and 2) the assigned codes for seldom seen conditions (e.g. W55.22XA: Struck by cow, initial encounter; and V91.07XA: Burn due to water-skis on fire, initial encounter).\n\nCriticism\nThe expansion of healthcare delivery systems and changes in global health trends prompted a need for codes with improved clinical accuracy and specificity. The alphanumeric coding in ICD-10 is an improvement from ICD-9 which had a limited number of codes and a restrictive structure. Early concerns in the implementation of ICD-10 included the cost and the availability of resources for training healthcare workers and professional coders.\n\nSee also\nDiagnostic and Statistical Manual of Mental Disorders, used in psychiatry\nDSM-5: its current version\n\nNotes\nReferences\nExternal links\n\nOfficial website \nICD-10 online browser (WHO)\nICD-10 online training direct access (WHO)\nICD-11 Home Page","92":"An International Standard Serial Number (ISSN) is an eight-digit serial number used to uniquely identify a serial publication (periodical), such as a magazine. The ISSN is especially helpful in distinguishing between serials with the same title. ISSNs are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature.\nThe ISSN system was first drafted as an International Organization for Standardization (ISO) international standard in 1971 and published as ISO 3297 in 1975. ISO subcommittee TC 46\/SC 9 is responsible for maintaining the standard.\nWhen a serial with the same content is published in more than one media type, a different ISSN is assigned to each media type. For example, many serials are published both in print and electronic media. The ISSN system refers to these types as print ISSN (p-ISSN) and electronic ISSN (e-ISSN). Consequently, as defined in ISO 3297:2007, every serial in the ISSN system is also assigned a linking ISSN (ISSN-L), typically the same as the ISSN assigned to the serial in its first published medium, which links together all ISSNs assigned to the serial in every medium.\n\nCode format\nAn ISSN is an eight-digit code, divided by a hyphen into two four-digit numbers. The last digit, which may be zero through nine or an X, is a check digit, so the ISSN is uniquely represented by its first seven digits. Formally, the general form of the ISSN (also named \"ISSN structure\" or \"ISSN syntax\") can be expressed as follows:\n\nwhere N is in the set {0,1,2,...,9}, a decimal digit character, and C is in {0,1,2,...,9,X}; or by a Perl Compatible Regular Expressions (PCRE) regular expression:\n\nFor example, the ISSN of the journal Hearing Research, is 0378-5955, where the final 5 is the check digit, that is C=5. To calculate the check digit, the following algorithm may be used:\n\n \nTo confirm the check digit, calculate the sum of all eight digits of the ISSN multiplied by their position in the number, counting from the right. (If the check digit is X, add 10 to the sum.) The remainder of the sum modulo 11 must be 0. There is an online ISSN checker that can validate an ISSN, based on the above algorithm.\n\nIn EANs\nISSNs can be encoded in EAN-13 bar codes with a 977 \"country code\" (compare the 978 country code (\"bookland\") for ISBNs), followed by the 7 main digits of the ISSN (the check digit is not included), followed by 2 publisher-defined digits, followed by the EAN check digit (which need not match the ISSN check digit).\n\nCode assignment, maintenance and look-up\nISSN codes are assigned by a network of ISSN National Centres, usually located at national libraries and coordinated by the ISSN International Centre based in Paris. The International Centre is an intergovernmental organization created in 1974 through an agreement between UNESCO and the French government.\n\nLinking ISSN\nISSN-L is a unique identifier for all versions of the serial containing the same content across different media. As defined by ISO 3297:2007, the \"linking ISSN (ISSN-L)\" provides a mechanism for collocation or linking among the different media versions of the same continuing resource. The ISSN-L is one of a serial's existing ISSNs, so does not change the use or assignment of \"ordinary\" ISSNs; it is based on the ISSN of the first published medium version of the publication. If the print and online versions of the publication are published at the same time, the ISSN of the print version is chosen as the basis of the ISSN-L.\nWith ISSN-L is possible to designate one single ISSN for all those media versions of the title. The use of ISSN-L facilitates search, retrieval and delivery across all media versions for services like OpenURL, library catalogues, search engines or knowledge bases.\n\nRegister\nThe International Centre maintains a database of all ISSNs assigned worldwide, the ISDS Register (International Serials Data System), otherwise known as the ISSN Register. At the end of 2016, the ISSN Register contained records for 1,943,572 items. The Register is not freely available for interrogation on the web, but is available by subscription.\n\nThe print version of a serial typically will include the ISSN code as part of the publication information.\nMost serial websites contain ISSN code information.\nDerivative lists of publications will often contain ISSN codes; these can be found through on-line searches with the ISSN code itself or serial title.\nWorldCat permits searching its catalog by ISSN, by entering \"issn:\" before the code in the query field. One can also go directly to an ISSN's record by appending it to \"https:\/\/www.worldcat.org\/ISSN\/\", e.g. n2:1021-9749 \u2013 Search Results. This does not query the ISSN Register itself, but rather shows whether any WorldCat library holds an item with the given ISSN.\n\nComparison with other identifiers\nISSN and ISBN codes are similar in concept, where ISBNs are assigned to individual books. An ISBN might be assigned for particular issues of a serial, in addition to the ISSN code for the serial as a whole. An ISSN, unlike the ISBN code, is an anonymous identifier associated with a serial title, containing no information as to the publisher or its location. For this reason a new ISSN is assigned to a serial each time it undergoes a major title change.\n\nExtensions\nSince the ISSN applies to an entire serial, other identifiers have been built on top of it to allow references to specific volumes, articles, or other identifiable components (like the table of contents): the Publisher Item Identifier (PII) and the Serial Item and Contribution Identifier (SICI).\n\nMedia versus content\nSeparate ISSNs are needed for serials in different media (except reproduction microforms). Thus, the print and electronic media versions of a serial need separate ISSNs, and CD-ROM versions and web versions require different ISSNs. However, the same ISSN can be used for different file formats (e.g. PDF and HTML) of the same online serial.\nThis \"media-oriented identification\" of serials made sense in the 1970s. In the 1990s and onward, with personal computers, better screens, and the Web, it makes sense to consider only content, independent of media. This \"content-oriented identification\" of serials was a repressed demand during a decade, but no ISSN update or initiative occurred. A natural extension for ISSN, the unique-identification of the articles in the serials, was the main demand application. An alternative serials' contents model arrived with the indecs Content Model and its application, the digital object identifier (DOI), an ISSN-independent initiative, consolidated in the 2000s.\nOnly later, in 2007, ISSN-L was defined in the new ISSN standard (ISO 3297:2007) as an \"ISSN designated by the ISSN Network to enable collocation or versions of a continuing resource linking among the different media\".\n\nUse in URNs\nAn ISSN can be encoded as a uniform resource name (URN) by prefixing it with \"urn:ISSN:\". For example, Rail could be referred to as \"urn:ISSN:0953-4563\". URN namespaces are case-sensitive, and the ISSN namespace is all caps. If the checksum digit is \"X\" then it is always encoded in uppercase in a URN.\n\nProblems\nThe URNs are content-oriented, but ISSN is media-oriented:\n\nISSN is not unique when the concept is \"a journal is a set of contents, generally copyrighted content\": the same journal (same contents and same copyrights) may have two or more ISSN codes. A URN needs to point to \"unique content\" (a \"unique journal\" as a \"set of contents\" reference).\nExample: Nature has an ISSN for print, 0028-0836, and another for the same content on the Web, 1476-4687; only the oldest (0028-0836) is used as a unique identifier. As the ISSN is not unique, the U.S. National Library of Medicine needed to create, prior to 2007, the NLM Unique ID (JID).\nISSN does not offer resolution mechanisms like a digital object identifier (DOI) or a URN does, so the DOI is used as a URN for articles, with (for historical reasons) no need for an ISSN's existence.\nExample: the DOI name \"10.1038\/nature13777\" can be represented as an HTTP string by https:\/\/doi.org\/10.1038\/nature13777, and is redirected (resolved) to the current article's page; but there is no ISSN online service, like http:\/\/dx.issn.org\/, to resolve the ISSN of the journal (in this sample 1476-4687).\nA unique URN for serials simplifies the search, recovery and delivery of data for various services including, in particular, search systems and knowledge databases. ISSN-L (see Linking ISSN above) was created to fill this gap.\n\nMedia category labels\nThe two standard categories of media in which serials are most available are print and electronic. In metadata contexts (e.g., JATS), these may have standard labels.\n\nPrint ISSN\np-ISSN is a standard label for \"Print ISSN\", the ISSN for the print media (paper) version of a serial. Usually it is the \"default media\" and so the \"default ISSN\".\n\nElectronic ISSN\ne-ISSN (or eISSN) is a standard label for \"Electronic ISSN\", the ISSN for the electronic media (online) version of a serial.\n\nROAD\nROAD: Directory of Open Access Scholarly Resources (est. 2013), produced by the ISSN International Centre and UNESCO\n\nSee also\nCODEN\nWorldCat\u2014an ISSN-resolve service\n\nReferences\nExternal links\n\nISSN International Centre\nISSN Portal\nList of 63800 ISSN numbers and titles\nISSN InterNational Centre (January 2015), ISSN Manual (PDF) (2015 ed.), Paris: ISSN InterNational Centre, archived from the original (PDF) on 12 July 2020, retrieved 22 October 2018.\nHow U.S. publishers can obtain an ISSN, United States: Library of Congress.\nISSN Canada, Ottawa: Library and Archives Canada, 8 January 2020, retrieved 3 April 2020..\nGetting an ISSN in the UK, British Library, archived from the original on 15 July 2014, retrieved 8 October 2008.\nGetting an ISSN in France (in French), Biblioth\u00e8que nationale de France, 15 June 2023\nGetting an ISSN in Germany (in German), Deutsche Nationalbibliothek, archived from the original on 11 December 2017, retrieved 8 March 2012\nGetting an ISSN in South Africa, National Library of South Africa, archived from the original on 24 December 2017, retrieved 7 January 2015","93":"The ideomotor phenomenon is a psychological phenomenon wherein a subject makes motions unconsciously. Also called ideomotor response (or ideomotor reflex) and abbreviated to IMR, it is a concept in hypnosis and psychological research. It is derived from the terms \"ideo\" (idea, or mental representation) and \"motor\" (muscular action). The phrase is most commonly used in reference to the process whereby a thought or mental image brings about a seemingly \"reflexive\" or automatic muscular reaction, often of minuscule degree, and potentially outside of the awareness of the subject. As in responses to pain, the body sometimes reacts reflexively with an ideomotor effect to ideas alone without the person consciously deciding to take action. The effects of automatic writing, dowsing, facilitated communication, applied kinesiology, and ouija boards have been attributed to the phenomenon.\nThe associated term \"ideo-dynamic response\" (or \"reflex\") applies to a wider domain, and extends to the description of all bodily reactions (including ideo-motor and ideo-sensory responses) caused in a similar manner by certain ideas, e.g., the salivation often caused by imagining sucking a lemon, which is a secretory response. The notion of an ideo-dynamic response contributed to James Braid's first neuropsychological explanation of the principle through which suggestion operated in hypnotism.\n\nHistory of scientific investigation\nWith the rise of Spiritualism in 1840s, mediums devised and refined a variety of techniques for communicating, ostensibly, with the spirit world including table-turning and planchette writing boards (the precursor to later Ouija boards). These phenomena and devices quickly became the subject of scientific investigation.\nThe term ideomotor was first used by William Benjamin Carpenter in 1852. In a scientific paper that specifically discussed the means through which James Braid's \"hypnotism\" produced its effects, Carpenter derived the word ideomotor from the components ideo, meaning \"idea\" or \"mental representation\", and motor, meaning \"muscular action\".  In the paper, Carpenter explained his theory that muscular movement can be independent of conscious desires or emotions; hence the alternative term \"Carpenter effect\".\nCarpenter was a friend and collaborator of James Braid, the founder of modern hypnotism. Braid soon adopted Carpenter's ideo-motor terminology, to facilitate the transmission of his most fundamental views, based upon those of his teacher, the philosopher Thomas Brown, that the efficacy of hypnotic suggestion was contingent upon the subject's concentration upon a single (thus, \"dominant\") idea.\n\nIn 1855, Braid explained his decision to abandon his earlier term \"mono-ideo-motor\", based on Carpenter's (1852) \"ideo-motor principle\", and adopt the more appropriate and more descriptive term \"mono-ideo-dynamic\". His decision was based upon suggestions made to Carpenter (in 1854) by their friend in common, Daniel Noble, that the activity that Carpenter was describing would be more accurately understood in its wider applications (viz., wider than pendulums and ouija boards) if it were to denominated the \"ideo-dynamic principle\":In order that I may do full justice to two esteemed friends, I beg to state, in connection with this term monoideo-dynamics, that, several years ago, Dr. W. B. Carpenter introduced the term ideo-motor to characterise the reflex or automatic muscular motions which arise merely from ideas associated with motion existing in the mind, without any conscious effort of volition. In 1853, in referring to this term, Daniel Noble said, \"Ideo-dynamic would probably constitute a phraseology more appropriate, as applicable to a wider range of phenomena.\" In this opinion I quite concurred, because I was well aware that an idea could arrest as well as excite motion automatically, not only in the muscles of voluntary motion, but also as regards the condition of every other function of the body. I have, therefore, adopted the term monoideo-dynamics, as still more comprehensive and characteristic as regards the true mental relations which subsist during all dynamic changes which take place, in every other function of the body, as well as in the muscles of voluntary motion.Scientific tests by the English scientist Michael Faraday, Manchester surgeon James Braid, the French chemist Michel Eug\u00e8ne Chevreul, and the American psychologists William James and Ray Hyman have demonstrated that many phenomena attributed to spiritual or paranormal forces, or to mysterious \"energies\", are actually due to ideomotor action. Furthermore, these tests demonstrate that \"honest, intelligent people can unconsciously engage in muscular activity that is consistent with their expectations\". They also show that suggestions that can guide behavior can be given by subtle clues (Hyman 1977).\nSome operators claim to use ideomotor responses to communicate with a subject's \"unconscious mind\" using a system of physical signals (such as finger movements) for the unconscious mind to indicate \"yes\", \"no\", \"I don't know\", or \"I'm not ready to know that consciously\".\nA simple experiment to demonstrate the ideomotor effect is to allow a hand-held pendulum to hover over a sheet of paper. The paper has words such as \"yes\", \"no\", and \"maybe\" printed on it. Small movements in the hand, in response to questions, can cause the pendulum to move towards the words on the paper. This technique has been used for experiments in extrasensory perception, lie detection, and ouija boards. This type of experiment was used by Kreskin and has also been used by illusionists such as Derren Brown.\n\nA 2019 study of automatic pendulum movements using a motion capture system showed that pendulum effect is produced when the fingers holding the pendulum generate an oscillating frequency close to the resonant frequency of the pendulum. At an appropriate frequency, very small driving movements of the arm are sufficient to produce relatively large pendulum motion.\n\nUses\nResponding to questions\nIt is strongly associated with the practice of analytical hypnotherapy based on \"uncovering techniques\" such as Watkins' \"affect bridge\", whereby a subject's \"yes\", \"no\", \"I don't know\", or \"I don't want to answer\" responses to an operator's questions are indicated by physical movements rather than verbal signals; and are produced per medium of a pre-determined (between operator and subject) and pre-calibrated set of responses.\n\nSee also\nFootnotes\nReferences\nExternal links\n Media related to Ideomotor phenomenon at Wikimedia Commons","94":"The International Standard Book Number (ISBN) is a numeric commercial book identifier that is intended to be unique. Publishers purchase or receive ISBNs from an affiliate of the International ISBN Agency.\nA different ISBN is assigned to each separate edition and variation of a publication, but not to a simple reprinting of an existing item. For example, an e-book, a paperback and a hardcover edition of the same book must each have a different ISBN, but an unchanged reprint of the hardcover edition keeps the same ISBN. The ISBN is ten digits long if assigned before 2007, and thirteen digits long if assigned on or after 1 January 2007. The method of assigning an ISBN is nation-specific and varies between countries, often depending on how large the publishing industry is within a country.\nThe first version of the ISBN identification format was devised in 1967, based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (any 9-digit SBN can be converted to a 10-digit ISBN by prefixing it with a zero).\nPrivately published books sometimes appear without an ISBN. The International ISBN Agency sometimes assigns ISBNs to such books on its own initiative.\nA separate identifier code of a similar kind, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines and newspapers. The International Standard Music Number (ISMN) covers musical scores.\n\nHistory\nThe Standard Book Number (SBN) is a commercial system using nine-digit code numbers to identify books. In 1965, British bookseller and stationers WHSmith announced plans to implement a standard numbering system for its books. They hired consultants to work on their behalf, and the system was devised by Gordon Foster, emeritus professor of statistics at Trinity College Dublin. The International Organization for Standardization (ISO) Technical Committee on Documentation sought to adapt the British SBN for international use. The ISBN identification format was conceived in 1967 in the United Kingdom by David Whitaker (regarded as the \"Father of the ISBN\") and in 1968 in the United States by Emery Koltay (who later became director of the U.S. ISBN agency R. R. Bowker).\nThe 10-digit ISBN format was developed by the ISO and was published in 1970 as international standard ISO 2108. The United Kingdom continued to use the nine-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46\/Subcommittee 9 TC 46\/SC 9. The ISO on-line facility only refers back to 1978.\n\nAn SBN may be converted to an ISBN by prefixing the digit \"0\". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has \"SBN 340 01381 8\", where \"340\" indicates the publisher, \"01381\" is the serial number assigned by the publisher, and \"8\" is the check digit. By prefixing a zero, this can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated. Some publishers, such as Ballantine Books, would sometimes use 12-digit SBNs where the last three digits indicated the price of the book; for example, Woodstock Handmade Houses had a 12-digit Standard Book Number of 345-24223-8-595 (valid SBN: 345-24223-8, ISBN: 0-345-24223-8), and it cost US$5.95.\nSince 1 January 2007, ISBNs have contained thirteen digits, a format that is compatible with \"Bookland\" European Article Numbers, which have 13 digits.\nThe United States, with 3.9 million registered ISBNs in 2020, was by far the biggest user of the ISBN identifier in 2020, followed by the Republic of Korea (329,582), Germany (284,000), China (263,066), the UK (188,553) and Indonesia (144,793). Lifetime ISBNs registered in the United States are over 39 million as of 2020.\n\nOverview\nA separate ISBN is assigned to each edition and variation (except reprintings) of a publication. For example, an ebook, audiobook, paperback, and hardcover edition of the same book must each have a different ISBN assigned to it.:\u200a12\u200a The ISBN is thirteen digits long if assigned on or after 1 January 2007, and ten digits long if assigned before 2007. An International Standard Book Number consists of four parts (if it is a 10-digit ISBN) or five parts (for a 13-digit ISBN).\nSection 5 of the International ISBN Agency's official user manual:\u200a11\u200a describes the structure of the 13-digit ISBN, as follows:\n\nfor a 13-digit ISBN, a prefix element \u2013 a GS1 prefix: so far 978 or 979 have been made available by GS1,\nthe registration group element (language-sharing country group, individual country or territory),\nthe registrant element,\nthe publication element, and\na checksum character or check digit.\nA 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.\n\nIssuing process\nISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from the government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.\nA full directory of ISBN agencies is available on the International ISBN Agency website. A list for a few countries is given below:\n\nAustralia \u2013 Thorpe-Bowker\nBrazil \u2013 The National Library of Brazil; (Up to 28 February 2020)\nBrazil \u2013 C\u00e2mara Brasileira do Livro (From 1 March 2020)\nCanada \u2013 English Library and Archives Canada, a government agency; French Biblioth\u00e8que et Archives nationales du Qu\u00e9bec;\nColombia \u2013 C\u00e1mara Colombiana del Libro, an NGO\nHong Kong \u2013 Books Registration Office (BRO), under the Hong Kong Public Libraries\nIceland \u2013 Landsb\u00f3kasafn (National and University Library of Iceland)\nIndia \u2013 The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development\nIsrael \u2013 The Israel Center for Libraries\nItaly \u2013 EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association)\nKenya \u2013 National Library of Kenya\nLatvia - Latvian ISBN Agency\nLebanon \u2013 Lebanese ISBN Agency\nMaldives \u2013 The National Bureau of Classification (NBC)\nMalta \u2013 The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb)\nMorocco \u2013 The National Library of Morocco\nNew Zealand \u2013 The National Library of New Zealand\nNigeria \u2013 National Library of Nigeria\nPakistan \u2013 National Library of Pakistan\nPhilippines \u2013 National Library of the Philippines\nSouth Africa \u2013 National Library of South Africa\nSpain \u2013 Spanish ISBN Agency \u2013 Agencia del ISBN\nTurkey \u2013 General Directorate of Libraries and Publications, a branch of the Ministry of Culture\nUnited Kingdom and Republic of Ireland \u2013 Nielsen Book Services Ltd, part of NIQ\nUnited States \u2013 R. R. Bowker\n\nRegistration group element\nThe ISBN registration group element is a 1-to-5-digit number that is valid within a single prefix element (i.e. one of 978 or 979),:\u200a11\u200a and can be separated between hyphens, such as \"978-1-...\". Registration groups have primarily been allocated within the 978 prefix element. The single-digit registration groups within the 978-prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. Example 5-digit registration groups are 99936 and 99980, for Bhutan. The allocated registration groups are: 0\u20135, 600\u2013631, 65, 7, 80\u201394, 950\u2013989, 9910\u20139989, and 99901\u201399993. Books published in rare languages typically have longer group elements.\nWithin the 979 prefix element, the registration group 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN. The registration groups within prefix element 979 that have been assigned are 8 for the United States of America, 10 for France, 11 for the Republic of Korea, and 12 for Italy.\nThe original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero to a 9-digit SBN creates a valid 10-digit ISBN.\n\nRegistrant element\nThe national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not legally required to assign an ISBN, although most large bookstores only handle publications that have ISBNs assigned to them.\nThe International ISBN Agency maintains the details of over one million ISBN prefixes and publishers in the Global Register of Publishers. This database is freely searchable over the internet.\nPublishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.\nBy using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements. Here are some sample ISBN-10 codes, illustrating block length variations.\n\nEnglish-language pattern\nEnglish-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:\n\nCheck digits\nA check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the 10-digit ISBN is an extension of that for SBNs, so the two systems are compatible; an SBN prefixed with a zero (the 10-digit ISBN) will give the same check digit as the SBN without the zero. The check digit is base eleven, and can be an integer between 0 and 9, or an 'X'. The system for 13-digit ISBNs is not compatible with SBNs and will, in general, give a different check digit from the corresponding 10-digit ISBN, so does not provide the same protection against transposition. This is because the 13-digit code was required to be compatible with the EAN format, and hence could not contain the letter 'X'.\n\nISBN-10 check digits\nAccording to the 2001 edition of the International ISBN Agency's official user manual, the ISBN-10 check digit (which is the last digit of the 10-digit ISBN) must range from 0 to 10 (the symbol 'X' is used for 10), and must be such that the sum of the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11. That is, if xi is the ith digit, then x10 must be chosen such that:\n\nFor example, for an ISBN-10 of 0-306-40615-2:\n\nFormally, using modular arithmetic, this is rendered\n\nIt is also true for ISBN-10s that the sum of all ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:\n\nFormally, this is rendered\n\nThe two most common errors in handling an ISBN (e.g. when typing it or writing it down) are a single altered digit or the transposition of adjacent digits. It can be proven mathematically that all pairs of valid ISBN-10s differ in at least two digits. It can also be proven that there are no pairs of valid ISBN-10s with eight identical digits and two transposed digits (these proofs are true because the ISBN is less than eleven digits long and because 11 is a prime number). The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e., if either of these types of error has occurred, the result will never be a valid ISBN\u2014the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error were to occur in the publishing house and remain undetected, the book would be issued with an invalid ISBN.\nIn contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).\n\nISBN-10 check digit calculation\nEach of the first nine digits of the 10-digit ISBN\u2014excluding the check digit itself\u2014is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.\nFor example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:\n\nAdding 2 to 130 gives a multiple of 11 (because 132 = 12\u00d711)\u2014this is the only number between 0 and 10 which does so. Therefore, the check digit has to be 2, and the complete sequence is ISBN 0-306-40615-2. If the value of \n  \n    \n      \n        \n          x\n          \n            10\n          \n        \n      \n    \n    {\\displaystyle x_{10}}\n  \n required to satisfy this condition is 10, then an 'X' should be used.\nAlternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation, the calculation could result in a check digit value of 11 \u2212 0 = 11, which is invalid. (Strictly speaking, the first \"modulo 11\" is not needed, but it may be considered to simplify the calculation.)\nFor example, the check digit for the ISBN of 0-306-40615-? is calculated as follows:\n\nThus the check digit is 2.\nIt is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:\n\nThe modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.\n\nISBN-13 check digit calculation\nAppendix 1 of the International ISBN Agency's official user manual:\u200a33\u200a describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10. As ISBN-13 is a subset of EAN-13, the algorithm for calculating the check digit is exactly the same for both.\nFormally, using modular arithmetic, this is rendered:\n\nThe calculation of an ISBN-13 check digit begins with the first twelve digits of the 13-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero replaces a ten, so, in all cases, a single check digit results.\nFor example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:\n\ns = 9\u00d71 + 7\u00d73 + 8\u00d71 + 0\u00d73 + 3\u00d71 + 0\u00d73 + 6\u00d71 + 4\u00d73 + 0\u00d71 + 6\u00d73 + 1\u00d71 + 5\u00d73\n  =   9 +  21 +   8 +   0 +   3 +   0 +   6 +  12 +   0 +  18 +   1 +  15\n  = 93\n93 \/ 10 = 9 remainder 3\n10 \u2013  3 = 7\n\nThus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.\nIn general, the ISBN check digit is calculated as follows.\nLet\n\nThen\n\nThis check system\u2014similar to the UPC check digit formula\u2014does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3 \u00d7 6 + 1 \u00d7 1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3 \u00d7 1 + 1 \u00d7 6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0\u20139 to express the check digit.\nAdditionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).\n\nISBN-10 to ISBN-13 conversion\nA 10-digit ISBN is converted to a 13-digit ISBN by prepending \"978\" to the  ISBN-10 and recalculating the final checksum digit using the ISBN-13 algorithm. The reverse process can also be performed, but not for numbers commencing with a prefix other than 978, which have no 10-digit equivalent.\n\nErrors in usage\nPublishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers. For example, ISBN 0-590-76484-5 is shared by two books\u2014Ninja gaiden: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.\nMost libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase \"Cancelled ISBN\". The International Union Library Catalog (a.k.a., WorldCat OCLC\u2014Online Computer Library Center system) often indexes by invalid ISBNs, if the book is indexed in that way by a member library.\n\neISBN\nOnly the term \"ISBN\" should be used; the terms \"eISBN\" and \"e-ISBN\" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic \"eISBN\" which encompasses all the e-book formats for a title.\n\nEAN format used in barcodes, and upgrading\nThe barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits called an EAN-5 for the currency and the recommended retail price. For 10-digit ISBNs, the number \"978\", the Bookland \"country code\", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN-13 formula (modulo 10, 1\u00d7 and 3\u00d7 weighting on alternating digits).\nPartly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a 13-digit ISBN (ISBN-13). The process began on 1 January 2005 and was planned to conclude on 1 January 2007. As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. The 10-digit ISMN codes differed visually as they began with an \"M\" letter; the bar code represents the \"M\" as a zero, and for checksum purposes it counted as a 3. All ISMNs are now thirteen digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.\nPublisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the 10-digit ISBN check digit generally is not the same as the 13-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.\nBarcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN in North America.\n\nSee also\nASIN (Amazon Standard Identification Number)\nBICI (Book Item and Component Identifier)\nBook sources search \u2013 a Wikipedia resource that allows search by ISBNs\nCODEN (serial publication identifier currently used by libraries; replaced by the ISSN for new works)\nDOI (Digital Object Identifier)\nESTC (English Short Title Catalogue)\nISAN (International Standard Audiovisual Number)\nISRC (International Standard Recording Code)\nISTC (International Standard Text Code)\nISWC (International Standard Musical Work Code)\nISSN (International Standard Serial Number)\nISWN (International Standard Wine Number)\nLCCN (Library of Congress Control Number)\nLicense number (East German books) (Book identification system used between 1951 and 1990 in the former GDR)\nList of group-0 ISBN publisher codes\nList of group-1 ISBN publisher codes\nList of ISBN registration groups\nSICI (Serial Item and Contribution Identifier)\nVD 16 (Verzeichnis der im deutschen Sprachbereich erschienenen Drucke des 16. Jahrhunderts, \"Bibliography of Books Printed in the German Speaking Countries of the Sixteenth Century\")\nVD 17 (Verzeichnis der im deutschen Sprachraum erschienenen Drucke des 17. Jahrhunderts, \"Bibliography of Books Printed in the German Speaking Countries of the Seventeenth Century\")\n\nExplanatory notes\nReferences\nExternal links\n\nISO 2108:2017 \u2013 International Standard Book Number (ISBN)\nInternational ISBN Agency \u2013 coordinates and supervises the worldwide use of the ISBN system\nNumerical List of Group Identifiers \u2013 List of language\/region prefixes\nFree conversion tool: ISBN-10 to ISBN-13 & ISBN-13 to ISBN-10 from the ISBN agency. Also shows correct hyphenation & verifies if ISBNs are valid or not.\n\"Guidelines for the Implementation of 13-Digit ISBNs\" (PDF). Archived from the original (PDF) on 12 September 2004.\nRFC 3187 \u2013 Using International Standard Book Numbers as Uniform Resource Names (URN)\nWorldwide Auto-Converter at Library of Congress","95":"Illusions of self-motion (or \"vection\") occur when one perceives bodily motion despite no movement taking place. One can experience illusory movements of the whole body or of individual body parts, such as arms or legs.\n\nVestibular illusions\nThe vestibular system is one of the major sources of information about one's own motion. Disorders of the visual system can lead to dizziness, vertigo, and feelings of instability. Vertigo is not associated with illusory self-motion as it does not typically make one feel as though they are moving; however, in a subclass of vertigo known as subjective vertigo one does experience their own motion. People experience themselves being pulled heavily in one direction. There are also specific self-motion illusions that can occur through abnormal stimulation of various parts of the vestibular system, often encountered in aviation. This includes an illusion of inversion, in which one feels like they're tumbling backwards. Through various stimuli, people can be made to feel as if they are moving when they are not, not moving when they are, tilted when they are not, or not tilted when they are.\n\nVisual illusions\nWhen a large part of the visual field moves, viewers feel like they have moved and that the world is stationary. For example, when one is in a train at a station, and a nearby train moves, one can have the illusion that one's own train has moved in the opposite direction. Common sorts of vection include circular vection, where an observer is placed at the center of rotation of a large vertically-oriented rotating drum, usually painted with vertical stripes; linear vection, where an observer views a field that either approaches or recedes;  and roll vection, where an observer views a patterned disk rotating around their line of sight. During circular vection, the observer feels like they are rotating and the drum is stationary. During linear vection, the observer feels like they have moved forwards or backwards and the stimulus has stayed stationary. During roll vection, the observer feels like they have rotated around the line of sight and the disk has stayed stationary.\nInducing vection can also induce motion sickness in susceptible individuals.\n\nAuditory illusions\nCompared to visually-induced vection, auditorily-induced vection is generally weaker. Auditory-induced vection can only be elicited in about 25% to 75% of the participants under laboratory conditions, and only when participants are blindfolded. Most of the research has focused on eliciting circular vection horizontally about the body. Researchers have induced circular vection by mechanically rotating a buzzer around a subject in the dark or by presenting sound sequentially in one of several speakers arranged in a circular array. Adding auditory stimuli can significantly enhance visual, vestibular, and biomechanical vections.\n\nBiomechanical illusions\nSea legs, dock rock, or stillness illness\nAfter being on a small boat for a few hours and then going back onto land, it may feel like there is still rising and falling, as if one is still on the boat. It can also occur on other situations, such as after a long journey by train or by aircraft, or after working up a swaying tree. It is not clear whether sea legs are a form of aftereffect to the predominant frequency of the stimulation (e.g., the waves or the rocking of the train), whether it is a form of learning to adjust one's gait and posture. The \"sea legs\" condition needs to be distinguished from mal de debarquement, which is much more long-lasting.\n\nTreadmills\nSubjects report a strong sense of self-rotation from stepping along a circular treadmill in the dark, which can be further enhanced through auditory cues.\nAfter spending more than 10 minutes on a linear treadmill, it is common to experience the visual illusion of moving at a substantially accelerated pace for 2-3 minutes once back on solid ground.\n\nSee also\nBalance disorder\nBroken escalator phenomenon\nChronic subjective dizziness\nIdeomotor phenomenon\nProprioception\nSeasickness\nSense of balance, also known as equilibrioception\nSensory illusions in aviation\nSpatial disorientation\nTetris effect\n\n\n== References ==","96":"Infarction is tissue death (necrosis) due to inadequate blood supply to the affected area. It may be caused by artery blockages, rupture, mechanical compression, or vasoconstriction. The resulting lesion is referred to as an infarct\n(from the Latin infarctus, \"stuffed into\").\n\nCauses\nInfarction occurs as a result of prolonged ischemia, which is the insufficient supply of oxygen and nutrition to an area of tissue due to a disruption in blood supply. The blood vessel supplying the affected area of tissue may be blocked due to an obstruction in the vessel (e.g., an arterial embolus, thrombus, or atherosclerotic plaque), compressed by something outside of the vessel causing it to narrow (e.g., tumor, volvulus, or hernia), ruptured by trauma causing a loss of blood pressure downstream of the rupture, or vasoconstricted, which is the narrowing of the blood vessel by contraction of the muscle wall rather than an external force (e.g., cocaine vasoconstriction leading to myocardial infarction).\n\nHypertension and atherosclerosis are risk factors for both atherosclerotic plaques and thromboembolism. In atherosclerotic formations, a plaque develops under a fibrous cap. When the fibrous cap is degraded by metalloproteinases released from macrophages or by intravascular shear force from blood flow, subendothelial thrombogenic material (extracellular matrix) is exposed to circulating platelets and thrombus formation occurs on the vessel wall occluding blood flow. Occasionally, the plaque may rupture and form an embolus which travels with the blood-flow downstream to where the vessel narrows and eventually clogs the vessel lumen.\n\nClassification\nBy histopathology\nInfarctions are divided into two types according to the amount of blood present:\n\nWhite infarctions (anemic infarcts) affect solid organs such as the spleen, heart and kidneys wherein the solidity of the tissue substantially limits the amount of nutrients (blood\/oxygen\/glucose\/fuel) that can flow into the area of ischaemic necrosis. Similar occlusion to blood flow and consequent necrosis can occur as a result of severe vasoconstriction as illustrated in severe Raynaud's phenomenon that can lead to irreversible gangrene.\nRed infarctions (hemorrhagic infarcts) generally affect the lungs or other loose organs (testis, ovary, small intestines). The occlusion consists more of red blood cells and fibrin strands. Characteristics of red infarcts include:\nocclusion of a vein\nloose tissues that allow blood to collect in the infarcted zone\ntissues with a dual circulatory system (lung, small intestines)\ntissues previously congested from sluggish venous outflow\nreperfusion (injury) of previously ischemic tissue that is associated with reperfusion-related diseases, such as myocardial infarction, stroke (cerebral infarction), shock-resuscitation, replantation surgery, frostbite, burns, and organ transplantation.\n\nBy localization\nHeart: Myocardial infarction (MI), commonly known as a heart attack, is an infarction of the heart, causing some heart cells to die. This is most commonly due to occlusion (blockage) of a coronary artery following the rupture of a vulnerable atherosclerotic plaque, which is an unstable collection of lipids (fatty acids) and white blood cells (especially macrophages) in the wall of an artery. The resulting ischemia (restriction in blood supply) and oxygen shortage, if left untreated for a sufficient period of time, can cause damage or kill heart muscle tissue (myocardium).\n\nBrain: Cerebral infarction is the ischemic kind of stroke due to a disturbance in the blood vessels supplying blood to the brain. It can be atherothrombotic or embolic. Stroke caused by cerebral infarction should be distinguished from two other kinds of stroke: cerebral hemorrhage and subarachnoid hemorrhage. Cerebral infarctions vary in their severity with one third of the cases resulting in death. In response to ischemia, the brain degenerates by the process of liquefactive necrosis.\nLung: Pulmonary infarction or lung infarction\nSpleen: Splenic infarction occurs when the splenic artery or one of its branches are occluded, for example by a blood clot. Although it can occur asymptomatically, the typical symptom is severe pain in the left upper quadrant of the abdomen, sometimes radiating to the left shoulder. Fever and chills develop in some cases. It has to be differentiated from other causes of acute abdomen.\nLimb: Limb infarction is an infarction of an arm or leg. Causes include arterial embolisms and skeletal muscle infarction as a rare complication of long standing, poorly controlled diabetes mellitus. A major presentation is painful thigh or leg swelling.\nBone: Infarction of bone results in avascular necrosis. Without blood, the bone tissue dies and the bone collapses. If avascular necrosis involves the bones of a joint, it often leads to destruction of the joint articular surfaces (see osteochondritis dissecans).\nTesticle: an infarction of a testicle is commonly caused by testicular torsion and may require removal of the affected testicle(s) if not undone by surgery quickly enough.\n\nEye: an infarction can occur to the central retinal artery which supplies the retina causing sudden visual loss.\nBowel: Bowel infarction is generally caused by mesenteric ischemia due to blockages in the arteries or veins that supply the bowel.\n\nAssociated diseases\nDiseases commonly associated with infarctions include:\n\nPeripheral artery occlusive disease (the most severe form of which is gangrene)\nAntiphospholipid syndrome\nSepsis\nGiant-cell arteritis (GCA)\nHernia\nVolvulus\nSickle-cell disease\n\nFirst aid\nEach type of infarction requires its own care.\nInfarction in the heart requires first aid for myocardial infarction (due to acute coronary syndrome).\nInfarction in the brain requires first aid for stroke (using a protocol named F.A.S.T.).\n\nReferences\nExternal links\n Media related to Infarction at Wikimedia Commons\n The dictionary definition of infarction at Wiktionary","97":"The inner ear (internal ear, auris interna) is the innermost part of the vertebrate ear. In vertebrates, the inner ear is mainly responsible for sound detection and balance. In mammals, it consists of the bony labyrinth, a hollow cavity in the temporal bone of the skull with a system of passages comprising two main functional parts:\n\nThe cochlea, dedicated to hearing; converting sound pressure patterns from the outer ear into electrochemical impulses which are passed on to the brain via the auditory nerve.\nThe vestibular system, dedicated to balance.\nThe inner ear is found in all vertebrates, with substantial variations in form and function. The inner ear is innervated by the eighth cranial nerve in all vertebrates.\n\nStructure\nThe labyrinth can be divided by layer or by region.\n\nBony and membranous labyrinths\nThe bony labyrinth, or osseous labyrinth, is the network of passages with bony walls lined with periosteum. The three major parts of the bony labyrinth are the vestibule of the ear, the semicircular canals, and the cochlea. The membranous labyrinth runs inside of the bony labyrinth, and creates three parallel fluid filled spaces. The two outer are filled with perilymph and the inner with endolymph.\n\nVestibular and cochlear systems\nIn the middle ear, the energy of pressure waves is translated into mechanical vibrations by the three auditory ossicles. Pressure waves move the tympanic membrane which in turns moves the malleus, the first bone of the middle ear. The malleus articulates to incus which connects to the stapes. The footplate of the stapes connects to the oval window, the beginning of the inner ear. When the stapes presses on the oval window, it causes the perilymph, the liquid of the inner ear to move. The middle ear thus serves to convert the energy from sound pressure waves to a force upon the perilymph of the inner ear. The oval window has only approximately 1\/18 the area of the tympanic membrane and thus produces a higher pressure. The cochlea propagates these mechanical signals as waves in the fluid and membranes and then converts them to nerve impulses which are transmitted to the brain.\nThe vestibular system is the region of the inner ear where the semicircular canals converge, close to the cochlea. The vestibular system works with the visual system to keep objects in view when the head is moved. Joint and muscle receptors are also important in maintaining balance. The brain receives, interprets, and processes the information from all these systems to create the sensation of balance.\nThe vestibular system of the inner ear is responsible for the sensations of balance and motion.  It uses the same kinds of fluids and detection cells (hair cells) as the cochlea uses, and sends information to the brain about the attitude, rotation, and linear motion of the head.  The type of motion or attitude detected by a hair cell depends on its associated mechanical structures, such as the curved tube of a semicircular canal or the calcium carbonate crystals (otolith) of the saccule and utricle.\n\nDevelopment\nThe human inner ear develops during week 4 of embryonic development from the auditory placode, a thickening of the ectoderm which gives rise to the bipolar neurons of the cochlear and vestibular ganglions. As the auditory placode invaginates towards the embryonic mesoderm, it forms the auditory vesicle or otocyst.\nThe auditory vesicle will give rise to the utricular and saccular components of the membranous labyrinth. They contain the sensory hair cells and otoliths of the macula of utricle and of the saccule, respectively, which respond to linear acceleration and the force of gravity. The utricular division of the auditory vesicle also responds to angular acceleration, as well as the endolymphatic sac and duct that connect the saccule and utricle.\nBeginning in the fifth week of development, the auditory vesicle also gives rise to the cochlear duct, which contains the spiral organ of Corti and the endolymph that accumulates in the membranous labyrinth. The vestibular wall will separate the cochlear duct from the perilymphatic scala vestibuli, a cavity inside the cochlea. The basilar membrane separates the cochlear duct from the scala tympani, a cavity within the cochlear labyrinth. The lateral wall of the cochlear duct is formed by the spiral ligament and the stria vascularis, which produces the endolymph. The hair cells develop from the lateral and medial ridges of the cochlear duct, which together with the tectorial membrane make up the organ of Corti.\n\nMicroanatomy\nRosenthal's canal or the spiral canal of the cochlea is a section of the bony labyrinth of the inner ear that is approximately 30 mm long and makes 2\u00be turns about the modiolus, the central axis of the cochlea that contains the spiral ganglion.\nSpecialized inner ear cell include: hair cells, pillar cells, Boettcher's cells, Claudius' cells, spiral ganglion neurons, and Deiters' cells (phalangeal cells).\nThe hair cells are the primary auditory receptor cells and they are also known as auditory sensory cells, acoustic hair cells, auditory cells or cells of Corti. The organ of Corti is lined with a single row of inner hair cells and three rows of outer hair cells. The hair cells have a hair bundle at the apical surface of the cell. The hair bundle consists of an array of actin-based stereocilia. Each stereocilium inserts as a rootlet into a dense filamentous actin mesh known as the cuticular plate. Disruption of these bundles results in hearing impairments and balance defects.\nInner and outer pillar cells in the organ of Corti support hair cells. Outer pillar cells are unique because they are free standing cells which only contact adjacent cells at the bases and apices. Both types of pillar cell have thousands of cross linked microtubules and actin filaments in parallel orientation. They provide mechanical coupling between the basement membrane and the mechanoreceptors on the hair cells.\nBoettcher's cells are found in the organ of Corti where they are present only in the lower turn of the cochlea. They lie on the basilar membrane beneath Claudius' cells and are organized in rows, the number of which varies between species. The cells interdigitate with each other, and project microvilli into the intercellular space. They are supporting cells for the auditory hair cells in the organ of Corti. They are named after German pathologist Arthur B\u00f6ttcher (1831\u20131889).\nClaudius' cells are found in the organ of Corti located above rows of Boettcher's cells. Like Boettcher's cells, they are considered supporting cells for the auditory hair cells in the organ of Corti. They contain a variety of aquaporin water channels and appear to be involved in ion transport. They also play a role in sealing off endolymphatic spaces. They are named after the German anatomist Friedrich Matthias Claudius (1822\u20131869).\nDeiters' cells (phalangeal cells) are a type of neuroglial cell found in the organ of Corti and organised in one row of inner phalangeal cells and three rows of outer phalangeal cells. They are the supporting cells of the hair cell area within the cochlea. They are named after the German pathologist Otto Deiters (1834\u20131863) who described them.\nHensen's cells are high columnar cells that are directly adjacent to the third row of Deiters' cells.\nHensen's stripe is the section of the tectorial membrane above the inner hair cell.\nNuel's spaces refer to the fluid-filled spaces between the outer pillar cells and adjacent hair cells and also the spaces between the outer hair cells.\nHardesty's membrane is the layer of the tectoria closest to the reticular lamina and overlying the outer hair cell region.\nReissner's membrane is composed of two cell layers and separates the scala media from the scala vestibuli.\nHuschke's teeth are the tooth-shaped ridges on the spiral limbus that are in contact with the tectoria and separated by interdental cells.\n\nBlood supply\nThe bony labyrinth receives its blood supply from three arteries:\n1 \u2013 Anterior tympanic branch (from maxillary artery).\n2 \u2013 Petrosal branch (from middle meningeal artery).\n3 \u2013 Stylomastoid branch (from posterior auricular artery).\nThe membranous labyrinth is supplied by the labyrinthine artery.\nVenous drainage of the inner ear is through the labyrinthine vein, which empties into the sigmoid sinus or inferior petrosal sinus.\n\nFunction\nNeurons within the ear respond to simple tones, and the brain serves to process other increasingly complex sounds. An average adult is typically able to detect sounds ranging between 20 and 20,000 Hz. The ability to detect higher pitch sounds decreases in older humans.\nThe human ear has evolved with two basic tools to encode sound waves; each is separate in detecting high and low-frequency sounds. Georg von B\u00e9k\u00e9sy (1899\u20131972) employed the use of a microscope in order to examine the basilar membrane located within the inner-ear of cadavers. He found that movement of the basilar membrane resembles that of a traveling wave; the shape of which varies based on the frequency of the pitch. In low-frequency sounds, the tip (apex) of the membrane moves the most, while in high-frequency sounds, the base of the membrane moves most.\n\nDisorders\nInterference with or infection of the labyrinth can result in a syndrome of ailments called labyrinthitis.  The symptoms of labyrinthitis include temporary nausea, disorientation, vertigo, and dizziness. Labyrinthitis can be caused by viral infections, bacterial infections, or physical blockage of the inner ear.\nAnother condition has come to be known as autoimmune inner ear disease (AIED). It is characterized by idiopathic, rapidly progressive, bilateral sensorineural hearing loss. It is a fairly rare disorder while at the same time, a lack of proper diagnostic testing has meant that its precise incidence cannot be determined.\n\nOther animals\nBirds have an auditory system similar to that of mammals, including a cochlea. Reptiles, amphibians, and fish do not have cochleas but hear with simpler auditory organs or vestibular organs, which generally detect lower-frequency sounds than the cochlea.  The cochlea of birds is also similar to that of crocodiles, consisting of a short, slightly curved bony tube within which lies the basilar membrane with its sensory structures.\n\nCochlear system\nIn reptiles, sound is transmitted to the inner ear by the stapes (stirrup) bone of the middle ear. This is pressed against the oval window, a membrane-covered opening on the surface of the vestibule. From here, sound waves are conducted through a short perilymphatic duct to a second opening, the round window, which equalizes pressure, allowing the incompressible fluid to move freely. Running parallel with the perilymphatic duct is a separate blind-ending duct, the lagena, filled with endolymph. The lagena is separated from the perilymphatic duct by a basilar membrane, and contains the sensory hair cells that finally translate the vibrations in the fluid into nerve signals. It is attached at one end to the saccule.\nIn most reptiles the perilymphatic duct and lagena are relatively short, and the sensory cells are confined to a small basilar papilla lying between them. However, in mammals, birds, and crocodilians, these structures become much larger and somewhat more complicated. In birds, crocodilians, and monotremes, the ducts are simply extended, together forming an elongated, more or less straight, tube. The endolymphatic duct is wrapped in a simple loop around the lagena, with the basilar membrane lying along one side. The first half of the duct is now referred to as the scala vestibuli, while the second half, which includes the basilar membrane, is called the scala tympani. As a result of this increase in length, the basilar membrane and papilla are both extended, with the latter developing into the organ of Corti, while the lagena is now called the cochlear duct. All of these structures together constitute the cochlea.\nIn therian mammals, the lagena is extended still further, becoming a coiled structure (cochlea) in order to accommodate its length within the head. The organ of Corti also has a more complex structure in mammals than it does in other amniotes.\nThe arrangement of the inner ear in living amphibians is, in most respects, similar to that of reptiles. However, they often lack a basilar papilla, having instead an entirely separate set of sensory cells at the upper edge of the saccule, referred to as the papilla amphibiorum, which appear to have the same function.\nAlthough many fish are capable of hearing, the lagena is, at best, a short diverticulum of the saccule, and appears to have no role in sensation of sound. Various clusters of hair cells within the inner ear may instead be responsible; for example, bony fish contain a sensory cluster called the macula neglecta in the utricle that may have this function. Although fish have neither an outer nor a middle ear, sound may still be transmitted to the inner ear through the bones of the skull, or by the swim bladder, parts of which often lie close by in the body.\n\nVestibular system\nBy comparison with the cochlear system, the vestibular system varies relatively little between the various groups of jawed vertebrates. The central part of the system consists of two chambers, the saccule and utricle, each of which includes one or two small clusters of sensory hair cells. All jawed vertebrates also possess three semicircular canals arising from the utricle, each with an ampulla containing sensory cells at one end.\nAn endolymphatic duct runs from the saccule up through the head and ending close to the brain. In cartilaginous fish, this duct actually opens onto the top of the head, and in some teleosts, it is simply blind-ending. In all other species, however, it ends in an endolymphatic sac. In many reptiles, fish, and amphibians this sac may reach considerable size. In amphibians the sacs from either side may fuse into a single structure, which often extends down the length of the body, parallel with the spinal canal.\nThe primitive lampreys and hagfish, however, have a simpler system. The inner ear in these species consists of a single vestibular chamber, although in lampreys, this is associated with a series of sacs lined by cilia. Lampreys have only two semicircular canals, with the horizontal canal being absent, while hagfish have only a single, vertical, canal.\n\nEquilibrium\nThe inner ear is primarily responsible for balance, equilibrium and orientation in three-dimensional space. The inner ear can detect both static and dynamic equilibrium. Three semicircular ducts and two chambers, which contain the saccule and utricle, enable the body to detect any deviation from equilibrium. The macula sacculi detects vertical acceleration while the macula utriculi is responsible for horizontal acceleration. These microscopic structures possess stereocilia and one kinocilium which are located within the gelatinous otolithic membrane. The membrane is further weighted with otoliths. Movement of the stereocilia and kinocilium enable the hair cells of the saccula and utricle to detect motion. The semicircular ducts are responsible for detecting rotational movement.\n\nAdditional images\nSee also\nHearing\nOuter ear\nTip link\n\nReferences\nRuckenstein, M. J. (2004). \"Autoimmune Inner Ear Disease\". Current Opinion in Otolaryngology & Head and Neck Surgery, 12(5), pp. 426\u2013430.\nSaladin, Anatomy and Physiology 6th ed., print\nAmerican Speech-Language-Hearing Association, \"The Middle Ear\",\n\nExternal links\n\nAnatomy photo:30:05-0101 at the SUNY Downstate Medical Center","98":"Ischemia or ischaemia is a restriction in blood supply to any tissue, muscle group, or organ of the body, causing a shortage of oxygen that is needed for cellular metabolism (to keep tissue alive).  Ischemia is generally caused by problems with blood vessels, with resultant damage to or dysfunction of tissue i.e. hypoxia and microvascular dysfunction. It also implies local hypoxia in a part of a body resulting from constriction (such as vasoconstriction, thrombosis, or embolism). \nIschemia causes not only insufficiency of oxygen, but also reduced availability of nutrients and inadequate removal of metabolic wastes. Ischemia can be partial (poor perfusion) or total blockage. The inadequate delivery of oxygenated blood to the organs must be resolved either by treating the cause of the inadequate delivery or reducing the oxygen demand of the system that needs it. For example, patients with myocardial ischemia have a decreased blood flow to the heart and are prescribed with medications that reduce chronotrophy and ionotrophy to meet the new level of blood delivery supplied by the stenosed vasculature so that it is adequate.\n\nSigns and symptoms\nThe signs and symptoms of ischemia vary, as they can occur anywhere in the body and depend on the degree to which blood flow is interrupted. For example, clinical manifestations of acute limb ischemia (which can be summarized as the \"six P's\") include pain, pallor, pulseless, paresthesia, paralysis, and poikilothermia.\nWithout immediate intervention, ischemia may progress quickly to tissue necrosis and gangrene within a few hours. Paralysis is a very late sign of acute arterial ischemia and signals the death of nerves supplying the extremity. Foot drop may occur as a result of nerve damage. Because nerves are extremely sensitive to hypoxia, limb paralysis or ischemic neuropathy may persist after revascularization and may be permanent.\n\nCardiac ischemia\nCardiac ischemia may be asymptomatic or may cause chest pain, known as angina pectoris. It occurs when the heart muscle, or myocardium, receives insufficient blood flow. This most frequently results from atherosclerosis, which is the long-term accumulation of cholesterol-rich plaques in the coronary arteries. In most Western countries, Ischemic heart disease is the most common cause of death in both men and women, and a major cause of hospital admissions.\n\nBowel\nBoth large and small intestines can be affected by ischemia. The blockage of blood flow to the large intestine (colon) is called ischemic colitis. Ischemia of the small bowel is called mesenteric ischemia.\n\nBrain\nBrain ischemia is insufficient blood flow to the brain, and can be acute or chronic. Acute ischemic stroke is a neurological emergency typically caused by a blood clot blocking blood flow in a vessel in the brain. Chronic ischemia of the brain may result in a form of dementia called vascular dementia. A sudden, brief episode (symptoms lasting only minutes) of ischemia affecting the brain is called a transient ischemic attack (TIA), often called a mini-stroke. TIAs can be a warning of future strokes, with approximately 1\/3 of TIA patients having a serious stroke within one year.\n\nLimb\nInadequate blood supply to a limb may result in acute limb ischemia or chronic limb threatening ischemia.\n\nCutaneous\nReduced blood flow to the skin layers may result in mottling or uneven, patchy discoloration of the skin.\n\nKidney ischemia\nKidney ischemia is a loss of blood flow to the kidney cells. Several physical symptoms include shrinkage of one or both kidneys, renovascular hypertension, acute renal failure, progressive azotemia, and acute pulmonary edema. It is a disease with high mortality rate and high morbidity. Failure to treat could cause chronic kidney disease and a need for renal surgery.\n\nCauses\nIschemia is a vascular disease involving an interruption in the arterial blood supply to a tissue, organ, or extremity that, if untreated, can lead to tissue death. It can be caused by embolism, thrombosis of an atherosclerotic artery, or trauma. Venous problems like venous outflow obstruction and low-flow states can cause acute arterial ischemia.  An aneurysm is one of the most frequent causes of acute arterial ischemia. Other causes are heart conditions including myocardial infarction, mitral valve disease, chronic atrial fibrillation, cardiomyopathies, and prosthesis, in all of which thrombi are prone to develop.\n\nOcclusion\nThe thrombi may dislodge and may travel anywhere in the circulatory system, where they may lead to pulmonary embolus, an acute arterial occlusion causing the oxygen and blood supply distal to the embolus to decrease suddenly. The degree and extent of symptoms depend on the size and location of the obstruction, the occurrence of clot fragmentation with embolism to smaller vessels, and the degree of peripheral arterial disease (PAD).\n\nThromboembolism (blood clots)\nEmbolism (foreign bodies in the circulation, e.g. amniotic fluid embolism)\n\nTrauma\nTraumatic injury to an extremity may produce partial or total occlusion of a vessel from compression, shearing, or laceration. Acute arterial occlusion may develop as a result of arterial dissection in the carotid artery or aorta or as a result of iatrogenic arterial injury (e.g., after angiography).\n\nOther\nAn inadequate flow of blood to a part of the body may be caused by any of the following:\n\nThoracic outlet syndrome (compression of the brachial plexus)\nAtherosclerosis (lipid-laden plaques obstructing the lumen of arteries)\nHypoglycemia (lower than normal level of glucose)\nTachycardia (abnormally rapid beating of the heart)\nRadiotherapy, therapeutic radiation used to treat cancer can cause a delayed side effect injury in adjacent tissue via progressive, proliferative endarteritis, inflamed arterial linings that disrupt the tissue's blood supply.\nHypotension (low blood pressure, e.g. in septic shock, heart failure)\nOutside compression of a blood vessel, e.g. by a tumor or in the case of superior mesenteric artery syndrome\nSickle cell disease (abnormally shaped red blood cells)\nInduced g-forces which restrict the blood flow and force the blood to the extremities of the body, as in acrobatics and military flying\nLocalized extreme cold, such as by frostbite or improper cold compression therapy\nTourniquet application\nAn increased level of glutamate receptor stimulation \nArteriovenous malformations and peripheral artery occlusive disease\nrupture of significant blood vessels supplying a tissue or organ.\nAnemia vasoconstricts the periphery so that red blood cells cannot work internally on vital organs such as the heart, brain, etc., thus causing lack of oxygen to the periphery.\nPremature discontinuation of any oral anticoagulant.\nUnconsciousness, such as due to the ingestion of excessive doses of central depressants like alcohol or opioids, can result in ischemia of the extremities due to unusual body positions that prevent normal circulation\n\nPathophysiology\nIschemia results in tissue damage in a process known as ischemic cascade. The damage is the result of the build-up of metabolic waste products, inability to maintain cell membranes, mitochondrial damage, and eventual leakage of autolyzing proteolytic enzymes into the cell and surrounding tissues.\nRestoration of blood supply to ischemic tissues can cause additional damage known as reperfusion injury that can be more damaging than the initial ischemia. Reintroduction of blood flow brings oxygen back to the tissues, causing a greater production of free radicals and reactive oxygen species that damage cells.  It also brings more calcium ions to the tissues causing further calcium overloading and can result in potentially fatal cardiac arrhythmias and also accelerates cellular self-destruction. The restored blood flow also exaggerates the inflammation response of damaged tissues, causing white blood cells to destroy damaged cells that may otherwise still be viable.\n\nTreatment\nEarly treatment is essential to keep the affected organ viable. The treatment options include injection of an anticoagulant, thrombolysis, embolectomy, surgical revascularization, or partial amputation. Anticoagulant therapy is initiated to prevent further enlargement of the thrombus. Continuous IV unfractionated heparin has been the traditional agent of choice.\nIf the condition of the ischemic limb is stabilized with anticoagulation, recently formed emboli may be treated with catheter-directed thrombolysis using intra-arterial infusion of a thrombolytic agent (e.g., recombinant tissue plasminogen activator (tPA), streptokinase, or urokinase). A percutaneous catheter inserted into the femoral artery and threaded to the site of the clot is used to infuse the drug. Unlike anticoagulants, thrombolytic agents work directly to resolve the clot over a period of 24 to 48 hours.\nDirect arteriotomy may be necessary to remove the clot. Surgical revascularization may be used in the setting of trauma (e.g., laceration of the artery). Amputation is reserved for cases where limb salvage is not possible. If the patient continues to have a risk of further embolization from some persistent source, such as chronic atrial fibrillation, treatment includes long-term oral anticoagulation to prevent further acute arterial ischemic episodes.\nDecrease in body temperature reduces the aerobic metabolic rate of the affected cells, reducing the immediate effects of hypoxia.  Reduction of body temperature also reduces the inflammation response and reperfusion injury. For frostbite injuries, limiting thawing and warming of tissues until warmer temperatures can be sustained may reduce reperfusion injury.\nIschemic stroke is at times treated with various levels of statin therapy at hospital discharge, followed by home time, in an attempt to lower the risk of adverse events.\n\nSociety and culture\nThe Infarct Combat Project (ICP) is an international nonprofit organization founded in 1998 to fight ischemic heart diseases through education and research.\n\nEtymology and pronunciation\nThe word ischemia () is from Greek \u1f34\u03c3\u03c7\u03b1\u03b9\u03bc\u03bf\u03c2 iskhaimos 'staunching blood', from \u1f34\u03c3\u03c7\u03c9 iskh\u03bf 'keep back, restrain' and \u03b1\u1f37\u03bc\u03b1 haima 'blood'.\n\nSee also\nReferences\nBibliography\nElizabeth (editor). Oxford Reference: Concise Medical Dictionary (1990, 3rd ed.). Oxford University Press: Market House Books, 1987, 2nd ed., pp. 107, ISBN 978-0-19-281991-8\n\nExternal links\n\n Media related to Ischemia at Wikimedia Commons","99":"The International Classification of Diseases (ICD) is a globally used medical classification used in epidemiology, health management and for clinical purposes. The ICD is maintained by the World Health Organization (WHO), which is the directing and coordinating authority for health within the United Nations System. The ICD is originally designed as a health care classification system, providing a system of diagnostic codes for classifying diseases, including nuanced classifications of a wide variety of signs, symptoms, abnormal findings, complaints, social circumstances, and external causes of injury or disease. This system is designed to map health conditions to corresponding generic categories together with specific variations, assigning for these a designated code, up to six characters long. Thus, major categories are designed to include a set of similar diseases.\nThe ICD is published by the WHO and used worldwide for morbidity and mortality statistics, reimbursement systems, and automated decision support in health care. This system is designed to promote international comparability in the collection, processing, classification, and presentation of these statistics. The ICD is a major project to statistically classify all health disorders, and provide diagnostic assistance. The ICD is a core statistically based classificatory diagnostic system for health care related issues of the WHO Family of International Classifications (WHO-FIC).\nThe ICD is revised periodically and is currently in its 11th revision. The ICD-11, as it is therefore known, was accepted by WHO's World Health Assembly (WHA) on 25 May 2019 and officially came into effect on 1 January 2022. On 11 February 2022, the WHO stated that 35 countries were using the ICD-11.\nThe ICD is part of a \"family\" of international classifications (WHOFIC) that complement each other, also including the International Classification of Functioning, Disability and Health (ICF) which focuses on the domains of functioning (disability) associated with health conditions, from both medical and social perspectives, and the International Classification of Health Interventions (ICHI) that classifies the whole range of medical, nursing, functioning and public health interventions.\nThe title of the ICD is formally the International Statistical Classification of Diseases and Related Health Problems, although the original title, International Classification of Diseases, is still informally the name by which it is usually known.\nIn the United States and some other countries, the Diagnostic and Statistical Manual of Mental Disorders (DSM) is preferred for the classification of mental disorders for some purposes.\n\nHistorical synopsis\nIn 1860, during the international statistical congress held in London, Florence Nightingale made a proposal that was to result in the development of the first model of systematic collection of hospital data. In 1893, a French physician, Jacques Bertillon, introduced the Bertillon Classification of Causes of Death at a congress of the International Statistical Institute in Chicago.\nA number of countries adopted Bertillon's system, which was based on the principle of distinguishing between general diseases and those localized to a particular organ or anatomical site, as used by the City of Paris for classifying deaths. Subsequent revisions represented a synthesis of English, German, and Swiss classifications, expanding from the original 44 titles to 161 titles. In 1898, the American Public Health Association (APHA) recommended that the registrars of Canada, Mexico, and the United States also adopt it. The APHA also recommended revising the system every 10 years to ensure the system remained current with medical practice advances. As a result, the first international conference to revise the International Classification of Causes of Death took place in 1900, with revisions occurring every ten years thereafter. At that time, the classification system was contained in one book, which included an Alphabetic Index as well as a Tabular List. The book was small compared with current coding texts.\nThe revisions that followed contained minor changes, until the sixth revision of the classification system. With the sixth revision, the classification system expanded to two volumes. The sixth revision included morbidity and mortality conditions, and its title was modified to reflect the changes: International Statistical Classification of Diseases, Injuries and Causes of Death (ICD). Prior to the sixth revision, responsibility for ICD revisions fell to the Mixed Commission, a group composed of representatives from the International Statistical Institute and the Health Organization of the League of Nations. In 1948, the WHO assumed responsibility for preparing and publishing the revisions to the ICD every ten years. WHO sponsored the seventh and eighth revisions in 1957 and 1968, respectively. It later became clear that the established ten year interval between revisions was too short.\nThe ICD is currently the most widely used statistical classification system for diseases in the world. In addition, some countries\u2014including Australia, Canada, and the United States\u2014have developed their own adaptations of ICD, with more procedure codes for classification of operative or diagnostic procedures.\n\nVersions of ICD\nICD-6\nThe ICD-6, published in 1949, was the first to be shaped to become suitable for morbidity reporting. Accordingly, the name changed from International List of Causes of Death to International Statistical Classification of Diseases. The combined code section for injuries and their associated accidents was split into two, a chapter for injuries, and a chapter for their external causes. With use for morbidity there was a need for coding mental conditions, and for the first time a section on mental disorders was added .\n\nICD-7\nThe International Conference for the Seventh Revision of the International Classification of Diseases was held in Paris under the auspices of WHO in February 1955. In accordance with a recommendation of the WHO Expert Committee on Health Statistics, this revision was limited to essential changes and amendments of errors and inconsistencies.\n\nICD-8\nThe 8th Revision Conference convened by WHO met in Geneva, from 6 to 12 July 1965. This revision was more radical than the Seventh but left unchanged the basic structure of the Classification and the general philosophy of classifying diseases, whenever possible, according to their etiology rather than a particular manifestation.\nDuring the years that the Seventh and Eighth Revisions of the ICD were in force, the use of the ICD for indexing hospital medical records increased rapidly and some countries prepared national adaptations which provided the additional detail needed for this application of the ICD.\n\nICDA-8 (United States)\nIn the US, a group of consultants was asked to study the ICD-8 for its applicability to various users in the United States. This group recommended that further detail be provided for coding hospital and morbidity data. The American Hospital Association's \"Advisory Committee to the Central Office on ICDA\" developed the needed adaptation proposals, resulting in the publication of the International Classification of Diseases, Adapted (ICDA). In 1968, the United States Public Health Service published the International Classification of Diseases, Adapted, 8th Revision for use in the United States (ICDA-8). Beginning in 1968, ICDA-8 served as the basis for coding diagnostic data for both official morbidity and mortality statistics in the United States.\n\nICD-9\nThe International Conference for the Ninth Revision of the International Statistical Classification of Diseases, Injuries, and Causes of Death, convened by WHO, met in Geneva from 30 September to 6 October 1975. In the discussions leading up to the conference, it had originally been intended that there should be little change other than updating of the classification. This was mainly because of the expense of adapting data processing systems each time the classification was revised.\nThere had been an enormous growth of interest in the ICD and ways had to be found of responding to this, partly by modifying the classification itself and partly by introducing special coding provisions. A number of representations were made by specialist bodies which had become interested in using the ICD for their own statistics. Some subject areas in the classification were regarded as inappropriately arranged and there was considerable pressure for more detail and for adaptation of the classification to make it more relevant for the evaluation of medical care, by classifying conditions to the chapters concerned with the part of the body affected rather than to those dealing with the underlying generalized disease.\nAt the other end of the scale, there were representations from countries and areas where a detailed and sophisticated classification was irrelevant, but which nevertheless needed a classification based on the ICD in order to assess their progress in health care and in the control of disease. A field test with a bi-axial classification approach\u2014one axis (criterion) for anatomy, with another for etiology\u2014showed the impracticability of such approach for routine use.\nThe final proposals presented to and accepted by the Conference in 1978 retained the basic structure of the ICD, although with much additional detail at the level of the four digit subcategories, and some optional five digit subdivisions. For the benefit of users not requiring such detail, care was taken to ensure that the categories at the three digit level were appropriate.\nAs the World Health Organization explains: \"For the benefit of users wishing to produce statistics and indexes oriented towards medical care, the 9th Revision included an optional alternative method of classifying diagnostic statements, including information about both an underlying general disease and a manifestation in a particular organ or site. This system became known as the 'dagger and asterisk system' and is retained in the Tenth Revision. A number of other technical innovations were included in the Ninth Revision, aimed at increasing its flexibility for use in a variety of situations.\"\nIt was eventually replaced by ICD-10, the version currently in use by the WHO and most countries. Given the widespread expansion in the tenth revision, it is not possible to convert ICD-9 data sets directly into ICD-10 data sets, although some tools are available to help guide users.\nPublication of ICD-9 without IP restrictions in a world with evolving electronic data systems led to a range of products based on ICD-9, such as MeDRA or the Read directory.\n\nInternational Classification of Procedures in Medicine (ICPM)\nWhen ICD-9 was published by the World Health Organization (WHO), the International Classification of Procedures in Medicine (ICPM) was also developed (1975) and published (1978). The ICPM surgical procedures fascicle was originally created by the United States, based on its adaptations of ICD (called ICDA), which had contained a procedure classification since 1962. ICPM is published separately from the ICD disease classification as a series of supplementary documents called fascicles (bundles or groups of items). Each fascicle contains a classification of modes of laboratory, radiology, surgery, therapy, and other diagnostic procedures. Many countries have adapted and translated the ICPM in parts or as a whole and are using it with amendments since then.\n\nICD-9-CM (United States)\nThe International Classification of Diseases, Clinical Modification (ICD-9-CM) was an adaptation created by the US National Center for Health Statistics (NCHS) and used in assigning diagnostic and procedure codes associated with inpatient, outpatient, and physician office utilization in the United States. The ICD-9-CM is based on the ICD-9 but provides for additional morbidity detail. It was updated annually on October 1.\nIt consists three volumes:\n\nVolumes 1 and 2 contain diagnosis codes. (Volume 1 is a tabular listing, and volume 2 is an index.) Extended for ICD-9-CM\nVolume 3 contains procedure codes for surgical, diagnostic, and therapeutic procedures. ICD-9-CM only\nThe NCHS and the Centers for Medicare and Medicaid Services are the US governmental agencies responsible for overseeing all changes and modifications to the ICD-9-CM.\n\nICD-10\nWork on ICD-10 began in 1983, and the new revision was endorsed by the Forty-third World Health Assembly in May 1990. The latest version came into use in WHO Member States starting in 1994. The classification system allows more than 55,000 different codes and permits tracking of many new diagnoses and procedures, a significant expansion on the 17,000 codes available in ICD-9.\nAdoption was relatively swift in most of the world. Several materials are made available online by WHO to facilitate its use, including a manual, training guidelines, a browser, and files for download. Some countries have adapted the international standard, such as the \"ICD-10-AM\" published in Australia in 1998 (also used in New Zealand), and the \"ICD-10-CA\" introduced in Canada in 2000.\n\nICD-10-CM (United States)\nAdoption of ICD-10-CM was slow in the United States. Since 1979, the US had required ICD-9-CM codes for Medicare and Medicaid claims, and most of the rest of the American medical industry followed suit. On 1 January 1999 the ICD-10 (without clinical extensions) was adopted for reporting mortality, but ICD-9-CM was still used for morbidity. Meanwhile, NCHS received permission from the WHO to create a clinical modification of the ICD-10, and has production of all these systems:\n\nICD-10-CM, for diagnosis codes, replaces volumes 1 and 2. Annual updates are provided.\nICD-10-PCS, for procedure codes, replaces volume 3. Annual updates are provided.\nOn 21 August 2008, the US Department of Health and Human Services (HHS) proposed new code sets to be used for reporting diagnoses and procedures on health care transactions. Under the proposal, the ICD-9-CM code sets would be replaced with the ICD-10-CM code sets, effective 1 October 2013. On 17 April 2012 the Department of Health and Human Services (HHS) published a proposed rule that would delay, from 1 October 2013 to 1 October 2014, the compliance date for the ICD-10-CM and PCS. Once again, Congress delayed implementation date to 1 October 2015, after it was inserted into \"Doc Fix\" Bill without debate over objections of many.\nRevisions to ICD-10-CM Include:\n\nRelevant information for ambulatory and managed care encounter.\nExpanded injury codes.\nNew combination codes for diagnosis\/symptoms to reduce the number of codes needed to describe a problem fully.\nAddition of sixth and seventh digit classification.\nClassification specific to laterality.\nClassification refinement for increased data granularity.\n\nICD-10-CA (Canada)\nICD-10-CA is a clinical modification of ICD-10 developed by the Canadian Institute for Health Information for morbidity classification in Canada. ICD-10-CA applies beyond acute hospital care, and includes conditions and situations that are not diseases but represent risk factors to health, such as occupational and environmental factors, lifestyle and psycho-social circumstances.\n\nICD-11\nThe eleventh revision of the International Classification of Diseases, or the ICD-11, is almost five times as big as the ICD-10. It was created following a decade of development involving over 300 specialists from 55 countries. Following an alpha version in May 2011 and a beta draft in May 2012, a stable version of the ICD-11 was released on 18 June 2018, and officially endorsed by all WHO members during the 72nd World Health Assembly on 25 May 2019.\nFor the ICD-11, the WHO decided to differentiate between the core of the system and its derived specialty versions, such as the ICD-O for oncology. As such, the collection of all ICD entities is called the Foundation Component. From this common core, subsets can be derived. The primary derivative of the Foundation is called the ICD-11 MMS, and it is this system that is commonly referred to and recognized as \"the ICD-11\". MMS stands for Mortality and Morbidity Statistics.\nICD-11 comes with an implementation package that includes transition tables from and to ICD-10, a translation tool, a coding tool, web-services, a manual, training material, and more. All tools are accessible after self-registration from the Maintenance Platform.\nThe ICD-11 officially came into effect on 1 January 2022, although the WHO admitted that \"not many countries are likely to adapt that quickly\". In the United States, the advisory body of the Secretary of Health and Human Services has given an expected release year of 2025, but if a clinical modification is determined to be needed (similar to the ICD-10-CM), this could become 2027.\n\nUsage in the United States\nIn the United States, the US Public Health Service published The International Classification of Diseases, Adapted for Indexing of Hospital Records and Operation Classification (ICDA), completed in 1962 and expanding the ICD-7 in a number of areas to more completely meet the indexing needs of hospitals. The US Public Health Service later published the Eighth Revision, International Classification of Diseases, Adapted for Use in the United States, commonly referred to as ICDA-8, for official national morbidity and mortality statistics. This was followed by the ICD, 9th Revision, Clinical Modification, known as ICD-9-CM, published by the US Department of Health and Human Services and used by hospitals and other healthcare facilities to better describe the clinical picture of the patient. The diagnosis component of ICD-9-CM is completely consistent with ICD-9 codes, and remains the data standard for reporting morbidity. National adaptations of the ICD-10 progressed to incorporate both clinical code (ICD-10-CM) and procedure code (ICD-10-PCS) with the revisions completed in 2003. In 2009, the US Centers for Medicare and Medicaid Services announced that it would begin using ICD-10 on April 1, 2010, with full compliance by all involved parties by 2013.\nHowever, the US extended the deadline twice and did not formally require transitioning to ICD-10-CM (for most clinical encounters) until October 1, 2015.\nThe years for which causes of death in the United States have been classified by each revision as follows:\n\nCause of death on United States death certificates, statistically compiled by the Centers for Disease Control and Prevention (CDC), are coded in the ICD, which does not include codes for human and system factors commonly called medical errors.\n\nMental health conditions\nThe various ICD editions include sections that classify mental and behavioural disorders. The ICD-10 Classification of Mental and Behavioural Disorders: Clinical Descriptions and Diagnostic Guidelines \u2013 also known as the \"blue book\" \u2013 is derived from Chapter V of ICD-10 and gives the diagnostic criteria for the conditions listed at each category therein. The blue book was developed separately to, but coexists with, the Diagnostic and Statistical Manual of Mental Disorders (DSM) of the American Psychiatric Association\u2014though both seek to use the same diagnostic classifications. A survey of psychiatrists in 66 countries comparing use of the ICD-10 and DSM-IV found that the former was more often used for clinical diagnosis while the latter was more valued for research. \nAs part of the development of the ICD-11, WHO established an \"International Advisory Group\" to guide what would become the chapter on \"Mental, behavioural or neurodevelopmental disorders\". The working group proposed that ICD-11 should declassify the categories within ICD-10 at \"F66 Psychological and behavioural disorders that are associated with sexual development and orientation\". The group reported to WHO that there was \"no evidence\" these classifications were clinically useful, as they do not \"contribute to health service delivery or treatment selection nor provide essential information for public health surveillance.\" Adding that; despite ICD-10 explicitly stating \"sexual orientation by itself is not to be considered a disorder\", the inclusion of such categories \"suggest that mental disorders exist that are uniquely linked to sexual orientation and gender expression.\" A position already recognised by the DSM, as well as other classification systems. \nThe ICD is actually the official system for the US, although many mental health professionals do not realize this due to the dominance of the DSM. \nA psychologist has stated: \"Serious problems with the clinical utility of both the ICD and the DSM are widely acknowledged.\"\n\nSee also\nClinical coder\nMedical classifications\nClassification of mental disorders\nClassification of Pharmaco-Therapeutic Referrals\nInternational Classification of Primary Care (ICPC)\nResearch Domain Criteria (RDoC), a framework being developed by the National Institute of Mental Health\nMedical diagnosis\nDiagnosis-related group (DRG)\nMedical terminology\nCurrent Procedural Terminology\nMedDRA (Medical Dictionary for Regulatory Activities)\nSNOMED CT\nWHO Family of International Classifications\nInternational Classification of Functioning, Disability and Health\nInternational Classification of Health Interventions\n\nReferences\nExternal links\n\nNote: Since adoption of ICD-10 CM in the US, several online tools have been mushrooming.  They all refer to that particular modification and thus are not linked here.\n\nOfficial website at World Health Organization (WHO)\nICD-10 online browser (WHO)\nICD-10 online training direct access (WHO)\nICD-10-CM (USA \u2013 modification) at Centers for Disease Control and Prevention (CDC)\nICD-11 release\nICD-11 maintenance","100":"In physiology, isobaric counterdiffusion (ICD) is the diffusion of different gases into and out of tissues while under a constant ambient pressure, after a change of gas composition, and the physiological effects of this phenomenon. The term inert gas counterdiffusion is sometimes used as a synonym, but can also be applied to situations where the ambient pressure changes. It has relevance in mixed gas diving and anesthesiology.\n\nBackground\nIsobaric counterdiffusion was first described by Graves, Idicula, Lambertsen, and Quinn in 1973 in subjects who breathed one gas mixture (in which the inert component was nitrogen or neon) while being surrounded by another (helium based).\n\nClinical relevance\nIn medicine, ICD is the diffusion of gases in different directions that can increase the pressure inside open air spaces of the body and surrounding equipment.\nAn example of this would be a patient breathing nitrous oxide in an operating room (surrounded by air). Cuffs on the endotracheal tubes must be monitored as nitrous oxide will diffuse into the air filled space causing the volume to increase. In laparoscopic surgery, nitrous oxide is avoided since the gas will diffuse into the abdominal or pelvic cavities causing an increase in internal pressure. In the case of a tympanoplasty, the skin flap will not lay down as the nitrous oxide will be diffusing into the middle ear.\n\nRelevance to diving\nIn underwater diving, ICD is the diffusion of one inert gas into body tissues while another inert gas is diffusing out. While not strictly speaking a phenomenon of decompression, it is a complication that can occur during decompression, and that can result in the formation or growth of bubbles without changes in the environmental pressure. If the gas that is diffusing into a tissue does so at a rate which exceeds the rate of the other leaving the tissue, it can raise the combined gas concentration in the tissue to a supersaturation sufficient to cause the formation or growth of bubbles, without changes in the environmental pressure, and in particular, without concurrent decompression. Two forms of this phenomenon have been described by Lambertsen:\n\nSuperficial ICD\nSuperficial ICD (also known as Steady State Isobaric Counterdiffusion) occurs when the inert gas breathed by the diver diffuses more slowly into the body than the inert gas surrounding the body.\nAn example of this would be breathing air in a heliox environment. The helium in the heliox diffuses into the skin quickly, while the nitrogen diffuses more slowly from the capillaries to the skin and out of the body. The resulting effect generates supersaturation in certain sites of the superficial tissues and the formation of inert gas bubbles. These isobaric skin lesions (urticaria) do not occur when the ambient gas is nitrogen and the breathing gas is helium.\n\nDeep tissue ICD\nDeep tissue ICD (also known as Transient Isobaric Counterdiffusion) occurs when different inert gases are breathed by the diver in sequence. The rapidly diffusing gas is transported into the tissue faster than the slower diffusing gas is transported out of the tissue.\nAn example of this was shown in the literature by Harvey in 1977 as divers switched from a nitrogen mixture to a helium mixture (diffusivity of helium is 2.65 times faster than nitrogen), they quickly developed itching followed by joint pain. Saturation divers breathing hydreliox switched to a heliox mixture and developed symptoms of decompression sickness during Hydra V. In 2003 Doolette and Mitchell described ICD as the basis for inner ear decompression sickness and suggest \"breathing-gas switches should be scheduled deep or shallow to avoid the period of maximum supersaturation resulting from decompression\". It can also happen when saturation divers breathing hydreliox switch to a heliox mixture.\nThere is another effect which can manifest as a result of the disparity in solubility between inert breathing gas diluents, which occurs in isobaric gas switches near the decompression ceiling between a low solubility gas (typically helium, and a higher solubility gas, typically nitrogen)\nAn inner ear decompression model by Doolette and Mitchell suggests that a transient increase in gas tension after a switch from helium to nitrogen in breathing gas may result from the difference in gas transfer between compartments. If the transport of nitrogen into the vascular compartment by perfusion exceeds removal of helium by perfusion, while transfer of helium into the vascular compartment by diffusion from the perilymph and endolymph exceeds the counterdiffusion of nitrogen, this may result in a temporary increase in total gas tension, as the input of nitrogen exceeds the removal of helium, which can result in bubble formation and growth. This model suggests that diffusion of gases from the middle ear across the round window is negligible. The model is not necessarily applicable to all tissue types.\n\nICD prevention\nLambertsen made suggestions to help avoid ICD while diving. If the diver is surrounded by or saturated with nitrogen, they should not breathe helium rich gases. Lambertson also proposed that gas switches that involve going from helium rich mixtures to nitrogen rich mixtures would be acceptable, but changes from nitrogen to helium should include recompression. However Doolette and Mitchell's more recent study of inner ear decompression sickness (IEDCS) now shows that the inner ear may not be well-modelled by common (e.g. B\u00fchlmann) algorithms. Doolette and Mitchell propose that a switch from a helium-rich mix to a nitrogen-rich mix, as is common in technical diving when switching from trimix to nitrox on ascent, may cause a transient supersaturation of inert gas within the inner ear and result in IEDCS. A similar hypothesis to explain the incidence of IEDCS when switching from trimix to nitrox was proposed by Steve Burton, who considered the effect of the much greater solubility of nitrogen than helium in producing transient increases in total inert gas pressure, which could lead to DCS under isobaric conditions. Recompression with oxygen is effective for relief of symptoms resulting from ICD.  However, Burton's model for IEDCS does not agree with Doolette and Mitchell's model of the inner ear.  Doolette and Mitchell model the inner ear using solubility coefficients close to that of water.  They suggest that breathing-gas switches from helium-rich to nitrogen-rich mixtures should be carefully scheduled either deep (with due consideration to nitrogen narcosis) or shallow to avoid the period of maximum supersaturation resulting from the decompression. Switches should also be made during breathing of the largest inspired oxygen partial pressure that can be safely tolerated with due consideration to oxygen toxicity.\nA similar hypothesis to explain the incidence of IEDCS when switching from trimix to nitrox was proposed by Steve Burton, who considered the effect of the much greater solubility of nitrogen than helium in producing transient increases in total inert gas pressure, which could lead to DCS under isobaric conditions.\nBurton argues that effect of switching to Nitrox from Trimix with a large increase of nitrogen fraction at constant pressure has the effect of increasing the overall gas loading within particularly the faster tissues, since the loss of helium is more than compensated by the increase in nitrogen. This could cause immediate bubble formation and growth in the fast tissues. A simple rule for avoidance of ICD when gas switching at a decompression ceiling is suggested:\n\nAny increase in gas fraction of nitrogen in the decompression gas should be limited to 1\/5 of the decrease in gas fraction of helium.\nThis rule has been found to successfully avoid ICD on hundreds of deep trimix dives.\nA decompression planning software tool called Ultimate Planner attempts to predict ICD through modeling the inner ear as either aqueous (Mitchell and Doolette's approach) or lipid tissue (Burton's approach).\n\nSee also\nDecompression sickness \u2013 Disorder caused by dissolved gases forming bubbles in tissues\nDecompression theory \u2013 Theoretical modelling of decompression physiology\nInner ear decompression sickness \u2013 Medical condition caused by inert gas bubbles forming out of solution\n\n\n== References ==","101":"Influenza, commonly known as \"the flu\" or just \"flu\", is an infectious disease caused by influenza viruses. Symptoms range from mild to severe and often include fever, runny nose, sore throat, muscle pain, headache, coughing, and fatigue. These symptoms begin one to four (typically two) days after exposure to the virus and last for about two to eight days. Diarrhea and vomiting can occur, particularly in children. Influenza may progress to pneumonia from the virus or a subsequent bacterial infection. Other complications include acute respiratory distress syndrome, meningitis, encephalitis, and worsening of pre-existing health problems such as asthma and cardiovascular disease.\nThere are four types of influenza virus: types A, B, C, and D. Aquatic birds are the primary source of influenza A virus (IAV), which is also widespread in various mammals, including humans and pigs. Influenza B virus (IBV) and influenza C virus (ICV) primarily infect humans, and influenza D virus (IDV) is found in cattle and pigs. Influenza A virus and influenza B virus circulate in humans and cause seasonal epidemics, and influenza C virus causes a mild infection, primarily in children. Influenza D virus can infect humans but is not known to cause illness. In humans, influenza viruses are primarily transmitted through respiratory droplets from coughing and sneezing. Transmission through aerosols and surfaces contaminated by the virus also occur.\nFrequent hand washing and covering one's mouth and nose when coughing and sneezing reduce transmission. Annual vaccination can help to provide protection against influenza. Influenza viruses, particularly influenza A virus, evolve quickly, so flu vaccines are updated regularly to match which influenza strains are in circulation. Vaccines provide protection against influenza A virus subtypes H1N1 and H3N2 and one or two influenza B virus subtypes. Influenza infection is diagnosed with laboratory methods such as antibody or antigen tests and a polymerase chain reaction (PCR) to identify viral nucleic acid. The disease can be treated with supportive measures and, in severe cases, with antiviral drugs such as oseltamivir. In healthy individuals, influenza is typically self-limiting and rarely fatal, but it can be deadly in high-risk groups.\nIn a typical year, five to 15 percent of the population contracts influenza. There are 3 to 5 million severe cases annually, with up to 650,000 respiratory-related deaths globally each year. Deaths most commonly occur in high-risk groups, including young children, the elderly, and people with chronic health conditions. In temperate regions, the number of influenza cases peaks during winter, whereas in the tropics, influenza can occur year-round. Since the late 1800s, pandemic outbreaks of novel influenza strains have occurred every 10 to 50 years. Five flu pandemics have occurred since 1900: the Spanish flu from 1918 to 1920, which was the most severe; the Asian flu in 1957; the Hong Kong flu in 1968; the Russian flu in 1977; and the swine flu pandemic in 2009.\n\nSigns and symptoms\nThe symptoms of influenza are similar to those of a cold, although usually more severe and less likely to include a runny nose. The time between exposure to the virus and development of symptoms (the incubation period) is one to four days, most commonly one to two days. Many infections are asymptomatic. The onset of symptoms is sudden, and initial symptoms are predominately non-specific, including fever, chills, headaches, muscle pain, malaise, loss of appetite, lack of energy, and confusion. These are usually accompanied by respiratory symptoms such as a dry cough, sore or dry throat, hoarse voice, and a stuffy or runny nose. Coughing is the most common symptom. Gastrointestinal symptoms may also occur, including nausea, vomiting, diarrhea, and gastroenteritis, especially in children. The standard influenza symptoms typically last for two to eight days. Some studies suggest influenza can cause long-lasting symptoms in a similar way to long COVID.\nSymptomatic infections are usually mild and limited to the upper respiratory tract, but progression to pneumonia is relatively common. Pneumonia may be caused by the primary viral infection or a secondary bacterial infection. Primary pneumonia is characterized by rapid progression of fever, cough, labored breathing, and low oxygen levels that cause bluish skin. It is especially common among those who have an underlying cardiovascular disease such as rheumatic heart disease. Secondary pneumonia typically has a period of improvement in symptoms for one to three weeks followed by recurrent fever, sputum production, and fluid buildup in the lungs, but can also occur just a few days after influenza symptoms appear. About a third of primary pneumonia cases are followed by secondary pneumonia, which is most frequently caused by the bacteria Streptococcus pneumoniae and Staphylococcus aureus.\n\nVirology\nTypes of virus\nInfluenza viruses comprise four species, each the sole member of its own genus. The four influenza genera comprise four of the seven genera in the family Orthomyxoviridae. They are:\n\nInfluenza A virus , genus Alphainfluenzavirus\nInfluenza B virus , genus Betainfluenzavirus\nInfluenza C virus , genus Gammainfluenzavirus\nInfluenza D virus , genus Deltainfluenzavirus\nInfluenza A virus is responsible for most cases of severe illness as well as seasonal epidemics and occasional pandemics. It infects people of all ages but tends to disproportionately cause severe illness in the elderly, the very young, and those with chronic health issues. Birds are the primary reservoir of influenza A virus, especially aquatic birds such as ducks, geese, shorebirds, and gulls, but the virus also circulates among mammals, including pigs, horses, and marine mammals.\nSubtypes of Influenza A are defined by the combination of the antigenic viral proteins haemagglutinin (H) and neuraminidase (N) in the viral envelope; for example, \"H1N1\" designates an IAV subtype that has a type-1 hemagglutinin (H) protein and a type-1 neuraminidase (N) protein. Almost all possible combinations of H (1 thru 16) and N (1 thru 11) have been isolated from wild birds. In addition H17, H18, N10 and N11 have been found in bats. The influenza A virus subtypes in circulation among humans as of 2018 are H1N1 and H3N2.\nInfluenza B virus mainly infects humans but has been identified in seals, horses, dogs, and pigs. Influenza B virus does not have subtypes like influenza A virus but has two antigenically distinct lineages, termed the B\/Victoria\/2\/1987-like and B\/Yamagata\/16\/1988-like lineages, or simply (B\/)Victoria(-like) and (B\/)Yamagata(-like). Both lineages are in circulation in humans, disproportionately affecting children. However, the B\/Yamagata lineage might have become extinct in 2020\/2021 due to COVID-19 pandemic measures. Influenza B viruses contribute to seasonal epidemics alongside influenza A viruses but have never been associated with a pandemic.\nInfluenza C virus, like influenza B virus, is primarily found in humans, though it has been detected in pigs, feral dogs, dromedary camels, cattle, and dogs. Influenza C virus infection primarily affects children and is usually asymptomatic or has mild cold-like symptoms, though more severe symptoms such as gastroenteritis and pneumonia can occur. Unlike influenza A virus and influenza B virus, influenza C virus has not been a major focus of research pertaining to antiviral drugs, vaccines, and other measures against influenza. Influenza C virus is subclassified into six genetic\/antigenic lineages.\nInfluenza D virus has been isolated from pigs and cattle, the latter being the natural reservoir. Infection has also been observed in humans, horses, dromedary camels, and small ruminants such as goats and sheep. Influenza D virus is distantly related to influenza C virus. While cattle workers have occasionally tested positive to prior influenza D virus infection, it is not known to cause disease in humans. Influenza C virus and influenza D virus experience a slower rate of antigenic evolution than influenza A virus and influenza B virus. Because of this antigenic stability, relatively few novel lineages emerge.\n\nInfluenza virus nomenclature\nEvery year, millions of influenza virus samples are analysed to monitor changes in the virus' antigenic properties, and to inform the development of vaccines.\nTo unambiguously describe a specific isolate of virus, researchers use the internationally accepted influenza virus nomenclature,i which describes, among other things, the species of animal from which the virus was isolated, and the place and year of collection. As an example \u2013 A\/chicken\/Nakorn-Patom\/Thailand\/CU-K2\/04(H5N1):\n\nA stands for the genus of influenza (A, B, C or D).\nchicken is the animal species the isolate was found in (note: human isolates lack this component term and are thus identified as human isolates by default)\nNakorn-Patom\/Thailand is the place this specific virus was isolated\nCU-K2 is the laboratory reference number that identifies it from other influenza viruses isolated at the same place and year\n04 represents the year of isolation 2004\nH5 stands for the fifth of several known types of the protein hemagglutinin.\nN1 stands for the first of several known types of the protein neuraminidase.\nThe nomenclature for influenza B, C and D, which are less variable, is simpler. Examples are B\/Santiago\/29615\/2020 and C\/Minnesota\/10\/2015.\n\nGenome and structure\nInfluenza viruses have a negative-sense, single-stranded RNA genome that is segmented. The negative sense of the genome means it can be used as a template to synthesize messenger RNA (mRNA). Influenza A virus and influenza B virus have eight genome segments that encode 10 major proteins. Influenza C virus and influenza D virus have seven genome segments that encode nine major proteins. \nThree segments encode three subunits of an RNA-dependent RNA polymerase (RdRp) complex: PB1, a transcriptase, PB2, which recognizes 5' caps, and PA (P3 for influenza C virus and influenza D virus), an endonuclease. The M1 matrix protein and M2 proton channel share a segment, as do the non-structural protein (NS1) and the nuclear export protein (NEP). For influenza A virus and influenza B virus, hemagglutinin (HA) and neuraminidase (NA) are encoded on one segment each, whereas influenza C virus and influenza D virus encode a hemagglutinin-esterase fusion (HEF) protein on one segment that merges the functions of HA and NA. The final genome segment encodes the viral nucleoprotein (NP). Influenza viruses also encode various accessory proteins, such as PB1-F2 and PA-X, that are expressed through alternative open reading frames and which are important in host defense suppression, virulence, and pathogenicity.\nThe virus particle, called a virion, is pleomorphic and varies between being filamentous, bacilliform, or spherical in shape. Clinical isolates tend to be pleomorphic, whereas strains adapted to laboratory growth typically produce spherical virions. Filamentous virions are about 250 nanometers (nm) by 80 nm, bacilliform 120\u2013250 by 95 nm, and spherical 120 nm in diameter. \nThe core of the virion comprises one copy of each segment of the genome bound to NP nucleoproteins in separate ribonucleoprotein (RNP) complexes for each segment. There is a copy of the RdRp, all subunits included, bound to each RNP. The genetic material is encapsulated by a layer of M1 matrix protein which provides structural reinforcement to the outer layer, the viral envelope. The envelope comprises a lipid bilayer membrane incorporating HA and NA (or HEF) proteins extending outward from its exterior surface. HA and HEF proteins have a distinct \"head\" and \"stalk\" structure. M2 proteins form proton channels through the viral envelope that are required for viral entry and exit. Influenza B viruses contain a surface protein named NB that is anchored in the envelope, but its function is unknown.\n\nLife cycle\nThe viral life cycle begins by binding to a target cell. Binding is mediated by the viral HA proteins on the surface of the envelope, which bind to cells that contain sialic acid receptors on the surface of the cell membrane. For N1 subtypes with the \"G147R\" mutation and N2 subtypes, the NA protein can initiate entry. Prior to binding, NA proteins promote access to target cells by degrading mucus, which helps to remove extracellular decoy receptors that would impede access to target cells. After binding, the virus is internalized into the cell by an endosome that contains the virion inside it. The endosome is acidified by cellular vATPase to have lower pH, which triggers a conformational change in HA that allows fusion of the viral envelope with the endosomal membrane. At the same time, hydrogen ions diffuse into the virion through M2 ion channels, disrupting internal protein-protein interactions to release RNPs into the host cell's cytosol. The M1 protein shell surrounding RNPs is degraded, fully uncoating RNPs in the cytosol.\nRNPs are then imported into the nucleus with the help of viral localization signals. There, the viral RNA polymerase transcribes mRNA using the genomic negative-sense strand as a template. The polymerase snatches 5' caps for viral mRNA from cellular RNA to prime mRNA synthesis and the 3'-end of mRNA is polyadenylated at the end of transcription. Once viral mRNA is transcribed, it is exported out of the nucleus and translated by host ribosomes in a cap-dependent manner to synthesize viral proteins. RdRp also synthesizes complementary positive-sense strands of the viral genome in a complementary RNP complex which are then used as templates by viral polymerases to synthesize copies of the negative-sense genome. During these processes, RdRps of avian influenza viruses (AIVs) function optimally at a higher temperature than mammalian influenza viruses.\nNewly synthesized viral polymerase subunits and NP proteins are imported to the nucleus to further increase the rate of viral replication and form RNPs. HA, NA, and M2 proteins are trafficked with the aid of M1 and NEP proteins to the cell membrane through the Golgi apparatus and inserted into the cell's membrane. Viral non-structural proteins including NS1, PB1-F2, and PA-X regulate host cellular processes to disable antiviral responses. PB1-F2 also interacts with PB1 to keep polymerases in the nucleus longer. M1 and NEP proteins localize to the nucleus during the later stages of infection, bind to viral RNPs and mediate their export to the cytoplasm where they migrate to the cell membrane with the aid of recycled endosomes and are bundled into the segments of the genome.\nProgeny viruses leave the cell by budding from the cell membrane, which is initiated by the accumulation of M1 proteins at the cytoplasmic side of the membrane. The viral genome is incorporated inside a viral envelope derived from portions of the cell membrane that have HA, NA, and M2 proteins. At the end of budding, HA proteins remain attached to cellular sialic acid until they are cleaved by the sialidase activity of NA proteins. The virion is then released from the cell. The sialidase activity of NA also cleaves any sialic acid residues from the viral surface, which helps prevent newly assembled viruses from aggregating near the cell surface and improving infectivity. Similar to other aspects of influenza replication, optimal NA activity is temperature- and pH-dependent. Ultimately, presence of large quantities of viral RNA in the cell triggers apoptosis (programmed cell death), which is initiated by cellular factors to restrict viral replication.\n\nAntigenic drift and shift\nTwo key processes that influenza viruses evolve through are antigenic drift and antigenic shift. Antigenic drift is when an influenza virus' antigens change due to the gradual accumulation of mutations in the antigen's (HA or NA) gene. This can occur in response to evolutionary pressure exerted by the host immune response. Antigenic drift is especially common for the HA protein, in which just a few amino acid changes in the head region can constitute antigenic drift. The result is the production of novel strains that can evade pre-existing antibody-mediated immunity. Antigenic drift occurs in all influenza species but is slower in B than A and slowest in C and D. Antigenic drift is a major cause of seasonal influenza, and requires that flu vaccines be updated annually. HA is the main component of inactivated vaccines, so surveillance monitors antigenic drift of this antigen among circulating strains. Antigenic evolution of influenza viruses of humans appears to be faster than in swine and equines. In wild birds, within-subtype antigenic variation appears to be limited but has been observed in poultry.\nAntigenic shift is a sudden, drastic change in an influenza virus' antigen, usually HA. During antigenic shift, antigenically different strains that infect the same cell can reassort genome segments with each other, producing hybrid progeny. Since all influenza viruses have segmented genomes, all are capable of reassortment. Antigenic shift only occurs among influenza viruses of the same genus and most commonly occurs among influenza A viruses. In particular, reassortment is very common in AIVs, creating a large diversity of influenza viruses in birds, but is uncommon in human, equine, and canine lineages. Pigs, bats, and quails have receptors for both mammalian and avian influenza A viruses, so they are potential \"mixing vessels\" for reassortment. If an animal strain reassorts with a human strain, then a novel strain can emerge that is capable of human-to-human transmission. This has caused pandemics, but only a limited number, so it is difficult to predict when the next will happen.\nThe Global Influenza Surveillance and Response System of the World Health Organization (GISRS) tests several millions of specimens annually to monitor the spread and evolution of influenza viruses.\n\nMechanism\nTransmission\nPeople who are infected can transmit influenza viruses through breathing, talking, coughing, and sneezing, which spread respiratory droplets and aerosols that contain virus particles into the air. A person susceptible to infection can contract influenza by coming into contact with these particles. Respiratory droplets are relatively large and travel less than two meters before falling onto nearby surfaces. Aerosols are smaller and remain suspended in the air longer, so they take longer to settle and can travel further. Inhalation of aerosols can lead to infection, but most transmission is in the area about two meters around an infected person via respiratory droplets that come into contact with mucosa of the upper respiratory tract. Transmission through contact with a person, bodily fluids, or intermediate objects (fomites) can also occur, since influenza viruses can survive for hours on non-porous surfaces. If one's hands are contaminated, then touching one's face can cause infection.\nInfluenza is usually transmissible from one day before the onset of symptoms to 5\u20137 days after. In healthy adults, the virus is shed for up to 3\u20135 days. In children and the immunocompromised, the virus may be transmissible for several weeks. Children ages 2\u201317 are considered to be the primary and most efficient spreaders of influenza. Children who have not had multiple prior exposures to influenza viruses shed the virus at greater quantities and for a longer duration than other children. People at risk of exposure to influenza include health care workers, social care workers, and those who live with or care for people vulnerable to influenza. In long-term care facilities, the flu can spread rapidly. A variety of factors likely encourage influenza transmission, including lower temperature, lower absolute and relative humidity, less ultraviolet radiation from the sun, and crowding. Influenza viruses that infect the upper respiratory tract like H1N1 tend to be more mild but more transmissible, whereas those that infect the lower respiratory tract like H5N1 tend to cause more severe illness but are less contagious.\n\nPathophysiology\nIn humans, influenza viruses first cause infection by infecting epithelial cells in the respiratory tract. Illness during infection is primarily the result of lung inflammation and compromise caused by epithelial cell infection and death, combined with inflammation caused by the immune system's response to infection. Non-respiratory organs can become involved, but the mechanisms by which influenza is involved in these cases are unknown. Severe respiratory illness can be caused by multiple, non-exclusive mechanisms, including obstruction of the airways, loss of alveolar structure, loss of lung epithelial integrity due to epithelial cell infection and death, and degradation of the extracellular matrix that maintains lung structure. In particular, alveolar cell infection appears to drive severe symptoms since this results in impaired gas exchange and enables viruses to infect endothelial cells, which produce large quantities of pro-inflammatory cytokines.\nPneumonia caused by influenza viruses is characterized by high levels of viral replication in the lower respiratory tract, accompanied by a strong pro-inflammatory response called a cytokine storm. Infection with H5N1 or H7N9 especially produces high levels of pro-inflammatory cytokines. In bacterial infections, early depletion of macrophages during influenza creates a favorable environment in the lungs for bacterial growth since these white blood cells are important in responding to bacterial infection. Host mechanisms to encourage tissue repair may inadvertently allow bacterial infection. Infection also induces production of systemic glucocorticoids that can reduce inflammation to preserve tissue integrity but allow increased bacterial growth.\nThe pathophysiology of influenza is significantly influenced by which receptors influenza viruses bind to during entry into cells. Mammalian influenza viruses preferentially bind to sialic acids connected to the rest of the oligosaccharide by an \u03b1-2,6 link, most commonly found in various respiratory cells, such as respiratory and retinal epithelial cells. AIVs prefer sialic acids with an \u03b1-2,3 linkage, which are most common in birds in gastrointestinal epithelial cells and in humans in the lower respiratory tract. Cleavage of the HA protein into HA1, the binding subunit, and HA2, the fusion subunit, is performed by different proteases, affecting which cells can be infected. For mammalian influenza viruses and low pathogenic AIVs, cleavage is extracellular, which limits infection to cells that have the appropriate proteases, whereas for highly pathogenic AIVs, cleavage is intracellular and performed by ubiquitous proteases, which allows for infection of a greater variety of cells, thereby contributing to more severe disease.\n\nImmunology\nCells possess sensors to detect viral RNA, which can then induce interferon production. Interferons mediate expression of antiviral proteins and proteins that recruit immune cells to the infection site, and they notify nearby uninfected cells of infection. Some infected cells release pro-inflammatory cytokines that recruit immune cells to the site of infection. Immune cells control viral infection by killing infected cells and phagocytizing viral particles and apoptotic cells. An exacerbated immune response can harm the host organism through a cytokine storm. To counter the immune response, influenza viruses encode various non-structural proteins, including NS1, NEP, PB1-F2, and PA-X, that are involved in curtailing the host immune response by suppressing interferon production and host gene expression.\nB cells, a type of white blood cell, produce antibodies that bind to influenza antigens HA and NA (or HEF) and other proteins to a lesser degree. Once bound to these proteins, antibodies block virions from binding to cellular receptors, neutralizing the virus. In humans, a sizeable antibody response occurs about one week after viral exposure. This antibody response is typically robust and long-lasting, especially for influenza C virus and influenza D virus. People exposed to a certain strain in childhood still possess antibodies to that strain at a reasonable level later in life, which can provide some protection to related strains. There is, however, an \"original antigenic sin\", in which the first HA subtype a person is exposed to influences the antibody-based immune response to future infections and vaccines.\n\nPrevention\nVaccination\nAnnual vaccination is the primary and most effective way to prevent influenza and influenza-associated complications, especially for high-risk groups. Vaccines against the flu are trivalent or quadrivalent, providing protection against an H1N1 strain, an H3N2 strain, and one or two influenza B virus strains corresponding to the two influenza B virus lineages. Two types of vaccines are in use: inactivated vaccines that contain \"killed\" (i.e. inactivated) viruses and live attenuated influenza vaccines (LAIVs) that contain weakened viruses. There are three types of inactivated vaccines: whole virus, split virus, in which the virus is disrupted by a detergent, and subunit, which only contains the viral antigens HA and NA. Most flu vaccines are inactivated and administered via intramuscular injection. LAIVs are sprayed into the nasal cavity.\nVaccination recommendations vary by country. Some recommend vaccination for all people above a certain age, such as 6 months, whereas other countries limit recommendations to high-risk groups. Young infants cannot receive flu vaccines for safety reasons, but they can inherit passive immunity from their mother if vaccinated during pregnancy. Influenza vaccination helps to reduce the probability of reassortment.\n\nIn general, influenza vaccines are only effective if there is an antigenic match between vaccine strains and circulating strains. Most commercially available flu vaccines are manufactured by propagation of influenza viruses in embryonated chicken eggs, taking 6\u20138 months. Flu seasons are different in the northern and southern hemisphere, so the WHO meets twice a year, once for each hemisphere, to discuss which strains should be included based on observation from HA inhibition assays. Other manufacturing methods include an MDCK cell culture-based inactivated vaccine and a recombinant subunit vaccine manufactured from baculovirus overexpression in insect cells.\n\nAntiviral chemoprophylaxis\nInfluenza can be prevented or reduced in severity by post-exposure prophylaxis with the antiviral drugs oseltamivir, which can be taken orally by those at least three months old, and zanamivir, which can be inhaled by those above seven years. Chemoprophylaxis is most useful for individuals at high risk for complications and those who cannot receive the flu vaccine. Post-exposure chemoprophylaxis is only recommended if oseltamivir is taken within 48 hours of contact with a confirmed or suspected case and zanamivir within 36 hours. It is recommended for people who have yet to receive a vaccine for the current flu season, who have been vaccinated less than two week since contact, if there is a significant mismatch between vaccine and circulating strains, or during an outbreak in a closed setting regardless of vaccination history.\n\nInfection control\nHand hygiene is important in reducing the spread of influenza. This includes frequent hand washing with soap and water, using alcohol-based hand sanitizers, and not touching one's eyes, nose, and mouth with one's hands. Covering one's nose and mouth when coughing or sneezing is important. Other methods to limit influenza transmission include staying home when sick, avoiding contact with others until one day after symptoms end, and disinfecting surfaces likely to be contaminated by the virus.\nResearch thus far has not shown a significant reduction in seasonal influenza with mask usage. The effectiveness of screening at points of entry into countries is not well researched. Social distancing measures such as school closures, isolation or quarantine, and limiting mass gatherings may reduce transmission, but these measures are often expensive, unpopular, and difficult to implement. Consequently, the commonly recommended methods of infection control are respiratory etiquette, hand hygiene, and mask wearing, which are inexpensive and easy. Pharmaceutical measures are effective but may not be available in the early stages of an outbreak.\nIn health care settings, infected individuals may be cohorted or assigned to individual rooms. Protective clothing such as masks, gloves, and gowns is recommended when coming into contact with infected individuals if there is a risk of exposure to infected bodily fluids. Keeping patients in negative pressure rooms and avoiding aerosol-producing activities may help, but special air handling and ventilation systems are not considered necessary to prevent the spread of influenza in the air. In residential homes, new admissions may need to be closed until the spread of influenza is controlled.\nSince influenza viruses circulate in animals such as birds and pigs, prevention of transmission from these animals is important. Water treatment, indoor raising of animals, quarantining sick animals, vaccination, and biosecurity are the primary measures used. Placing poultry houses and piggeries on high ground away from high-density farms, backyard farms, live poultry markets, and bodies of water helps to minimize contact with wild birds. Closure of live poultry markets appears to the most effective measure and has shown to be effective at controlling the spread of H5N1, H7N9, and H9N2. Other biosecurity measures include cleaning and disinfecting facilities and vehicles, banning visits to poultry farms, not bringing birds intended for slaughter back to farms, changing clothes, disinfecting foot baths, and treating food and water.\nIf live poultry markets are not closed, then \"clean days\" when unsold poultry is removed and facilities are disinfected and \"no carry-over\" policies to eliminate infectious material before new poultry arrive can be used to reduce the spread of influenza viruses. If a novel influenza viruses has breached the aforementioned biosecurity measures, then rapid detection to stamp it out via quarantining, decontamination, and culling may be necessary to prevent the virus from becoming endemic. Vaccines exist for avian H5, H7, and H9 subtypes that are used in some countries. In China, for example, vaccination of domestic birds against H7N9 successfully limited its spread, indicating that vaccination may be an effective strategy if used in combination with other measures to limit transmission. In pigs and horses, management of influenza is dependent on vaccination with biosecurity.\n\nDiagnosis\nDiagnosis based on symptoms is fairly accurate in otherwise healthy people during seasonal epidemics and should be suspected in cases of pneumonia, acute respiratory distress syndrome (ARDS), sepsis, or if encephalitis, myocarditis, or breakdown of muscle tissue occur. Because influenza is similar to other viral respiratory tract illnesses, laboratory diagnosis is necessary for confirmation. Common sample collection methods for testing include nasal and throat swabs. Samples may be taken from the lower respiratory tract if infection has cleared the upper but not lower respiratory tract. Influenza testing is recommended for anyone hospitalized with symptoms resembling influenza during flu season or who is connected to an influenza case. For severe cases, earlier diagnosis improves patient outcome. Diagnostic methods that can identify influenza include viral cultures, antibody- and antigen-detecting tests, and nucleic acid-based tests.\nViruses can be grown in a culture of mammalian cells or embryonated eggs for 3\u201310 days to monitor cytopathic effect. Final confirmation can then be done via antibody staining, hemadsorption using red blood cells, or immunofluorescence microscopy. Shell vial cultures, which can identify infection via immunostaining before a cytopathic effect appears, are more sensitive than traditional cultures with results in 1\u20133 days. Cultures can be used to characterize novel viruses, observe sensitivity to antiviral drugs, and monitor antigenic drift, but they are relatively slow and require specialized skills and equipment.\nSerological assays can be used to detect an antibody response to influenza after natural infection or vaccination. Common serological assays include hemagglutination inhibition assays that detect HA-specific antibodies, virus neutralization assays that check whether antibodies have neutralized the virus, and enzyme-linked immunoabsorbant assays. These methods tend to be relatively inexpensive and fast but are less reliable than nucleic-acid based tests.\nDirect fluorescent or immunofluorescent antibody (DFA\/IFA) tests involve staining respiratory epithelial cells in samples with fluorescently-labeled influenza-specific antibodies, followed by examination under a fluorescent microscope. They can differentiate between influenza A virus and influenza B virus but can not subtype influenza A virus. Rapid influenza diagnostic tests (RIDTs) are a simple way of obtaining assay results, are low cost, and produce results in less than 30 minutes, so they are commonly used, but they can not distinguish between influenza A virus and influenza B virus or between influenza A virus subtypes and are not as sensitive as nucleic-acid based tests.\nNucleic acid-based tests (NATs) amplify and detect viral nucleic acid. Most of these tests take a few hours, but rapid molecular assays are as fast as RIDTs. Among NATs, reverse transcription polymerase chain reaction (RT-PCR) is the most traditional and considered the gold standard for diagnosing influenza because it is fast and can subtype influenza A virus, but it is relatively expensive and more prone to false-positives than cultures. Other NATs that have been used include loop-mediated isothermal amplification-based assays, simple amplification-based assays, and nucleic acid sequence-based amplification. Nucleic acid sequencing methods can identify infection by obtaining the nucleic acid sequence of viral samples to identify the virus and antiviral drug resistance. The traditional method is Sanger sequencing, but it has been largely replaced by next-generation methods that have greater sequencing speed and throughput.\n\nManagement\nTreatment in cases of mild or moderate illness is supportive and includes anti-fever medications such as acetaminophen and ibuprofen, adequate fluid intake to avoid dehydration, and rest. Cough drops and throat sprays may be beneficial for sore throat. It is recommended to avoid alcohol and tobacco use while ill. Aspirin is not recommended to treat influenza in children due to an elevated risk of developing Reye syndrome. Corticosteroids are not recommended except when treating septic shock or an underlying medical condition, such as chronic obstructive pulmonary disease or asthma exacerbation, since they are associated with increased mortality. If a secondary bacterial infection occurs, then antibiotics may be necessary.\n\nAntivirals\nAntiviral drugs are primarily used to treat severely ill patients, especially those with compromised immune systems. Antivirals are most effective when started in the first 48 hours after symptoms appear. Later administration may still be beneficial for those who have underlying immune defects, those with more severe symptoms, or those who have a higher risk of developing complications if these individuals are still shedding the virus. Antiviral treatment is also recommended if a person is hospitalized with suspected influenza instead of waiting for test results to return and if symptoms are worsening. Most antiviral drugs against influenza fall into two categories: neuraminidase (NA) inhibitors and M2 inhibitors. Baloxavir marboxil is a notable exception, which targets the endonuclease activity of the viral RNA polymerase and can be used as an alternative to NA and M2 inhibitors for influenza A virus and influenza B virus.\nNA inhibitors target the enzymatic activity of NA receptors, mimicking the binding of sialic acid in the active site of NA on influenza A virus and influenza B virus virions so that viral release from infected cells and the rate of viral replication are impaired. NA inhibitors include oseltamivir, which is consumed orally in a prodrug form and converted to its active form in the liver, and zanamivir, which is a powder that is inhaled nasally. Oseltamivir and zanamivir are effective for prophylaxis and post-exposure prophylaxis, and research overall indicates that NA inhibitors are effective at reducing rates of complications, hospitalization, and mortality and the duration of illness. Additionally, the earlier NA inhibitors are provided, the better the outcome, though late administration can still be beneficial in severe cases. Other NA inhibitors include laninamivir and peramivir, the latter of which can be used as an alternative to oseltamivir for people who cannot tolerate or absorb it.\nThe adamantanes amantadine and rimantadine are orally administered drugs that block the influenza virus' M2 ion channel, preventing viral uncoating. These drugs are only functional against influenza A virus but are no longer recommended for use because of widespread resistance to them among influenza A viruses. Adamantane resistance first emerged in H3N2 in 2003, becoming worldwide by 2008. Oseltamivir resistance is no longer widespread because the 2009 pandemic H1N1 strain (H1N1 pdm09), which is resistant to adamantanes, seemingly replaced resistant strains in circulation. Since the 2009 pandemic, oseltamivir resistance has mainly been observed in patients undergoing therapy, especially the immunocompromised and young children. Oseltamivir resistance is usually reported in H1N1, but has been reported in H3N2 and influenza B viruss less commonly. Because of this, oseltamivir is recommended as the first drug of choice for immunocompetent people, whereas for the immunocompromised, oseltamivir is recommended against H3N2 and influenza B virus and zanamivir against H1N1 pdm09. Zanamivir resistance is observed less frequently, and resistance to peramivir and baloxavir marboxil is possible.\n\nPrognosis\nIn healthy individuals, influenza infection is usually self-limiting and rarely fatal. Symptoms usually last for 2\u20138 days. Influenza can cause people to miss work or school, and it is associated with decreased job performance and, in older adults, reduced independence. Fatigue and malaise may last for several weeks after recovery, and healthy adults may experience pulmonary abnormalities that can take several weeks to resolve. Complications and mortality primarily occur in high-risk populations and those who are hospitalized. Severe disease and mortality are usually attributable to pneumonia from the primary viral infection or a secondary bacterial infection, which can progress to ARDS.\nOther respiratory complications that may occur include sinusitis, bronchitis, bronchiolitis, excess fluid buildup in the lungs, and exacerbation of chronic bronchitis and asthma. Middle ear infection and croup may occur, most commonly in children. Secondary S. aureus infection has been observed, primarily in children, to cause toxic shock syndrome after influenza, with hypotension, fever, and reddening and peeling of the skin. Complications affecting the cardiovascular system are rare and include pericarditis, fulminant myocarditis with a fast, slow, or irregular heartbeat, and exacerbation of pre-existing cardiovascular disease. Inflammation or swelling of muscles accompanied by muscle tissue breaking down occurs rarely, usually in children, which presents as extreme tenderness and muscle pain in the legs and a reluctance to walk for 2\u20133 days.\nInfluenza can affect pregnancy, including causing smaller neonatal size, increased risk of premature birth, and an increased risk of child death shortly before or after birth. Neurological complications have been associated with influenza on rare occasions, including aseptic meningitis, encephalitis, disseminated encephalomyelitis, transverse myelitis, and Guillain\u2013Barr\u00e9 syndrome. Additionally, febrile seizures and Reye syndrome can occur, most commonly in children. Influenza-associated encephalopathy can occur directly from central nervous system infection from the presence of the virus in blood and presents as sudden onset of fever with convulsions, followed by rapid progression to coma. An atypical form of encephalitis called encephalitis lethargica, characterized by headache, drowsiness, and coma, may rarely occur sometime after infection. In survivors of influenza-associated encephalopathy, neurological defects may occur. Primarily in children, in severe cases the immune system may rarely dramatically overproduce white blood cells that release cytokines, causing severe inflammation.\nPeople who are at least 65 years of age, due to a weakened immune system from aging or a chronic illness, are a high-risk group for developing complications, as are children less than one year of age and children who have not been previously exposed to influenza viruses multiple times. Pregnant women are at an elevated risk, which increases by trimester and lasts up to two weeks after childbirth. Obesity, in particular a body mass index greater than 35\u201340, is associated with greater amounts of viral replication, increased severity of secondary bacterial infection, and reduced vaccination efficacy. People who have underlying health conditions are also considered at-risk, including those who have congenital or chronic heart problems or lung (e.g. asthma), kidney, liver, blood, neurological, or metabolic (e.g. diabetes) disorders, as are people who are immunocompromised from chemotherapy, asplenia, prolonged steroid treatment, splenic dysfunction, or HIV infection. Tobacco use, including past use, places a person at risk. The role of genetics in influenza is not well researched, but it may be a factor in influenza mortality.\n\nEpidemiology\nInfluenza is typically characterized by seasonal epidemics and sporadic pandemics. Most of the burden of influenza is a result of flu seasons caused by influenza A virus and influenza B virus. Among influenza A virus subtypes, H1N1 and H3N2 circulate in humans and are responsible for seasonal influenza. Cases disproportionately occur in children, but most severe causes are among the elderly, the very young, and the immunocompromised. In a typical year, influenza viruses infect 5\u201315% of the global population, causing 3\u20135 million cases of severe illness annually and accounting for 290,000\u2013650,000 deaths each year due to respiratory illness. 5\u201310% of adults and 20\u201330% of children contract influenza each year. The reported number of influenza cases is usually much lower than the actual number.\nDuring seasonal epidemics, it is estimated that about 80% of otherwise healthy people who have a cough or sore throat have the flu. Approximately 30\u201340% of people hospitalized for influenza develop pneumonia, and about 5% of all severe pneumonia cases in hospitals are due to influenza, which is also the most common cause of ARDS in adults. In children, influenza and respiratory syncytial virus are the two most common causes of ARDS. About 3\u20135% of children each year develop otitis media due to influenza. Adults who develop organ failure from influenza and children who have PIM scores and acute renal failure have higher rates of mortality. During seasonal influenza, mortality is concentrated in the very young and the elderly, whereas during flu pandemics, young adults are often affected at a high rate.\n\nIn temperate regions, the number of influenza cases varies from season to season. Lower vitamin D levels, presumably due to less sunlight, lower humidity, lower temperature, and minor changes in virus proteins caused by antigenic drift contribute to annual epidemics that peak during the winter season. In the northern hemisphere, this is from October to May (more narrowly December to April), and in the southern hemisphere, this is from May to October (more narrowly June to September). There are therefore two distinct influenza seasons every year in temperate regions, one in the northern hemisphere and one in the southern hemisphere. In tropical and subtropical regions, seasonality is more complex and appears to be affected by various climatic factors such as minimum temperature, hours of sunshine, maximum rainfall, and high humidity. Influenza may therefore occur year-round in these regions. Influenza epidemics in modern times have the tendency to start in the eastern or southern hemisphere, with Asia being a key reservoir.\nInfluenza A virus and influenza B virus co-circulate, so have the same patterns of transmission. The seasonality of influenza C virus, however, is poorly understood. Influenza C virus infection is most common in children under the age of two, and by adulthood most people have been exposed to it. Influenza C virus-associated hospitalization most commonly occurs in children under the age of three and is frequently accompanied by co-infection with another virus or a bacterium, which may increase the severity of disease. When considering all hospitalizations for respiratory illness among young children, influenza C virus appears to account for only a small percentage of such cases. Large outbreaks of influenza C virus infection can occur, so incidence varies significantly.\nOutbreaks of influenza caused by novel influenza viruses are common. Depending on the level of pre-existing immunity in the population, novel influenza viruses can spread rapidly and cause pandemics with millions of deaths. These pandemics, in contrast to seasonal influenza, are caused by antigenic shifts involving animal influenza viruses. To date, all known flu pandemics have been caused by influenza A viruses, and they follow the same pattern of spreading from an origin point to the rest of the world over the course of multiple waves in a year. Pandemic strains tend to be associated with higher rates of pneumonia in otherwise healthy individuals. Generally after each influenza pandemic, the pandemic strain continues to circulate as the cause of seasonal influenza, replacing prior strains. From 1700 to 1889, influenza pandemics occurred about once every 50\u201360 years. Since then, pandemics have occurred about once every 10\u201350 years, so they may be getting more frequent over time.\n\nHistory\nThe first influenza epidemic may have occurred around 6,000 BC in China, and possible descriptions of influenza exist in Greek writings from the 5th century BC. In both 1173\u20131174 AD and 1387 AD, epidemics occurred across Europe that were named \"influenza\". Whether these epidemics or others were caused by influenza is unclear since there was then no consistent naming pattern for epidemic respiratory diseases, and \"influenza\" did not become clearly associated with respiratory disease until centuries later. Influenza may have been brought to the Americas as early as 1493, when an epidemic disease resembling influenza killed most of the population of the Antilles.\nThe first convincing record of an influenza pandemic was in 1510. It began in East Asia before spreading to North Africa and then Europe. Following the pandemic, seasonal influenza occurred, with subsequent pandemics in 1557 and 1580. The flu pandemic in 1557 was potentially the first time influenza was connected to miscarriage and death of pregnant women. The 1580 influenza pandemic originated in Asia during summer, spread to Africa, then Europe, and finally America. By the end of the 16th century, influenza was beginning to become understood as a specific, recognizable disease with epidemic and endemic forms. In 1648, it was discovered that horses also experience influenza.\nInfluenza data after 1700 is more accurate, so it is easier to identify flu pandemics after this point. The first flu pandemic of the 18th century started in 1729 in Russia in spring, spreading worldwide over the course of three years with distinct waves, the later ones being more lethal. Another flu pandemic occurred in 1781\u20131782, starting in China in autumn. From this pandemic, influenza became associated with sudden outbreaks of febrile illness. The next flu pandemic was from 1830 to 1833, beginning in China in winter. This pandemic had a high attack rate, but the mortality rate was low.\nA minor influenza pandemic occurred from 1847 to 1851 at the same time as the third cholera pandemic and was the first flu pandemic to occur with vital statistics being recorded, so influenza mortality was clearly recorded for the first time. Fowl plague (now recognised as highly pathogenic avian influenza) was recognized in 1878 and was soon linked to transmission to humans. By the time of the 1889 pandemic, which may have been caused by an H2N2 strain, the flu had become an easily recognizable disease.\nThe microbial agent responsible for influenza was incorrectly identified in 1892 by R. F. J. Pfeiffer as the bacteria species Haemophilus influenzae, which retains \"influenza\" in its name. From 1901 to 1903, Italian and Austrian researchers were able to show that avian influenza, then called \"fowl plague\", was caused by a microscopic agent smaller than bacteria by using filters with pores too small for bacteria to pass through. The fundamental differences between viruses and bacteria, however, were not yet fully understood.\n\nFrom 1918 to 1920, the Spanish flu pandemic became the most devastating influenza pandemic and one of the deadliest pandemics in history. The pandemic, caused by an H1N1 strain of influenza A, likely began in the United States before spreading worldwide via soldiers during and after the First World War. The initial wave in the first half of 1918 was relatively minor and resembled past flu pandemics, but the second wave later that year had a much higher mortality rate. A third wave with lower mortality occurred in many places a few months after the second. By the end of 1920, it is estimated that about a third to half of all people in the world had been infected, with tens of millions of deaths, disproportionately young adults. During the 1918 pandemic, the respiratory route of transmission was clearly identified and influenza was shown to be caused by a \"filter passer\", not a bacterium, but there remained a lack of agreement about influenza's cause for another decade and research on influenza declined. After the pandemic, H1N1 circulated in humans in seasonal form until the next pandemic.\nIn 1931, Richard Shope published three papers identifying a virus as the cause of swine influenza, a then newly recognized disease among pigs that was characterized during the second wave of the 1918 pandemic. Shope's research reinvigorated research on human influenza, and many advances in virology, serology, immunology, experimental animal models, vaccinology, and immunotherapy have since arisen from influenza research. Just two years after influenza viruses were discovered, in 1933, influenza A virus was identified as the agent responsible for human influenza. Subtypes of influenza A virus were discovered throughout the 1930s, and influenza B virus was discovered in 1940.\nDuring the Second World War, the US government worked on developing inactivated vaccines for influenza, resulting in the first influenza vaccine being licensed in 1945 in the United States. Influenza C virus was discovered two years later in 1947. In 1955, avian influenza was confirmed to be caused by influenza A virus. Four influenza pandemics have occurred since WWII. The first of these was the Asian flu from 1957 to 1958, caused by an H2N2 strain and beginning in China's Yunnan province. The number of deaths probably exceeded one million, mostly among the very young and very old. This was the first flu pandemic to occur in the presence of a global surveillance system and laboratories able to study the novel influenza virus. After the pandemic, H2N2 was the influenza A virus subtype responsible for seasonal influenza. The first antiviral drug against influenza, amantadine, was approved in 1966, with additional antiviral drugs being used since the 1990s.\nIn 1968, H3N2 was introduced into humans through a rearrangement between an avian H3N2 strain and an H2N2 strain that was circulating in humans. The novel H3N2 strain emerged in Hong Kong and spread worldwide, causing the Hong Kong flu pandemic, which resulted in 500,000\u20132,000,000 deaths. This was the first pandemic to spread significantly by air travel. H2N2 and H3N2 co-circulated after the pandemic until 1971 when H2N2 waned in prevalence and was completely replaced by H3N2. In 1977, H1N1 reemerged in humans, possibly after it was released from a freezer in a laboratory accident, and caused a pseudo-pandemic. This H1N1 strain was antigenically similar to the H1N1 strains that circulated prior to 1957. Since 1977, both H1N1 and H3N2 have circulated in humans as part of seasonal influenza. In 1980, the classification system used to subtype influenza viruses was introduced.\n\nAt some point, influenza B virus diverged into two strains, named the B\/Victoria-like and B\/Yamagata-like lineages, both of which have been circulating in humans since 1983.\nIn 1996, a highly pathogenic H5N1 subtype of influenza A was detected in geese in Guangdong, China and a year later emerged in poultry in Hong Kong, gradually spreading worldwide from there. A small H5N1 outbreak in humans in Hong Kong occurred then, and sporadic human cases have occurred since 1997, carrying a high case fatality rate.\nThe most recent flu pandemic was the 2009 swine flu pandemic, which originated in Mexico and resulted in hundreds of thousands of deaths. It was caused by a novel H1N1 strain that was a reassortment of human, swine, and avian influenza viruses. The 2009 pandemic had the effect of replacing prior H1N1 strains in circulation with the novel strain but not any other influenza viruses. Consequently, H1N1, H3N2, and both influenza B virus lineages have been in circulation in seasonal form since the 2009 pandemic.\nIn 2011, influenza D virus was discovered in pigs in Oklahoma, USA, and cattle were later identified as the primary reservoir of influenza D virus.\nIn the same year, avian H7N9 was detected in China and began to cause human infections in 2013, starting in Shanghai and Anhui and remaining mostly in China. Highly pathogenic H7N9 emerged sometime in 2016 and has occasionally infected humans incidentally. Other avian influenza viruses have less commonly infected humans since the 1990s, including H5N1, H5N5, H5N6, H5N8, H6N1, H7N2, H7N7, and H10N7, and have begun to spread throughout much of the world since the 2010s. Future flu pandemics, which may be caused by an influenza virus of avian origin, are viewed as almost inevitable, and increased globalization has made it easier for a pandemic virus to spread, so there are continual efforts to prepare for future pandemics and improve the prevention and treatment of influenza.\n\nEtymology\nThe word influenza comes from the Italian word influenza, from medieval Latin influentia, originally meaning 'visitation' or 'influence'. Terms such as influenza di freddo, meaning 'influence of the cold', and influenza di stelle, meaning 'influence of the stars' are attested from the 14th century. The latter referred to the disease's cause, which at the time was ascribed by some to unfavorable astrological conditions. As early as 1504, influenza began to mean a 'visitation' or 'outbreak' of any disease affecting many people in a single place at once. During an outbreak of influenza in 1743 that started in Italy and spread throughout Europe, the word reached the English language and was anglicized in pronunciation. Since the mid-1800s, influenza has also been used to refer to severe colds. The shortened form of the word, \"flu\", is first attested in 1839 as flue with the spelling flu confirmed in 1893. Other names that have been used for influenza include epidemic catarrh, la grippe from French, sweating sickness, and, especially when referring to the 1918 pandemic strain, Spanish fever.\n\nIn animals\nBirds\nAquatic birds such as ducks, geese, shorebirds, and gulls are the primary reservoir of influenza A viruses (IAVs).\nBecause of the impact of avian influenza on economically important chicken farms, a classification system was devised in 1981 which divided avian virus strains as either highly pathogenic (and therefore potentially requiring vigorous control measures) or low pathogenic. The test for this is based solely on the effect on chickens \u2013 a virus strain is highly pathogenic avian influenza (HPAI) if 75% or more of chickens die after being deliberately infected with it. The alternative classification is low pathogenic avian influenza (LPAI) which produces mild or no symptoms. This classification system has since been modified to take into account the structure of the virus' haemagglutinin protein. At the genetic level, an AIV can be identified as an HPAI virus if it has a multibasic cleavage site in the HA protein, which contains additional residues in the HA gene. Other species of birds, especially water birds, can become infected with HPAI virus without experiencing severe symptoms and can spread the infection over large distances; the exact symptoms depend on the species of bird and the strain of virus. Classification of an avian virus strain as HPAI or LPAI does not predict how serious the disease might be if it infects humans or other mammals.\nSymptoms of HPAI infection in chickens include lack of energy and appetite, decreased egg production, soft-shelled or misshapen eggs, swelling of the head, comb, wattles, and hocks, purple discoloration of wattles, combs, and legs, nasal discharge, coughing, sneezing, incoordination, and diarrhea; birds infected with an HPAI virus may also die suddenly without any signs of infection. Notable HPAI viruses include influenza A (H5N1) and A (H7N9). HPAI viruses have been a major disease burden in the 21st century, resulting in the death of large numbers of birds. In H7N9's case, some circulating strains were originally low pathogenic but became high pathogenic by mutating to acquire the HA multibasic cleavage site. Avian H9N2 is also of concern because although it is low pathogenic, it is a common donor of genes to H5N1 and H7N9 during reassortment.\nMigratory birds can spread influenza across long distances. An example of this was when an H5N1 strain in 2005 infected birds at Qinghai Lake, China, which is a stopover and breeding site for many migratory birds, subsequently spreading the virus to more than 20 countries across Asia, Europe, and the Middle East. AIVs can be transmitted from wild birds to domestic free-range ducks and in turn to poultry through contaminated water, aerosols, and fomites. Ducks therefore act as key intermediates between wild and domestic birds. Transmission to poultry typically occurs in backyard farming and live animal markets where multiple species interact with each other. From there, AIVs can spread to poultry farms in the absence of adequate biosecurity. Among poultry, HPAI transmission occurs through aerosols and contaminated feces, cages, feed, and dead animals. Back-transmission of HPAI viruses from poultry to wild birds has occurred and is implicated in mass die-offs and intercontinental spread.\nAIVs have occasionally infected humans through aerosols, fomites, and contaminated water. Direction transmission from wild birds is rare. Instead, most transmission involves domestic poultry, mainly chickens, ducks, and geese but also a variety of other birds such as guinea fowl, partridge, pheasants, and quails. The primary risk factor for infection with AIVs is exposure to birds in farms and live poultry markets. Typically, infection with an AIV has an incubation period of 3\u20135 days but can be up to 9 days. H5N1 and H7N9 cause severe lower respiratory tract illness, whereas other AIVs such as H9N2 cause a more mild upper respiratory tract illness, commonly with conjunctivitis. Limited transmission of avian H2, H5-7, H9, and H10 subtypes from one person to another through respiratory droplets, aerosols, and fomites has occurred, but sustained human-to-human transmission of AIVs has not occurred.\n\nPigs\nInfluenza in pigs is a respiratory disease similar to influenza in humans and is found worldwide. Asymptomatic infections are common. Symptoms typically appear 1\u20133 days after infection and include fever, lethargy, anorexia, weight loss, labored breathing, coughing, sneezing, and nasal discharge. In sows, pregnancy may be aborted. Complications include secondary infections and potentially fatal bronchopneumonia. Pigs become contagious within a day of infection and typically spread the virus for 7\u201310 days, which can spread rapidly within a herd. Pigs usually recover within 3\u20137 days after symptoms appear. Prevention and control measures include inactivated vaccines and culling infected herds. Influenza A virus subtypes H1N1, H1N2, and H3N2 are usually responsible for swine flu.\nSome influenza A viruses can be transmitted via aerosols from pigs to humans and vice versa. Pigs, along with bats and quails, are recognized as a mixing vessel of influenza viruses because they have both \u03b1-2,3 and \u03b1-2,6 sialic acid receptors in their respiratory tract. Because of that, both avian and mammalian influenza viruses can infect pigs. If co-infection occurs, reassortment is possible. A notable example of this was the reassortment of a swine, avian, and human influenza virus that caused the 2009 flu pandemic. Spillover events from humans to pigs appear to be more common than from pigs to humans.\n\nOther animals\nInfluenza viruses have been found in many other animals, including cattle, horses, dogs, cats, and marine mammals. Nearly all influenza A viruses are apparently descended from ancestral viruses in birds. The exception are bat influenza-like viruses, which have an uncertain origin. These bat viruses have HA and NA subtypes H17, H18, N10, and N11. H17N10 and H18N11 are unable to reassort with other influenza A viruses, but they are still able to replicate in other mammals.\nEquine influenza A viruses include H7N7 and two lineages of H3N8. H7N7, however, has not been detected in horses since the late 1970s, so it may have become extinct in horses. H3N8 in equines spreads via aerosols and causes respiratory illness. Equine H3N8 preferentially binds to \u03b1-2,3 sialic acids, so horses are usually considered dead-end hosts, but transmission to dogs and camels has occurred, raising concerns that horses may be mixing vessels for reassortment. In canines, the only influenza A viruses in circulation are equine-derived H3N8 and avian-derived H3N2. Canine H3N8 has not been observed to reassort with other subtypes. H3N2 has a much broader host range and can reassort with H1N1 and H5N1. An isolated case of H6N1, likely from a chicken, was found infecting a dog, so other AIVs may emerge in canines.\nA wide range of other mammals have been affected by avian influenza A viruses, generally due to eating birds which had been infected. There have been instances where transmission of the disease between mammals, including seals and cows, may have occurred. Various mutations have been identified that are associated with AIVs adapting to mammals. Since HA proteins vary in which sialic acids they bind to, mutations in the HA receptor binding site can allow AIVs to infect mammals. Other mutations include mutations affecting which sialic acids NA proteins cleave and a mutation in the PB2 polymerase subunit that improves tolerance of lower temperatures in mammalian respiratory tracts and enhances RNP assembly by stabilizing NP and PB2 binding.\nInfluenza B virus is mainly found in humans but has also been detected in pigs, dogs, horses, and seals. Likewise, influenza C virus primarily infects humans but has been observed in pigs, dogs, cattle, and dromedary camels. Influenza D virus causes an influenza-like illness in pigs but its impact in its natural reservoir, cattle, is relatively unknown. It may cause respiratory disease resembling human influenza on its own, or it may be part of a bovine respiratory disease (BRD) complex with other pathogens during co-infection. BRD is a concern for the cattle industry, so influenza D virus' possible involvement in BRD has led to research on vaccines for cattle that can provide protection against influenza D virus. Two antigenic lineages are in circulation: D\/swine\/Oklahoma\/1334\/2011 (D\/OK) and D\/bovine\/Oklahoma\/660\/2013 (D\/660).\n\nReferences\nFurther reading\n\nBrown J (2018). Influenza: The Hundred Year Hunt to Cure the Deadliest Disease in History. New York: Atria. ISBN 978-1501181245.\nSt Mouritz AA (1921). The Flu: A Brief History of Influenza in U.S. America, Europe, Hawaii. Honolulu, Hawaii, U.S. America: Advertiser Publishing Co.","102":"Joanna Marguerite Wardlaw  (born 4 November 1958) is a Scottish physician, radiologist, and academic specialising in neuroradiology and pathophysiology. Wardlaw worked as a junior doctor before specialising as a radiologist. She continues to practice medicine as an Honorary Consultant Neuroradiologist with NHS Lothian. She has spent her entire academic career at the University of Edinburgh.\n\nEarly life and education\nWardlaw was born on 4 November 1958 in London, England. She was educated at Park School, an all-girls school in Glasgow, Scotland. She read medicine at the University of Edinburgh, taking a first class BSc in 1979, and Bachelor of Medicine, Bachelor of Surgery (MBChB) in 1982. In 1994, she completed a Doctor of Medicine (MD). Her doctoral thesis concerned the pathophysiology and treatment of ischaemic stroke, and was titled \"Imaging and treatment of acute ischaemic stroke: the application and verification of non-invasive imaging techniques in the investigation and treatment of acute ischaemic stroke\".\n\nCareer and research\nHaving worked as a junior doctor, Wardlaw specialised as a radiologist. In 1986 she became a Member of the Royal Colleges of Physicians of the United Kingdom (MRCP), and in 1988 a Fellow of both the Royal College of Physicians (FRCP) and the Royal College of Radiologist (FRCR). From 1992 to 1994 she worked as a consultant neuroradiologist at the Institute of Neurological Sciences in Glasgow (now part of Queen Elizabeth University Hospital). Since 1994 she has been an honorary consultant neuroradiologist with NHS Lothian.\nFrom 1994 to 1998, Wardlaw was a MRC senior lecturer at the University of Edinburgh. In 1997 or 1998, she established the Brain Imaging Research Centre at the university, now grouped with the Clinical Research Imaging Centre into Edinburgh Imaging and continues to serve as its director. She was a Reader from 1998 to 2001. She has been Head of the Division of Neuroimaging since 2001. She was appointed to a personal chair as Professor of Applied Neuroimaging in 2002. She was the founding director of the Scottish Imaging Network: A Platform for Scientific Excellence (SINAPSE), leading the organisation until 2010.\nWardlaw is recognised as an expert in brain blood vessel diseases and neuroimaging. Her current research is focused on the prevention, diagnosis, and treatment of strokes, particularly cerebral small vessel diseases. She is also interested in the use of imaging in pathophysiology.\n\nAwards and honours\nIn 2005, Wardlaw was elected a Fellow of the Academy of Medical Sciences (FMedSci). In 2011 she was elected a Fellow of the Royal Society of Edinburgh (FRSE), Scotland's national academy of science and letters. She was made a Fellow of the American Heart Association in 2014. In the 2016 New Year Honours, she was appointed a Commander of the Order of the British Empire (CBE) \"for services to neuroimaging and clinical science\".\nIn 2008, Wardlaw was awarded the President's Medal of the British Society of Neuroradiologists. In May 2017, she was awarded the Presidential Award of the European Stroke Organisation. In 2018, she received both the Karolinska Stroke Award for Lifetime Contribution to Excellence in Advancing Knowledge in Stroke and the American Stroke Associations' William M. Feinberg Award for Excellence in Clinical Stroke.\n\nSelected works\nHankey, Graeme J.; Wardlaw, Joanna M. (2002). Clinical neurology. London: Manson. ISBN 978-1840760101.\nWarlow, Charles P.; van Gijn, Jan; Dennis, Martin S.; Wardlaw, Joanna M.; Bamford, John M.; Hankey, Graeme J.; Sandercock, Peter A. G.; Rinkel, Gabriel; Langhorne, Peter; Sudlow, Cathie; Rothwell, Peter (2008). Stroke: practical management (3rd ed.). Malden, MA: Blackwell Publishing. ISBN 978-1405127660.\nWardlaw, Joanna M.; Doubal, Fergus; Armitage, Paul; Chappell, Francesca; Carpenter, Trevor; Mu\u00f1oz Maniega, Susana; Farrall, Andrew; Sudlow, Cathie; Dennis, Martin; Dhillon, Baljean (February 2009). \"Lacunar stroke is associated with diffuse blood-brain barrier dysfunction\". Annals of Neurology. 65 (2): 194\u2013202. doi:10.1002\/ana.21549. PMID 19260033. S2CID 25226289.\nSandercock, PAG; Wardlaw, JM; Lindley, RI (June 2012). \"The benefits and harms of intravenous thrombolysis with recombinant tissue plasminogen activator within 6 h of acute ischaemic stroke (the third international stroke trial [IST-3]): a randomised controlled trial\". The Lancet. 379 (9834): 2352\u20132363. doi:10.1016\/S0140-6736(12)60768-5. PMC 3386495. PMID 22632908.\nWardlaw, Joanna M; Murray, Veronica; Berge, Eivind; del Zoppo, Gregory; Sandercock, Peter; Lindley, Richard L; Cohen, Geoff (June 2012). \"Recombinant tissue plasminogen activator for acute ischaemic stroke: an updated systematic review and meta-analysis\". The Lancet. 379 (9834): 2364\u20132372. doi:10.1016\/S0140-6736(12)60738-7. PMC 3386494. PMID 22632907.\nWardlaw, Joanna M; Smith, Colin; Dichgans, Martin (May 2013). \"Mechanisms of sporadic cerebral small vessel disease: insights from neuroimaging\". The Lancet Neurology. 12 (5): 483\u2013497. doi:10.1016\/S1474-4422(13)70060-7. PMC 3836247. PMID 23602162.\nWardlaw, Joanna M; Smith, Eric E; Biessels, Geert J; Cordonnier, Charlotte; Fazekas, Franz; Frayne, Richard; Lindley, Richard I; O'Brien, John T; Barkhof, Frederik; Benavente, Oscar R; Black, Sandra E; Brayne, Carol; Breteler, Monique; Chabriat, Hugues; DeCarli, Charles; de Leeuw, Frank-Erik; Doubal, Fergus; Duering, Marco; Fox, Nick C; Greenberg, Steven; Hachinski, Vladimir; Kilimann, Ingo; Mok, Vincent; Oostenbrugge, Robert van; Pantoni, Leonardo; Speck, Oliver; Stephan, Blossom C M; Teipel, Stefan; Viswanathan, Anand; Werring, David; Chen, Christopher; Smith, Colin; van Buchem, Mark; Norrving, Bo; Gorelick, Philip B; Dichgans, Martin (August 2013). \"Neuroimaging standards for research into small vessel disease and its contribution to ageing and neurodegeneration\". The Lancet Neurology. 12 (8): 822\u2013838. doi:10.1016\/S1474-4422(13)70124-8. PMC 3714437. PMID 23867200.\nGorelick, Philip B.; Testai, Fernando; Hankey, Graeme; Wardlaw, Joanna M., eds. (2014). Hankey's Clinical Neurology (2nd ed.). London: Manson Publishing. ISBN 978-1840761931.\nEmberson, Jonathan; Lees, Kennedy R; Lyden, Patrick; Blackwell, Lisa; Albers, Gregory; Bluhmki, Erich; Brott, Thomas; Cohen, Geoff; Davis, Stephen; Donnan, Geoffrey; Grotta, James; Howard, George; Kaste, Markku; Koga, Masatoshi; von Kummer, Ruediger; Lansberg, Maarten; Lindley, Richard I; Murray, Gordon; Olivot, Jean Marc; Parsons, Mark; Tilley, Barbara; Toni, Danilo; Toyoda, Kazunori; Wahlgren, Nils; Wardlaw, Joanna; Whiteley, William; del Zoppo, Gregory J; Baigent, Colin; Sandercock, Peter; Hacke, Werner (November 2014). \"Effect of treatment delay, age, and stroke severity on the effects of intravenous thrombolysis with alteplase for acute ischaemic stroke: a meta-analysis of individual patient data from randomised trials\". The Lancet. 384 (9958): 1929\u20131935. doi:10.1016\/S0140-6736(14)60584-5. PMC 4441266. PMID 25106063.\n\n\n== References ==","103":"A labyrinthine fistula is an abnormal opening in the inner ear. This can result in leakage of the perilymph into the middle ear. This includes specifically a perilymph fistula (PLF), an abnormal connection between the fluid of the inner ear and the air-filled middle ear. This is caused by a rupture of the round window or oval window ligaments separating the inner and middle ear.\nAnother type of labyrinthine fistula is the superior semicircular canal dehiscence, which allows the inner ear to be influenced by the intracranial pressure directly.\n\nSigns and symptoms\nPLF usually induces one or all the following pathological states: aural fullness, fluctuating or non-fluctuating hearing loss, tinnitus and dizziness, which may sometimes include vertigo and balance disorders.\n\nCauses\nLabyrinthine fistula can be both congenital or develop over time with the thinning of the otic capsule by the persistent pulsations of the intracranial pressures against the bones of the skull. Finally, medical conditions (e.g. cholesteatoma) can result in a labyrinthine fistula. Traumatic events, with excessive pressure changes to the inner ear such as in scuba diving, head trauma, or an extremely loud noise can lead to rupture and leakage.\nThe most common causes of PLF are: head or ear traumas, rapid increases of intracranial pressure, congenital abnormalities (in children), complication of stapedectomy, barotraumas (e.g. slap\/suction, scuba diving, skydiving, strong and repetitive nose-blowing or sneezing, heavy lifting).\n\nDiagnosis\nWhen diagnosing, PLF should be differentiated from M\u00e9ni\u00e8re's disease. Tympanostomy has been reported to be a way to diagnose and cure PLF.\n\nTreatment\nPatients are advised to treat with bed rest and avoiding activities that increase intracranial pressure (i.e. weightlifting, Valsalva maneuver, scuba diving, flying in airplanes) with the hopes of the membrane healing on its own. Appropriate Physical therapy \/ vestibular rehabilitation techniques can be helpful in managing symptoms of movement sensitivity.\n\nReferences\nExternal links\n\nhttp:\/\/www.dizziness-and-balance.com\/disorders\/unilat\/fistula.html","104":"The bony labyrinth (also osseous labyrinth or otic capsule) is the rigid, bony outer wall of the inner ear in the temporal bone. It consists of three parts: the vestibule, semicircular canals, and cochlea. These are cavities hollowed out of the substance of the bone, and lined by periosteum. They contain a clear fluid, the perilymph, in which the membranous labyrinth is situated.\nA fracture classification system in which temporal bone fractures detected by computed tomography are delineated based on disruption of the otic capsule has been found to be predictive for complications of temporal bone trauma such as facial nerve injury, sensorineural deafness and cerebrospinal fluid otorrhea. On radiographic images, the otic capsule is the densest portion of the temporal bone.\nIn otospongiosis, a leading cause of adult-onset hearing loss, the otic capsule is exclusively affected. This area normally undergoes no remodeling in adult life and is extremely dense. With otospongiosis, the normally dense enchondral bone is replaced by Haversian bone, a spongy and vascular matrix that results in sensorineural hearing loss due to compromise of the conductive capacity of the inner ear ossicles. This results in hypodensity on CT, with the portion first affected usually being the fissula ante fenestram.\nThe bony labyrinth is studied in paleoanthropology as it is a good indicator for distinguishing Neanderthals and modern humans.\n\nReferences\n\n This article incorporates text in the public domain from page 1047 of the 20th edition of Gray's Anatomy (1918)","105":"Labyrinthitis is inflammation of the labyrinth, a maze of fluid-filled channels in the inner ear. Vestibular neuritis is inflammation of the vestibular nerve (the nerve in the ear that sends messages related to motion and position to the brain). Both conditions involve inflammation of the inner ear. Labyrinths that house the vestibular system sense changes in the head's position or the head's motion. Inflammation of these inner ear parts results in a vertigo (sensation of the world spinning) and also possible hearing loss or tinnitus (ringing in the ears). It can occur as a single attack, a series of attacks, or a persistent condition that diminishes over three to six weeks. It may be associated with nausea, vomiting, and eye nystagmus.\nThe cause is often not clear. It may be due to a virus, but it can also arise from bacterial infection, head injury, extreme stress, an allergy, or as a reaction to medication. 30% of affected people had a common cold prior to developing the disease. Either bacterial or viral labyrinthitis can cause a permanent hearing loss in rare cases. This appears to result from an imbalance of neuronal input between the left and right inner ears.\n\nSigns and symptoms\nThe main symptoms are severe vertigo and nystagmus. The most common symptom for vestibular neuritis is the onset of vertigo that has formed from an ongoing infection or trauma. The dizziness sensation that is associated with vertigo is thought to be from the inner ear labyrinth. Rapid and undesired eye motion (nystagmus) often results from the improper indication of rotational motion.  Nausea, anxiety, and a general ill feeling are common due to the distorted balance signals that the brain receives from the inner ear system. Other common symptoms include tinnitus, ear ache, and a feeling of fullness in the ear.\n\nCauses\nSome people will report having an upper respiratory infection (common cold) or flu prior to the onset of the symptoms of vestibular neuritis; others will have no viral symptoms prior to the vertigo attack.  \nSome cases of vestibular neuritis are thought to be caused by an infection of the vestibular ganglion by the herpes simplex type 1 virus. However, the cause of this condition is not fully understood, and in fact, many different viruses may be capable of infecting the vestibular nerve.\nAcute localized ischemia of these structures also may be an important cause. Especially in children, vestibular neuritis may be preceded by symptoms of a common cold.  However, the causative mechanism remains uncertain.\nThis can also be brought on by pressure changes such as those experienced while flying or scuba diving.\n\nMechanism\nIn the vestibular system, there are three canals that are semicircular in shape that input sensory clues. These canals allow the brain to sense rotational motion and linear motion changes. The brain then uses the sensory input clues and the visual input clues from the vestibular system to retain balance. The vestibulo\u2013ocular reflex retains continuous visual focus during motion which is also the vestibular systems job during activity.\n\nTreatment\nThe treatment for vestibular neuritis depends on the cause. However, symptoms of vertigo can be treated in the same way as other vestibular dysfunctions with vestibular rehabilitation.\n\nPhysical therapy\nTypical treatments include combinations of head and eye movements, postural changes, and walking exercises. Specifically, exercises that may be prescribed include keeping eyes fixated on a specific target while moving the head, moving the head right to left at two targets at a significant distance apart, walking while keeping eyes fixated on a specific target, and walking while keeping eyes fixated on a specific target while also turning the head in different directions.\nThe main function behind repeating a combination of head and eye movements, postural changes and walking is that through this repetition, compensatory changes for the dysfunctions arising from peripheral vestibular structures may be promoted in the central vestibular system (brainstem and cerebellum).\nVestibular rehabilitation therapy is a highly effective way to substantially reduce or eliminate residual dizziness from labyrinthitis. VRT works by causing the brain to use already existing neural mechanisms for adaptation, neuroplasticity, and compensation. Vestibular neuritis rehabilitation is an effective and safe management to improve symptoms. The vestibular neuritis rehabilitation can improve symptoms or resolve the symptoms which is dependent on each individual. \nRehabilitation strategies most commonly used are:\n\nGaze stability exercises \u2013 moving the head from side to side while fixated on a stationary object (aimed at assisting the eye to fixate during head rotation without the input from the lost canal vestibulo\u2013ocular reflex).  An advanced progression of this exercise would be walking in a straight line while looking side to side by turning the head.\nHabituation exercises \u2013 movements designed to provoke symptoms and subsequently reduce the negative vestibular response upon repetition. Examples of these include Brandt\u2013Daroff exercises.\nFunctional retraining \u2013 including postural control, relaxation, and balance training.\nThese exercises function by challenging the vestibular system.  Progression occurs by increasing the amplitude of the head or focal point movements, increasing the speed of movement, and combining movements such as walking and head turning.  \nOne study found that patients who believed their illness was out of their control showed the slowest progression to full recovery, long after the initial vestibular injury had healed. The study revealed that the patient who compensated well was one who, at the psychological level, was not afraid of the symptoms and had some positive control over them. Notably, a reduction in negative beliefs over time was greater in those patients treated with rehabilitation than in those untreated. \"Of utmost importance, baseline beliefs were the only significant predictor of change in a handicap at 6 months follow-up.\"\n\nMedication\nVestibular neuritis is generally a self-limiting disease. Treatment with drugs is neither necessary nor possible. The effect of glucocorticoids has been studied, but they have not been found to significantly affect long-term outcome.\nSymptomatic treatment with antihistaminics such as cinnarizine, however, can be used to suppress the symptoms of vestibular neuritis while it spontaneously regresses. Prochlorperazine is another commonly prescribed medication to help alleviate the symptoms of vertigo and nausea.\n\nMental disorders\nBecause mood disorders can hamper recovery from labyrinthitis, treatment may also include any co-occurring anxiety disorder or depression. Severe anxiety episodes are usually addressed by short-term benzodiazepine therapy.\n\nPrognosis\nRecovery from acute labyrinthine inflammation generally takes from one to six weeks, but it is not uncommon for residual symptoms such as dysequilibrium and dizziness to last for a couple of months.\nRecovery from a temporarily damaged inner ear typically follows two phases:\n\nAn acute period, which may include severe vertigo and vomiting\napproximately two weeks of sub-acute symptoms and rapid recovery\n\nEpidemiology\nLabyrinthitis affects approximately 35 million people per year (approximately 3.5 cases per 100,000 people). It typically occurs in those between 30 and 60 years of age, and there are no significant differences between male and female incidence rates. In 95% of cases, sufferers experience a single attack and fully recover. Vestibular rehabilitation showed a statistically significant increase in controlling symptoms over no intervention in people who have vestibular neuritis.\n\nReferences\nExternal links\n\nLabyrinthitis at Curlie","106":"Lateral medullary syndrome is a neurological disorder causing a range of symptoms due to ischemia in the lateral part of the medulla oblongata in the brainstem. The ischemia is a result of a blockage most commonly in the vertebral artery or the posterior inferior cerebellar artery. Lateral medullary syndrome is also called Wallenberg's syndrome, posterior inferior cerebellar artery (PICA) syndrome and vertebral artery syndrome.\n\nSigns and symptoms\nThis syndrome is characterized by sensory deficits that affect the trunk and extremities contralaterally (opposite to the lesion), and sensory deficits of the face and cranial nerves ipsilaterally (same side as the lesion). Specifically a loss of pain and temperature sensation if the lateral spinothalamic tract is involved. The cross body finding is a highly suggestive symptom from which the diagnosis may be considered (however, this a symptom common to all brainstem pathology).\nPatients often have difficulty walking or maintaining balance (ataxia), or difference in temperature of an object based on which side of the body the object of varying temperature is touching. Some patients may walk with a slant or experience skew deviation and illusions of room tilt. The nystagmus is commonly associated with vertigo spells. These vertigo spells can result in falling, caused from the involvement of the region of Deiters' nucleus.\nCommon symptoms with lateral medullary syndrome may include difficulty swallowing, or dysphagia. This can be caused by the involvement of the nucleus ambiguus, as it supplies the vagus and glossopharyngeal nerves. Slurred speech (dysarthria), and disordered vocal quality (dysphonia) are also common.   The damage to the cerebellum or the inferior cerebellar peduncle can cause ataxia. Damage to the hypothalamospinal fibers disrupts sympathetic nervous system relay and gives symptoms that are similar to the symptoms caused by Horner's syndrome \u2013 such as miosis, anhidrosis and partial ptosis.\nPalatal myoclonus, the twitching of the muscles of the mouth, may be observed due to disruption of the central tegmental tract. Other symptoms include: hoarseness, nausea, vomiting, a decrease in sweating, problems with body temperature sensation, dizziness, difficulty walking, and difficulty maintaining balance. Lateral medullary syndrome can also cause bradycardia, a slow heart rate, and increases or decreases in the patients average blood pressure.\n\nBased on location\nCause\nIt is the clinical manifestation resulting from occlusion of the posterior inferior cerebellar artery (PICA) or one of its branches or of the vertebral artery, in which the lateral part of the medulla oblongata infarcts, resulting in a typical pattern. The most commonly affected artery is PICA, specifically the lateral medullary segment.\n\nDiagnosis\nSince lateral medullary syndrome is often caused by a stroke, diagnosis is time dependent. Diagnosis is usually done by assessing vestibular-related symptoms in order to determine where in the medulla that the infarction has occurred. Head Impulsive Nystagmus Test of Skew (HINTS) examination of oculomotor function is often performed, along with computed tomography (CT) or magnetic resonance imaging (MRI) to assist in stroke detection. Standard stroke assessment must be done to rule out a concussion or other head trauma.\n\nTreatment\nTreatment for lateral medullary syndrome is dependent on how quickly it is identified. Treatment for lateral medullary syndrome involves focusing on relief of symptoms and active rehabilitation to help patients return to their daily activities. Many patients undergo speech therapy. Depressed mood and withdrawal from society can be seen in patients following the initial onslaught of symptoms.\nIn more severe cases, a feeding tube may need to be inserted through the mouth or a gastrostomy may be necessary if swallowing is impaired. In some cases, medication may be used to reduce or eliminate residual pain. Some studies have reported success in mitigating the chronic neuropathic pain associated with the syndrome with anti-epileptics such as gabapentin. Long-term treatment generally involves the use of antiplatelets like aspirin or clopidogrel and statin regimen for the rest of their lives in order to minimize the risk of another stroke. Warfarin is used if atrial fibrillation is present. Other medications may be necessary in order to suppress high blood pressure and risk factors associated with strokes. A blood thinner may be prescribed to a patient in order to break up the infarction and reestablish blood flow and to try to prevent future infarctions.\nOne of the most unusual and difficult to treat symptoms that occur due to Wallenberg syndrome are interminable, violent hiccups.  The hiccups can be so severe that patients often struggle to eat, sleep and carry on conversations.  Depending on the severity of the blockage caused by the stroke, the hiccups can last for weeks.  Unfortunately there are very few successful medications available to mediate the inconvenience of constant hiccups.\nFor dysphagia symptoms, repetitive transcranial magnetic stimulation has been shown to assist in rehabilitation.  Overall, traditional stroke assessment and outcomes are used to treat patients, since lateral medullary syndrome is often caused by a stroke in the lateral medulla.\n\nPrognosis\nThe outlook for someone with lateral medullary syndrome depends upon the size and location of the area of the brain stem damaged by the stroke.  Some individuals may see a decrease in their symptoms within weeks or months.  Others may be left with significant neurological disabilities for years after the initial symptoms appeared. However, more than 85% of patients have seen minimal symptoms present at six months from the time of the original stroke, and have been able to independently accomplish average daily within a year.\n\nEpidemiology\nThe lateral medullary syndrome is the most common form of posterior ischemic stroke syndrome. It is estimated that there are around 600,000 new cases of this syndrome in the United States alone. Those at the overall highest risk for lateral medullary syndrome are men at an average age of 55.06. Having a history of hypertension, diabetes and smoking all increase the risk of large artery atherosclerosis. Large artery atherosclerosis is thought to be the greatest risk factor for lateral medullary syndrome due to the deposits of cholesterol, fatty substances, cellular waste products, calcium and fibrin. Otherwise known as plaque build up in the arteries.\n\nHistory\nThe earliest description of lateral medullary syndrome was first written by Gaspard Vieusseux at the Medical and Chirurgical Society of London describing the symptoms observed at the time. Adolf Wallenberg further reinforced these signs after completing his first case report in 1895. He was able to make an accurate localization of the lesion and soon after proved it following a postmortem examination. Wallenberg accomplished three more published articles about lateral medullary syndrome.\n\nAdolf Wallenberg\nAdolf Wallenberg was a renowned neurologist and neuroanatomist most widely known for his clinical descriptions of Lateral Medullary Syndrome. He completed his doctorate at Leipzig University in 1886. By 1928 he had spent 2 years (1886-1888) as an assistant at the city hospital in Danzig, 21 years (1907-1928) as the director of internal and psychiatric departments and 18 years (1910-1928) as a titular professor. In 1929, Wallenberg received the Erb Commemorative Medal for his work in the field of anatomy, physiology and pathology of the nervous system.\nWallenberg's first patient in 1885 was a 38-year-old male with symptoms of vertigo, numbness, loss of pain and temperature sensitivity, paralysis of multiple locations, ataxia and more. His background in neuroanatomy helped him in correctly locating the patient's lesion to the lateral medulla and connected it to a blockage of the ipsilateral posterior inferior cerebral artery. After the death of his patient in 1899, he was able to prove his findings after a postmortem examination. He continued his work with many patients and by 1922 he had reported his 15th patient with clinicopathological correlations. In 1938, Adolf Wallenberg was forced to end his career as a physician by the German occupation.  When the Nazis came to power, he was stripped of his research laboratory and forced to stop working because he was Jewish.  He emigrated to Great Britain in 1938, then relocated to the United States in 1943.\n\nSee also\nAlternating hemiplegia of childhood\nBenedikt syndrome\nLateral pontine syndrome\nMedial medullary syndrome\nWeber's syndrome\n\nReferences\nExternal links\n\nMRI of Lateral Medullary Infarction (Wallenberg) MedPix Images","107":"The lateral vestibular nucleus (Deiters's nucleus) is the continuation upward and lateralward of the principal nucleus, and in it terminate many of the ascending branches of the vestibular nerve.\n\nStructure\nIt consists of very large multipolar cells whose axons form an important part of the posterior longitudinal bundle (aka medial longitudinal fasciculus) of the same and the opposite side.\nThe axons bifurcate as they enter the posterior longitudinal bundle, \n\nthe ascending branches send terminals and collaterals to the motor nuclei of the abducens, trochlear and oculomotor nerves via the ascending component of the medial longitudinal fasciculus, and are concerned in coordinating the movements of the eyes with alterations in the position of the head;\nthe descending branches pass down in the posterior longitudinal bundle into the anterior funiculus of the spinal cord as the vestibulospinal fasciculus (anterior marginal bundle) and are distributed to motor nuclei of the anterior column by terminals and collaterals.\nOther fibers are said to pass directly to the vestibulospinal fasciculus without passing into the posterior longitudinal bundle.\nThe fibers which pass into the vestibulospinal fasciculus are intimately concerned with equilibratory reflexes.\nOther axons from Deiters\u2019s nucleus are supposed to cross and ascend in the opposite medial lemniscus to the ventro-lateral nuclei of the thalamus; still other fibers pass into the cerebellum with the inferior peduncle and are distributed to the cortex of the vermis and the roof nuclei of the cerebellum; according to Cajal they merely pass through the nucleus fastigii on their way to the cortex of the vermis and the hemisphere.\n\nHistory\nEponym\nDeiter's nucleus was named after German neuroanatomist Otto Friedrich Karl Deiters (1834\u20131863).\n\nReferences\n This article incorporates text in the public domain from page 860 of the 20th edition of Gray's Anatomy (1918)","108":"The following is a list of codes for International Statistical Classification of Diseases and Related Health Problems.\n\nList of ICD-9 codes 001\u2013139: infectious and parasitic diseases\nList of ICD-9 codes 140\u2013239: neoplasms\nList of ICD-9 codes 240\u2013279: endocrine, nutritional and metabolic diseases, and immunity disorders\nList of ICD-9 codes 280\u2013289: diseases of the blood and blood-forming organs\nList of ICD-9 codes 290\u2013319: mental disorders\nList of ICD-9 codes 320\u2013389: diseases of the nervous system and sense organs\nList of ICD-9 codes 390\u2013459: diseases of the circulatory system\nList of ICD-9 codes 460\u2013519: diseases of the respiratory system\nList of ICD-9 codes 520\u2013579: diseases of the digestive system\nList of ICD-9 codes 580\u2013629: diseases of the genitourinary system\nList of ICD-9 codes 630\u2013679: complications of pregnancy, childbirth, and the puerperium\nList of ICD-9 codes 680\u2013709: diseases of the skin and subcutaneous tissue\nList of ICD-9 codes 710\u2013739: diseases of the musculoskeletal system and connective tissue\nList of ICD-9 codes 740\u2013759: congenital anomalies\nList of ICD-9 codes 760\u2013779: certain conditions originating in the perinatal period\nList of ICD-9 codes 780\u2013799: symptoms, signs, and ill-defined conditions\nList of ICD-9 codes 800\u2013999: injury and poisoning\nList of ICD-9 codes E and V codes: external causes of injury and supplemental classification\n\nSee also\nInternational Statistical Classification of Diseases and Related Health Problems: ICD-9 \u2013 provides multiple external links for looking up ICD codes\nMS Access MDB file at United States Department of Health and Human Services in the downloads section at the bottom\n\n\n== References ==","109":"A lesion is any damage or abnormal change in the tissue of an organism, usually caused by injury or diseases. Lesion is derived from the Latin laesio meaning \"injury\". Lesions may occur in plants as well as animals.\n\nTypes\nThere is no designated classification or naming convention for lesions. Since lesions can occur anywhere in the body and the definition of a lesion is so broad, the varieties of lesions are virtually endless. Generally, lesions may be classified by their patterns, their sizes, their locations, or their causes. They can also be named after the person who discovered them. For example, Ghon lesions, which are found in the lungs of those with tuberculosis, are named after the lesion's discoverer, Anton Ghon. The characteristic skin lesions of a varicella zoster virus infection are called chickenpox. Lesions of the teeth are usually called dental caries, or \"cavities\".\n\nLocation\nLesions are often classified by their tissue types or locations. For example, a \"skin lesion\" or a \"brain lesion\" are named for the tissue where they are found. If there is an added significance to regions within the tissue\u2014such as in neural injuries where different locations correspond to different neurological deficits\u2014they are further classified by location. For example, a lesion in the central nervous system is called a central lesion, and a lesion in the peripheral nervous system is called a peripheral lesion. A myocardial lesion results from damage to the heart muscle, and a coronary lesion is a subtype that describes a lesion in the coronary arteries. Coronary lesions are then further classified according to the side of the heart that is affected and the diameter of the artery in which they form.\n\nCause and behavior\nIf a lesion is caused by a tumor, it can be classified as malignant or benign after analysis of a biopsy. A benign lesion that is evolving into a malignant lesion is called \"premalignant\". Cancerous lesions are sometimes classified by their growth kinetics, such as the Lodwick classification, which characterizes classes of bone lesions. Another type is an excitotoxic lesion, which can be caused by excitatory amino acids like kainic acid that kill neurons through overstimulation.\n\nSize and shape\nLesion size may be specified as gross, meaning it is visible to the unaided eye, or histologic, meaning a microscope is needed to see it. A space-occupying lesion, as the name suggests, has a recognizable volume and may impinge on nearby structures, whereas a non space-occupying lesion is simply a hole in the tissue, e.g. a small area of the brain that has turned to fluid following a stroke.\nLesions may also be classified by the shape they form. This is the case with many ulcers, which can have a bullseye or 'target' appearance. A coin lesion as seen in an X-ray has the appearance of a coin sitting on the patient's chest.\n\nResearch using lesions\nBrain lesions may help researchers understand brain function. Research involving lesions relies on two assumptions: that brain damage can affect different aspects of cognition independently, and that a locally damaged brain functions identically to a normal brain in its \"undamaged\" parts.\nSham lesion is the name given to a control procedure during a lesion experiment. In a sham lesion, an animal may be placed in a stereotaxic apparatus and electrodes inserted as in the experimental condition, but no current is passed, and therefore damage to the tissue should be minimal.\n\nResearch with humans\nHumans with brain lesions are often the subjects of research with the goal of establishing the function of the area where their lesion occurred.\nA drawback to the use of human subjects is the difficulty in finding subjects who have a lesion to the area the researcher wishes to study. As such, transcranial magnetic stimulation is often used in cognition and neuroscience-related tests to imitate the effect.\n\nResearch with animals\nUsing animal subjects gives researchers the ability to study lesions in specific body parts of the subjects, allowing them to quickly acquire a large group of subjects. An example of such a study is the lesioning of rat hippocampi to establish the role of the hippocampus in object recognition and object recency.\n\nNotable lesions\nSee also\nAblation\n\nReferences\n\n\n== External links ==","110":"Magnetic resonance imaging (MRI) is a medical imaging technique used in radiology to form pictures of the anatomy and the physiological processes inside the body. MRI scanners use strong magnetic fields, magnetic field gradients, and radio waves to generate images of the organs in the body. MRI does not involve X-rays or the use of ionizing radiation, which distinguishes it from computed tomography (CT) and positron emission tomography (PET) scans. MRI is a medical application of nuclear magnetic resonance (NMR) which can also be used for imaging in other NMR applications, such as NMR spectroscopy.\nMRI is widely used in hospitals and clinics for medical diagnosis, staging and follow-up of disease. Compared to CT, MRI provides better contrast in images of soft tissues, e.g. in the brain or abdomen. However, it may be perceived as less comfortable by patients, due to the usually longer and louder measurements with the subject in a long, confining tube, although \"open\" MRI designs mostly relieve this. Additionally, implants and other non-removable metal in the body can pose a risk and may exclude some patients from undergoing an MRI examination safely.\nMRI was originally called NMRI (nuclear magnetic resonance imaging), but \"nuclear\" was dropped to avoid negative associations. Certain atomic nuclei are able to absorb radio frequency (RF) energy when placed in an external magnetic field; the resultant evolving spin polarization can induce an RF signal in a radio frequency coil and thereby be detected. In other words, the nuclear magnetic spin of protons in the hydrogen nuclei resonates with the RF incident waves and emit coherent radiation with compact direction, energy (frequency) and phase. This coherent amplified radiation is easily detected by RF antenas close to the subject being examined. It is a process similar to masers. In clinical and research MRI, hydrogen atoms are most often used to generate a macroscopic polarized radiation that is detected by the antennas. Hydrogen atoms are naturally abundant in humans and other biological organisms, particularly in water and fat. For this reason, most MRI scans essentially map the location of water and fat in the body. Pulses of radio waves excite the nuclear spin energy transition, and magnetic field gradients localize the polarization in space. By varying the parameters of the pulse sequence, different contrasts may be generated between tissues based on the relaxation properties of the hydrogen atoms therein.\nSince its development in the 1970s and 1980s, MRI has proven to be a versatile imaging technique. While MRI is most prominently used in diagnostic medicine and biomedical research, it also may be used to form images of non-living objects, such as mummies. Diffusion MRI and functional MRI extend the utility of MRI to capture neuronal tracts and blood flow respectively in the nervous system, in addition to detailed spatial images. The sustained increase in demand for MRI within health systems has led to concerns about cost effectiveness and overdiagnosis.\n\nMechanism\nConstruction and physics\nIn most medical applications, hydrogen nuclei, which consist solely of a proton, that are in tissues create a signal that is processed to form an image of the body in terms of the density of those nuclei in a specific region. Given that the protons are affected by fields from other atoms to which they are bonded, it is possible to separate responses from hydrogen in specific compounds. To perform a study, the person is positioned within an MRI scanner that forms a strong magnetic field around the area to be imaged. First, energy from an oscillating magnetic field is temporarily applied to the patient at the appropriate resonance frequency. Scanning with X and Y gradient coils causes a selected region of the patient to experience the exact magnetic field required for the energy to be absorbed. The atoms are excited by a RF pulse and the resultant signal is measured by a receiving coil. The RF signal may be processed to deduce position information by looking at the changes in RF level and phase caused by varying the local magnetic field using gradient coils. As these coils are rapidly switched during the excitation and response to perform a moving line scan, they create the characteristic repetitive noise of an MRI scan as the windings move slightly due to magnetostriction. The contrast between different tissues is determined by the rate at which excited atoms return to the equilibrium state. Exogenous contrast agents may be given to the person to make the image clearer.\nThe major components of an MRI scanner are the main magnet, which polarizes the sample, the shim coils for correcting shifts in the homogeneity of the main magnetic field, the gradient system which is used to localize the region to be scanned and the RF system, which excites the sample and detects the resulting NMR signal. The whole system is controlled by one or more computers.\n\nMRI requires a magnetic field that is both strong and uniform to a few parts per million across the scan volume. The field strength of the magnet is measured in teslas \u2013 and while the majority of systems operate at 1.5 T, commercial systems are available between 0.2 and 7 T. Whole-body MRI systems for research applications operate in e.g. 9.4T, 10.5T, 11.7T. Even higher field whole-body MRI systems e.g. 14 T and beyond are in conceptual proposal or in engineering design. Most clinical magnets are superconducting magnets, which require liquid helium to keep them at low temperatures. Lower field strengths can be achieved with permanent magnets, which are often used in \"open\" MRI scanners for claustrophobic patients. Lower field strengths are also used in a portable MRI scanner approved by the FDA in 2020. Recently, MRI has been demonstrated also at ultra-low fields, i.e., in the microtesla-to-millitesla range, where sufficient signal quality is made possible by prepolarization (on the order of 10\u2013100 mT) and by measuring the Larmor precession fields at about 100 microtesla with highly sensitive superconducting quantum interference devices (SQUIDs).\n\nT1 and T2\nEach tissue returns to its equilibrium state after excitation by the independent relaxation processes of T1 (spin-lattice; that is, magnetization in the same direction as the static magnetic field) and T2 (spin-spin; transverse to the static magnetic field).\nTo create a T1-weighted image, magnetization is allowed to recover before measuring the MR signal by changing the repetition time (TR). This image weighting is useful for assessing the cerebral cortex, identifying fatty tissue, characterizing focal liver lesions, and in general, obtaining morphological information, as well as for post-contrast imaging.\n\nTo create a T2-weighted image, magnetization is allowed to decay before measuring the MR signal by changing the echo time (TE). This image weighting is useful for detecting edema and inflammation, revealing white matter lesions, and assessing zonal anatomy in the prostate and uterus.\nThe information from MRI scans comes in the form of image contrasts based on differences in the rate of relaxation of nuclear spins following their perturbation by an oscillating magnetic field (in the form of radiofrequency pulses through the sample). The relaxation rates are a measure of the time it takes for a signal to decay back to an equilibrium state from either the longitudinal or transverse plane.\nMagnetization builds up along the z-axis in the presence of a magnetic field, B0, such that the magnetic dipoles in the sample will, on average, align with the z-axis summing to a total magnetization Mz. This magnetization along z is defined as the equilibrium magnetization; magnetization is defined as the sum of all magnetic dipoles in a sample. Following the equilibrium magnetization, a 90\u00b0 radiofrequency (RF) pulse flips the direction of the magnetization vector in the xy-plane, and is then switched off. The initial magnetic field B0, however, is still applied. Thus, the spin magnetization vector will slowly return from the xy-plane back to the equilibrium state. The time it takes for the magnetization vector to return to its equilibrium value, Mz, is referred to as the longitudinal relaxation time, T1. Subsequently, the rate at which this happens is simply the reciprocal of the relaxation time: \n  \n    \n      \n        \n          \n            1\n            \n              T\n              1\n            \n          \n        \n        =\n        R\n        1\n      \n    \n    {\\displaystyle {\\frac {1}{T1}}=R1}\n  \n. Similarly, the time in which it takes for Mxy to return to zero is T2, with the rate \n  \n    \n      \n        \n          \n            1\n            \n              T\n              2\n            \n          \n        \n        =\n        R\n        2\n      \n    \n    {\\displaystyle {\\frac {1}{T2}}=R2}\n  \n. Magnetization as a function of time is defined by the Bloch equations.\nT1 and T2 values are dependent on the chemical environment of the sample; hence their utility in MRI. Soft tissue and muscle tissue relax at different rates, yielding the image contrast in a typical scan.\nThe standard display of MR images is to represent fluid characteristics in black-and-white images, where different tissues turn out as follows:\n\nDiagnostics\nUsage by organ or system\nMRI has a wide range of applications in medical diagnosis and around 50,000 scanners are estimated to be in use worldwide. MRI affects diagnosis and treatment in many specialties although the effect on improved health outcomes is disputed in certain cases.\n\nMRI is the investigation of choice in the preoperative staging of rectal and prostate cancer and has a role in the diagnosis, staging, and follow-up of other tumors, as well as for determining areas of tissue for sampling in biobanking.\n\nNeuroimaging\nMRI is the investigative tool of choice for neurological cancers over CT, as it offers better visualization of the posterior cranial fossa, containing the brainstem and the cerebellum. The contrast provided between grey and white matter makes MRI the best choice for many conditions of the central nervous system, including demyelinating diseases, dementia, cerebrovascular disease, infectious diseases, Alzheimer's disease and epilepsy. Since many images are taken milliseconds apart, it shows how the brain responds to different stimuli, enabling researchers to study both the functional and structural brain abnormalities in psychological disorders. MRI also is used in guided stereotactic surgery and radiosurgery for treatment of intracranial tumors, arteriovenous malformations, and other surgically treatable conditions using a device known as the N-localizer. New tools that implement artificial intelligence in healthcare have demonstrated higher image quality and morphometric analysis in neuroimaging with the application of a denoising system.\nThe record for the highest spatial resolution of a whole intact brain (postmortem) is 100 microns, from Massachusetts General Hospital. The data was published in NATURE on 30 October 2019.\nThough MRI is used widely in research on mental disabilities, based on a 2024 systematic literature review and meta analysis commissioned by the Patient-Centered Outcomes Research Institute (PCORI), available research using MRI scans to diagnose ADHD showed great variability. The authors conclude that MRI cannot be reliably used to assist in making a clinical diagnosis of ADHD.\n\nCardiovascular\nCardiac MRI is complementary to other imaging techniques, such as echocardiography, cardiac CT, and nuclear medicine. It can be used to assess the structure and the function of the heart. Its applications include assessment of myocardial ischemia and viability, cardiomyopathies, myocarditis, iron overload, vascular diseases, and congenital heart disease.\n\nMusculoskeletal\nApplications in the musculoskeletal system include spinal imaging, assessment of joint disease, and soft tissue tumors. Also, MRI techniques can be used for diagnostic imaging of\nsystemic muscle diseases including genetic muscle diseases.\nSwallowing movement of throat and oesophagus can cause motion artifact over the imaged spine. Therefore, a saturation pulse applied over this region the throat and oesophagus can help to avoid this artifact. Motion artifact arising due to pumping of the heart can be reduced by timing the MRI pulse according to heart cycles. Blood vessels flow artifacts can be reduced by applying saturation pulses above and below the region of interest.\n\nLiver and gastrointestinal\nHepatobiliary MR is used to detect and characterize lesions of the liver, pancreas, and bile ducts. Focal or diffuse disorders of the liver may be evaluated using diffusion-weighted, opposed-phase imaging and dynamic contrast enhancement sequences. Extracellular contrast agents are used widely in liver MRI, and newer hepatobiliary contrast agents also provide the opportunity to perform functional biliary imaging. Anatomical imaging of the bile ducts is achieved by using a heavily T2-weighted sequence in magnetic resonance cholangiopancreatography (MRCP). Functional imaging of the pancreas is performed following administration of secretin. MR enterography provides non-invasive assessment of inflammatory bowel disease and small bowel tumors. MR-colonography may play a role in the detection of large polyps in patients at increased risk of colorectal cancer.\n\nAngiography\nMagnetic resonance angiography (MRA) generates pictures of the arteries to evaluate them for stenosis (abnormal narrowing) or aneurysms (vessel wall dilatations, at risk of rupture). MRA is often used to evaluate the arteries of the neck and brain, the thoracic and abdominal aorta, the renal arteries, and the legs (called a \"run-off\"). A variety of techniques can be used to generate the pictures, such as administration of a paramagnetic contrast agent (gadolinium) or using a technique known as \"flow-related enhancement\" (e.g., 2D and 3D time-of-flight sequences), where most of the signal on an image is due to blood that recently moved into that plane (see also FLASH MRI).\nTechniques involving phase accumulation (known as phase contrast angiography) can also be used to generate flow velocity maps easily and accurately. Magnetic resonance venography (MRV) is a similar procedure that is used to image veins. In this method, the tissue is now excited inferiorly, while the signal is gathered in the plane immediately superior to the excitation plane\u2014thus imaging the venous blood that recently moved from the excited plane.\n\nContrast agents\nMRI for imaging anatomical structures or blood flow do not require contrast agents since the varying properties of the tissues or blood provide natural contrasts. However, for more specific types of imaging, exogenous contrast agents may be given intravenously, orally, or intra-articularly. Most contrast agents are either paramagnetic (e.g.: gadolinium, manganese, europium), and are used to shorten T1 in the tissue they accumulate in, or super-paramagnetic (SPIONs), and are used to shorten T2 and T2* in healthy tissue reducing its signal intensity (negative contrast agents). The most commonly used intravenous contrast agents are based on chelates of gadolinium, which is highly paramagnetic. In general, these agents have proved safer than the iodinated contrast agents used in X-ray radiography or CT. Anaphylactoid reactions are rare, occurring in approx. 0.03\u20130.1%. Of particular interest is the lower incidence of nephrotoxicity, compared with iodinated agents, when given at usual doses\u2014this has made contrast-enhanced MRI scanning an option for patients with renal impairment, who would otherwise not be able to undergo contrast-enhanced CT.\nGadolinium-based contrast reagents are typically octadentate complexes of gadolinium(III). The complex is very stable (log K > 20) so that, in use, the concentration of the un-complexed Gd3+ ions should be below the toxicity limit. The 9th place in the metal ion's coordination sphere is occupied by a water molecule which exchanges rapidly with water molecules in the reagent molecule's immediate environment, affecting the magnetic resonance relaxation time.\nIn December 2017, the Food and Drug Administration (FDA) in the United States announced in a drug safety communication that new warnings were to be included on all gadolinium-based contrast agents (GBCAs). The FDA also called for increased patient education and requiring gadolinium contrast vendors to conduct additional animal and clinical studies to assess the safety of these agents.\nAlthough gadolinium agents have proved useful for patients with kidney impairment, in patients with severe kidney failure requiring dialysis there is a risk of a rare but serious illness, nephrogenic systemic fibrosis, which may be linked to the use of certain gadolinium-containing agents. The most frequently linked is gadodiamide, but other agents have been linked too. Although a causal link has not been definitively established, current guidelines in the United States are that dialysis patients should only receive gadolinium agents where essential and that dialysis should be performed as soon as possible after the scan to remove the agent from the body promptly.\nIn Europe, where more gadolinium-containing agents are available, a classification of agents according to potential risks has been released. In 2008, a new contrast agent named gadoxetate, brand name Eovist (US) or Primovist (EU), was approved for diagnostic use: This has the theoretical benefit of a dual excretion path.\n\nSequences\nAn MRI sequence is a particular setting of radiofrequency pulses and gradients, resulting in a particular image appearance. The T1 and T2 weighting can also be described as MRI sequences.\n\nOverview table\nThis table does not include uncommon and experimental sequences.\n\nSpecialized configurations\nMagnetic resonance spectroscopy\nMagnetic resonance spectroscopy (MRS) is used to measure the levels of different metabolites in body tissues, which can be achieved through a variety of single voxel or imaging-based techniques. The MR signal produces a spectrum of resonances that corresponds to different molecular arrangements of the isotope being \"excited\". This signature is used to diagnose certain metabolic disorders, especially those affecting the brain, and to provide information on tumor metabolism.\nMagnetic resonance spectroscopic imaging (MRSI) combines both spectroscopic and imaging methods to produce spatially localized spectra from within the sample or patient. The spatial resolution is much lower (limited by the available SNR), but the spectra in each voxel contains information about many metabolites. Because the available signal is used to encode spatial and spectral information, MRSI requires high SNR achievable only at higher field strengths (3 T and above). The high procurement and maintenance costs of MRI with extremely high field strengths inhibit their popularity. However, recent compressed sensing-based software algorithms (e.g., SAMV) have been proposed to achieve super-resolution without requiring such high field strengths.\n\nReal-time\nInterventional MRI\nThe lack of harmful effects on the patient and the operator make MRI well-suited for interventional radiology, where the images produced by an MRI scanner guide minimally invasive procedures. Such procedures use no ferromagnetic instruments.\nA specialized growing subset of interventional MRI is intraoperative MRI, in which an MRI is used in surgery. Some specialized MRI systems allow imaging concurrent with the surgical procedure. More typically, the surgical procedure is temporarily interrupted so that MRI can assess the success of the procedure or guide subsequent surgical work.\n\nMagnetic resonance guided focused ultrasound\nIn guided therapy, high-intensity focused ultrasound (HIFU) beams are focused on a tissue, that are controlled using MR thermal imaging. Due to the high energy at the focus, the temperature rises to above 65 \u00b0C (150 \u00b0F) which completely destroys the tissue. This technology can achieve precise ablation of diseased tissue. MR imaging provides a three-dimensional view of the target tissue, allowing for the precise focusing of ultrasound energy. The MR imaging provides quantitative, real-time, thermal images of the treated area. This allows the physician to ensure that the temperature generated during each cycle of ultrasound energy is sufficient to cause thermal ablation within the desired tissue and if not, to adapt the parameters to ensure effective treatment.\n\nMultinuclear imaging\nHydrogen has the most frequently imaged nucleus in MRI because it is present in biological tissues in great abundance, and because its high gyromagnetic ratio gives a strong signal. However, any nucleus with a net nuclear spin could potentially be imaged with MRI. Such nuclei include helium-3, lithium-7, carbon-13, fluorine-19, oxygen-17, sodium-23, phosphorus-31 and xenon-129. 23Na and 31P are naturally abundant in the body, so they can be imaged directly. Gaseous isotopes such as 3He or 129Xe must be hyperpolarized and then inhaled as their nuclear density is too low to yield a useful signal under normal conditions. 17O and 19F can be administered in sufficient quantities in liquid form (e.g. 17O-water) that hyperpolarization is not a necessity. Using helium or xenon has the advantage of reduced background noise, and therefore increased contrast for the image itself, because these elements are not normally present in biological tissues.\nMoreover, the nucleus of any atom that has a net nuclear spin and that is bonded to a hydrogen atom could potentially be imaged via heteronuclear magnetization transfer MRI that would image the high-gyromagnetic-ratio hydrogen nucleus instead of the low-gyromagnetic-ratio nucleus that is bonded to the hydrogen atom. In principle, heteronuclear magnetization transfer MRI could be used to detect the presence or absence of specific chemical bonds.\nMultinuclear imaging is primarily a research technique at present. However, potential applications include functional imaging and imaging of organs poorly seen on 1H MRI (e.g., lungs and bones) or as alternative contrast agents. Inhaled hyperpolarized 3He can be used to image the distribution of air spaces within the lungs. Injectable solutions containing 13C or stabilized bubbles of hyperpolarized 129Xe have been studied as contrast agents for angiography and perfusion imaging. 31P can potentially provide information on bone density and structure, as well as functional imaging of the brain. Multinuclear imaging holds the potential to chart the distribution of lithium in the human brain, this element finding use as an important drug for those with conditions such as bipolar disorder.\n\nMolecular imaging by MRI\nMRI has the advantages of having very high spatial resolution and is very adept at morphological imaging and functional imaging. MRI does have several disadvantages though. First, MRI has a sensitivity of around 10\u22123 mol\/L  to 10\u22125 mol\/L, which, compared to other types of imaging, can be very limiting. This problem stems from the fact that the population difference between the nuclear spin states is very small at room temperature. For example, at 1.5 teslas, a typical field strength for clinical MRI, the difference between high and low energy states is approximately 9 molecules per 2 million. Improvements to increase MR sensitivity include increasing magnetic field strength and hyperpolarization via optical pumping or dynamic nuclear polarization. There are also a variety of signal amplification schemes based on chemical exchange that increase sensitivity.\nTo achieve molecular imaging of disease biomarkers using MRI, targeted MRI contrast agents with high specificity and high relaxivity (sensitivity) are required. To date, many studies have been devoted to developing targeted-MRI contrast agents to achieve molecular imaging by MRI. Commonly, peptides, antibodies, or small ligands, and small protein domains, such as HER-2 affibodies, have been applied to achieve targeting. To enhance the sensitivity of the contrast agents, these targeting moieties are usually linked to high payload MRI contrast agents or MRI contrast agents with high relaxivities. A new class of gene targeting MR contrast agents has been introduced to show gene action of unique mRNA and gene transcription factor proteins. These new contrast agents can trace cells with unique mRNA, microRNA and virus; tissue response to inflammation in living brains. The MR reports change in gene expression with positive correlation to TaqMan analysis, optical and electron microscopy.\n\nParallel MRI\nIt takes time to gather MRI data using sequential applications of magnetic field gradients. Even for the most streamlined of MRI sequences, there are physical and physiologic limits to the rate of gradient switching. Parallel MRI circumvents these limits by gathering some portion of the data simultaneously, rather than in a traditional sequential fashion. This is accomplished using arrays of radiofrequency (RF) detector coils, each with a different 'view' of the body. A reduced set of gradient steps is applied, and the remaining spatial information is filled in by combining signals from various coils, based on their known spatial sensitivity patterns. The resulting acceleration is limited by the number of coils and by the signal to noise ratio (which decreases with increasing acceleration), but two- to four-fold accelerations may commonly be achieved with suitable coil array configurations, and substantially higher accelerations have been demonstrated with specialized coil arrays. Parallel MRI may be used with most MRI sequences.\nAfter a number of early suggestions for using arrays of detectors to accelerate imaging went largely unremarked in the MRI field, parallel imaging saw widespread development and application following the introduction of the SiMultaneous Acquisition of Spatial Harmonics (SMASH) technique in 1996\u20137. The SENSitivity Encoding (SENSE) and Generalized Autocalibrating Partially Parallel Acquisitions (GRAPPA) techniques are the parallel imaging methods in most common use today. The advent of parallel MRI resulted in extensive research and development in image reconstruction and RF coil design, as well as in a rapid expansion of the number of receiver channels available on commercial MR systems. Parallel MRI is now used routinely for MRI examinations in a wide range of body areas and clinical or research applications.\n\nQuantitative MRI\nMost MRI focuses on qualitative interpretation of MR data by acquiring spatial maps of relative variations in signal strength which are \"weighted\" by certain parameters. Quantitative methods instead attempt to determine spatial maps of accurate tissue relaxometry parameter values or magnetic field, or to measure the size of certain spatial features.\nExamples of quantitative MRI methods are:\n\nT1-mapping (notably used in cardiac magnetic resonance imaging)\nT2-mapping\nQuantitative susceptibility mapping (QSM)\nQuantitative fluid flow MRI (i.e. some cerebrospinal fluid flow MRI)\nMagnetic resonance elastography (MRE)\nQuantitative MRI aims to increase the reproducibility of MR images and interpretations, but has historically require longer scan times.\nQuantitative MRI (or qMRI) sometimes more specifically refers to multi-parametric quantitative MRI, the mapping of multiple tissue relaxometry parameters in a single imaging session.\nEfforts to make multi-parametric quantitative MRI faster have produced sequences which map multiple parameters simultaneously, either by building separate encoding methods for each parameter into the sequence,\nor by fitting MR signal evolution to a multi-parameter model.\n\nHyperpolarized gas MRI\nTraditional MRI generates poor images of lung tissue because there are fewer water molecules with protons that can be excited by the magnetic field. Using hyperpolarized gas an MRI scan can identify ventilation defects in the lungs. Before the scan, a patient is asked to inhale hyperpolarized xenon mixed with a buffer gas of helium or nitrogen. The resulting lung images are much higher quality than with traditional MRI.\n\nSafety\nMRI is, in general, a safe technique, although injuries may occur as a result of failed safety procedures or human error. Contraindications to MRI include most cochlear implants and cardiac pacemakers, shrapnel, and metallic foreign bodies in the eyes. Magnetic resonance imaging in pregnancy appears to be safe, at least during the second and third trimesters if done without contrast agents. Since MRI does not use any ionizing radiation, its use is generally favored in preference to CT when either modality could yield the same information. Some patients experience claustrophobia and may require sedation or shorter MRI protocols. Amplitude and rapid switching of gradient coils during image acquisition may cause peripheral nerve stimulation.\nMRI uses powerful magnets and can therefore cause magnetic materials to move at great speeds, posing a projectile risk, and may cause fatal accidents. However, as millions of MRIs are performed globally each year, fatalities are extremely rare.\nMRI machines can produce loud noise, up to 120 dB(A). This can cause hearing loss, tinnitus and hyperacusis, so appropriate hearing protection is essential for anyone inside the MRI scanner room during the examination.\n\nOveruse\nMedical societies issue guidelines for when physicians should use MRI on patients and recommend against overuse. MRI can detect health problems or confirm a diagnosis, but medical societies often recommend that MRI not be the first procedure for creating a plan to diagnose or manage a patient's complaint. A common case is to use MRI to seek a cause of low back pain; the American College of Physicians, for example, recommends against imaging (including MRI) as unlikely to result in a positive outcome for the patient.\n\nArtifacts\nAn MRI artifact is a visual artifact, that is, an anomaly during visual representation. Many different artifacts can occur during magnetic resonance imaging (MRI), some affecting the diagnostic quality, while others may be confused with pathology. Artifacts can be classified as patient-related, signal processing-dependent and hardware (machine)-related.\n\nNon-medical use\nMRI is used industrially mainly for routine analysis of chemicals. The nuclear magnetic resonance technique is also used, for example, to measure the ratio between water and fat in foods, monitoring of flow of corrosive fluids in pipes, or to study molecular structures such as catalysts.\nBeing non-invasive and non-damaging, MRI can be used to study the anatomy of plants, their water transportation processes and water balance. It is also applied to veterinary radiology for diagnostic purposes. Outside this, its use in zoology is limited due to the high cost; but it can be used on many species.\nIn palaeontology it is used to examine the structure of fossils.\nForensic imaging provides graphic documentation of an autopsy, which manual autopsy does not. CT scanning provides quick whole-body imaging of skeletal and parenchymal alterations, whereas MR imaging gives better representation of soft tissue pathology. All that being said, MRI is more expensive, and more time-consuming to utilize. Moreover, the quality of MR imaging deteriorates below 10 \u00b0C.\n\nHistory\nIn 1971 at Stony Brook University, Paul Lauterbur applied magnetic field gradients in all three dimensions and a back-projection technique to create NMR images. He published the first images of two tubes of water in 1973 in the journal Nature, followed by the picture of a living animal, a clam, and in 1974 by the image of the thoracic cavity of a mouse. Lauterbur called his imaging method zeugmatography, a term which was replaced by (N)MR imaging. In the late 1970s, physicists Peter Mansfield and Paul Lauterbur developed MRI-related techniques, like the echo-planar imaging (EPI) technique.\nRaymond Damadian\u2019s work into nuclear magnetic resonance (NMR) has been incorporated into MRI, having built one of the first scanners.\nAdvances in semiconductor technology were crucial to the development of practical MRI, which requires a large amount of computational power. This was made possible by the rapidly increasing number of transistors on a single integrated circuit chip. Mansfield and Lauterbur were awarded the 2003 Nobel Prize in Physiology or Medicine for their \"discoveries concerning magnetic resonance imaging\".\n\nSee also\nReferences\nFurther reading\nExternal links\n\nRinck PA (ed.). \"MRI: A Peer-Reviewed, Critical Introduction\". European Magnetic Resonance Forum (EMRF)\/The Round Table Foundation (TRTF).\nA Guided Tour of MRI: An introduction for laypeople National High Magnetic Field Laboratory\nThe Basics of MRI. Underlying physics and technical aspects.\nVideo: What to Expect During Your MRI Exam from the Institute for Magnetic Resonance Safety, Education, and Research (IMRSER)\nRoyal Institution Lecture \u2013 MRI: A Window on the Human Body\nA Short History of Magnetic Resonance Imaging from a European Point of View\nHow MRI works explained simply using diagrams\nReal-time MRI videos: Biomedizinische NMR Forschungs GmbH.\nPaul C. Lauterbur, Genesis of the MRI (Magnetic Resonance Imaging) notebook, September 1971 (all pages freely available for download in variety of formats from Science History Institute Digital Collections at digital.sciencehistory.org)","111":"Latin (lingua Latina, pronounced [\u02c8l\u026a\u014b\u0261\u02b7a \u026ba\u02c8ti\u02d0na], or Latinum [\u026ba\u02c8ti\u02d0n\u028a\u0303]) is a classical language belonging to the Italic branch of the Indo-European languages. Classical Latin is considered a dead language as it is no longer used to produce major texts, while Vulgar Latin evolved into the Romance Languages. Latin was originally spoken by the Latins in Latium (now known as Lazio), the lower Tiber area around Rome, Italy. Through the expansion of the Roman Republic it became the dominant language in the Italian Peninsula and subsequently throughout the Roman Empire. Even after the fall of Western Rome, Latin remained the common language of international communication, science, scholarship and academia in Europe until well into the early 19th century, when regional vernaculars supplanted it in common academic and political usage\u2014including its own descendants, the Romance languages.\nLatin grammar is highly fusional, with classes of inflections for case, number, person, gender, tense, mood, voice, and aspect. The Latin alphabet is directly derived from the Etruscan and Greek alphabets.\nBy the late Roman Republic, Old Latin had evolved into standardized Classical Latin. Vulgar Latin was the colloquial register with less prestigious variations attested in inscriptions and some literary works such as those of the comic playwrights Plautus and Terence and the author Petronius. Late Latin is the literary language from the 3rd century AD onward, and Vulgar Latin's various regional dialects had developed by the 6th to 9th centuries into the ancestors of the modern Romance languages.\nIn Latin's usage beyond the early medieval period, it lacked native speakers. Medieval Latin was used across Western and Catholic Europe during the Middle Ages as a working and literary language from the 9th century to the Renaissance, which then developed a classicizing form, called Renaissance Latin. This was the basis for Neo-Latin which evolved during the early modern period. In these periods Latin was used productively and generally taught to be written and spoken, at least until the late seventeenth century, when spoken skills began to erode. It then became increasingly taught only to be read.\nLatin remains the official language of the Holy See and the Roman Rite of the Catholic Church at the Vatican City. The church continues to adapt concepts from modern languages to Ecclesiastical Latin of the Latin language. Contemporary Latin is more often studied to be read rather than spoken or actively used.\nLatin has greatly influenced the English language, along with a large amount of others, and historically contributed many words to the English lexicon, particularly after the Christianization of the Anglo-Saxons and the Norman Conquest. Latin and Ancient Greek roots are heavily used in English vocabulary in theology, the sciences, medicine, and law.\n\nHistory\nA number of phases of the language have been recognized, each distinguished by subtle differences in vocabulary, usage, spelling, and syntax. There are no hard and fast rules of classification; different scholars emphasize different features. As a result, the list has variants, as well as alternative names.\nIn addition to the historical phases, Ecclesiastical Latin refers to the styles used by the writers of the Roman Catholic Church from late antiquity onward, as well as by Protestant scholars.\n\nOld Latin\nThe earliest known form of Latin is Old Latin, also called Archaic or Early Latin, which was spoken from the Roman Kingdom, traditionally founded in 753 BC, through the later part of the Roman Republic, up to 75 BC, i.e. before the age of Classical Latin. It is attested both in inscriptions and in some of the earliest extant Latin literary works, such as the comedies of Plautus and Terence. The Latin alphabet was devised from the Etruscan alphabet. The writing later changed from what was initially either a right-to-left or a boustrophedon script to what ultimately became a strictly left-to-right script.\n\nClassical Latin\nDuring the late republic and into the first years of the empire, from about 75 BC to AD 200, a new Classical Latin arose, a conscious creation of the orators, poets, historians and other literate men, who wrote the great works of classical literature, which were taught in grammar and rhetoric schools. Today's instructional grammars trace their roots to such schools, which served as a sort of informal language academy dedicated to maintaining and perpetuating educated speech.\n\nVulgar Latin\nPhilological analysis of Archaic Latin works, such as those of Plautus, which contain fragments of everyday speech, gives evidence of an informal register of the language, Vulgar Latin (termed sermo vulgi, \"the speech of the masses\", by Cicero). Some linguists, particularly in the nineteenth century, believed this to be a separate language, existing more or less in parallel with the literary or educated Latin, but this is now widely dismissed.\nThe term 'Vulgar Latin' remains difficult to define, referring both to informal speech at any time within the history of Latin, and the kind of informal Latin that had begun to move away from the written language significantly in the post-Imperial period, that led ultimately to the Romance languages.\nDuring the Classical period, informal language was rarely written, so philologists have been left with only individual words and phrases cited by classical authors, inscriptions such as Curse tablets and those found as graffiti. In the Late Latin period, language changes reflecting spoken (non-classical) norms tend to be found in greater quantities in texts.\nAs it was free to develop on its own, there is no reason to suppose that the speech was uniform either diachronically or geographically. On the contrary, Romanised European populations developed their own dialects of the language, which eventually led to the differentiation of Romance languages.\n\nLate Latin\nLate Latin is a kind of written Latin used in the 3rd to 6th centuries. This began to diverge from Classical forms at a faster pace. It is characterised by greater use of prepositions, and word order that is closer to modern Romance languages, for example, while grammatically retaining more or less the same formal rules as Classical Latin.\nUltimately, Latin diverged into a distinct written form, where the commonly spoken form was perceived as a separate language, for instance early French or Italian dialects, that could be transcribed differently. It took some time for these to be viewed as wholly different from Latin however.\nAfter the Western Roman Empire fell in 476 and Germanic kingdoms took its place, the Germanic people adopted Latin as a language more suitable for legal and other, more formal uses.\n\nRomance languages\nWhile the written form of Latin was increasingly standardized into a fixed form, the spoken forms began to diverge more greatly. Currently, the five most widely spoken Romance languages by number of native speakers are Spanish, Portuguese, French, Italian, and Romanian. Despite dialectal variation, which is found in any widespread language, the languages of Spain, France, Portugal, and Italy have retained a remarkable unity in phonological forms and developments, bolstered by the stabilising influence of their common Christian (Roman Catholic) culture.\nIt was not until the Muslim conquest of Spain in 711, cutting off communications between the major Romance regions, that the languages began to diverge seriously. The spoken Latin that would later become Romanian diverged somewhat more from the other varieties, as it was largely separated from the unifying influences in the western part of the Empire.\nSpoken Latin began to diverge into distinct languages by the 9th century at the latest, when the earliest extant Romance writings begin to appear. They were, throughout the period, confined to everyday speech, as Medieval Latin was used for writing.\nFor many Italians using Latin, though, there was no complete separation between Italian and Latin, even into the beginning of the Renaissance. Petrarch for example saw Latin as a literary version of the spoken language.\n\nMedieval Latin\nMedieval Latin is the written Latin in use during that portion of the post-classical period when no corresponding Latin vernacular existed, that is from around 700 to 1500 AD. The spoken language had developed into the various Romance languages; however, in the educated and official world, Latin continued without its natural spoken base. Moreover, this Latin spread into lands that had never spoken Latin, such as the Germanic and Slavic nations. It became useful for international communication between the member states of the Holy Roman Empire and its allies.\nWithout the institutions of the Roman Empire that had supported its uniformity, Medieval Latin was much more liberal in its linguistic cohesion: for example, in classical Latin sum and eram are used as auxiliary verbs in the perfect and pluperfect passive, which are compound tenses. Medieval Latin might use fui and fueram instead. Furthermore, the meanings of many words were changed and new words were introduced, often under influence from the vernacular. Identifiable individual styles of classically incorrect Latin prevail.\n\nRenaissance and Neo-Latin\nRenaissance Latin, 1300 to 1500, and the classicised Latin that followed through to the present are often grouped together as Neo-Latin, or New Latin, which have in recent decades become a focus of renewed study, given their importance for the development of European culture, religion and science. The vast majority of written Latin belongs to this period, but its full extent is unknown.\nThe Renaissance reinforced the position of Latin as a spoken and written language by the scholarship by the Renaissance humanists. Petrarch and others began to change their usage of Latin as they explored the texts of the Classical Latin world. Skills of textual criticism evolved to create much more accurate versions of extant texts through the fifteenth and sixteenth centuries, and some important texts were rediscovered. Comprehensive versions of authors' works were published by Isaac Casaubon, Joseph Scaliger and others. Nevertheless, despite the careful work of Petrarch, Politian and others, first the demand for manuscripts, and then the rush to bring works into print, led to the circulation of inaccurate copies for several centuries following.\nNeo-Latin literature was extensive and prolific, but less well known or understood today. Works covered poetry, prose stories and early novels, occasional pieces and collections of letters, to name a few. Famous and well regarded writers included Petrarch, Erasmus, Salutati, Celtis, George Buchanan and Thomas More. Non fiction works were long produced in many subjects, including the sciences, law, philosophy, historiography and theology. Famous examples include Isaac Newton's Principia. Latin was also used as a convenient medium for translations of important works first written in a vernacular, such as those of Descartes.\nLatin education underwent a process of reform to classicise written and spoken Latin. Schooling remained largely Latin medium until approximately 1700. Until the end of the 17th century, the majority of books and almost all diplomatic documents were written in Latin. Afterwards, most diplomatic documents were written in French (a Romance language) and later native or other languages. Education methods gradually shifted towards written Latin, and eventually concentrating solely on reading skills. The decline of Latin education took several centuries and proceeded much more slowly than the decline in written Latin output.\n\nContemporary Latin\nDespite having no native speakers, Latin is still used for a variety of purposes in the contemporary world.\n\nReligious use\nThe largest organisation that retains Latin in official and quasi-official contexts is the Catholic Church. The Catholic Church required that Mass be carried out in Latin until the Second Vatican Council of 1962\u20131965, which permitted the use of the vernacular. Latin remains the language of the Roman Rite. The Tridentine Mass (also known as the Extraordinary Form or Traditional Latin Mass) is celebrated in Latin. Although the Mass of Paul VI (also known as the Ordinary Form or the Novus Ordo) is usually celebrated in the local vernacular language, it can be and often is said in Latin, in part or in whole, especially at multilingual gatherings. It is the official language of the Holy See, the primary language of its public journal, the Acta Apostolicae Sedis, and the working language of the Roman Rota. Vatican City is also home to the world's only automatic teller machine that gives instructions in Latin. In the pontifical universities postgraduate courses of Canon law are taught in Latin, and papers are written in the same language.\nThere are a small number of Latin services held in the Anglican church. These include an annual service in Oxford, delivered with a Latin sermon; a relic from the period when Latin was the normal spoken language of the university.\n\nUse of Latin for mottos\nIn the Western world, many organizations, governments and schools use Latin for their mottos due to its association with formality, tradition, and the roots of Western culture.\nCanada's motto A mari usque ad mare (\"from sea to sea\") and most provincial mottos are also in Latin. The Canadian Victoria Cross is modelled after the British Victoria Cross which has the inscription \"For Valour\". Because Canada is officially bilingual, the Canadian medal has replaced the English inscription with the Latin Pro Valore.\nSpain's motto Plus ultra, meaning \"even further\", or figuratively \"Further!\", is also Latin in origin. It is taken from the personal motto of Charles V, Holy Roman Emperor and King of Spain (as Charles I), and is a reversal of the original phrase Non terrae plus ultra (\"No land further beyond\", \"No further!\"). According to legend, this phrase was inscribed as a warning on the Pillars of Hercules, the rocks on both sides of the Strait of Gibraltar and the western end of the known, Mediterranean world. Charles adopted the motto following the discovery of the New World by Columbus, and it also has metaphorical suggestions of taking risks and striving for excellence.\nIn the United States the unofficial national motto until 1956 was E pluribus unum meaning \"Out of many, one\". The motto continues to be featured on the Great Seal. It also appears on the flags and seals of both houses of congress and the flags of the states of Michigan, North Dakota, New York, and Wisconsin. The motto's 13 letters symbolically represent the original Thirteen Colonies which revolted from the British Crown. The motto is featured on all presently minted coinage and has been featured in most coinage throughout the nation's history.\nSeveral states of the United States have Latin mottos, such as:\n\nArizona's Ditat deus (\"God enriches\");\nConnecticut's Qui transtulit sustinet (\"He who transplanted sustains\");\nKansas's Ad astra per aspera (\"Through hardships, to the stars\");\nColorado's Nil sine numine (\"Nothing without providence\");\nMichigan's Si quaeris peninsulam amoenam, circumspice (\"If you seek a pleasant peninsula, look about you\"), is based on that of Sir Christopher Wren, in St. Paul's Cathedral;\nMissouri's Salus populi suprema lex esto (\"The health of the people should be the highest law\");\nNew York's Excelsior (\"Ever upward\");\nNorth Carolina's Esse Quam Videri (\"To be rather than to seem\");\nSouth Carolina's Dum spiro spero (\"While I breathe, I hope\");\nVirginia's Sic semper tyrannis (\"Thus always to tyrants\"); and\nWest Virginia's Montani Semper Liberi (\"Mountaineers [are] always free\").\nMany military organizations today have Latin mottos, such as:\n\nSemper Paratus (\"always ready\"), the motto of the United States Coast Guard;\nSemper Fidelis (\"always faithful\"), the motto of the United States Marine Corps;\nSemper Supra (\"always above\"), the motto of the United States Space Force;\nPer ardua ad astra (\"Through adversity\/struggle to the stars\"), the motto of the Royal Air Force (RAF); and\nVigilamus pro te (\"We stand on guard for thee\"), the motto of the Canadian Armed Forces.\nSome law governing bodies in the Philippines have Latin mottos, such as:\n\nJustitiae Pax Opus (\"Justice, peace, work\"), the motto of the Department of Justice (Philippines);\nSome colleges and universities have adopted Latin mottos, for example Harvard University's motto is Veritas (\"truth\"). Veritas was the goddess of truth, a daughter of Saturn, and the mother of Virtue.\n\nOther modern uses\nSwitzerland has adopted the country's Latin short name Helvetia on coins and stamps, since there is no room to use all of the nation's four official languages. For a similar reason, it adopted the international vehicle and internet code CH, which stands for Confoederatio Helvetica, the country's full Latin name.\nSome film and television in ancient settings, such as Sebastiane, The Passion of the Christ and Barbarians (2020 TV series), have been made with dialogue in Latin. Occasionally, Latin dialogue is used because of its association with religion or philosophy, in such film\/television series as The Exorcist and Lost (\"Jughead\"). Subtitles are usually shown for the benefit of those who do not understand Latin. There are also songs written with Latin lyrics. The libretto for the opera-oratorio Oedipus rex by Igor Stravinsky is in Latin.\nThe continued instruction of Latin is seen by some as a highly valuable component of a liberal arts education. Latin is taught at many high schools, especially in Europe and the Americas. It is most common in British public schools and grammar schools, the Italian liceo classico and liceo scientifico, the German Humanistisches Gymnasium and the Dutch gymnasium.\n\n Occasionally, some media outlets, targeting enthusiasts, broadcast in Latin. Notable examples include Radio Bremen in Germany, YLE radio in Finland (the Nuntii Latini broadcast from 1989 until it was shut down in June 2019), and Vatican Radio & Television, all of which broadcast news segments and other material in Latin.\nA variety of organisations, as well as informal Latin 'circuli' ('circles'), have been founded in more recent times to support the use of spoken Latin. Moreover, a number of university classics departments have begun incorporating communicative pedagogies in their Latin courses. These include the University of Kentucky, the University of Oxford and also Princeton University.\nThere are many websites and forums maintained in Latin by enthusiasts. The Latin Wikipedia has more than 130,000 articles.\n\nLegacy\nItalian, French, Portuguese, Spanish, Romanian, Catalan, Romansh and other Romance languages are direct descendants of Latin. There are also many Latin borrowings in English and Albanian, as well as a few in German, Dutch, Norwegian, Danish and Swedish. Latin is still spoken in Vatican City, a city-state situated in Rome that is the seat of the Catholic Church.\n\nLiterature\nThe works of several hundred ancient authors who wrote in Latin have survived in whole or in part, in substantial works or in fragments to be analyzed in philology. They are in part the subject matter of the field of classics. Their works were published in manuscript form before the invention of printing and are now published in carefully annotated printed editions, such as the Loeb Classical Library, published by Harvard University Press, or the Oxford Classical Texts, published by Oxford University Press.\nLatin translations of modern literature such as: The Hobbit, Treasure Island, Robinson Crusoe, Paddington Bear, Winnie the Pooh, The Adventures of Tintin, Asterix, Harry Potter, Le Petit Prince, Max and Moritz, How the Grinch Stole Christmas!, The Cat in the Hat, and a book of fairy tales, \"fabulae mirabiles\", are intended to garner popular interest in the language. Additional resources include phrasebooks and resources for rendering everyday phrases and concepts into Latin, such as Meissner's Latin Phrasebook.\n\nInscriptions\nSome inscriptions have been published in an internationally agreed, monumental, multivolume series, the Corpus Inscriptionum Latinarum (CIL). Authors and publishers vary, but the format is about the same: volumes detailing inscriptions with a critical apparatus stating the provenance and relevant information. The reading and interpretation of these inscriptions is the subject matter of the field of epigraphy. About 270,000 inscriptions are known.\n\nInfluence on present-day languages\nThe Latin influence in English has been significant at all stages of its insular development. In the Middle Ages, borrowing from Latin occurred from ecclesiastical usage established by Saint Augustine of Canterbury in the 6th century or indirectly after the Norman Conquest, through the Anglo-Norman language. From the 16th to the 18th centuries, English writers cobbled together huge numbers of new words from Latin and Greek words, dubbed \"inkhorn terms\", as if they had spilled from a pot of ink. Many of these words were used once by the author and then forgotten, but some useful ones survived, such as 'imbibe' and 'extrapolate'. Many of the most common polysyllabic English words are of Latin origin through the medium of Old French. Romance words make respectively 59%, 20% and 14% of English, German and Dutch vocabularies. Those figures can rise dramatically when only non-compound and non-derived words are included.\n\nThe influence of Roman governance and Roman technology on the less-developed nations under Roman dominion led to the adoption of Latin phraseology in some specialized areas, such as science, technology, medicine, and law. For example, the Linnaean system of plant and animal classification was heavily influenced by Historia Naturalis, an encyclopedia of people, places, plants, animals, and things published by Pliny the Elder. Roman medicine, recorded in the works of such physicians as Galen, established that today's medical terminology would be primarily derived from Latin and Greek words, the Greek being filtered through the Latin. Roman engineering had the same effect on scientific terminology as a whole. Latin law principles have survived partly in a long list of Latin legal terms.\nA few international auxiliary languages have been heavily influenced by Latin. Interlingua is sometimes considered a simplified, modern version of the language. Latino sine Flexione, popular in the early 20th century, is Latin with its inflections stripped away, among other grammatical changes.\nThe Logudorese dialect of the Sardinian language and Standard Italian are the two closest contemporary languages to Latin.\n\nEducation\nThroughout European history, an education in the classics was considered crucial for those who wished to join literate circles. This also was true in the United States where many of the nation's founders obtained a classically based education in grammar schools or from tutors. Admission to Harvard in the Colonial era required that the applicant \"Can readily make and speak or write true Latin prose and has skill in making verse . . .\" Latin Study and the classics were emphasized in American secondary schools and colleges well into the Antebellum era.\nInstruction in Latin is an essential aspect. In today's world, a large number of Latin students in the US learn from Wheelock's Latin: The Classic Introductory Latin Course, Based on Ancient Authors. This book, first published in 1956, was written by Frederic M. Wheelock, who received a PhD from Harvard University. Wheelock's Latin has become the standard text for many American introductory Latin courses.\nThe numbers of people studying Latin varies significantly by country. In the United Kingdom, Latin is available in around 2.3% of state primary schools, representing a significant increase in availability. In Germany, over 500,000 students study Latin each year, representing a decrease from over 800,000 in 2008. Latin is still required for some University courses, but this has become less frequent.\nThe Living Latin movement attempts to teach Latin in the same way that living languages are taught, as a means of both spoken and written communication. It is available in Vatican City and at some institutions in the US, such as the University of Kentucky and Iowa State University. The British Cambridge University Press is a major supplier of Latin textbooks for all levels, such as the Cambridge Latin Course series. It has also published a subseries of children's texts in Latin by Bell & Forte, which recounts the adventures of a mouse called Minimus.\nIn the United Kingdom, the Classical Association encourages the study of antiquity through various means, such as publications and grants. The University of Cambridge, the Open University, a number of independent schools, for example Eton, Harrow, Haberdashers' Aske's Boys' School, Merchant Taylors' School, and Rugby, and The Latin Programme\/Via Facilis, a London-based charity, run Latin courses. In the United States and in Canada, the American Classical League supports every effort to further the study of classics. Its subsidiaries include the National Junior Classical League (with more than 50,000 members), which encourages high school students to pursue the study of Latin, and the National Senior Classical League, which encourages students to continue their study of the classics into college. The league also sponsors the National Latin Exam. Classicist Mary Beard wrote in The Times Literary Supplement in 2006 that the reason for learning Latin is because of what was written in it.\n\nOfficial status\nLatin was or is the official language of European states:\n\n Hungary \u2013 Latin was an official language in the Kingdom of Hungary from the 11th century to the mid 19th century, when Hungarian became the exclusive official language in 1844. The best known Latin language poet of Croatian-Hungarian origin was Janus Pannonius.\n Croatia \u2013 Latin was the official language of Croatian Parliament (Sabor) from the 13th to the 19th century (1847). The oldest preserved records of the parliamentary sessions (Congregatio Regni totius Sclavonie generalis) \u2013 held in Zagreb (Zagabria), Croatia \u2013 date from 19 April 1273. An extensive Croatian Latin literature exists. Latin was used on Croatian coins on even years until 1 January 2023, when Croatia adopted the Euro as its official currency.\n Poland, Kingdom of Poland \u2013 officially recognised and widely used between the 10th and 18th centuries, commonly used in foreign relations and popular as a second language among some of the nobility.\n\nPhonology\nThe ancient pronunciation of Latin has been reconstructed; among the data used for reconstruction are explicit statements about pronunciation by ancient authors, misspellings, puns, ancient etymologies, the spelling of Latin loanwords in other languages, and the historical development of Romance languages.\n\nConsonants\nThe consonant phonemes of Classical Latin are as follows:\n\n\/z\/ was not native to Classical Latin. It appeared in Greek loanwords starting around the first century BC, when it was probably pronounced (at least by educated speakers) [z] initially and doubled [zz] between vowels, in accordance with its pronunciation in Koine Greek. In Classical Latin poetry, the letter \u27e8z\u27e9 between vowels always counts as two consonants for metrical purposes. The consonant \u27e8b\u27e9 usually sounds as [b]; however, when \u27e8t\u27e9 or \u27e8s\u27e9 follows \u27e8b\u27e9 then it is pronounced as in [pt] or [ps]. In Latin, \u27e8q\u27e9 is always followed by the vowel \u27e8u\u27e9. Together they make a [k\u02b7] sound.\nIn Old and Classical Latin, the Latin alphabet had no distinction between uppercase and lowercase, and the letters \u27e8J U W\u27e9 did not exist. In place of \u27e8J U\u27e9, \u27e8I V\u27e9 were used, respectively; \u27e8I V\u27e9 represented both vowels and consonants. Most of the letter forms were similar to modern uppercase, as can be seen in the inscription from the Colosseum shown at the top of the article.\nThe spelling systems used in Latin dictionaries and modern editions of Latin texts, however, normally use \u27e8j u\u27e9 in place of Classical-era \u27e8i v\u27e9. Some systems use \u27e8j v\u27e9 for the consonant sounds \/j w\/ except in the combinations \u27e8gu su qu\u27e9 for which \u27e8v\u27e9 is never used.\nSome notes concerning the mapping of Latin phonemes to English graphemes are given below:\n\nIn Classical Latin, as in modern Italian, double consonant letters were pronounced as long consonant sounds distinct from short versions of the same consonants. Thus the nn in Classical Latin annus \"year\" (and in Italian anno) is pronounced as a doubled \/nn\/ as in English unnamed. (In English, distinctive consonant length or doubling occurs only at the boundary between two words or morphemes, as in that example.)\n\nVowels\nSimple vowels\nIn Classical Latin, \u27e8U\u27e9 did not exist as a letter distinct from V; the written form \u27e8V\u27e9 was used to represent both a vowel and a consonant. \u27e8Y\u27e9 was adopted to represent upsilon in loanwords from Greek, but it was pronounced like \u27e8u\u27e9 and \u27e8i\u27e9 by some speakers. It was also used in native Latin words by confusion with Greek words of similar meaning, such as sylva and \u1f55\u03bb\u03b7.\nClassical Latin distinguished between long and short vowels. Then, long vowels, except for \u27e8i\u27e9, were frequently marked using the apex, which was sometimes similar to an acute accent \u27e8\u00c1 \u00c9 \u00d3 V\u0301 \u00dd\u27e9. Long \/i\u02d0\/ was written using a taller version of \u27e8I\u27e9, called i longa \"long I\": \u27e8\ua7fe\u27e9. In modern texts, long vowels are often indicated by a macron \u27e8\u0101 \u0113 \u012b \u014d \u016b\u27e9, and short vowels are usually unmarked except when it is necessary to distinguish between words, when they are marked with a breve \u27e8\u0103 \u0115 \u012d \u014f \u016d\u27e9. However, they would also signify a long vowel by writing the vowel larger than other letters in a word or by repeating the vowel twice in a row. The acute accent, when it is used in modern Latin texts, indicates stress, as in Spanish, rather than length.\nAlthough called long vowels, their exact quality in Classical Latin is different from short vowels. The difference is described in the table below:\n\nThis difference in quality is posited by W. Sidney Allen in his book Vox Latina. However, Andrea Calabrese has disputed this assertion, based in part upon the observation that in Sardinian and some Lucanian dialects, each long and short vowel pair merged, as opposed to in Italo-Western languages in which short \/i\/ and \/u\/ merged with long \/e\u02d0\/ and \/o:\/ (c.f. Latin 'siccus', Italian 'secco', and Sardinian 'siccu').\nA vowel letter followed by \u27e8m\u27e9 at the end of a word, or a vowel letter followed by \u27e8n\u27e9 before \u27e8s\u27e9 or \u27e8f\u27e9, represented a short nasal vowel, as in monstrum [m\u00f5\u02d0str\u0169].\n\nDiphthongs\nClassical Latin had several diphthongs. The two most common were \u27e8ae au\u27e9. The former pronounced like the 'i' in mine, and the latter like the 'ow' in power. \u27e8oe\u27e9 was fairly rare, and \u27e8ui eu ei\u27e9 were very rare, at least in native Latin words. There has also been debate over whether \u27e8ui\u27e9 is truly a diphthong in Classical Latin, due to its rarity, absence in works of Roman grammarians, and the roots of Classical Latin words (i.e. hui ce to huic, quoi to cui, etc.) not matching or being similar to the pronunciation of classical words if \u27e8ui\u27e9 were to be considered a diphthong.\nThe sequences sometimes did not represent diphthongs. \u27e8ae\u27e9 and \u27e8oe\u27e9 also represented a sequence of two vowels in different syllables in a\u0113nus [a\u02c8e\u02d0.n\u028as] \"bronze\" and co\u0113pit [k\u0254\u02c8e\u02d0.p\u026at] \"began\", and \u27e8au ui eu ei ou\u27e9 represented sequences of two vowels or of a vowel and one of the semivowels \/j w\/, in cav\u0113 [\u02c8ka.we\u02d0] \"beware!\", cuius [\u02c8k\u028aj.j\u028as] \"whose\", monu\u012b [\u02c8m\u0254n.\u028a.i\u02d0] \"I warned\", solv\u012b [\u02c8s\u0254\u026b.wi\u02d0] \"I released\", d\u0113l\u0113v\u012b [de\u02d0\u02c8le\u02d0.wi\u02d0] \"I destroyed\", eius [\u02c8\u025bj.j\u028as] \"his\", and novus [\u02c8n\u0254.w\u028as] \"new\".\nOld Latin had more diphthongs, but most of them changed into long vowels in Classical Latin. The Old Latin diphthong \u27e8ai\u27e9 and the sequence \u27e8\u0101\u012b\u27e9 became Classical \u27e8ae\u27e9. Old Latin \u27e8oi\u27e9 and \u27e8ou\u27e9 changed to Classical \u27e8\u016b\u27e9, except in a few words whose \u27e8oi\u27e9 became Classical \u27e8oe\u27e9. These two developments sometimes occurred in different words from the same root: for instance, Classical poena \"punishment\" and p\u016bn\u012bre \"to punish\". Early Old Latin \u27e8ei\u27e9 usually monophthongized to a later Old Latin \u27e8\u0113\u27e9, to Classical \u27e8\u012b\u27e9.\nBy the late Roman Empire, \u27e8ae oe\u27e9 had merged with \u27e8e \u0113\u27e9. During the Classical period this sound change was present in some rural dialects, but deliberately avoided by well-educated speakers.\n\nSyllables\nSyllables in Latin are signified by the presence of diphthongs and vowels. The number of syllables is the same as the number of vowel sounds.\nFurther, if a consonant separates two vowels, it will go into the syllable of the second vowel. When there are two consonants between vowels, the last consonant will go with the second vowel. An exception occurs when a phonetic stop and liquid come together. In this situation, they are thought to be a single consonant, and as such, they will go into the syllable of the second vowel.\n\nLength\nSyllables in Latin are considered either long or short (less often called \"heavy\" and \"light\" respectively). Within a word, a syllable may either be long by nature or long by position. A syllable is long by nature if it has a diphthong or a long vowel. On the other hand, a syllable is long by position if the vowel is followed by more than one consonant.\n\nStress\nThere are two rules that define which syllable is stressed in Classical Latin.\n\nIn a word with only two syllables, the emphasis will be on the first syllable.\nIn a word with more than two syllables, there are two cases.\nIf the second-to-last syllable is long, that syllable will have stress.\nIf the second-to-last syllable is not long, the syllable before that one will be stressed instead.\n\nOrthography\nLatin was written in the Latin alphabet (A, B, C, D, E, F, G, H, I, K, L, M, N, O, P, Q, R, S, T, V, X), derived from the Etruscan alphabet, which was in turn drawn from the Greek alphabet and ultimately the Phoenician alphabet. This alphabet has continued to be used over the centuries as the script for the Romance, Celtic, Germanic, Baltic, Finnic and many Slavic languages (Polish, Slovak, Slovene, Croatian, Bosnian, Serbian and Czech); and it has been adopted by many languages around the world, including Vietnamese, the Austronesian languages, many Turkic languages, and most languages in sub-Saharan Africa, the Americas and Oceania, making it by far the world's single most widely used writing system.\nThe number of letters in the Latin alphabet has varied. When it was first derived from the Etruscan alphabet, it contained only 21 letters. Later, G was added to represent \/\u0261\/, which had previously been spelled C, and Z ceased to be included in the alphabet, as the language then had no voiced alveolar fricative. The letters K, Y, and Z were later added to represent Greek letters kappa, upsilon, and zeta respectively, in Greek loanwords.\nW was created in the 11th century from VV in some areas and uu in others. It represented \/w\/ in Germanic languages, not Latin, which still uses V for the purpose. J was distinguished from the original I only during the late Middle Ages, as was the letter U from V. Although some Latin dictionaries use J, it is rarely used for Latin text, as it was not used in classical times, but many other languages use it.\n\nPunctuation\nClassical Latin did not contain sentence punctuation, letter case, or interword spacing, but apices were sometimes used to distinguish length in vowels and the interpunct was used at times to separate words.\nThe first line of Catullus 3 (\"Mourn, O Venuses and Cupids\") was originally written as:\n\nIt would be rendered in a modern edition as:\n\nThe Roman cursive script is commonly found on the many wax tablets excavated at sites such as forts, an especially extensive set having been discovered at Vindolanda on Hadrian's Wall in Britain. Most notable is the fact that while most of the Vindolanda tablets show spaces between words, spaces were avoided in monumental inscriptions from that era.\n\nAlternative scripts\nOccasionally, Latin has been written in other scripts:\n\nThe Praeneste fibula is a 7th-century BC pin with an Old Latin inscription written using the Etruscan script.\nThe rear panel of the early 8th-century Franks Casket has an inscription that switches from Old English in Anglo-Saxon runes to Latin in Latin script and to Latin in runes.\n\nGrammar\nLatin is a synthetic, fusional language in the terminology of linguistic typology. Words involve an objective semantic element and markers (usually suffixes) specifying the grammatical use of the word, expressing gender, number, and case in adjectives, nouns, and pronouns (declension) and verbs to denote person, number, tense, voice, mood, and aspect (conjugation). Some words are uninflected and undergo neither process, such as adverbs, prepositions, and interjections.\nLatin inflection can result in words with much ambiguity: For example, am\u0101bit, \"he\/she\/it will love\", is formed from am\u0101-, a future tense morpheme -bi- and a third person singular morpheme, -t, the last of which -t does not express masculine, feminine, or neuter gender. A major task in understanding Latin phrases and clauses is to clarify such ambiguities by an analysis of context.\n\nNouns\nA regular Latin noun belongs to one of five main declensions, a group of nouns with similar inflected forms. The declensions are identified by the genitive singular form of the noun.\n\nThe first declension, with a predominant ending letter of a, is signified by the genitive singular ending of -ae.\nThe second declension, with a predominant ending letter of us, is signified by the genitive singular ending of -i.\nThe third declension, with a predominant ending letter of i, is signified by the genitive singular ending of -is.\nThe fourth declension, with a predominant ending letter of u, is signified by the genitive singular ending of -\u016bs.\nThe fifth declension, with a predominant ending letter of e, is signified by the genitive singular ending of -ei.\nThere are seven Latin noun cases, which also apply to adjectives and pronouns and mark a noun's syntactic role in the sentence by means of inflections. Thus, word order is not as important in Latin as it is in English, which is less inflected. The general structure and word order of a Latin sentence can therefore vary. The cases are as follows:\n\nNominative \u2013 used when the noun is the subject or a predicate nominative. The thing or person acting: the girl ran: puella cucurrit, or cucurrit puella\nGenitive \u2013 used when the noun is the possessor of or connected with an object: \"the horse of the man\", or \"the man's horse\"; in both instances, the word man would be in the genitive case when it is translated into Latin. It also indicates the partitive, in which the material is quantified: \"a group of people\"; \"a number of gifts\": people and gifts would be in the genitive case. Some nouns are genitive with special verbs and adjectives: The cup is full of wine. (Poculum pl\u0113num v\u012bn\u012b est.) The master of the slave had beaten him. (Dominus serv\u012b eum verber\u0101verat.)\nDative \u2013 used when the noun is the indirect object of the sentence, with special verbs, with certain prepositions, and if it is used as agent, reference, or even possessor: The merchant hands the stola to the woman. (Merc\u0101tor f\u0113minae stolam tr\u0101dit.)\nAccusative \u2013 used when the noun is the direct object of the subject, as the object of a preposition demonstrating place to which, and sometimes to indicate a duration of time: The man killed the boy. (Vir puerum nec\u0101vit.)\nAblative \u2013 used when the noun demonstrates separation or movement from a source, cause, agent or instrument or when the noun is used as the object of certain prepositions, and to indicate a specific place in time.; adverbial: You walked with the boy. (Cum puer\u014d ambul\u0101vist\u012b.)\nVocative \u2013 used when the noun is used in a direct address. The vocative form of a noun is often the same as the nominative, with the exception of second-declension nouns ending in -us. The -us becomes an -e in the vocative singular. If it ends in -ius (such as f\u012blius), the ending is just -\u012b (fil\u012b), as distinct from the nominative plural (fili\u012b) in the vocative singular: \"Master!\" shouted the slave. (\"Domine!\" cl\u0101m\u0101vit servus.)\nLocative \u2013 used to indicate a location (corresponding to the English \"in\" or \"at\"). It is far less common than the other six cases of Latin nouns and usually applies to cities and small towns and islands along with a few common nouns, such as the words domus (house), humus (ground), and rus (country). In the singular of the first and second declensions, its form coincides with the genitive (Roma becomes Romae, \"in Rome\"). In the plural of all declensions and the singular of the other declensions, it coincides with the ablative (Ath\u0113nae becomes Ath\u0113n\u012bs, \"at Athens\"). In the fourth-declension word domus, the locative form, dom\u012b (\"at home\") differs from the standard form of all other cases.\nLatin lacks both definite and indefinite articles so puer currit can mean either \"the boy is running\" or \"a boy is running\".\n\nAdjectives\nThere are two types of regular Latin adjectives: first- and second-declension and third-declension. They are so-called because their forms are similar or identical to first- and second-declension and third-declension nouns, respectively. Latin adjectives also have comparative and superlative forms. There are also a number of Latin participles.\nLatin numbers are sometimes declined as adjectives. See Numbers below.\nFirst- and second-declension adjectives are declined like first-declension nouns for the feminine forms and like second-declension nouns for the masculine and neuter forms. For example, for mortuus, mortua, mortuum (dead), mortua is declined like a regular first-declension noun (such as puella (girl)), mortuus is declined like a regular second-declension masculine noun (such as dominus (lord, master)), and mortuum is declined like a regular second-declension neuter noun (such as auxilium (help)).\nThird-declension adjectives are mostly declined like normal third-declension nouns, with a few exceptions. In the plural nominative neuter, for example, the ending is -ia (omnia (all, everything)), and for third-declension nouns, the plural nominative neuter ending is -a or -ia (capita (heads), animalia (animals)) They can have one, two or three forms for the masculine, feminine, and neuter nominative singular.\n\nParticiples\nLatin participles, like English participles, are formed from a verb. There are a few main types of participles: Present Active Participles, Perfect Passive Participles, Future Active Participles, and Future Passive Participles.\n\nPrepositions\nLatin sometimes uses prepositions, depending on the type of prepositional phrase being used. Most prepositions are followed by a noun in either the accusative or ablative case: \"apud puerum\" (with the boy), with \"puerum\" being the accusative form of \"puer\", boy, and \"sine puero\" (without the boy), \"puero\" being the ablative form of \"puer\". A few adpositions, however, govern a noun in the genitive (such as \"gratia\" and \"tenus\").\n\nVerbs\nA regular verb in Latin belongs to one of four main conjugations. A conjugation is \"a class of verbs with similar inflected forms.\" The conjugations are identified by the last letter of the verb's present stem. The present stem can be found by omitting the -re (-r\u012b in deponent verbs) ending from the present infinitive form. The infinitive of the first conjugation ends in -\u0101-re or -\u0101-ri (active and passive respectively): am\u0101re, \"to love\", hort\u0101r\u012b, \"to exhort\"; of the second conjugation by -\u0113-re or -\u0113-r\u012b: mon\u0113re, \"to warn\", ver\u0113r\u012b, \"to fear;\" of the third conjugation by -ere, -\u012b: d\u016bcere, \"to lead\", \u016bt\u012b, \"to use\"; of the fourth by -\u012b-re, -\u012b-r\u012b: aud\u012bre, \"to hear\", exper\u012br\u012b, \"to attempt\". The stem categories descend from Indo-European and can therefore be compared to similar conjugations in other Indo-European languages.\nIrregular verbs are verbs that do not follow the regular conjugations in the formation of the inflected form. Irregular verbs in Latin are esse, \"to be\"; velle, \"to want\"; ferre, \"to carry\"; edere, \"to eat\"; dare, \"to give\"; ire, \"to go\"; posse, \"to be able\"; fieri, \"to happen\"; and their compounds.\nThere are six simple tenses in Latin (present, imperfect, future, perfect, pluperfect and future perfect), three moods (indicative, imperative and subjunctive, in addition to the infinitive, participle, gerund, gerundive and supine), three persons (first, second and third), two numbers (singular and plural), two voices (active and passive) and two aspects (perfective and imperfective). Verbs are described by four principal parts:\n\nThe first principal part is the first-person singular, present tense, active voice, indicative mood form of the verb. If the verb is impersonal, the first principal part will be in the third-person singular.\nThe second principal part is the present active infinitive.\nThe third principal part is the first-person singular, perfect active indicative form. Like the first principal part, if the verb is impersonal, the third principal part will be in the third-person singular.\nThe fourth principal part is the supine form, or alternatively, the nominative singular of the perfect passive participle form of the verb. The fourth principal part can show one gender of the participle or all three genders (-us for masculine, -a for feminine and -um for neuter) in the nominative singular. The fourth principal part will be the future participle if the verb cannot be made passive. Most modern Latin dictionaries, if they show only one gender, tend to show the masculine; but many older dictionaries instead show the neuter, as it coincides with the supine. The fourth principal part is sometimes omitted for intransitive verbs, but strictly in Latin, they can be made passive if they are used impersonally, and the supine exists for such verbs.\nThe six simple tenses of Latin are divided into two systems: the present system, which is made up of the present, imperfect and future forms, and the perfect system, which is made up of the perfect, pluperfect and future perfect forms. Each simple tense has a set of endings corresponding to the person, number, and voice of the subject. Subject (nominative) pronouns are generally omitted for the first (I, we) and second (you) persons except for emphasis.\nThe table below displays the common inflected endings for the indicative mood in the active voice in all six tenses. For the future tense, the first listed endings are for the first and second conjugations, and the second listed endings are for the third and fourth conjugations:\n\nDeponent verbs\nSome Latin verbs are deponent, causing their forms to be in the passive voice but retain an active meaning: hortor, hort\u0101r\u012b, hort\u0101tus sum (to urge).\n\nVocabulary\nAs Latin is an Italic language, most of its vocabulary is likewise Italic, ultimately from the ancestral Proto-Indo-European language. However, because of close cultural interaction, the Romans not only adapted the Etruscan alphabet to form the Latin alphabet but also borrowed some Etruscan words into their language, including persona \"mask\" and histrio \"actor\". Latin also included vocabulary borrowed from Oscan, another Italic language.\nAfter the Fall of Tarentum (272 BC), the Romans began Hellenising, or adopting features of Greek culture, including the borrowing of Greek words, such as camera (vaulted roof), sumbolum (symbol), and balineum (bath). This Hellenisation led to the addition of \"Y\" and \"Z\" to the alphabet to represent Greek sounds. Subsequently, the Romans transplanted Greek art, medicine, science and philosophy to Italy, paying almost any price to entice Greek skilled and educated persons to Rome and sending their youth to be educated in Greece. Thus, many Latin scientific and philosophical words were Greek loanwords or had their meanings expanded by association with Greek words, as ars (craft) and \u03c4\u03ad\u03c7\u03bd\u03b7 (art).\nBecause of the Roman Empire's expansion and subsequent trade with outlying European tribes, the Romans borrowed some northern and central European words, such as beber (beaver), of Germanic origin, and bracae (breeches), of Celtic origin. The specific dialects of Latin across Latin-speaking regions of the former Roman Empire after its fall were influenced by languages specific to the regions. The dialects of Latin evolved into different Romance languages.\nDuring and after the adoption of Christianity into Roman society, Christian vocabulary became a part of the language, either from Greek or Hebrew borrowings or as Latin neologisms. Continuing into the Middle Ages, Latin incorporated many more words from surrounding languages, including Old English and other Germanic languages.\nOver the ages, Latin-speaking populations produced new adjectives, nouns, and verbs by affixing or compounding meaningful segments. For example, the compound adjective, omnipotens, \"all-powerful\", was produced from the adjectives omnis, \"all\", and potens, \"powerful\", by dropping the final s of omnis and concatenating. Often, the concatenation changed the part of speech, and nouns were produced from verb segments or verbs from nouns and adjectives.\n\nNumbers\nIn ancient times, numbers in Latin were written only with letters. Today, the numbers can be written with the Arabic numbers as well as with Roman numerals. The numbers 1, 2 and 3 and every whole hundred from 200 to 900 are declined as nouns and adjectives, with some differences.\n\nThe numbers from 4 to 100 do not change their endings. As in modern descendants such as Spanish, the gender for naming a number in isolation is masculine, so that \"1, 2, 3\" is counted as \u016bnus, duo, tr\u0113s.\n\nExample text\nCommentarii de Bello Gallico, also called De Bello Gallico (The Gallic War), written by Gaius Julius Caesar, begins with the following passage:\n\nGallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur. Hi omnes lingua, institutis, legibus inter se differunt. Gallos ab Aquitanis Garumna flumen, a Belgis Matrona et Sequana dividit. Horum omnium fortissimi sunt Belgae, propterea quod a cultu atque humanitate provinciae longissime absunt, minimeque ad eos mercatores saepe commeant atque ea quae ad effeminandos animos pertinent important, proximique sunt Germanis, qui trans Rhenum incolunt, quibuscum continenter bellum gerunt. Qua de causa Helvetii quoque reliquos Gallos virtute praecedunt, quod fere cotidianis proeliis cum Germanis contendunt, cum aut suis finibus eos prohibent aut ipsi in eorum finibus bellum gerunt. Eorum una pars, quam Gallos obtinere dictum est, initium capit a flumine Rhodano, continetur Garumna flumine, Oceano, finibus Belgarum; attingit etiam ab Sequanis et Helvetiis flumen Rhenum; vergit ad septentriones. Belgae ab extremis Galliae finibus oriuntur; pertinent ad inferiorem partem fluminis Rheni; spectant in septentrionem et orientem solem. Aquitania a Garumna flumine ad Pyrenaeos montes et eam partem Oceani quae est ad Hispaniam pertinet; spectat inter occasum solis et septentriones.\nThe same text may be marked for all long vowels (before any possible elisions at word boundary) with apices over vowel letters, including customarily before \"nf\" and \"ns\" where a long vowel is automatically produced:\n\nGallia est omnis d\u00edv\u00edsa in part\u00e9s tr\u00e9s, qu\u00e1rum \u00fanam incolunt Belgae, aliam Aqu\u00edt\u00e1n\u00ed, tertiam qu\u00ed ips\u00f3rum lingu\u00e1 Celtae, nostr\u00e1 Gall\u00ed appellantur. H\u00ed omn\u00e9s lingu\u00e1, \u00ednstit\u00fat\u00eds, l\u00e9gibus inter s\u00e9 differunt. Gall\u00f3s ab Aqu\u00edt\u00e1n\u00eds Garumna fl\u00famen, \u00e1 Belg\u00eds M\u00e1trona et S\u00e9quana d\u00edvidit. H\u00f3rum omnium fortissim\u00ed sunt Belgae, proptere\u00e1 quod \u00e1 cult\u00fa atque h\u00fam\u00e1nit\u00e1te pr\u00f3vinciae longissim\u00e9 absunt, minim\u00e9que ad e\u00f3s merc\u00e1t\u00f3r\u00e9s saepe commeant atque ea quae ad eff\u00e9minand\u00f3s anim\u00f3s pertinent important, proxim\u00edque sunt Germ\u00e1n\u00eds, qu\u00ed tr\u00e1ns Rh\u00e9num incolunt, quibuscum continenter bellum gerunt. Qu\u00e1 d\u00e9 caus\u00e1 Helv\u00e9ti\u00ed quoque reliqu\u00f3s Gall\u00f3s virt\u00fate praec\u00e9dunt, quod fer\u00e9 cot\u00eddi\u00e1n\u00eds proeli\u00eds cum Germ\u00e1n\u00eds contendunt, cum aut su\u00eds f\u00ednibus e\u00f3s prohibent aut ips\u00ed in e\u00f3rum f\u00ednibus bellum gerunt. E\u00f3rum \u00fana pars, quam Gall\u00f3s obtin\u00e9re dictum est, initium capit \u00e1 fl\u00famine Rhodan\u00f3, contin\u00e9tur Garumn\u00e1 fl\u00famine, \u00d3cean\u00f3, f\u00ednibus Belg\u00e1rum; attingit etiam ab S\u00e9quan\u00eds et Helv\u00e9ti\u00eds fl\u00famen Rh\u00e9num; vergit ad septentri\u00f3n\u00e9s. Belgae ab extr\u00e9m\u00eds Galliae f\u00ednibus oriuntur; pertinent ad \u00ednferi\u00f3rem partem fl\u00faminis Rh\u00e9n\u00ed; spectant in septentri\u00f3nem et orientem s\u00f3lem. Aqu\u00edt\u00e1nia \u00e1 Garumn\u00e1 fl\u00famine ad P\u00fdr\u00e9nae\u00f3s mont\u00e9s et eam partem \u00d3cean\u00ed quae est ad Hisp\u00e1niam pertinet; spectat inter occ\u00e1sum s\u00f3lis et septentri\u00f3n\u00e9s.\n\nSee also\nReferences\nBibliography\nExternal links\nLanguage tools\n\"Latin Dictionary Headword Search\". Perseus Hopper. Tufts University. Searches Lewis & Short's A Latin Dictionary and Lewis's An Elementary Latin Dictionary. Online results.\n\"Online Latin Dictionary with conjugator and declension tool\". Olivetti Media Communication. Search on line Latin-English and English-Latin dictionary with complete declension or conjugation. Online results.\n\"Latin Word Study Tool\". Perseus Hopper. Tufts University. Identifies the grammatical functions of words entered. Online results.\nAversa, Alan. \"Latin Inflector\". Retrieved 8 June 2023. Identifies the grammatical functions of all the words in sentences entered, using Perseus.\n\"Latin Verb Conjugator\". Verbix. Displays complete conjugations of verbs entered in first-person present singular form.\n\"Online Latin Verb Conjugator\". Archived from the original on 18 May 2016. Retrieved 30 September 2014. Displays conjugation of verbs entered in their infinitive form.\nWhittaker, William. \"Words\". Notre Dame Archives. Archived from the original on 18 June 2006. Identifies Latin words entered. Translates English words entered.\n\"Alpheios\". Alpheios Project. Combines Whittakers Words, Lewis and Short, Bennett's grammar and inflection tables in a browser addon.\nLatin Dictionaries at Curlie\nDymock, John (1830). A new abridgment of Ainsworth's Dictionary, English and Latin, for the use of Grammar Schools (4th ed.). Glasgow: Hutchison & Brookman.\n\"Collatinus web\". Online lemmatizer and morphological analysis for Latin texts.\n\nCourses\nCommunity courses on Memrise\nLatin Lessons (free online through the Linguistics Research Center at UT Austin)\nFree 47-Lesson Online Latin Course, Learnlangs\nLearn Latin Archived 8 March 2022 at the Wayback Machine Grammar, vocabulary and audio\nLatin Links and Resources, Compiled by Fr. Gary Coulter\nder Millner, Evan (2007). \"Latinum\". Latin Latin Course on YouTube and audiobooks. Molendinarius. Retrieved 2 February 2012.\nByrne, Carol (1999). \"Simplicissimus\" (PDF). The Latin Mass Society of England and Wales. Archived from the original (PDF) on 30 April 2011. Retrieved 20 April 2011. (a course in ecclesiastical Latin).\nLatin course: Ludus Latinus (Bibliotheca Augustana)\nBeginners' Latin on The National Archives (United Kingdom)\n\nGrammar and study\nBennett, Charles E. (2005) [1908]. New Latin Grammar (2nd ed.). Project Gutenberg. ISBN 978-1-176-19706-0.\nGriffin, Robin (1992). A student's Latin Grammar (3rd ed.). University of Cambridge. ISBN 978-0-521-38587-9.\nLehmann, Winifred P.; Slocum, Jonathan (2008). \"Latin Online\". The University of Texas at Austin. Retrieved 17 April 2020.\n\u00d8rberg, Hans (1991). LINGVA LATINA PER SE ILLVSTRATA \u2013 Pars I FAMILIA ROMANA. Museum Tusculanum Press. ISBN 87-997016-5-0.\n\u00d8rberg, Hans (2007). LINGVA LATINA PER SE ILLVSTRATA - Pars II ROMA AETERNA. Hackett Publishing Company, Incorporated. ISBN 978-1-58510-067-5.\nAllen and Greenough (1903). New Latin Grammar. Athan\u00e6um Press.\n\nPhonetics\nCui, Ray (2005). \"Phonetica Latinae-How to pronounce Latin\". Ray Cui. Retrieved 25 June 2010.\nWilkins, Augustus Samuel; Conway, Robert Seymour (1911). \"Latin Language\" . Encyclop\u00e6dia Britannica. Vol. 16 (11th ed.). pp. 244\u2013257.\nRanieri, Luke (31 August 2018). \"Latin Pronunciation (for Classical Latin)\". YouTube. Archived from the original on 27 October 2021. Retrieved 31 August 2018.\n\nLibraries\nThe latin library, ancient Latin books and writings (without translations) ordered by author\nLacusCurtius, a small collection of Greek and Roman authors along with their books and writings (original texts are in Latin and Greek, translations in English and occasionally in a few other languages are available)\n\nLatin language media\nEphemeris, online Latin newspaper: nuntii latini universi = news in Latin of the universe (whole world)\nEphemeris archive, archived copy of online Latin newspaper\nNuntii Latini, from Finnish YLE Radio 1\nNuntii Latini, monthly review from German Radio Bremen (Bremen Zwei)\nClassics Podcasts in Latin and Ancient Greek, Haverford College\nGrex Latine Loquentium (Flock of those Speaking Latin)\nCirculus Latinus Interretialis (Internet Latin Circle)\nLatinitas Foundation, at the Vatican","112":"The mastoid part of the temporal bone is the posterior (back) part of the temporal bone, one of the bones of the skull. Its rough surface gives attachment to various muscles (via tendons) and it has openings for blood vessels. From its borders, the mastoid part articulates with two other bones.\n\nEtymology\nThe word \"mastoid\" is derived from the Greek word for \"breast\", a reference to the shape of this bone.\n\nSurfaces\nOuter surface\nIts outer surface is rough and gives attachment to the occipitalis and posterior auricular muscles. It is perforated by numerous foramina (holes); for example, the mastoid foramen is situated near the posterior border and transmits a vein to the transverse sinus and a small branch of the occipital artery to the dura mater. The position and size of this foramen are very variable; it is not always present; sometimes it is situated in the occipital bone, or in the suture between the temporal and the occipital.\n\nMastoid process\nThe mastoid process is located posterior and inferior to the ear canal, lateral to the styloid process, and appears as a conical or pyramidal projection. It forms a bony prominence behind and below the ear. It has variable size and form (e.g. it is larger in the male than in the female). It is also filled with sinuses, or mastoid cells. The mastoid process serves for the attachment of the sternocleidomastoid, the posterior belly of the digastric muscle, splenius capitis, and longissimus capitis. On the medial side of the process is a deep groove, the mastoid notch, for the attachment of the digastric muscle; medial to this is a shallow furrow, the occipital groove, which lodges the occipital artery. The facial nerve passes close to the mastoid process.\n\nInner surface\nThe inner surface of the mastoid portion presents a deep, curved groove, the sigmoid sulcus, which lodges part of the transverse sinus; in it may be seen in the opening of the mastoid foramen.\nThe groove for the transverse sinus is separated from the innermost of the mastoid cells by a very thin lamina of bone, and even this may be partly deficient.\n\nBorders\nThe superior border of the mastoid part is broad and serrated, for articulation with the mastoid angle of the parietal.\nThe posterior border, also serrated, articulates with the inferior border of the occipital between the lateral angle and jugular process.\nAnteriorly, the mastoid portion is fused with the descending process of the squama above; below, it enters into the formation of the ear canal and the tympanic cavity.\n\nSpaces\nA section of the mastoid process shows it to be hollowed out into a number of spaces, the mastoid cells, which exhibit the greatest possible variety as to their size and number. At the upper and front part of the process, they are large and irregular and contain air, but toward the lower part, they diminish in size, while those at the apex of the process are frequently quite small and contain marrow; occasionally, they are entirely absent, and the mastoid is then solid throughout.\nIn addition to these a large irregular cavity is situated at the upper and front part of the bone. It is called the tympanic antrum and must be distinguished from the mastoid cells, though it communicates with them. Like the mastoid cells, it is filled with air and lined by a prolongation of the mucous membrane of the tympanic cavity, with which it communicates. The tympanic antrum is bounded above by a thin plate of bone, the tegmen tympani, which separates it from the middle fossa of the base of the skull, below by the mastoid process, laterally by the squama just below the temporal line, and medially by the lateral semicircular canal of the internal ear, which projects into its cavity. It opens in front into that portion of the tympanic cavity which is known as the attic or epitympanic recess. The tympanic antrum is a cavity of some considerable size at the time of birth; the mastoid air cells may be regarded as diverticula from the antrum and begin to appear at or before birth. By the fifth year, they are well-marked, but their development is not completed until toward puberty.\n\nDevelopment\nThe mastoid process is absent or rudimentary in the neonatal skull. It forms postnatally (starts to develop after 1 year old), as the sternocleidomastoid muscle develops and pulls on the bone. It usually finishes structural development by 2 years old.\n\nClinical significance\nMastoid process\nBecause of the late postnatal development of the mastoid process, antenatal injuries to the region often recover spontaneously. The largest size is found in South Africans and least found in North American Indians.\nRarely, lesions can develop on the mastoid process.\n\nSee also\nmastoiditis\nmastoidectomy\n\nReferences\nThis article incorporates text in the public domain from page 141 of the 20th edition of Gray's Anatomy (1918)\n\nExternal links\n\nAnatomy photo:22:os-0403 at the SUNY Downstate Medical Center\n\"Anatomy diagram: 34257.000-1\". Roche Lexicon - illustrated navigator. Elsevier. Archived from the original on 2012-07-22.\nlesson5 at The Anatomy Lesson by Wesley Norman (Georgetown University)\nDiagram - #5 (source here)\n\"Anatomy diagram: 34257.000-1\". Roche Lexicon - illustrated navigator. Elsevier. Archived from the original on 2012-07-22.","113":"Mastoiditis is the result of an infection that extends to the air cells of the skull behind the ear.  Specifically, it is an inflammation of the mucosal lining of the mastoid antrum and mastoid air cell system inside the mastoid process. The mastoid process is the portion of the temporal bone of the skull that is behind the ear. The mastoid process contains open, air-containing spaces. Mastoiditis is usually caused by untreated acute otitis media (middle ear infection) and used to be a leading cause of child mortality. With the development of antibiotics, however, mastoiditis has become quite rare in developed countries where surgical treatment is now much less frequent and more conservative, unlike former times.\nThere is no evidence that the drop in antibiotic prescribing for otitis media has increased the incidence of mastoiditis, raising the possibility that the drop in reported cases is due to a confounding factor such as childhood immunizations against Haemophilus and Streptococcus. Untreated, the infection can spread to surrounding structures, including the brain, causing serious complications. While the use of antibiotics has reduced the incidence of mastoiditis, the risk of masked mastoiditis, a subclinical infection without the typical findings of mastoiditis has increased with the inappropriate use of antibiotics and the emergence of multidrug-resistant bacteria.\n\nSigns and symptoms\nSome common symptoms and signs of mastoiditis include pain, tenderness, and swelling in the mastoid region. There may be ear pain (otalgia), and the ear or mastoid region may be red (erythematous). Fever or headaches may also be present. Infants usually show nonspecific symptoms, including anorexia, diarrhea, or irritability. Drainage from the ear occurs in more serious cases often manifests as brown discharge on the pillowcase upon waking.\n\nPathophysiology\nThe pathophysiology of mastoiditis is straightforward: bacteria spread from the middle ear to the mastoid air cells, where the inflammation causes damage to the bony structures. Streptococcus pneumoniae, Streptococcus pyogenes, Staphylococcus aureus, Haemophilus influenzae, and Moraxella catarrhalis are the most common organisms recovered in acute mastoiditis. Organisms that are rarely found are Pseudomonas aeruginosa and other Gram-negative aerobic bacilli, and anaerobic bacteria. P. aeruginosa, Enterobacteriaceae, S. aureus and anaerobic bacteria (Prevotella, Bacteroides, Fusobacterium, and Peptostreptococcus spp.) are the most common isolates in chronic mastoiditis. Rarely, Mycobacterium species can also cause the infection. Some mastoiditis is caused by cholesteatoma, which is a sac of keratinizing squamous epithelium in the middle ear that usually results from repeated middle-ear infections. If left untreated, the cholesteatoma can erode into the mastoid process, producing mastoiditis, as well as other complications.\n\nDiagnosis\nThe diagnosis of mastoiditis is clinical\u2014based on the medical history and physical examination. Imaging studies provide additional information; The standard method of diagnosis is via MRI scan although a CT scan is a common alternative as it gives a clearer and more useful image to see how close the damage may have gotten to the brain and facial nerves. Planar (2-D) X-rays are not as useful. If there is drainage, it is often sent for culture, although this will often be negative if the patient has begun taking antibiotics. Exploratory surgery is often used as a last resort method of diagnosis to see the mastoid and surrounding areas.\n\nTreatment\nIf ear infections are treated in a reasonable amount of time, the antibiotics will usually cure the infection and prevent its spread. For this reason, mastoiditis is rare in developed countries. Most ear infections occur in infants as the eustachian tubes are not fully developed and don't drain readily.\nIn all developed countries with up-to-date modern healthcare the primary treatment for mastoiditis is administration of intravenous antibiotics. Initially, broad-spectrum antibiotics are given, such as ceftriaxone. As culture results become available, treatment can be switched to more specific antibiotics directed at the eradication of the recovered aerobic and anaerobic bacteria. Long-term antibiotics may be necessary to completely eradicate the infection. If the condition does not quickly improve with antibiotics, surgical procedures may be performed (while continuing the medication). The most common procedure is a myringotomy, a small incision in the tympanic membrane (eardrum), or the insertion of a tympanostomy tube into the eardrum. These serve to drain the pus from the middle ear, helping to treat the infection. The tube is extruded spontaneously after a few weeks to months, and the incision heals naturally. If there are complications, or the mastoiditis does not respond to the above treatments, it may be necessary to perform a mastoidectomy: a procedure in which a portion of the bone is removed and the infection drained.\n\nPrognosis\nWith prompt treatment, it is possible to cure mastoiditis. Seeking medical care early is important. However, it is difficult for antibiotics to penetrate to the interior of the mastoid process and so it may not be easy to cure the infection; it also may recur. Mastoiditis has many possible complications, all connected to the infection spreading to surrounding structures. Hearing loss is likely, or inflammation of the labyrinth of the inner ear (labyrinthitis) may occur, producing vertigo and an ear ringing may develop along with the hearing loss, making it more difficult to communicate. The infection may also spread to the facial nerve (cranial nerve VII), causing facial-nerve palsy, producing weakness or paralysis of some muscles of facial expression, on the same side of the face. Other complications include Bezold's abscess, an abscess (a collection of pus surrounded by inflamed tissue) behind the sternocleidomastoid muscle in the neck, or a subperiosteal abscess, between the periosteum and mastoid bone (resulting in the typical appearance of a protruding ear). Serious complications result if the infection spreads to the brain. These include meningitis (inflammation of the protective membranes surrounding the brain), epidural abscess (abscess between the skull and outer membrane of the brain), dural venous thrombophlebitis (inflammation of the venous structures of the brain), or brain abscess.\n\nEpidemiology\nIn the United States and other developed countries, the incidence of mastoiditis is quite low, around 0.004%, although it is higher in developing countries.  The condition most commonly affects children aged from two to thirteen months, when ear infections most commonly occur. Males and females are equally affected.\n\nSee also\nthe case of Simon Guggenheim's son\nPete Browning\n\nReferences\nFurther reading\nDurand, Marlene & Joseph, Michael. (2001). Infections of the Upper Respiratory Tract. In Eugene Braunwald, Anthony S. Fauci, Dennis L. Kasper, Stephen L. Hauser, Dan L. Longo, & J. Larry Jameson (Eds.), Harrison's Principles of Internal Medicine (15th Edition), p. 191. New York: McGraw-Hill\nCummings CW, Flint PW, Haughey BH, et al. Otolaryngology: Head & Neck Surgery. 4th ed. St Louis, Mo; Mosby; 2005:3019\u20133020.\nMastoiditis E Medicine\n\n\n== External links ==","114":"Meclizine, sold under the brand name Bonine, among others, is an antihistamine used to treat motion sickness and dizziness (vertigo). It is taken by mouth. Effects generally begin in an hour and last for up to a day.\nCommon side effects include sleepiness and dry mouth. Serious side effects may include allergic reactions. Use in pregnancy appears safe, but has not been well studied; use in breastfeeding is of unclear safety. It is believed to work in part by anticholinergic and antihistamine mechanisms.\nMeclizine was patented in 1951 and came into medical use in 1953. It is available as a generic medication and often over the counter. In 2021, it was the 136th most commonly prescribed medication in the United States, with more than 4 million prescriptions.\n\nMedical uses\nMeclizine is used to treat symptoms of motion sickness.\n\nMotion sickness\nMeclizine is effective in inhibiting nausea, vomiting, and dizziness caused by motion sickness.\nThe drug is safe for treating nausea in pregnancy and is a first-line therapy for this use. Meclizine may not be strong enough for especially sickening motion stimuli, and second-line defenses should be tried in those cases.\n\nVertigo\nMeclizine may be used to treat vertigo, such as in those with M\u00e9ni\u00e8re's disease.\n\nSide effects\nSome common side effects such as drowsiness, dry mouth, and tiredness may occur. Meclizine has been shown to have fewer dry mouth side effects than the traditional treatment for motion sickness, transdermal scopolamine. A very serious allergic reaction to this drug is unlikely, but immediate medical attention should be sought if it occurs. Symptoms of a serious allergic reaction may include rash, itching, swelling, severe dizziness, and trouble breathing.\n\nPharmacology\nPharmacodynamics\nMeclizine is an antagonist at H1 receptors (Ki = 250 nM). It possesses anticholinergic, central nervous system depressant, and local anesthetic effects. Its antiemetic and antivertigo effects are not fully understood, but its central anticholinergic properties are partially responsible. The drug depresses labyrinth excitability and vestibular stimulation, and it may affect the medullary chemoreceptor trigger zone. The drug has been shown to reduce the magnitude of the vestibulo-ocular reflex in healthy volunteers. At the same time the drug was found to have only a small (and statistically insignificant) effect on the motion sensitivity of the utricles. Much as motion sickness arises from a discrepancy between multiple senses, meclizine most likely affects a wide array of sensory mechanisms related to self-motion while leaving the core vestibular response intact.\nMeclizine also has been reported to be a weak dopamine antagonist at D1-like and D2-like receptors but it does not cause catalepsy in mice, perhaps because of its anticholinergic activity. The drug does not effect dopamine or serotonin reuptake.\n\nPharmacokinetics\nMeclizine reaches peak plasma concentration in about 1.5 hours and has an elimination half-life of 5-6 hours. Despite its relatively short half-life, the drug is reported to remain effective for motion sickness for 12 - 24 hours. Meclizine has low bioavailability (22\u201332%) and a delayed onset to action in part due to its poor solubility in water (0.1 mg\/ml) and gastrointestinal fluid. In children it has been found that taking meclizine with food increases its bioavailability slightly. It is metabolized in the liver by the CYP2D6 enzyme. Ten metabolites have been identified. In rats, the main metabolite is norchlorcyclizine, which distributes extensively through body tissue.\n\nChemistry\nMeclizine is a first-generation antihistamine (nonselective H1 antagonist) of the piperazine class. It is structurally and pharmacologically similar to buclizine, cyclizine, and hydroxyzine.\n\nSynthesis\n(4-Chlorophenyl)-phenylmethanol is halogenated with thionyl chloride before adding acetylpiperazine. The acetyl group is cleaved with diluted sulfuric acid. An N-alkylation of the piperazine ring with 3-methylbenzylchloride completes the synthesis.\n\nAlternatively, the last step can be replaced by a reductive N-alkylation with 3-methylbenzaldehyde. The reductive agent is hydrogen, and Raney nickel is used as a catalyst.\n\nMeclizine is obtained and used as a racemate, a 1:1 mixture of the two stereoisomers. Drug forms contain the racemic dihydrochloride.\n\nSociety and culture\nBrand names\nMeclizine is an international nonproprietary name.\nIt is sold under the brand names Bonine, Bonamine, Antivert, Postafen, Sea Legs, and Dramamine II (Less Drowsy Formulation). Emesafene is a combination of meclizine (1\/3) and pyridoxine (2\/3). In Canada, Antivert Tab was a combination of meclizine and nicotinic acid.\n\n\n== References ==","115":"The medial vestibular nucleus (Schwalbe nucleus) is one of the vestibular nuclei. It is located in the medulla oblongata.\nLateral vestibulo-spinal tract (lateral vestibular nucleus \u201cDeiters\u201d)- via ventrolateral medulla and spinal cord to ventral funiculus (lumbo-sacral segments). ..Ipsilaterally for posture\nMedial vestibulo-spinal tract (medial, lateral, inferior, vestibular nuclei), bilateral projection via descending medial longitudinal fasciculus to cervical segments. DESCENDING MLF..Bilaterally for head\/neck\/eye movements\nIt is one of the nuclei that corresponds to CN VIII, corresponding to the vestibular nerve, which joins with the cochlear nerve.\nIt receives its blood supply from the Posterior Inferior Cerebellar Artery, which is compromised in the lateral medullary syndrome.\n\nSee also\nVestibular nerve\nVestibular system\n\nReferences\nThis article incorporates text in the public domain from the 20th edition of Gray's Anatomy (1918)\n\nExternal links\nhttps:\/\/web.archive.org\/web\/20111109232230\/http:\/\/www.neuroanatomy.wisc.edu\/virtualbrain\/BrainStem\/13VNAN.html","116":"Medical Subject Headings (MeSH) is a comprehensive controlled vocabulary for the purpose of indexing journal articles and books in the life sciences. It serves as a thesaurus that facilitates searching. Created and updated by the United States National Library of Medicine (NLM), it is used by the MEDLINE\/PubMed article database and by NLM's catalog of book holdings. MeSH is also used by ClinicalTrials.gov registry to classify which diseases are studied by trials registered in ClinicalTrials.\nMeSH was introduced in the 1960s, with the NLM's own index catalogue and the subject headings of the Quarterly Cumulative Index Medicus (1940 edition) as precursors. The yearly printed version of MeSH was discontinued in 2007; MeSH is now available only online. It can be browsed and downloaded free of charge through PubMed. Originally in English, MeSH has been translated into numerous other languages and allows retrieval of documents from different origins.\n\nStructure\nMeSH vocabulary is divided into four types of terms. The main ones are the \"headings\" (also known as MeSH headings or descriptors), which describe the subject of each article (e.g., \"Body Weight\", \"Brain Edema\" or \"Critical Care Nursing\"). Most of these are accompanied by a short description or definition, links to related descriptors, and a list of synonyms or very similar terms (known as entry terms). MeSH contains approximately 30,000 entries (as of 2022) and is updated annually to reflect changes in medicine and medical terminology. MeSH terms are arranged in alphabetic order and in a hierarchical structure by subject categories with more specific terms arranged beneath broader terms. When we search for a MeSH term, the most specific MeSH terms are automatically included in the search. This is known as the extended search or explode of that MeSH term. This additional information and the hierarchical structure (see below) make the MeSH essentially a thesaurus, rather than a plain subject headings list.\nThe second type of term, MeSH subheadings or qualifiers (see below), can be used with MeSH terms to more completely describe a particular aspect of a subject, such as adverse, diagnostic or genetic effects. For example, the drug therapy of asthma is displayed as asthma\/drug therapy.\nThe remaining two types of term are those that describe the type of material that the article represents (publication types), and supplementary concept records (SCR) which describes substances such as chemical products and drugs that are not included in the headings (see below as \"Supplements\").\n\nDescriptor hierarchy\nThe descriptors or subject headings are arranged in a hierarchy. A given descriptor may appear at several locations in the hierarchical tree. The tree locations carry systematic labels known as tree numbers, and consequently one descriptor can carry several tree numbers. For example, the descriptor \"Digestive System Neoplasms\" has the tree numbers C06.301 and C04.588.274; C stands for Diseases, C06 for Digestive System Diseases and C06.301 for Digestive System Neoplasms; C04 for Neoplasms, C04.588 for Neoplasms By Site, and C04.588.274 also for Digestive System Neoplasms. The tree numbers of a given descriptor are subject to change as MeSH is updated. Every descriptor also carries a unique alphanumerical ID that will not change.\n\nDescriptions\nMost subject headings come with a short description or definition. See the MeSH description for diabetes type 2 as an example. The explanatory text is written by the MeSH team based on their standard sources if not otherwise stated. References are mostly encyclopaedias and standard textbooks of the subject areas. References for specific statements in the descriptions are not given; instead, readers are referred to the bibliography.\n\nQualifiers\nIn addition to the descriptor hierarchy, MeSH contains a small number of standard qualifiers (also known as subheadings), which can be added to descriptors to narrow down the topic. For example, \"Measles\" is a descriptor and \"epidemiology\" is a qualifier; \"Measles\/epidemiology\" describes the subheading of epidemiological articles about Measles. The \"epidemiology\" qualifier can be added to all other disease descriptors. Not all descriptor\/qualifier combinations are allowed since some of them may be meaningless. In all there are 83 different qualifiers.\n\nSupplements\nIn addition to the descriptors, MeSH also contains some 318,000 supplementary concept records. These do not belong to the controlled vocabulary as such; instead they enlarge the thesaurus and contain links to the closest fitting descriptor to be used in a MEDLINE search. Many of these records describe chemical substances.\n\nUse in Medline\/PubMed\nIn MEDLINE\/PubMed, every journal article is indexed with about 10\u201315 subject headings, subheadings and supplementary concept records, with some of them designated as major and marked with an asterisk, indicating the article's major topics. When performing a MEDLINE search via PubMed, entry terms are automatically translated into (i.e., mapped to) the corresponding descriptors with a good degree of reliability; it is recommended to check the 'Details tab' in PubMed to see how a search formulation was translated. By default, a search for a descriptor will include all the descriptors in the hierarchy below the given one. PubMed does not apply automatic mapping of the term in the following circumstances: by writing the quoted phrase (e.g. \"kidney allograft\"), when truncated on the asterisk (e.g. kidney allograft *), and when looking with field labels (e.g. Cancer [ti]).\n\nUse at ClinicalTrials.gov\nAt ClinicalTrials.gov, each trial has keywords that describe the trial. The ClinicalTrials.gov team assigns each trial two sets of MeSH terms. One set is for the conditions studied by the trial and the other for the set of interventions used in the trial. The XML file that can be downloaded for each trial contains these MeSH keywords. The XML file also has a comment that says: \"the assignment of MeSH keywords is done by imperfect algorithm\".\n\nCategories\nThe top-level categories in the MeSH descriptor hierarchy are:\n\nAnatomy [A]\nOrganisms [B]\nDiseases [C]\nChemicals and Drugs [D]\nAnalytical, Diagnostic and Therapeutic Techniques, and Equipment [E]\nPsychiatry and Psychology [F]\nPhenomena and Processes [G]\nDisciplines and Occupations [H]\nAnthropology, Education, Sociology and Social Phenomena [I]\nTechnology, Industry, and Agriculture [J]\nHumanities [K]\nInformation Science [L]\nNamed Groups [M]\nHealth Care [N]\nPublication Characteristics [V]\nGeographicals [Z]\n\nSee also\nMedical classification\nMedical literature retrieval\n\nReferences\nExternal links\n\nMedical Subject Heading Home provided by National Library of Medicine, National Institutes of Health (U.S.)\nMeSH  tutorials\nAutomatic Term Mapping\nBrowsing MeSH:\nEntrez\nMeSH Browser\nVisual MeSH Browser mapping drug-disease relationships in research\nReference.MD\nof qualifiers \u2013 2009","117":"A medical specialty is a branch of medical practice that is focused on a defined group of patients, diseases, skills, or philosophy. Examples include those branches of medicine that deal exclusively with children (paediatrics), cancer (oncology), laboratory medicine (pathology), or primary care (family medicine). After completing medical school or other basic training, physicians or surgeons and other clinicians usually further their medical education in a specific specialty of medicine by completing a multiple-year residency to become a specialist.\n\nHistory of medical specialization\nTo a certain extent, medical practitioners have long been specialized. According to Galen, specialization was common among Roman physicians. The particular system of modern medical specialties evolved gradually during the 19th century. Informal social recognition of medical specialization evolved before the formal legal system. The particular subdivision of the practice of medicine into various specialties varies from country to country, and is somewhat arbitrary.\n\nClassification of medical specialization\nMedical specialties can be classified along several axes. These are:\n\nSurgical or internal medicine\nAge range of patients\nDiagnostic or therapeutic\nOrgan-based or technique-based\nThroughout history, the most important has been the division into surgical and internal medicine specialties. The surgical specialties are those in which an important part of diagnosis and treatment is achieved through major surgical techniques. The internal medicine specialties are the specialties in which the main diagnosis and treatment is never major surgery. In some countries, anesthesiology is classified as a surgical discipline, since it is vital in the surgical process, though anesthesiologists never perform major surgery themselves.\nMany specialties are organ-based. Many symptoms and diseases come from a particular organ. Others are based mainly around a set of techniques, such as radiology, which was originally based around X-rays.\nThe age range of patients seen by any given specialist can be quite variable. Pediatricians handle most complaints and diseases in children that do not require surgery, and there are several subspecialties (formally or informally) in pediatrics that mimic the organ-based specialties in adults. Pediatric surgery may or may not be a separate specialty that handles some kinds of surgical complaints in children.\nA further subdivision is the diagnostic versus therapeutic specialties. While the diagnostic process is of great importance in all specialties, some specialists perform mainly or only diagnostic examinations, such as pathology, clinical neurophysiology, and radiology. This line is becoming somewhat blurred with interventional radiology, an evolving field that uses image expertise to perform minimally invasive procedures.\n\nSpecialties that are common worldwide\nList of specialties recognized in the European Union and European Economic Area\nThe European Union publishes a list of specialties recognized in the European Union, and by extension, the European Economic Area. There is substantial overlap between some of the specialties and it is likely that for example \"Clinical radiology\" and \"Radiology\" refer to a large degree to the same pattern of practice across Europe.\n\nList of North American medical specialties and others\nIn this table, as in many healthcare arenas, medical specialties are organized into the following groups:\n\nSurgical specialties focus on manually operative and instrumental techniques to treat disease.\nMedical specialties that focus on the diagnosis and non-surgical treatment of disease.\nDiagnostic specialties focus more purely on diagnosis of disorders.\n\nSalaries\nAccording to the 2022 Medscape Physician Compensation Report, physicians on average earn $339K annually. Primary care physicians earn $260K annually while specialists earned $368K annually.\nThe table below details the average range of salaries for physicians in the US of medical specialties:\n\nSpecialties by country\nAustralia and New Zealand\nThere are 15 recognised specialty medical Colleges in Australia. The majority of these are Australasian Colleges and therefore also oversee New Zealand specialist doctors. These Colleges are:\n\nIn addition, the Royal Australasian College of Dental Surgeons supervises training of specialist medical practitioners specializing in Oral and Maxillofacial Surgery in addition to its role in the training of dentists. There are approximately 260 faciomaxillary surgeons in Australia.\nThe Royal New Zealand College of General Practitioners is a distinct body from the Australian Royal Australian College of General Practitioners. There are approximately 5100 members of the RNZCGP.\nWithin some of the larger Colleges, there are sub-faculties, such as: Australasian Faculty of Rehabilitation Medicine Archived 2014-12-11 at the Wayback Machine within the Royal Australasian College of Physicians\nThere are some collegiate bodies in Australia that are not officially recognised as specialities by the Australian Medical Council but have a college structure for members, such as: Australasian College of Physical Medicine\nThere are some collegiate bodies in Australia of Allied Health non-medical practitioners with specialisation. They are not recognised as medical specialists, but can be treated as such by private health insurers, such as: Australasian College of Podiatric Surgeons\n\nCanada\nSpecialty training in Canada is overseen by the Royal College of Physicians and Surgeons of Canada and the College of Family Physicians of Canada. For specialists working in the province of Quebec, the Coll\u00e8ge des m\u00e9decins du Qu\u00e9bec also oversees the process.\n\nGermany\nIn Germany these doctors use the term Facharzt.\n\nIndia\nSpecialty training in India is overseen by the Medical Council of India, responsible for recognition of post graduate training and by the National Board of Examinations. Education of Ayurveda in overseen by Central Council of Indian Medicine (CCIM), the council conducts UG and PG courses all over India, while Central Council of Homoeopathy does the same in the field of Homeopathy.\n\nSweden\nIn Sweden, a medical license is required before commencing specialty training. Those graduating from Swedish medical schools are first required to do a rotational internship of about 1.5 to 2 years in various specialties before attaining a medical license. The specialist training lasts 5 years.\n\nUnited States\nThere are three agencies or organizations in the United States that collectively oversee physician board certification of MD and DO physicians in the United States in the 26 approved medical specialties recognized in the country. These organizations are the American Board of Medical Specialties (ABMS) and the American Medical Association (AMA); the American Osteopathic Association Bureau of Osteopathic Specialists (AOABOS) and the American Osteopathic Association; the American Board of Physician Specialties (ABPS) and the American Association of Physician Specialists (AAPS). Each of these agencies and their associated national medical organization functions as its various specialty academies, colleges and societies.\n\nAll boards of certification now require that medical practitioners demonstrate, by examination, continuing mastery of the core knowledge and skills for a chosen specialty. Recertification varies by particular specialty between every seven and every ten years.\nIn the United States there are hierarchies of medical specialties in the cities of a region. Small towns and cities have primary care, middle sized cities offer secondary care, and metropolitan cities have tertiary care. Income, size of population, population demographics, distance to the doctor, all influence the numbers and kinds of specialists and physicians located in a city.\n\nDemography\nA population's income level determines whether sufficient physicians can practice in an area and whether public subsidy is needed to maintain the health of the population. Developing countries and poor areas usually have shortages of physicians and specialties, and those in practice usually locate in larger cities. For some underlying theory regarding physician location, see central place theory.\nThe proportion of men and women in different medical specialties varies greatly. Such sex segregation is largely due to differential application.\n\nSatisfaction and burnout\nA survey of physicians in the United States came to the result that dermatologists are most satisfied with their choice of specialty followed by radiologists, oncologists, plastic surgeons, and gastroenterologists. In contrast, primary care physicians were the least satisfied, followed by nephrologists, obstetricians\/gynecologists, and pulmonologists. Surveys have also revealed high levels of depression among medical students (25 - 30%) as well as among physicians in training (22 - 43%), which for many specialties, continue into regular practice. A UK survey conducted of cancer-related specialties in 1994 and 2002 found higher job satisfaction in those specialties with more patient contact. Rates of burnout also varied by specialty.\n\nSee also\nBranches of medicine\nInterdisciplinary sub-specialties of medicine, including\nOccupational medicine \u2013 branch of clinical medicine that provides health advice to organizations and individuals concerning work-related health and safety issues and standards. See occupational safety and health.\nDisaster medicine \u2013 branch of medicine that provides healthcare services to disaster survivors; guides medically related disaster preparation, disaster planning, disaster response and disaster recovery throughout the disaster life cycle and serves as a liaison between and partner to the medical contingency planner, the emergency management professional, the incident command system, government and policy makers.\nPreventive medicine \u2013 part of medicine engaged with preventing disease rather than curing it. It can be contrasted not only with curative medicine, but also with public health methods (which work at the level of population health rather than individual health).\nMedical genetics \u2013 the application of genetics to medicine. Medical genetics is a broad and varied field. It encompasses many different individual fields, including clinical genetics, biochemical genetics, cytogenetics, molecular genetics, the genetics of common diseases (such as neural tube defects), and genetic counseling.\nSpecialty Registrar\nFederation of National Specialty Societies of Canada\nSociety of General Internal Medicine\n\n\n== References ==","118":"Metoprolol, sold under the brand name Lopressor among others, is a medication used to treat angina and a number of conditions involving an abnormally fast heart rate. It is also used to prevent further heart problems after myocardial infarction and to prevent headaches in those with migraines. It is a selective \u03b21 receptor blocker medication. It is taken by mouth or is given intravenously.\nCommon side effects include trouble sleeping, feeling tired, feeling faint, and abdominal discomfort. Large doses may cause serious toxicity. Risk in pregnancy has not been ruled out. It appears to be safe in breastfeeding. The metabolism of metoprolol can vary widely among patients, often as a result of hepatic impairment or CYP2D6 polymorphism.\nMetoprolol was first made in 1969, patented in 1970, and approved for medical use in 1978. It is on the World Health Organization's List of Essential Medicines. It is available as a generic medication. In 2022, it was the sixth most commonly prescribed medication in the United States, with more than 65 million prescriptions.\n\nMedical uses\nMetoprolol is used for a number of conditions, including angina, acute myocardial infarction, supraventricular tachycardia, ventricular tachycardia, congestive heart failure, and prevention of migraine headaches. It is an adjunct in the treatment of hyperthyroidism.\nBoth oral and intravenous forms of metoprolol are available for administration.\nThe different salt versions of metoprolol \u2013 metoprolol tartrate and metoprolol succinate \u2013 are approved for different conditions and are not interchangeable.\nOff-label uses include supraventricular tachycardia and thyroid storm.\n\nAdverse effects\nAdverse effects, especially with higher doses, include dizziness, drowsiness, fatigue, diarrhea, unusual dreams, trouble sleeping, depression, and vision problems such as blurred vision or dry eyes. \u03b2-blockers, including metoprolol, reduce salivary flow via inhibition of the direct sympathetic innervation of the salivary glands. Metoprolol may also cause the hands and feet to feel cold. Due to the high penetration across the blood\u2013brain barrier, lipophilic beta blockers such as propranolol and metoprolol are more likely than other less lipophilic beta blockers to cause sleep disturbances such as insomnia, vivid dreams and nightmares.\nPatients should be cautious while driving or operating machinery due to its potential to cause decreased alertness.\nThere may also be an impact on blood sugar levels and it can potentially mask signs of low blood sugar.\nThe safety of metoprolol during pregnancy is not fully established.\n\nPrecautions\nMetoprolol reduces long-term mortality and hospitalisation due to worsening heart failure. A meta-analysis further supports reduced incidence of heart failure worsening in patients treated with beta-blockers compared to placebo. However, in some circumstances, particularly when initiating metoprolol in patients with more symptomatic disease, an increased prevalence of hospitalisation and mortality has been reported within the first two months of starting. Patients should monitor for swelling of extremities, fatigue, and shortness of breath. \nA Cochrane Review concluded that although metoprolol reduces the risk of atrial fibrillation recurrence, it is unclear whether the long-term benefits outweigh the risks.\nThis medicine may cause changes in blood sugar levels or cover up signs of low blood sugar, such as a rapid pulse rate. It also may cause some people to become less alert than they are normally, making it dangerous for them to drive or use machines.\n\nPregnancy and breastfeeding\nRisk for the fetus has not been ruled out, per being rated pregnancy category C in Australia, meaning that it may be suspected of causing harmful effects on the human fetus (but no malformations). It appears to be safe in breastfeeding.\n\nOverdose\nExcessive doses of metoprolol can cause bradycardia, hypotension, metabolic acidosis, seizures, and cardiorespiratory arrest. Blood or plasma concentrations may be measured to confirm a diagnosis of overdose or poisoning in hospitalized patients or to assist in a medicolegal death investigation. Plasma levels are usually less than 200 \u03bcg\/L during therapeutic administration, but can range from 1\u201320 mg\/L in overdose victims.\n\nPharmacology\nMechanism of action\nMetoprolol is a beta blocker, or an antagonist of the \u03b2-adrenergic receptors. It is specifically a selective antagonist of the \u03b21-adrenergic receptor and has no intrinsic sympathomimetic activity.\nMetoprolol exerts its effects by blocking the action of certain neurotransmitters, specifically adrenaline and noradrenaline. It does this by selectively binding to and antagonizing \u03b2-1 adrenergic receptors in the body. When adrenaline (epinephrine) or noradrenaline (norepinephrine) are released from nerve endings or secreted by the adrenal glands, they bind to \u03b2-1 adrenergic receptors found primarily in cardiac tissues such as the heart. This binding activates these receptors, leading to various physiological responses, including an increase in heart rate, force of contraction (inotropic effect), conduction speed through electrical pathways in the heart, and release of renin from the kidneys. Metoprolol competes with adrenaline and noradrenaline for binding sites on these \u03b2-1 receptors. By occupying these receptor sites without activating them, metoprolol blocks or inhibits their activation by endogenous catecholamines like adrenaline or noradrenaline.\nMetoprolol blocks \u03b21-adrenergic receptors in heart muscle cells, thereby decreasing the slope of phase 4 in the nodal action potential (reducing Na+ uptake) and prolonging repolarization of phase 3 (slowing down K+ release). It also suppresses the norepinephrine-induced increase in the sarcoplasmic reticulum (SR) Ca2+ leak and the spontaneous SR Ca2+ release, which are the major triggers for atrial fibrillation.\nThrough this mechanism of selective blockade at beta-(\u03b2)-1 receptors, metoprolol exerts the following effects:\n\nHeart rate reduction, i.e., decrease of the resting heart rate (negative chronotropic effect) and reduction of excessive elevations resulting from exercise or stress.\nReduction of the force of contraction, i.e., decrease in contractility (negative inotropic effect), which lessens how hard each heartbeat contracts.\nDecrease in cardiac output, i.e., decrease in both heart rate and contractility within myocardium cells, where beta-(\u03b2)-1 is predominantly located, overall blood output per minute lowers called cardiac output\/dysfunction, allowing decreased demands placed onto impaired hearts, reducing oxygen demand-supply mismatch.\nLowering of blood pressure.\nAntiarrhythmic effects, such as supraventricular tachycardia prevention. Metoprolol also prevents electrical wave propagation.\n\nPharmacokinetics\nMetoprolol is mostly absorbed from the intestine with an absorption fraction of 0.95. The systemic bioavailability after oral administration is approximately 50%. Less than 5% of an orally administered dose of metoprolol is excreted unchanged in urine; most of it is eliminated in metabolized form through feces via bile secretion into the intestines.\nMetoprolol undergoes extensive metabolism in the liver, mainly \u03b1-hydroxylation and O-demethylation through various cytochrome P450 enzymes such as CYP2D6 (primary), CYP3A4, CYP2B6, and CYP2C9. The primary metabolites formed are \u03b1-hydroxymetoprolol and O-demethylmetoprolol.\nMetoprolol is classified as a moderately lipophilic beta blocker. More lipophilic beta blockers tend to cross the blood\u2013brain barrier more readily, with greater potential for effects in the central nervous system as well as associated neuropsychiatric side effects. Metoprolol binds mainly to human serum albumin with an unbound fraction of 0.88. It has a large volume of distribution at steady state (3.2 L\/kg), indicating extensive distribution throughout the body.\n\nChemistry\nMetoprolol was synthesized and its activity discovered in 1969. The specific agent in on-market formulations of metoprolol is either metoprolol tartrate or metoprolol succinate, where tartrate is an immediate-release formulation and the succinate is an extended-release formulation (with 100 mg metoprolol tartrate corresponding to 95 mg metoprolol succinate).\n\nStereochemistry\nMetoprolol contains a stereocenter and consists of two enantiomers. This is a racemate, i.e. a 1:1 mixture of (R)- and the (S)-form:\n\nSociety and culture\nLegal status\nMetoprolol was approved for medical use in the United States in August 1978.\n\nEconomics\nIn the 2000s, a lawsuit was brought against the manufacturers of Toprol XL (a time-release formula version of metoprolol) and its generic equivalent (metoprolol succinate) claiming that to increase profits, lower cost generic versions of Toprol XL were intentionally kept off the market. It alleged that the pharmaceutical companies AstraZeneca AB, AstraZeneca LP, AstraZeneca Pharmaceuticals LP, and Aktiebolaget Hassle violated antitrust and consumer protection law. In a settlement by the companies in 2012, without admission to the claims, they agreed to a settlement pay-out of US$ 11 million.\n\nSport\nBecause beta blockers can be used to reduce heart rate and minimize tremors, which can enhance performance in sports such as archery, metoprolol is banned by the world anti-doping agency in some sports.\n\nReferences\nFurther reading\nDean L (2017). \"Metoprolol Therapy and CYP2D6 Genotype\". In Pratt VM, McLeod HL, Rubinstein WS, et al. (eds.). Medical Genetics Summaries. National Center for Biotechnology Information (NCBI). PMID 28520381. Bookshelf ID: NBK425389.","119":"Methylprednisolone (Depo-Medrol, Medrol, Solu-Medrol) is a synthetic glucocorticoid, primarily prescribed for its anti-inflammatory and immunosuppressive effects. It is either used at low doses for chronic illnesses or used concomitantly at high doses during acute flares. Methylprednisolone and its derivatives can be administered orally or parenterally.\nRegardless of route of administration, methylprednisolone integrates systemically as exhibited by its effectiveness to quickly reduce inflammation during acute flares. It is associated with many adverse reactions that require tapering off the drug as soon as the disease is under control. Serious side effects include iatrogenic Cushing's syndrome, hypertension, osteoporosis, diabetes, infection, and skin atrophy.\nChemically, methylprednisolone is a synthetic pregnane steroid hormone derived from hydrocortisone and prednisolone. It belongs to a class of synthetic glucocorticoids and more generally, corticosteroids. It acts as a mineralocorticoid and glucocorticoid receptor agonist. In comparison to other exogenous glucocorticoids, methylprednisolone has a higher affinity to glucocorticoid receptors than to mineralocorticoid receptors.\nGlucocorticoid's name was derived after the discovery of their involvement in regulating carbohydrate metabolism. The cellular functions of glucocorticoids, such as methylprednisolone, are now understood to regulate homeostasis, metabolism, development, cognition, and inflammation. They play a critical role in adapting and responding to environmental, physical and emotional stress.\nMethylprednisolone was first synthesized and manufactured by The Upjohn Company (now Viatris) and FDA approved in the United States in October 1957. In 2021, it was the 168th most commonly prescribed medication in the United States, with more than 3 million prescriptions. Methylprednisolone is also on the World Health Organization's List of Essential Medicines for its effects against lymphoid leukemia.\n\nMedical uses\nThe primary use of methylprednisolone is to suppress inflammatory and immune responses. Methylprednisolone achieves this primarily by regulating the number and function of leukocytes, cytokines and chemokines. Its widespread inflammatory control is conducive in use across multiple disorders regardless of pathology. Methylprednisolone is commonly prescribed as short-term therapy for acute flares, as seen with acute gouty arthritis. It can be prescribed during on-going therapy in lower doses contingent upon monitorization of adverse effects. Dosage strength and formulation are optimized per medical use.\n\nAsthma\nIn 2001\u20132002, 11.4% of patients diagnosed with asthma and seen at an outpatient visit were prescribed oral corticosteroids as a long-term control therapy. The National Asthma Education and Prevention Program (NAEPP) indicates systemic methylprednisolone in both short and long term therapies to quickly control and to suppress persistent asthma, respectively. For exacerbations that result in a visit to the Emergency Department (ED), oral methylprednisolone is preferred over intravenous administration, unless there are issues with adherence or vomiting. Oral methylprednisolone is less invasive and studies have shown that equivalent efficacy compared to intravenous methylprednisolone. Dosage above 60\u201380 mg\/day or 2 mg\/kg\/day is not recommended as it has not been shown to alter pulmonary function, rate of admission, or length of stay in the hospital compared to lower doses. Following ED discharge, it is advised to prescribed a five-day course of methylprednisolone to decrease the probability of relapse or withdrawal symptoms.\n\nRheumatic diseases\nMethylprednisolone is used to treat several rheumatic diseases, such as Systemic Lupus Erythematosus (SLE) and Rheumatoid Arthritis (RA). Methylprednisolone dosage and administration for these diseases is highly variable due to varied pathophysiology between the diseases and within patients diagnosed with a given disease. In Lupus Nephritis, a common manifestation of SLE, patients are often prescribed methylprednisolone concomitantly with immunosuppressants. Severe manifestations are often treated with Cyclophosphamide or Rituximab and three doses of methylprednisolone IV-pulse treatment (as recommended by ACR guidelines) prior to switching to oral prednisolone and azathioprine for maintenance.\nIntra-articular corticosteroid injections (IACI) are a second line therapy to relieve joint pain resulting from rheumatoid arthritis. It is most commonly injected into the joints of the knees and shoulders. Although the injection is local, studies have shown systemic absorption as evidenced by beneficial effects on distant joints. In an attempt to minimize HPA suppression, FDA guidelines have restricted IACIs to three per year, with a minimum of 30 days in between injections.\n\nPrimary or secondary adrenocortical insufficiency\nMethylprednisolone is not typically recommended for primary or secondary adrenocortical insufficiency compared to other corticosteroids which have a higher affinity for mineralocorticoid receptor and salt retaining properties.\n\nLabeled indications\nThe labeled indications below are categorized by route of administration then by medical discipline.\n\nOral methylprednisolone\nAllergy and immunology: angioneurotic edema, asthma, urticaria, seasonal or perennial allergic rhinitis, drug hypersensitivity reactions, and serum sickness.\nDermatology: toxic epidermal necrolysis, atopic dermatitis, contact dermatitis, pemphigus, erythema multiforme, Steven-Johnson syndrome, bullous dermatitis herpetiformis, severe seborrheic dermatitis, exfoliative dermatitis, mycosis fungoides, and severe psoriasis.\nEndocrinology: congenital adrenal hyperplasia, hypercalcemia associated with cancer, nonsuppurative thyroiditis, and primary or secondary adrenocortical insufficiency.\nGastroenterology: inflammatory bowel disease and ulcerative colitis.\nHematology: acquired (autoimmune) hemolytic anemia, idiopathic thrombocytopenic purpura, secondary thrombocytopenia, erythroblastopenia, leukemia, lymphoma and congenital (erythroid) hypoplastic anemia.\nPulmonary: aspiration pneumonitis, chronic beryllium disease, eosinophilic pneumonia, symptomatic sarcoidosis, and pulmonary tuberculosis in conjunction with antituberculosis chemotherapy.\nNephrology: nephrotic syndrome, idiopathic type or secondary to lupus nephritis.\nNeurology: multiple sclerosis.\nOphthalmology: scleritis, retinal vasculitis, uveitis, choroiditis, iritis, iridocyclitis, keratitis, optic neuritis, allergic conjunctivitis, allergic corneal marginal ulcers, herpes zoster ophthalmicus, sympathetic ophthalmia, and chorioretinitis.\nRheumatology: rheumatoid arthritis, rheumatic carditis, acute gouty arthritis, ankylosing spondylitis, dermatomyositis and polymyositis, psoriatic arthritis, systemic lupus erythematosus, acute and subacute bursitis, synovitis of osteoarthritis, post-traumatic osteoarthritis, and epicondylitis.\nMiscellaneous: trichinosis with neurologic or myocardial involvement.\n\nParenteral methylprednisolone\nIntra-articular or soft tissue injections: acute gouty arthritis, acute and subacute bursitis, acute tenosynovitis, epicondylitis, and synovitis of osteoarthritis.\nIntralesional injections: alopecia areata, discoid lupus erythematosus, keloids, granuloma annulare, lichen planus, lichen simplex chronicus, psoriatic plaques, necrobiosis lipoidica diabeticorum.\nIntramuscular injections are prescribed treat many of the same conditions indicated for oral administration. Intramuscular injections are administered as an alternative to oral therapy.\n\nOff-label indications\nSome of the off-label indications of methylprednisolone include acute spinal cord injury, acute respiratory distress syndrome, alcoholic hepatitis, hormonal resuscitation in cadaveric organ recovery, and chronic obstructive pulmonary disease.\n\nAvailable forms\nFootnotes:\n1Signifies varying strengths of available forms. Is not indicative of frequency nor daily cumulative dose; varies per patient and condition.\n2Benzyl alcohol should not to be used on neonates\n3Preservative free formulation\n\nContraindications\nMethylprednisolone should not be taken orally by people who have systemic fungal infections, with the exception of Depo-Medrol when administered as an intra-articular injection for localized joint conditions. Methylprednisolone is contraindicated in those with known hypersensitivity to methylprednisolone or its components Steroids should be used with caution in patients with ulcerative colitis, heart disease or hypertension, peptic ulcer, renal insufficiency, osteoporosis, myasthenia gravis, glaucoma, and diabetes. Psychic manifestations may appear while taking methylprednisolone, ranging from euphoria, insomnia, personality changes to depression. Caution is required for patients with predisposed psychoses, as psychotic tendencies may be exacerbated while taking corticosteroids.\nSolu-Medrol 40 mg dosage contains lactose monohydrate produced from cow's milk; It should not be taken by anyone with known hypersensitivity to dairy products or its components. Severe medical events have been associated with epidural administration of Solu-Medrol and Depo-Medrol, including spinal cord infarction, paraplegia, quadriplegia, cortical blindness and stroke. Intramuscular injections should not be administered to those with idiopathic thrombocytopenic purpura. Formulations of Solu-Medrol and Depo-Medrol containing benzyl alcohol are contraindicated for use in premature infants. Exposure of neural tissue to excessive amounts of benzyl alcohol has been associated with toxicity and in rare events has resulted in death.\n\nAdverse effects\nAdverse reactions may overshadow the therapeutic effects of methylprednisolone.\n\nCentral nervous system\nThere is minimal clinical diagnostic criteria to define the psychic adverse effects (PAE) associated with methylprednisolone use in patients with systemic lupus erythematosus (SLE). The prevalence varies from 1.3 to 62% of adult treated patients. The type and severity of neuropsychiatric symptoms also varies significantly between patients, with 33% of patients reporting mild to moderate PAE and 5-10% reporting severe PAE. Methylprednisolone dose and duration have been implicated in PAE development. 20 mg\/day of prednisone (16 mg\/day of methylprednisolone) is the threshold dosage for PAE development agreed upon by many studies. Short-term pulse IV therapy at high doses is associated with rapid onset of manic and hypomanic symptoms, whereas long term therapy gives rise to depressive symptoms (suicide attempts infrequent). PAE are reversible with treatment reduction or discontinuation.\n\nMetabolic and endocrine\nIatrogenic Cushing's Syndrome is a direct complication of glucocorticoid therapy, and the most common cause of exogenous Cushing's Syndrome. Clinical features of Cushing's Syndrome is inclusive of many adverse effects in glucocorticoid therapy. Traditional symptoms include weight gain, myopathy, osteoporosis, increased risk of infection, hypertension and psychological effect. Fat deposition is centralized on the trunk, in between shoulders (\"buffalo hump\"), and on the face (\"moon face\"). Patient education and provider monitoring is the first step in recognizing and diagnosing Iatrogenic Cushing's Syndrome. Exogenous glucocorticoids suppress adrenocorticotropic hormone (ATCH) production, which can be verified by AM biochemical analysis. The onset of side effects varies; neuropsychiatric symptoms can arise within a few hours, while osteoporosis would take months to develop.\nThe metabolic effects of taking methylprednisolone involve the continuous breakdown of proteins for gluconeogenesis increase necessity for insulin. This results in hyperlipidemia, weight gain, myopathy that may prompt a patient to cease treatment.\n\nInfections\nThe Immunodeficiency section tabulates known pathogens of concern in glucocorticoid induced immunodeficiency.\n\nMusculoskeletal\nOsteoporosis is a type of bone disease characterized by a loss of bone density, mass and architecture that leaves a patient susceptible to fractures. The World Health Organization (WHO) defines osteoporosis in caucasian postmenopausal women as a bone mineral density (BMD) and a T-score of -2.5 or less. The prevalence of osteoporosis in patients with SLE varies geographically and some attribute it to BMD and T-score diagnostic appropriateness. British 10.3%, Chinese 21.7%The Canadian Clinical Practice Guidelines and The American College of Rheumatology have switched to using a Z-score as a diagnostic marker for osteoporosis but have failed to find a clinical diagnostic threshold. Additionally, a UK-based study showed that BMD may underrepresent a patient with SLE, as their risk for fractures is 22% higher than the healthy individual.\nExogenous corticosteroids induce osteoporosis by increasing bone resorption and reducing bone formation. Bone loss can be pronounced within the first few months of initiating methylprednisolone with a steady decrease with chronic use. Trabecular bone loss in the lumbar spine precedes cortical bone loss in the femoral neck.\n\nExhaustive list\nAllergic: allergic or hypersensitivity reactions, anaphylactoid reaction, anaphylaxis, and urticaria.\nCardiovascular: hypertension, congestive heart failure in susceptible patients, premature atherosclerotic disease, arrhythmias, and possible hyperlipidemia.\nDermatologic: impaired wound healing, petechiae and ecchymoses, thinning of the skin, facial erythema, and increased sweating.\nEndocrine: Cushingoid features, growth suppression in children, secondary adrenocortical and pituitary unresponsiveness, menstrual irregularities, decreased carbohydrate intolerance, and latent diabetes mellitus. In patients with diabetes, increased requirements of insulin or oral hypoglycemic agents.\nFluid and electrolyte disturbances: sodium retention, fluid retention, potassium loss, hypokalemic alkalosis, or congestive heart failure in susceptible patients.\nGastrointestinal: peptic ulcer, pancreatitis, abdominal distention, and ulcerative esophagitis.\nMetabolic: protein catabolism which causes negative nitrogen balance.\nMusculoskeletal: muscle weakness, loss of muscle mass, steroid myopathy, osteoporosis, tendon rupture (especially Achilles), vertebral compression fractures, aseptic necrosis of femoral and humeral heads, and pathologic fracture of long bones.\nNeurological: increased intracranial pressure with papilledema, convulsions, vertigo, and headache.\nOphthalmic: posterior sub-capsular cataracts, increased intraocular pressure, glaucoma, and exophthalmos.\n\nWithdrawal\nFeedback of the exogenous glucocorticoids at the hypothalamic\u2013pituitary\u2013adrenal (HPA) axis inhibits the secretion of the corticotropin-releasing hormone (CRH) and the adrenocorticotropic hormone (ATCH) at the hypothalamus and pituitary glands, respectively. Prolonged suppression leads to inadequate responses to physical and emotional stresses, such as illness and trauma. Suppression of ATCH may result in adrenal hypoplasia or secondary adrenal gland atrophy within 6 weeks of methylprednisolone therapy, leaving a patient at risk for developing life-threatening adrenal insufficiency crisis. Factors that contribute to the extent of HPA axis suppression include steroid hormone potency (type of compound and route of administration), cumulative dose, duration of treatment and concomitant drug use. Any individual who has taken steroid hormones for 2+ weeks is at risk for developing HPA axis suppression. Systemic methylprednisolone risk has been marked as moderate within the class of synthetic glucocorticoids.\nConsult with your physician prior to discontinuing methylprednisolone for any reason. Abrupt termination of the drug commonly causes transient non-specific symptoms such as loss of appetite, upset stomach, vomiting, drowsiness, confusion, headache, fever, joint and muscle pain, peeling skin, and weight loss. These symptoms can be attributed to steroid withdrawal syndrome, adrenal insufficiency or disease relapse. Those who have been taking methylprednisolone as a long-term treatment may be gradually be tapered off to minimize withdrawal symptoms and potential for relapse. If symptoms are exacerbated, temporarily increasing methylprednisolone dosage has shown clinical relevancy. Studies retesting patients upon methylprednisolone withdrawal showed persistent adrenal insufficiency, with one study showing 15% after 3 years. However, there was a wide range of prevalence and lack of uniformity in the follow-up timeline.\n\nDrug interactions\nCaution is advised when taking methylprednisolone concurrently with the medications described below.\n\nEnzyme inducers\nAll drugs that fall within the class of enzyme inducers increase the clearance and decrease the half-life of methylprednisolone when co-administered. Phenobarbital, phenytoin, rifampin, carbamazepine and barbiturates, increase hepatic enzymes and rate of elimination, thus reducing the immunosuppressive effect of methylprednisolone. Increased dosages may be required to achieve desired effect of methylprednisolone.\n\nCytochrome P450 (CYP) 3A4 inhibitors\nTroleandomycin, ketoconazole, and Clarithromycin inhibit metabolism; and may decrease rate of elimination and increase half-life of methylprednisolone. Dosages should be decreased accordingly to avoid side effects. Another CYP 3A4 inhibitor, grapefruit juice, prolongs half-life of oral methylprednisolone.\n\nOral contraceptives\nOral contraceptives inhibit oxidative processes, as highlighted by its ability to decrease methylprednisolone clearance.\n\nP-glycoprotein inhibitors\nMethylprednisolone is shown to be a substrate of P-glycoprotein; its inhibition is thought to increase methylprednisone absorption and distribution. No clinical relevance has been linked.\n\nCiclosporin, tacrolimus, sirolimus (Rapamycin)\nMethylprednisolone and cyclosporin inhibit metabolism and therefore increase the likelihood of experiencing side effects associated with either of the individual drugs. In addition to known individual side effects, convulsions have been reported.\n\nCox1 inhibitors\nMethylprednisolone may increase rate of elimination with chronic high dose aspirin. Patients are susceptible to increased salicylate serum levels or salicylate toxicity upon termination of methylprednisolone. Excessive caution should be taken when prescribing methylprednisolone and aspirin to patients with hypoprothrombinemia.\n\nAnticoagulants\nAnticoagulants exhibit variable interactions; monitoring coagulation indices is recommended to achieve the desired effect.\n\nPharmacology\nMethylprednisolone is a synthetic glucocorticoid (GCs) that exhibits pleiotropic effects on a variety of physiological mechanisms. However, they have been prescribed extensively for their effects on inflammation and immunity. The effects of synthetic glucocorticoids, such as methylprednisolone, is dependent on its association with intracellular glucocorticoid receptors (GRs), and to a lesser extent, mineralocorticoid receptors (MRs). GRs are widely distributed in contrast to MRs that show a restricted tissue distribution. By this mechanism, the ligand-bound receptor translocate to the nucleus and modulate gene expression.\n\nSignal transduction\nIn the absence of endogenous or synthetic GCs, monomeric GRs are located in the cytoplasm and form multiprotein complexes with heat shock proteins (HSPs), immunophilins, and other chaperones such as src, and p23. The GR acts in a ligand-dependent manner, with the complex holding the GR in an inactive form with high specificity for the ligand. When methylprednisolone from the plasma or interstitial fluid diffuses passively across the cell membrane, it binds to the GR inducing a conformational change and GC-GR dimerization. It was previously thought that this conformational change was necessary to dissociate the multiprotein complex to allow the ligand bound receptor to translocate to the nucleus. However, recent studies have indicated that chaperones play a role in nuclear import. The now active methylprednisolone-GR complex can either transduce non-genomic changes in the cytoplasm or translocate to the nucleus and regulate transcriptional activity of target genes by direct, tethering or composite mechanisms.\n\nGenomic signaling\nGenomic mechanisms, regardless of which type, elicit responses with a slow onset and a slow dissipation. This is attributed to the duration of mRNA transcription and translation. Glucocorticoids have the ability to regulate roughly 100 to 1000 genes with specificity to cell-type.\nThree major mechanisms have described how the MP-GR complex alter gene expression by either binding to DNA or transcription modulators. One mechanism of genomic signaling occurs when the MP-GR complex directly binds to DNA sequences called glucocorticoid response elements (GREs). GREs are located in regulatory regions of target genes and mediate their transactivation or transrepression. For example, the activation of lipocortin 1 (ANAX1) negatively interferes with the production of prostaglandins and leukotrienes, known pro-inflammatory signals. Likewise, negative GREs (nGREs) are responsible for repressing genes involved in immune cell activation.\n\nPost transcriptional modifications\nPost translational modifications (PTMs) also contribute to methylprednisolone signaling and can produce genomic and non-genomic effects. The GR contains several sites for phosphorylation, sumoylation, ubiquitination, and acetylation that primarily occurs after intracellular methylprednisolone binding to the GR. PTMs modulate many functions including nuclear translocation, strength and duration of receptor signaling and cofactor interaction. A specific example is the deacetylation by histone deacetylase 2 (HDACe) was necessary for transrepression of NF-\u03baB.\n\nNon-genomic signaling\nThe mechanisms of non-genomic signaling are distinct from genomic signaling, yet mediate similar pathways and provide therapeutic relevance. These mechanisms are characterized as having a rapid onset (less than 15 minutes), because they do not rely on time-consuming transcription or translation and are not modified by inhibitors of transcription.\nMethylprednisolone induced non-genomic signaling is classified by three mechanisms: (1) cytoplasmic glucocorticoid receptor (cGR)-mediated non-genomic effects, (2) membrane-bound glucocorticoid receptor (mGR) non-genomic effects, and (3) physiochemical interactions with cellular membranes (non-specific non-genomic effects).\nProteins that dissociate from the activated GC-cGR complex, initiate intracellular transcription-independent mechanisms. It is evidence that dissociated SRC is responsible for inhibiting the release of arachidonic acid (AA) from cell membrane phospholipids. AA is required for the synthesis of inflammatory mediators (prostaglandins and leukotrienes) and thus AA inhibition mediates several important pathways such as cellular growth, metabolism and inflammation.\nPrevious studies identified mGRs in lymphoma cells, but it wasn't until 2004 that mGRs were identified in human peripheral mononuclear cells. The prevalence of mGRs ranges per cell type, with the highest concentration in B lymphocytes at up to 12.3%, up to 9.2% in monocytes, and absent from T lymphocytes. Studies have shown a positive correlation between the mGR-positive cells and disease related activity. There are no proven signaling pathways associated with mGR at this time. Some researchers hypothesize that high disease activity activates mGR expression and upon administering methylprednisolone, creates a negative feedback loop by inducing apoptosis.\nHigh concentrations of methylprednisolone intercalate in plasma and mitochondrial cellular membranes. This association changes physiochemical properties of the membrane; activating membrane proteins, altering cellular functions and ultimately influencing cation transport through the plasma membrane and stimulating the proton leak across the inner mitochondrial membrane. Hindered oxidative phosphorylation compromises ATP production, a major energy source for cellular energy metabolism and immune function. In vivo studies of Con-A stimulated thymocytes (in rats) and human immune cells that were administered high doses of methylprednisolone has been shown to inhibit respiration in a dose-dependent manner, inhibit plasma calcium and sodium uptake, and increase cytoplasmic calcium concentration. The summative process is as follows: Methylprednisolone intercalates in the plasma membrane, causes physiochemical changes, activates membrane proteins that inhibit plasma calcium and sodium uptake (mimicking an energy deficit state). ATP consumption drops (resembled by inhibited respiration), resulting in protein permeability at the inner mitochondrial membrane and uncoupling of oxidative phosphorylation. Of notable importance, DNA\/RNA synthesis was not hindered. The dependency of house keeping cells and immune cells on ATP, results in immunosuppression during ATP deficit. Specific immune functions effected by this process are cytokinesis, migration, phagocytosis, antigen processing and presenting, antibody synthesis, cytotoxicity and regulation.\n\nThe HPA\nThe activation of the hypothalamic-pituitary axis (HPA) stimulates the production of endogenous glucocorticoids within the adrenal cortex. The HPA interprets stimuli (stress, inflammation and circadian cues) and transduces a corresponding physiochemical response. Glucocorticoids released in the blood, serve as a messenger by binding to glucocorticoid receptors in a wide distribution across the body, including the HPA itself. Thus, the physiological range of GCs is monitored by the negative feedback loop GCs exert on any portion of the HPA.\n\nPharmacokinetics\nMethylprednisolone is approved for oral and parenteral administration. Methylprednisolone (Medrol) for oral administration is available in a tablet formulation in 2 mg, 4 mg, 8 mg, 16 mg or 32 mg strengths. Both methylprednisolone acetate (Depo-Medrol) and methylprednisolone succinate (Solu-Medrol) are approved for intramuscular injection. Depo-Medrol is additionally approved for intralesional, intra-articular, and soft tissue injections. Depo-Medrol is available as sterile aqueous solution in 20 mg\/mL, 40 mg\/mL, or 80 mg\/mL strengths. Solu-Medrol is the only derivative of methylprednisolone that is approved for intravenous infusion, as the sterile powder is soluble in water and can be mixed with a diluent. Strengths vary from 40 mg to 2 g.\nSynthetic glucocorticoids are similar to endogenous steroids in metabolism, but differ in affinity for glucocorticoid and mineralocorticoid receptors, affinity for protein-binding, rate of elimination, and metabolic products.\nOral methylprednisolone is readily absorbed from the gastrointestinal tract with a bioavailability of 89.9%. In contrast to endogenous GCs, methylprednisolone does not bind to the glycoprotein transcortin (corticosteroid binding globulin, CBG) but does have moderate protein binding to albumin. Thus, pharmacokinetics of methylprednisolone is linear and show no dose dependency. Patients exhibiting low albumin concentrations are at risk for adverse effects during glucocorticoid therapy. Oral methylprednisolone has a moderate distribution into tissue at 1.38L\/kg.\nMethylprednisolone is primarily eliminated by hepatic metabolism and renal excretion of metabolites; with renal excretion of unchanged methylprednisolone at only 1.3\u20139.2%. Methylprednisolone can be interconverted with methylprednisone. Hepatic metabolism is mediated by 11 beta-hydroxysteroid dehydrogenases (11[beta]-HSD) and 20-ketosteroid reductases. Methylprednisolone undergoes renal excretion of hydrophilic inactive metabolites, including 20-carboxymelthylprednisolone and 6[beta]-hydroxy-20[alpha]-hydroxymethylprednisolone.\n\nPhysical properties\nOral methylprednisolone (Medrol) and its derivatives are a white, odorless crystalline powder. Its solubility ranges from practically insoluble in water, very slightly soluble in ether, slightly soluble in acetone and chloroform to sparingly soluble in alcohol, dioxane and methanol. Methylprednisolone acetate suspension (Depo-Medrol) is a 6-methyl derivative of prednisolone that melts at 215 degrees Celsius with some decomposition. Methylprednisolone sodium succinate (Solu-Medrol) is the sodium succinate ester of methylprednisolone. Contrary to the solubilities above, methylprednisolone sodium succinate is soluble in water and alcohol, slightly soluble in acetone and insoluble in chloroform\n\nChemistry\nMethylprednisolone, or 6\u03b1-methylprednisolone, also known as 11\u03b2,17,21-trihydroxy-6\u03b1-methylpregna-1,4-diene-3,20-dione, is a synthetic pregnane steroid and a derivative of hydrocortisone (11\u03b2,17\u03b1,21-trihydroxypregn-4-ene-3,20-dione) and prednisolone (11\u03b2,17\u03b1,21-trihydroxypregn-1,4-diene-3,20-dione). A variety of methylprednisolone esters with differing characteristics exist and have been marketed for medical use. They include methylprednisolone aceponate (Advantan), methylprednisolone acetate (Depo-Medrol), methylprednisolone succinate (Solu-Medrol), and methylprednisolone suleptanate (Medrosol, Promedrol).\n\nSynthesis\nSynthetic steroids are synthesized from cholic acid and sapogenins obtained from cattle and plants, respectively.\n\nHistory\nMethylprednisolone was first synthesized and manufactured by The Upjohn Company (now Pfizer) and Food and Drug Administration (FDA) approved in the United States in October 1957. The patent has expired, and generics are available. In 2021, it was the 168th most commonly prescribed medication in the United States, with more than 3 million prescriptions.\n\nResearch\nMethylprednisolone has been a prescribed therapy amidst the COVID-19 pandemic, but there is no evidence it is either safe or effective for this purpose.\n\n\n== References ==","120":"Middle ear barotrauma (MEBT), also known to underwater divers as ear squeeze and reverse ear squeeze, is an injury caused by a difference in pressure between the external ear canal and the middle ear. It is common in underwater divers and usually occurs when the diver does not equalise sufficiently during descent or, less commonly, on ascent. Failure to equalise may be due to inexperience or eustachian tube dysfunction, which can have many possible causes. Unequalised ambient pressure increase during descent causes a pressure imbalance between the middle ear air space and the external auditory canal over the eardrum, referred to by divers as ear squeeze, causing inward stretching, serous effusion and haemorrhage, and eventual rupture. During ascent internal over-pressure is normally passively released through the eustachian tube, but if this does not happen the volume expansion of middle ear gas will cause outward bulging, stretching and eventual rupture of the eardrum known to divers as reverse ear squeeze. This damage causes local pain and hearing loss. Tympanic rupture during a dive can allow water into the middle ear, which can cause severe vertigo from caloric stimulation. This may cause nausea and vomiting underwater, which has a high risk of aspiration of vomit or water, with possibly fatal consequences.\nMiddle ear barotrauma can also be caused by shock waves and blows to the external ear, particularly in water, and large or fast changes in altitude.\n\nClassification\nDeformation stress trauma caused by externally applied (environmental) pressure differences on the middle ear.\n\nSigns and symptoms\nLocalised pain in one or both ears while the eardrums are stretched, which may be partly relieved if the eardrum ruptures, followed by longer term dull pain in the injured ears,and possible hearing loss.\n\nComplications\nUnequal pressures in the middle ears can cause alternobaric vertigo, disorientation and nausea.\nCold water ingress through a perforated eardrum can cause caloric vertigo, usually a short term effect.\nIngress of contaminated water through a perforated eardrum can cause infections of the middle ear.\nOver-vigorous attempts to equalise using the Valsalva maneuver can lead to inner ear barotrauma\nTemporary or permanent hearing deficit, vertigo, or balance problems.\nInfections of the external, middle or inner ear.\n\nCauses\nAny cause of sufficiently large and rapid environmental pressure change can potentially cause barotrauma. Several commonly recognised examples are listed below.\n\nDepth changes while diving\nWhen diving, the pressure differences which cause the barotrauma are changes in hydrostatic pressure:\nThere are two components to the surrounding pressure acting on the diver: the atmospheric pressure and the water pressure. A descent of 10 metres (33 feet) in water increases the ambient pressure by an amount approximately equal to the pressure of the atmosphere at sea level. So, a descent from the surface to 10 metres (33 feet) underwater results in a doubling of the pressure on the diver. This pressure change will reduce the volume of a flexible gas-filled space by half. Boyle's law describes the relationship between the volume of the gas space and the pressure in the gas.\nBarotraumas of descent are caused by preventing the free change of volume of the gas in a closed space in contact with the diver, resulting in a pressure difference between the tissues and the gas space, and the unbalanced force due to this pressure difference causes deformation of the tissues resulting in cell rupture.\nBarotraumas of ascent are also caused when the free change of volume of the gas in a closed space in contact with the diver is prevented. In this case the pressure difference causes a resultant tension in the surrounding tissues which exceeds their tensile strength.\n\nUse of a hyperbaric chamber\nPatients undergoing hyperbaric oxygen therapy must equalize their ears to avoid barotrauma. High risk of otic barotrauma is associated with unconscious patients.\n\nRapid decompression or pressurisation of an artificial environment\nExplosive decompression of a hyperbaric environment can produce severe barotrauma, followed by severe decompression bubble formation and other related injury. Rapid uncontrolled decompression from caissons, airlocks, pressurised aircraft, spacecraft, and pressure suits can have similar effects of decompression barotrauma.\nCollapse of a pressure resistant structure such as a submarine, submersible, or atmospheric diving suit can cause rapid compression barotrauma.\n\nRapid change of altitude\nA rapid change of altitude can cause barotrauma when internal air spaces cannot be equalised.\n\nSelf inflicted barotrauma\nExcessively strenuous efforts to equalise the ears using the Valsalva manoeuvre can overpressurise the middle ear, and can cause middle ear barotrauma. This is more likely to happen when one tube opens and the other remains blocked. When a Valsalva maneuver is performed during descent with the intention of opening the Eustachian tubes, but they do not open, intrathoracic pressure, central venous pressure, spinal fluid pressure, and inner ear pressure are raised further above ambient pressure, which increases the pressure difference between perilymph of the inner ear and the gas space of the middle ear. This can cause the round or oval window to rupture outwards, allowing leakage of perilymph into the middle ear.\n\nBlast-induced barotrauma\nAn explosive blast and explosive decompression create a pressure wave that can induce barotrauma. The difference in pressure between internal organs and the outer surface of the body causes injuries to internal organs that contain gas, such as the lungs, gastrointestinal tract, and ear.\n\nImpact over the external auditory canal\nBlows to the outer ear which seal the canal and compress the trapped gas or water can burst an eardrum or cause lesser barotrauma to the middle ear. This is a recognised hazard in several contact sports.\n\nMechanism\nThe middle ear is an air-filled space between the external and inner ears. it is separated from the outer ear canal by the eardrum, and connected to the nose and throat cavity by the Eustachian tube. Pressure in the middle ear should match the ambient pressure for normal functioning of hearing. Under-pressure equalisation is normally through periodic opening of the Eustachian tubes during swallowing and yawning, and over-pressure usually vents passively through the collapsed soft part of the tube, as the inner end of the tube is normally closed.\nMiddle ear barotrauma occurs when a pressure difference develops over the eardrum, causing bulging towards the low pressure side, stretching the tissues which in a severe case can rupture, which immediately equalises the pressure and removes the stretching forces, but leaves local trauma. Stretching of the eardrum to a lesser extent can also cause damage, including engorged blood vessels which exude serum into the surrounding tissues and cause inflammation. increased pressure difference will cause blood vessels to rupture, which may bleed into or inside of the membrane. In divers this usually occurs during descent, when the ambient pressure rises due to increasing hydrostatic pressure. Pressure on the outer side of the eardrum normally closely follows ambient pressure, and in the inner ear pressure equalises through the Eustachian tube, which must be open for gas to flow through. If the diver does not equalise sufficiently a pressure difference may develop that is large enough to damage the eardrum as described. During ascent, the convere occurs, with the internal pressure higher than external. This is usually passively released by the Eustachian tube, but in some cases it does not function correctly causing the eardrum to bulge and possibly rupture outward.\nThe pressure difference required to rupture the eardrum is thought to be approximately 100 kPA (1 bar or 10 msw).\n\nDiagnosis\nDiagnosis is by symptoms, otoscope examination and history.\n\nDifferential diagnosis\nDifferential diagnosis should consider alternative conditions which could produce the same symptoms. Depending on the actual symptoms presented, such conditions could include: otitis media, otitis externa, cerumen impaction, inner ear decompression sickness, caloric stimulation, benign paroxysmal positional vertigo (BPPV), vestibular neuronitis, M\u00e9ni\u00e8re's disease, acoustic neuroma, and possibly others.\nIf there is sensorineural hearing loss or vertigo after exposure to a large change in ambient pressure or a change of breathing gas, the possibility of concurrent barotrauma and inner ear decompression sickness (IEDCS) should be considered, because  the symptoms can be very similar, and IEDCS is treated with recompression and hyperbaric oxygen.\n\nPrevention\nAmong people playing underwater and swimming contact sports, such as water polo, underwater hockey or underwater rugby, a cap with perforated ear cups is often used, such as a water polo cap.\nThe a risk of stretched or burst eardrums, can be reduced by any of a variety of methods to let air into or out of the middle ears via the Eustachian tubes. Sometimes swallowing will open the Eustachian tubes and equalise the ears. Most of the methods are less likely than the Valsalva maneuver to cause collateral damage to the inner ear.\nThe Eustachian tubes will close completely with a pressure difference of about 3msw (10fsw) above the middle ear pressure, at which point none of the equalising maneuvers will work, and the pressure difference must be decreased to make it possible again, This implies ascending during a dive, venting some of the pressure from a hyperbaric chamber, and ascending to a higher altitude in an aircraft, which is not always practicable.\n\nTreatment and management\nIf inner ear barotrauma and decompression sickness can be excluded, treatment may include any combination of short term use of nasal decongestants, intranasal steroid sprays and antibiotics for secondary infections. Surgical repair of persistent perforation of the eardrum may be necessary.\nTreatment is in proportion to the injury, and may include education to reduce risk of repeat injury. It is often treated conservatively and usually resolves without medical intervention. Some cases are due to simple ambient pressure change and Eustachian tube dysfunction at the time, while others may be partly the consequence of a less obvious underlying condition.\nAntibiotics are not usually needed unless infection develops or the ear was exposed to contaminated water.\nMEBT may occur during pressurization for hyperbaric treatment for other conditions. If this happens, pressurization should be stopped and if necessary, reversed sufficiently to allow the Eustachian tubes to be opened more easily, and the middle ear to be cleared. Physician-prescribed oral decongestants may help. Compression should normally be aborted if equalization remains unsuccessful. In urgent clinical hyperbaric treatment, an emergency needle myringotomy or placement of tympanostomy ventilation tubes may be required. These will passively equalise the middle ear, and are effective with an unconscious person.\n\nOutcomes\nMild symptoms may resolve within 1 to 2 weeks. All symptoms should be resolved before diving or flying recommences, including healing of any perforations of the eardrum, and equalisation must be possible, with no abnormal sounds, and hearing is normal.\n\nEpidemiology\nMiddle ear barotrauma is the single most common diving disorder for which treatment is sought, at nearly 50% of all reported diving injuries. Many more milder cases may go unreported.\nA history of head and neck cancers, with associated radiation treatment, has been associated with a relatively higher incidence of MEBT, possibly due to radiation damage of the soft tissues of the Eustachian tubes or pharynx.\n\nSee also\n\n\n== References ==","121":"The middle ear is the portion of the ear medial to the eardrum, and distal to the oval window of the cochlea (of the inner ear).  \nThe mammalian middle ear contains three ossicles (malleus, incus, and stapes), which transfer the vibrations of the eardrum into waves in the fluid and membranes of the inner ear.  The hollow space of the middle ear is also known as the tympanic cavity and is surrounded by the tympanic part of the temporal bone. The auditory tube (also known as the Eustachian tube or the pharyngotympanic tube) joins the tympanic cavity with the nasal cavity (nasopharynx), allowing pressure to equalize between the middle ear and throat.\nThe primary function of the middle ear is to efficiently transfer acoustic energy from compression waves in air to fluid\u2013membrane waves within the cochlea.\n\nStructure\nOssicles\nThe middle ear contains three tiny bones known as the ossicles: malleus, incus, and stapes.  The ossicles were given their Latin names for their distinctive shapes; they are also referred to as the hammer, anvil, and stirrup, respectively. The ossicles directly couple sound energy from the eardrum to the oval window of the cochlea.  While the stapes is present in all tetrapods, the malleus and incus evolved from lower and upper jaw bones present in reptiles.\nThe ossicles are classically supposed to mechanically convert the vibrations of the eardrum into amplified pressure waves in the fluid of the cochlea (or inner ear), with a lever arm factor of 1.3. Since the effective vibratory area of the eardrum is about 14 fold larger than that of the oval window, the sound pressure is concentrated, leading to a pressure gain of at least 18.1. The eardrum is merged to the malleus, which connects to the incus, which in turn connects to the stapes. Vibrations of the stapes footplate introduce pressure waves in the inner ear. There is a steadily increasing body of evidence that shows that the lever arm ratio is actually variable, depending on frequency. Between 0.1 and 1 kHz it is approximately 2, it then rises to around 5 at 2 kHz and then falls off steadily above this frequency. The measurement of this lever arm ratio is also somewhat complicated by the fact that the ratio is generally given in relation to the tip of the malleus (also known as the umbo) and the level of the middle of the stapes. The eardrum is actually attached to the malleus handle over about a 0.5 cm distance. In addition, the eardrum itself moves in a very chaotic fashion at frequencies >3 kHz. The linear attachment of the eardrum to the malleus actually smooths out this chaotic motion and allows the ear to respond linearly over a wider frequency range than a point attachment. The auditory ossicles can also reduce sound pressure (the inner ear is very sensitive to overstimulation), by uncoupling each other through particular muscles.\nThe middle ear efficiency peaks at a frequency of around 1 kHz. The combined transfer function of the outer ear and middle ear gives humans a peak sensitivity to frequencies between 1 kHz and 3 kHz.\n\nMuscles\nThe movement of the ossicles may be stiffened by two muscles. The stapedius muscle, the smallest skeletal muscle in the body, connects to the stapes and is controlled by the facial nerve; the tensor tympani muscle is attached to the upper end of the medial surface of the handle of malleus and is under the control of the medial pterygoid nerve which is a branch of the mandibular nerve of the trigeminal nerve. These muscles contract in response to loud sounds, thereby reducing the transmission of sound to the inner ear. This is called the acoustic reflex.\n\nNerves\nOf surgical importance are two branches of the facial nerve that also pass through the middle ear space.  These are the horizontal portion of the facial nerve and the chorda tympani.  Damage to the horizontal branch during ear surgery can lead to  paralysis of the face (same side of the face as the ear).  The chorda tympani is the branch of the facial nerve that carries taste from the ipsilateral half (same side) of the tongue.\n\nFunction\nSound transfer\nOrdinarily, when sound waves in air strike liquid, most of the energy is reflected off the surface of the liquid.  The middle ear allows the impedance matching of sound traveling in air to acoustic waves traveling in a system of fluids and membranes in the inner ear.  This system should not be confused, however, with the propagation of sound as compression waves in liquid.\nThe acoustic impedance of air is about \n  \n    \n      \n        \n          Z\n          \n            1\n          \n        \n        =\n        400\n        \n        \n          P\n          a\n          \u22c5\n          s\n          \n            \/\n          \n          m\n        \n      \n    \n    {\\displaystyle Z_{1}=400\\;\\mathrm {Pa\\cdot s\/m} }\n  \n, while the impedance of cochlear fluids (\n  \n    \n      \n        \n          Z\n          \n            2\n          \n        \n        =\n        1.5\n        \u00d7\n        \n          10\n          \n            6\n          \n        \n        \n        \n          P\n          a\n          \u22c5\n          s\n          \n            \/\n          \n          m\n        \n      \n    \n    {\\displaystyle Z_{2}=1.5\\times 10^{6}\\;\\mathrm {Pa\\cdot s\/m} }\n  \n) is approximately equal to that of sea water. Because of this high impedance, only \n  \n    \n      \n        \n          \n            \n              2\n              \n                Z\n                \n                  1\n                \n              \n            \n            \n              \n                Z\n                \n                  1\n                \n              \n              +\n              \n                Z\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        0.05\n        %\n      \n    \n    {\\displaystyle {\\frac {2Z_{1}}{Z_{1}+Z_{2}}}=0.05\\%}\n  \n of incident energy could be directly transmitted from the air to cochlear fluids. \nThe middle ear's impedance matching mechanism increases the efficiency of sound transmission. Two processes are involved: \n\nArea Ratio: The area of the tympanic membrane is about 20 times larger than that of the stapes footplate in the cochlea. The forces collected over the eardrum are concentrated over a smaller area, thus increasing the pressure over the oval window.\nLever: The malleus is 1.3 times longer than the incus.\nTogether, they amplify pressure by 26 times, or about 30 dB. The actual value is around 20 dB across 200 to 10000 Hz.\nThe middle ear couples sound from air to the fluid via the oval window, using the principle of \"mechanical advantage\" in the form of the \"hydraulic principle\" and the \"lever principle\".  The vibratory portion of the tympanic membrane (eardrum) is many times the surface area of the footplate of the stapes (the third ossicular bone which attaches to the oval window); furthermore, the shape of the articulated ossicular chain is a complex lever, the long arm being the long process of the malleus, the fulcrum being the body of the incus, and the short arm being the lenticular process of the incus.  The collected pressure of sound vibration that strikes the tympanic membrane is therefore concentrated down to this much smaller area of the footplate, increasing the force but reducing the velocity and displacement, and thereby coupling the acoustic energy.\nThe middle ear is able to dampen sound conduction substantially when faced with very loud sound, by noise-induced reflex contraction of the middle-ear muscles.\n\nClinical significance\nThe middle ear is hollow in the tympanic cavity and Eustachian tube. In a high-altitude environment or on diving into water, there will be a pressure difference between the middle ear and the outside environment.  This pressure will pose a risk of bursting or otherwise damaging the tympanum (eardrum) if it is not relieved. If middle ear pressure remains low, the eardrum (tympanic membrane) may become retracted into the middle ear.  One of the functions of the Eustachian tubes that connect the middle ear to the nasopharynx is to help keep middle ear pressure the same as air pressure. The Eustachian tubes are normally pinched off at the nose end, to prevent being clogged with mucus, but they may be opened by lowering and protruding the jaw; this is why yawning or chewing helps relieve the pressure felt in the ears when on board an aircraft. Eustachian tube obstruction may result in fluid build up in the middle ear, which causes a conductive hearing loss. Otitis media is an inflammation of the middle ear.\n\nInjuries\nThe middle ear is well protected from most minor external injuries by its internal location, but is vulnerable to pressure injury (barotrauma).\n\nInfections\nRecent findings indicate that the middle ear mucosa could be subjected to human papillomavirus infection. Indeed, DNAs belonging to oncogenic HPVs, i.e., HPV16 and HPV18, have been detected in normal middle ear specimens, thereby indicating that the normal middle ear mucosa could potentially be a target tissue for HPV infection.\nInflammation of the middle ear, also known as otitis media, is a prevalent condition, especially among infants under six months old. This ailment occurs when the middle ear, typically filled with air, becomes infected or obstructed by fluid behind the eardrum. In certain cases, a surgical intervention called myringotomy and tube insertion may be necessary to alleviate the symptoms and complications associated with this condition. The risk of otitis media can be mitigated through various preventive measures, such as receiving pneumococcal and influenza vaccines, breastfeeding infants, and avoiding secondhand smoke. Furthermore, the use of analgesics plays a vital role in alleviating the pain caused by acute otitis media.\n\nDiversity and evolution\nThe middle ear of tetrapods is analogous with the spiracle of fishes, an opening from the pharynx to the side of the head in front of the main gill slits. In fish embryos, the spiracle forms as a pouch in the pharynx, which grows outward and breaches the skin to form an opening; in most tetrapods, this breach is never quite completed, and the final vestige of tissue separating it from the outside world becomes the eardrum. The inner part of the spiracle, still connected to the pharynx, forms the eustachian tube.\nIn reptiles, birds, and early fossil tetrapods, there is a single auditory ossicle, the columella which is homologous with the stapes, or \"stirrup\" of mammals. This is connected indirectly with the eardrum via a mostly cartilaginous extracolumella and medially to the inner-ear spaces via a widened footplate in the fenestra ovalis. The columella is an evolutionary derivative of the bone known as the hyomandibula in fish ancestors, a bone that supported the skull and braincase.\n\nAmphibians\nThe structure of the middle ear in living amphibians varies considerably and is often degenerate. In most frogs and toads, it is similar to that of reptiles, but in other amphibians, the middle ear cavity is often absent. In these cases, the stapes either is also missing or, in the absence of an eardrum, connects to the quadrate bone in the skull, although, it is presumed, it still has some ability to transmit vibrations to the inner ear. In many amphibians, there is also a second auditory ossicle, the operculum (not to be confused with the structure of the same name in fishes). This is a flat, plate-like bone, overlying the fenestra ovalis, and connecting it either to the stapes or, via a special muscle, to the scapula. It is not found in any other vertebrates.\n\nMammals\nMammals are unique in having evolved a three-ossicle middle-ear independently of the various single-ossicle middle ears of other land vertebrates, all during the Triassic period of geological history. Functionally, the mammalian middle ear is very similar to the single-ossicle ear of non-mammals, except that it responds to sounds of higher frequency, because these are better taken up by the inner ear (which also responds to higher frequencies than those of non-mammals). The malleus, or \"hammer\", evolved from the articular bone of the lower jaw, and the incus, or \"anvil\", from the quadrate. In other vertebrates, these bones form the primary jaw joint, but the expansion of the dentary bone in mammals led to the evolution of an entirely new jaw joint, freeing up the old joint to become part of the ear. For a period of time, both jaw joints existed together, one medially and one laterally. The evolutionary process leading to a three-ossicle middle ear was thus an \"accidental\" byproduct of the simultaneous evolution of the new, secondary jaw joint. In many mammals, the middle ear also becomes protected within a cavity, the auditory bulla, not found in other vertebrates. A bulla evolved late in time and independently numerous times in different mammalian clades, and it can be surrounded by membranes, cartilage or bone. The bulla in humans is part of the temporal bone.\nRecently found fossils such as Morganucodon show intermediary steps of middle ear evolution. A new morganucodontan-like species, Dianoconodon youngi, shows parts of the mandible (= dentary) that permit an auditory function, although these bones are still attached to the mandible.\n\nAdditional images\nSee also\nFacial canal \u2013 Hole in the temporal bone of the skull carrying the facial nerve\nHearing \u2013 Sensory perception of sound by living organisms\nExternal ear\nInner ear\n\nReferences\nExternal links\n\nPromenade Around the Cochlea - Middle ear at iurc.montp.inserm.fr","122":"Migraine (UK: , US: ) is a genetically influenced complex neurological disorder characterized by episodes of moderate-to-severe headache, most often unilateral and generally associated with nausea and light and sound sensitivity. Other characterizing symptoms may include vomiting, cognitive dysfunction, allodynia, and dizziness. Exacerbation of headache symptoms during physical activity is another distinguishing feature. Up to one-third of migraine sufferers experience aura, a premonitory period of sensory disturbance widely accepted to be caused by cortical spreading depression at the onset of a migraine attack. Although primarily considered to be a headache disorder, migraine is highly heterogenous in its clinical presentation and is better thought of as a spectrum disease rather than a distinct clinical entity. Disease burden can range from episodic discrete attacks to chronic disease.\nMigraine is believed to be caused by a mixture of environmental and genetic factors that influence the excitation and inhibition of nerve cells in the brain. An older \"vascular hypothesis\" postulated that the aura of migraine is produced by vasoconstriction and the headache of migraine is produced by vasodilation, but the vasoconstrictive mechanism has been disproven, and the role of vasodilation in migraine pathophysiology is uncertain. The accepted hypothesis suggests that multiple primary neuronal impairments lead to a series of intracranial and extracranial changes, triggering a physiological cascade that leads to migraine symptomatology.\nInitial recommended treatment for acute attacks is with over-the-counter analgesics (pain medication) such as ibuprofen and paracetamol (acetaminophen) for headache, antiemetics (anti-nausea medication) for nausea, and the avoidance of triggers. Specific medications such as triptans, ergotamines, or CGRP inhibitors may be used in those experiencing headaches that are refractory to simple pain medications. For individuals who experience four or more attacks per month, or could otherwise benefit from prevention, prophylactic medication is recommended. Commonly prescribed prophylactic medications include beta blockers like propranolol, anticonvulsants like sodium valproate, antidepressants like amitriptyline, and other off-label classes of medications. Preventive medications inhibit migraine pathophysiology through various mechanisms, such as blocking calcium and sodium channels, blocking gap junctions, and inhibiting matrix metalloproteinases, among other mechanisms. Nonpharmacological preventive therapies include nutritional supplementation, dietary interventions, sleep improvement, and aerobic exercise.\nGlobally, approximately 15% of people are affected by migraine. In the Global Burden of Disease Study, conducted in 2010, migraines ranked as the third-most prevalent disorder in the world. It most often starts at puberty and is worst during middle age. As of 2016, it is one of the most common causes of disability.\n\nSigns and symptoms\nMigraine typically presents with self-limited, recurrent severe headache associated with autonomic symptoms. About 15\u201330% of people living with migraine experience episodes with aura, and they also frequently experience episodes without aura. The severity of the pain, duration of the headache, and frequency of attacks are variable. A migraine attack lasting longer than 72 hours is termed status migrainosus. There are four possible phases to a migraine attack, although not all the phases are necessarily experienced:\n\nThe prodrome, which occurs hours or days before the headache\nThe aura, which immediately precedes the headache\nThe pain phase, also known as headache phase\nThe postdrome, the effects experienced following the end of a migraine attack\nMigraine is associated with major depression, bipolar disorder, anxiety disorders, and obsessive\u2013compulsive disorder. These psychiatric disorders are approximately 2\u20135 times more common in people without aura, and 3\u201310 times more common in people with aura.\n\nProdrome phase\nProdromal or premonitory symptoms occur in about 60% of those with migraines, with an onset that can range from two hours to two days before the start of pain or the aura. These symptoms may include a wide variety of phenomena, including altered mood, irritability, depression or euphoria, fatigue, craving for certain food(s), stiff muscles (especially in the neck), constipation or diarrhea, and sensitivity to smells or noise. This may occur in those with either migraine with aura or migraine without aura. Neuroimaging indicates the limbic system and hypothalamus as the origin of prodromal symptoms in migraine.\n\nAura phase\nAura is a transient focal neurological phenomenon that occurs before or during the headache. Aura appears gradually over a number of minutes (usually occurring over 5\u201360 minutes) and generally lasts less than 60 minutes. Symptoms can be visual, sensory or motoric in nature, and many people experience more than one. Visual effects occur most frequently: they occur in up to 99% of cases and in more than 50% of cases are not accompanied by sensory or motor effects. If any symptom remains after 60 minutes, the state is known as persistent aura.\nVisual disturbances often consist of a scintillating scotoma (an area of partial alteration in the field of vision which flickers and may interfere with a person's ability to read or drive). These typically start near the center of vision and then spread out to the sides with zigzagging lines which have been described as looking like fortifications or walls of a castle. Usually the lines are in black and white but some people also see colored lines. Some people lose part of their field of vision known as hemianopsia while others experience blurring.\nSensory aura are the second most common type; they occur in 30\u201340% of people with auras. Often a feeling of pins-and-needles begins on one side in the hand and arm and spreads to the nose\u2013mouth area on the same side. Numbness usually occurs after the tingling has passed with a loss of position sense. Other symptoms of the aura phase can include speech or language disturbances, world spinning, and less commonly motor problems. Motor symptoms indicate that this is a hemiplegic migraine, and weakness often lasts longer than one hour unlike other auras. Auditory hallucinations or delusions have also been described.\n\nPain phase\nClassically the headache is unilateral, throbbing, and moderate to severe in intensity. It usually comes on gradually and is aggravated by physical activity during a migraine attack. However, the effects of physical activity on migraine are complex, and some researchers have concluded that, while exercise can trigger migraine attacks, regular exercise may have a prophylactic effect and decrease frequency of attacks. The feeling of pulsating pain is not in phase with the pulse. In more than 40% of cases, however, the pain may be bilateral (both sides of the head), and neck pain is commonly associated with it. Bilateral pain is particularly common in those who have migraine without aura. Less commonly pain may occur primarily in the back or top of the head. The pain usually lasts 4 to 72 hours in adults; however, in young children frequently lasts less than 1 hour. The frequency of attacks is variable, from a few in a lifetime to several a week, with the average being about one a month.\nThe pain is frequently accompanied by nausea, vomiting, sensitivity to light, sensitivity to sound, sensitivity to smells, fatigue, and irritability. Many thus seek a dark and quiet room. In a basilar migraine, a migraine with neurological symptoms related to the brain stem or with neurological symptoms on both sides of the body, common effects include a sense of the world spinning, light-headedness, and confusion. Nausea occurs in almost 90% of people, and vomiting occurs in about one-third. Other symptoms may include blurred vision, nasal stuffiness, diarrhea, frequent urination, pallor, or sweating. Swelling or tenderness of the scalp may occur as can neck stiffness. Associated symptoms are less common in the elderly.\n\nSilent migraine\nSometimes, aura occurs without a subsequent headache. This is known in modern classification as a typical aura without headache, or acephalgic migraine in previous classification, or commonly as a silent migraine. However, silent migraine can still produce debilitating symptoms, with visual disturbance, vision loss in half of both eyes, alterations in color perception, and other sensory problems, like sensitivity to light, sound, and odors. It can last from 15 to 30 minutes, usually no longer than 60 minutes, and it can recur or appear as an isolated event.\n\nPostdrome\nThe migraine postdrome could be defined as that constellation of symptoms occurring once the acute headache has settled. Many report a sore feeling in the area where the migraine was, and some report impaired thinking for a few days after the headache has passed. The person may feel tired or \"hung over\" and have head pain, cognitive difficulties, gastrointestinal symptoms, mood changes, and weakness. According to one summary, \"Some people feel unusually refreshed or euphoric after an attack, whereas others note depression and malaise.\"\n\nCause\nThe underlying causes of migraines are unknown. However, they are believed to be related to a mix of environmental and genetic factors. They run in families in about two-thirds of cases and rarely occur due to a single gene defect. While migraines were once believed to be more common in those of high intelligence, this does not appear to be true. A number of psychological conditions are associated, including depression, anxiety, and bipolar disorder.\nSuccess of the surgical migraine treatment by decompression of extracranial sensory nerves adjacent to vessels suggests that migraineurs may have anatomical predisposition for neurovascular compression that may be caused by both intracranial and extracranial vasodilation due to migraine triggers. This, along with the existence of numerous cranial neural interconnections, may explain the multiple cranial nerve involvement and consequent diversity of migraine symptoms.\n\nGenetics\nStudies of twins indicate a 34\u201351% genetic influence of likelihood to develop migraine. This genetic relationship is stronger for migraine with aura than for migraines without aura. It is clear from family and populations studies, that migraine is a complex disorders, where numerous of genetic risk variants exist, and where each variant increase the risk of migraine marginally. It is also known that having several of these risk variants increase the risk by a small to moderate amount.\nSingle gene disorders that result in migraines are rare. One of these is known as familial hemiplegic migraine, a type of migraine with aura, which is inherited in an autosomal dominant fashion. Four genes have been shown to be involved in familial hemiplegic migraine. Three of these genes are involved in ion transport. The fourth is the axonal protein PRRT2, associated with the exocytosis complex. Another genetic disorder associated with migraine is CADASIL syndrome or cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy. One meta-analysis found a protective effect from angiotensin converting enzyme polymorphisms on migraine. The TRPM8 gene, which codes for a cation channel, has been linked to migraines.\nThe common forms migraine are Polygenetic, where common variants of numerous genes contributes to the predisposition for migraine.  These genes can be placed in three categories increasing the risk of migraine in general, specifically migraine with aura, or migraine without aura. Three of these genes, CALCA, CALCB, and HTR1F are already target for migraine specific treatments. Five genes are specific risk to migraine with aura, PALMD, ABO, LRRK2, CACNA1A and PRRT2, and 13 genes are specific to migraine without aura. Using the accumulated genetic risk of the common variations, into a so-called polygenetic risk, it is possible to assess e.g. the treatment response to triptans.\n\nTriggers\nMigraine may be induced by triggers, with some reporting it as an influence in a minority of cases and others the majority. Many things such as fatigue, certain foods, alcohol, and weather have been labeled as triggers; however, the strength and significance of these relationships are uncertain. Most people with migraines report experiencing triggers. Symptoms may start up to 24 hours after a trigger.\n\nPhysiological aspects\nCommon triggers quoted are stress, hunger, and fatigue (these equally contribute to tension headaches). Psychological stress has been reported as a factor by 50\u201380% of people. Migraine has also been associated with post-traumatic stress disorder and abuse. Migraine episodes are more likely to occur around menstruation. Other hormonal influences, such as menarche, oral contraceptive use, pregnancy, perimenopause, and menopause, also play a role. These hormonal influences seem to play a greater role in migraine without aura. Migraine episodes typically do not occur during the second and third trimesters of pregnancy, or following menopause.\n\nDietary aspects\nBetween 12% and 60% of people report foods as triggers.\nThere are many reports that tyramine \u2013 which is naturally present in chocolate, alcoholic beverages, most cheeses, processed meats, and other foods \u2013 can trigger migraine symptoms in some individuals. Monosodium glutamate (MSG) has been reported as a trigger for migraine, but a systematic review concluded that \"a causal relationship between MSG and headache has not been proven... It would seem premature to conclude that the MSG present in food causes headache\".\n\nEnvironmental aspects\nA 2009 review on potential triggers in the indoor and outdoor environment concluded that while there were insufficient studies to confirm environmental factors as causing migraine, \"migraineurs worldwide consistently report similar environmental triggers\".\n\nPathophysiology\nMigraine is believed to be primarily a neurological disorder, while others believe it to be a neurovascular disorder with blood vessels playing the key role, although evidence does not support this completely. Others believe both are likely important. One theory is related to increased excitability of the cerebral cortex and abnormal control of pain neurons in the trigeminal nucleus of the brainstem.\nSensitization of trigeminal pathways is a key pathophysiological phenomenon in migraine. It is debatable whether sensitization starts in the periphery or in the brain.\n\nAura\nCortical spreading depression, or spreading depression according to Le\u00e3o, is a burst of neuronal activity followed by a period of inactivity, which is seen in those with migraines with aura. There are a number of explanations for its occurrence, including activation of NMDA receptors leading to calcium entering the cell. After the burst of activity, the blood flow to the cerebral cortex in the area affected is decreased for two to six hours. It is believed that when depolarization travels down the underside of the brain, nerves that sense pain in the head and neck are triggered.\n\nPain\nThe exact mechanism of the head pain which occurs during a migraine episode is unknown. Some evidence supports a primary role for central nervous system structures (such as the brainstem and diencephalon), while other data support the role of peripheral activation (such as via the sensory nerves that surround blood vessels of the head and neck). The potential candidate vessels include dural arteries, pial arteries and extracranial arteries such as those of the scalp. The role of vasodilatation of the extracranial arteries, in particular, is believed to be significant.\n\nNeuromodulators\nAdenosine, a neuromodulator, may be involved. Released after the progressive cleavage of adenosine triphosphate (ATP), adenosine acts on adenosine receptors to put the body and brain in a low activity state by dilating blood vessels and slowing the heart rate, such as before and during the early stages of sleep. Adenosine levels have been found to be high during migraine attacks. Caffeine's role as an inhibitor of adenosine may explain its effect in reducing migraine. Low levels of the neurotransmitter serotonin, also known as 5-hydroxytryptamine (5-HT), are also believed to be involved.\nCalcitonin gene-related peptides (CGRPs) have been found to play a role in the pathogenesis of the pain associated with migraine, as levels of it become elevated during an attack.\n\nDiagnosis\nThe diagnosis of a migraine is based on signs and symptoms. Neuroimaging tests are not necessary to diagnose migraine, but may be used to find other causes of headaches in those whose examination and history do not confirm a migraine diagnosis. It is believed that a substantial number of people with the condition remain undiagnosed.\nThe diagnosis of migraine without aura, according to the International Headache Society, can be made according the \"5, 4, 3, 2, 1 criteria,\" which is as follows:\n\nFive or more attacks \u2013 for migraine with aura, two attacks are sufficient for diagnosis.\nFour hours to three days in duration\nTwo or more of the following:\nUnilateral (affecting one side of the head)\nPulsating\nModerate or severe pain intensity\nWorsened by or causing avoidance of routine physical activity\nOne or more of the following:\nNausea and\/or vomiting\nSensitivity to both light (photophobia) and sound (phonophobia)\nIf someone experiences two of the following: photophobia, nausea, or inability to work or study for a day, the diagnosis is more likely. In those with four out of five of the following: pulsating headache, duration of 4\u201372 hours, pain on one side of the head, nausea, or symptoms that interfere with the person's life, the probability that this is a migraine attack is 92%. In those with fewer than three of these symptoms, the probability is 17%.\n\nClassification\nMigraine was first comprehensively classified in 1988.\nThe International Headache Society updated their classification of headaches in 2004. A third version was published in 2018. According to this classification, migraine is a primary headache disorder along with tension-type headaches and cluster headaches, among others.\nMigraine is divided into six subclasses (some of which include further subdivisions):\n\nMigraine without aura, or \"common migraine\", involves migraine headaches that are not accompanied by aura.\nMigraine with aura, or \"classic migraine\", usually involves migraine headaches accompanied by aura. Less commonly, aura can occur without a headache, or with a nonmigraine headache. Two other varieties are familial hemiplegic migraine and sporadic hemiplegic migraine, in which a person has migraine with aura and with accompanying motor weakness. If a close relative has had the same condition, it is called \"familial\", otherwise it is called \"sporadic\". Another variety is basilar-type migraine, where a headache and aura are accompanied by difficulty speaking, world spinning, ringing in ears, or a number of other brainstem-related symptoms, but not motor weakness. This type was initially believed to be due to spasms of the basilar artery, the artery that supplies the brainstem. Now that this mechanism is not believed to be primary, the symptomatic term migraine with brainstem aura (MBA) is preferred. Retinal migraine (which is distinct from visual or optical migraine) involves migraine headaches accompanied by visual disturbances or even temporary blindness in one eye.\nChildhood periodic syndromes that are commonly precursors of migraine include cyclical vomiting (occasional intense periods of vomiting), abdominal migraine (abdominal pain, usually accompanied by nausea), and benign paroxysmal vertigo of childhood (occasional attacks of vertigo).\nComplications of migraine describe migraine headaches and\/or auras that are unusually long or unusually frequent, or associated with a seizure or brain lesion.\nProbable migraine describes conditions that have some characteristics of migraines, but where there is not enough evidence to diagnose it as a migraine with certainty (in the presence of concurrent medication overuse).\nChronic migraine is a complication of migraines, and is a headache that fulfills diagnostic criteria for migraine headache and occurs for a greater time interval. Specifically, greater or equal to 15 days\/month for longer than 3 months.\n\nAbdominal migraine\nThe diagnosis of abdominal migraine is controversial. Some evidence indicates that recurrent episodes of abdominal pain in the absence of a headache may be a type of migraine or are at least a precursor to migraines. These episodes of pain may or may not follow a migraine-like prodrome and typically last minutes to hours. They often occur in those with either a personal or family history of typical migraine. Other syndromes that are believed to be precursors include cyclical vomiting syndrome and benign paroxysmal vertigo of childhood.\n\nDifferential diagnosis\nOther conditions that can cause similar symptoms to a migraine headache include temporal arteritis, cluster headaches, acute glaucoma, meningitis and subarachnoid hemorrhage. Temporal arteritis typically occurs in people over 50 years old and presents with tenderness over the temple, cluster headache presents with one-sided nose stuffiness, tears and severe pain around the orbits, acute glaucoma is associated with vision problems, meningitis with fevers, and subarachnoid hemorrhage with a very fast onset. Tension headaches typically occur on both sides, are not pounding, and are less disabling.\nThose with stable headaches that meet criteria for migraines should not receive neuroimaging to look for other intracranial disease. This requires that other concerning findings such as papilledema (swelling of the optic disc) are not present. People with migraines are not at an increased risk of having another cause for severe headaches.\n\nManagement\nManagement of migraine includes prevention of migraine attacks and rescue treatment. There are three main aspects of treatment: trigger avoidance, acute (abortive), and preventive (prophylactic) control.\n\nPrognosis\n\"Migraine exists on a continuum of different attack frequencies and associated levels of disability.\" For those with occasional, episodic migraine, a \"proper combination of drugs for prevention and treatment of migraine attacks\" can limit the disease's impact on patients' personal and professional lives. But fewer than half of people with migraine seek medical care and more than half go undiagnosed and undertreated. \"Responsive prevention and treatment of migraine is incredibly important\" because evidence shows \"an increased sensitivity after each successive attack, eventually leading to chronic daily migraine in some individuals.\" Repeated migraine results in \"reorganization of brain circuitry,\" causing \"profound functional as well as structural changes in the brain.\" \"One of the most important problems in clinical migraine is the progression from an intermittent, self-limited inconvenience to a life-changing disorder of chronic pain, sensory amplification, and autonomic and affective disruption. This progression, sometimes termed chronification in the migraine literature, is common, affecting 3% of migraineurs in a given year, such that 8% of migraineurs have chronic migraine in any given year.\" Brain imagery reveals that the electrophysiological changes seen during an attack become permanent in people with chronic migraine; \"thus, from an electrophysiological point of view, chronic migraine indeed resembles a never-ending migraine attack.\" Severe migraine ranks in the highest category of disability, according to the World Health Organization, which uses objective metrics to determine disability burden for the authoritative annual Global Burden of Disease report. The report classifies severe migraine alongside severe depression, active psychosis, quadriplegia, and terminal-stage cancer.\nMigraine with aura appears to be a risk factor for ischemic stroke doubling the risk. Being a young adult, being female, using hormonal birth control, and smoking further increases this risk. There also appears to be an association with cervical artery dissection. Migraine without aura does not appear to be a factor. The relationship with heart problems is inconclusive with a single study supporting an association. Migraine does not appear to increase the risk of death from stroke or heart disease. Preventative therapy of migraines in those with migraine with aura may prevent associated strokes. People with migraine, particularly women, may develop higher than average numbers of white matter brain lesions of unclear significance.\n\nEpidemiology\nMigraine is common, with around 33% of women and 18% of men affected at some point in their lifetime.  Onset can be at any age, but prevalence rises sharply around puberty, and remains high until declining after age 50. Before puberty, boys and girls are equally impacted, with around 5% of children experiencing migraines. From puberty onwards, women experience migraines at greater rates than men. From age 30 to 50, up to 4 times as many women experience migraines as men., this is most pronounced in migraine without aura.\nWorldwide, migraine affects nearly 15% or approximately one billion people. In the United States, about 6% of men and 18% of women experience a migraine attack in a given year, with a lifetime risk of about 18% and 43% respectively. In Europe, migraines affect 12\u201328% of people at some point in their lives with about 6\u201315% of adult men and 14\u201335% of adult women getting at least one yearly. Rates of migraine are slightly lower in Asia and Africa than in Western countries. Chronic migraine occurs in approximately 1.4\u20132.2% of the population.\nDuring perimenopause symptoms often get worse before decreasing in severity. While symptoms resolve in about two-thirds of the elderly, in 3\u201310% they persist.\n\nHistory\nAn early description consistent with migraine is contained in the Ebers Papyrus, written around 1500 BCE in ancient Egypt.\nThe word migraine is from the Greek \u1f21\u03bc\u03b9\u03ba\u03c1\u1fb1\u03bd\u03af\u1fb1 (h\u0113mikr\u0101n\u00ed\u0101), 'pain in half of the head', from \u1f21\u03bc\u03b9- (h\u0113mi-), 'half' and \u03ba\u03c1\u1fb1\u03bd\u03af\u03bf\u03bd (kr\u0101n\u00edon), 'skull'.\n\nIn 200 BCE, writings from the Hippocratic school of medicine described the visual aura that can precede the headache and a partial relief occurring through vomiting.\nA second-century description by Aretaeus of Cappadocia divided headaches into three types: cephalalgia, cephalea, and heterocrania. Galen of Pergamon used the term hemicrania (half-head), from which the word migraine was eventually derived. He also proposed that the pain arose from the meninges and blood vessels of the head. Migraine was first divided into the two now used types \u2013 migraine with aura (migraine ophthalmique) and migraine without aura (migraine vulgaire) in 1887 by Louis Hyacinthe Thomas, a French Librarian. The mystical visions of Hildegard von Bingen, which she described as \"reflections of the living light\", are consistent with the visual aura experienced during migraines.\n\nTrepanation, the deliberate drilling of holes into a skull, was practiced as early as 7,000 BCE. While sometimes people survived, many would have died from the procedure due to infection. It was believed to work via \"letting evil spirits escape\". William Harvey recommended trepanation as a treatment for migraines in the 17th century. The association between trepanation and headaches in ancient history may simply be a myth or unfounded speculation that originated several centuries later. In 1913, the world-famous American physician William Osler misinterpreted the French anthropologist and physician Paul Broca's words about a set of children's skulls from the Neolithic age that he found during the 1870s. These skulls presented no evident signs of fractures that could justify this complex surgery for mere medical reasons. Trepanation was probably born of superstitions, to remove \"confined demons\" inside the head, or to create healing or fortune talismans with the bone fragments removed from the skulls of the patients. However, Osler wanted to make Broca's theory more palatable to his modern audiences, and explained that trepanation procedures were used for mild conditions such as \"infantile convulsions headache and various cerebral diseases believed to be caused by confined demons.\"\nWhile many treatments for migraine have been attempted, it was not until 1868 that use of a substance which eventually turned out to be effective began. This substance was the fungus ergot from which ergotamine was isolated in 1918  and first used to treat migraines in 1925. Methysergide was developed in 1959 and the first triptan, sumatriptan, was developed in 1988. During the 20th century with better study-design, effective preventive measures were found and confirmed.\n\nSociety and culture\nMigraine is a significant source of both medical costs and lost productivity. It has been estimated that migraine is the most costly neurological disorder in the European Community, costing more than \u20ac27 billion per year. In the United States, direct costs have been estimated at $17 billion, while indirect costs \u2013 such as missed or decreased ability to work \u2013 is estimated at $15 billion. Nearly a tenth of the direct cost is due to the cost of triptans. In those who do attend work during a migraine attack, effectiveness is decreased by around a third. Negative impacts also frequently occur for a person's family.\n\nResearch\nPrevention mechanisms\nTranscranial magnetic stimulation shows promise, as does transcutaneous supraorbital nerve stimulation. There is preliminary evidence that a ketogenic diet may help prevent episodic and long-term migraine.\n\nSex dependency\nStatistical data indicates that women may be more prone to having migraine, showing migraine incidence three times higher among women than men. The Society for Women's Health Research has also mentioned hormonal influences, mainly estrogen, as having a considerable role in provoking migraine pain. Studies and research related to the sex dependencies of migraine are still ongoing, and conclusions have yet to be achieved.\n\nSee also\nHeadache\nCluster headache\nTension headache\nTrigeminal neuralgia\n\nReferences\nFurther reading\nExternal links\n\nMigraine at Curlie","123":"Motion sickness occurs due to a difference between actual and expected motion. Symptoms commonly include nausea, vomiting, cold sweat, headache, dizziness, tiredness, loss of appetite, and increased salivation. Complications may rarely include dehydration, electrolyte problems, or a lower esophageal tear.\nThe cause of motion sickness is either real or perceived motion. This may include car travel, air travel, sea travel, space travel, or reality simulation. Risk factors include pregnancy, migraines, and M\u00e9ni\u00e8re's disease. The diagnosis is based on symptoms.\nTreatment may include behavioral measures or medications. Behavioral measures include keeping the head still and focusing on the horizon. Three types of medications are useful: antimuscarinics such as scopolamine, H1 antihistamines such as dimenhydrinate, and amphetamines such as dexamphetamine. Side effects, however, may limit the use of medications. A number of medications used for nausea such as ondansetron are not effective for motion sickness.\nNearly all people are affected with sufficient motion and most people will experience motion sickness at least once in their lifetime. Susceptibility, however, is variable, with about one-third of the population being highly susceptible while most other people are affected under extreme conditions. Women are more easily affected than men. Motion sickness has been described since at least the time of Homer (c. eighth century BC).\n\nSigns and symptoms\nSymptoms commonly include nausea, vomiting, cold sweat, headache, dizziness, tiredness, loss of appetite, and increased salivation. Occasionally, tiredness can last for hours to days after an episode of motion sickness, known as \"sopite syndrome\". Rarely severe symptoms such as the inability to walk, ongoing vomiting, or social isolation may occur while rare complications may include dehydration, electrolyte problems, or a lower esophageal tear from severe vomiting.\n\nCause\nMotion sickness can be divided into three categories:\n\nMotion sickness caused by motion that is felt but not seen i.e. terrestrial motion sickness;\nMotion sickness caused by motion that is seen but not felt i.e. space motion sickness;\nMotion sickness caused when both systems detect motion but they do not correspond i.e. either terrestrial or space motion sickness.\n\nMotion felt but not seen\nIn these cases, motion is sensed by the vestibular system and hence the motion is felt, but no motion or little motion is detected by the visual system, as in terrestrial motion sickness.\n\nCarsickness\nA specific form of terrestrial motion sickness, being carsick is quite common and evidenced by disorientation while reading a map, a book, or a small screen during travel. Carsickness results from the sensory conflict arising in the brain from differing sensory inputs. Motion sickness is caused by a conflict between signals arriving in the brain from the inner ear, which forms the base of the vestibular system, the sensory apparatus that deals with movement and balance, and which detects motion mechanically. If someone is looking at a stationary object within a vehicle, such as a magazine, their eyes will inform their brain that what they are viewing is not moving. Their inner ears, however, will contradict this by sensing the motion of the vehicle.\nVarying theories exist as to cause. The sensory conflict theory notes that the eyes view motion while riding in the moving vehicle while other body sensors sense stillness, creating conflict between the eyes and inner ear. Another suggests the eyes mostly see the interior of the car which is motionless while the vestibular system of the inner ear senses motion as the vehicle goes around corners or over hills and even small bumps. Therefore, the effect is worse when looking down but may be lessened by looking outside of the vehicle.\nIn the early 20th century, Austro-Hungarian scientist R\u00f3bert B\u00e1r\u00e1ny observed the back and forth movement of the eyes of railroad passengers as they looked out the side windows at the scenery whipping by. He called this \"railway nystagmus\", also known as \"optokinetic nystagmus\". His findings were published in the journal Laeger, 83:1516, Nov.17, 1921.\n\nAirsickness\nAir sickness is a kind of terrestrial motion sickness induced by certain sensations of air travel. It is a specific form of motion sickness and is considered a normal response in healthy individuals. It is essentially the same as carsickness but occurs in an airplane. An airplane may bank and tilt sharply, and unless passengers are sitting by a window, they are likely to see only the stationary interior of the plane due to the small window sizes and during flights at night. Another factor is that while in flight, the view out of windows may be blocked by clouds, preventing passengers from seeing the moving ground or passing clouds.\n\nSeasickness\nSeasickness is a form of terrestrial motion sickness characterized by a feeling of nausea and, in extreme cases, vertigo experienced after spending time on a boat.  It is essentially the same as carsickness, though the motion of a watercraft tends to be more regular. It is typically brought on by the rocking motion of the craft or movement while the craft is immersed in water. As with airsickness, it can be difficult to visually detect motion even if one looks outside the boat since water does not offer fixed points with which to visually judge motion. Poor visibility conditions, such as fog, may worsen seasickness. The greatest contributor to seasickness is the tendency for people being affected by the rolling or surging motions of the craft to seek refuge below decks, where they are unable to relate themselves to the boat's surroundings and consequent motion. Some people with carsickness are resistant to seasickness and vice versa. Adjusting to the craft's motion at sea is called \"gaining one's sea legs\"; it can take a significant portion of the time spent at sea after disembarking to regain a sense of stability \"post-sea legs\".\n\nCentrifuge motion sickness\nRotating devices such as centrifuges used in astronaut training and amusement park rides such as the Rotor, Mission: Space and the Gravitron can cause motion sickness in many people.  While the interior of the centrifuge does not appear to move, one will experience a sense of motion. In addition, centrifugal force can cause the vestibular system to give one the sense that downward is in the direction away from the center of the centrifuge rather than the true downward direction.\n\nDizziness due to spinning\nWhen one spins and stops suddenly, fluid in the inner ear continues to rotate causing a sense of continued spinning while one's visual system no longer detects motion.\n\nVirtual reality\nUsually, VR programs would detect the motion of the user's head and adjust the rotation of vision to avoid dizziness. However, some cases such as system lagging or software crashing could cause lags in the screen updates. In such cases, even some small head motions could trigger the motion sickness by the defense mechanism mentioned below: the inner ear transmits to the brain that it senses motion, but the eyes tell the brain that everything is still.\n\nMotion seen but not felt\nIn these cases, motion is detected by the visual system and hence the motion is seen, but no motion or little motion is sensed by the vestibular system. Motion sickness arising from such situations has been referred to as \"visually induced motion sickness\" (VIMS).\n\nSpace motion sickness\nZero gravity interferes with the vestibular system's gravity-dependent operations, so that the two systems, vestibular and visual, no longer provide a unified and coherent sensory representation. This causes unpleasant disorientation sensations often quite distinct from terrestrial motion sickness, but with similar symptoms. The symptoms may be more intense because a condition caused by prolonged weightlessness is usually quite unfamiliar.\nSpace motion sickness was effectively unknown during the earliest spaceflights because the very cramped conditions of the spacecraft allowed for only minimal bodily motion, especially head motion. Space motion sickness seems to be aggravated by being able to freely move around, and so is more common in larger spacecraft. Around 60% of Space Shuttle astronauts experienced it on their first flight; the first case of space motion sickness is now thought to be the Soviet cosmonaut Gherman Titov, in August 1961 onboard Vostok 2, who reported dizziness, nausea, and vomiting. The first severe cases were in early Apollo flights; Frank Borman on Apollo 8 and Rusty Schweickart on Apollo 9. Both experienced identifiable and quite unpleasant symptoms\u2014in the latter case causing the mission plan to be modified.\n\nScreen images\nThis type of terrestrial motion sickness is particularly prevalent when susceptible people are watching films presented on very large screens such as IMAX, but may also occur in regular format theaters or even when watching TV or playing games.  For the sake of novelty, IMAX and other panoramic type theaters often show dramatic motions such as flying over a landscape or riding a roller coaster.\nIn regular-format theaters, an example of a movie that caused motion sickness in many people is The Blair Witch Project.  Theaters warned patrons of its possible nauseating effects, cautioning pregnant women in particular.  Blair Witch was filmed with a handheld camcorder, which was subjected to considerably more motion than the average movie camera, and lacks the stabilization mechanisms of steadicams.\nHome movies, often filmed with a cell phone camera, also tend to cause motion sickness in those who view them.  The person holding the cell phone or other camera usually is unaware of this as the recording is being made since the sense of motion seems to match the motion seen through the camera's viewfinder.  Those who view the film afterward only see the movement, which may be considerable, without any sense of motion.  Using the zoom function seems to contribute to motion sickness as well since zooming is not a normal function of the eye.  The use of a tripod or a camera or cell phone with image stabilization while filming can reduce this effect.\n\nVirtual reality\nMotion sickness due to virtual reality is very similar to simulation sickness and motion sickness due to films.  In virtual reality the effect is made more acute as all external reference points are blocked from vision, the simulated images are three-dimensional and in some cases stereo sound that may also give a sense of motion. The NADS-1, a simulator located at the National Advanced Driving Simulator, is capable of accurately stimulating the vestibular system with a 360-degree horizontal field of view and 13 degrees of freedom motion base. Studies have shown that exposure to rotational motions in a virtual environment can cause significant increases in nausea and other symptoms of motion sickness.\nIn a study conducted by the U.S. Army Research Institute for the Behavioral and Social Sciences in a report published May 1995 titled \"Technical Report 1027 \u2013 Simulator Sickness in Virtual Environments\", out of 742 pilot exposures from 11 military flight simulators, \"approximately half of the pilots (334) reported post-effects of some kind: 250 (34%) reported that symptoms dissipated in less than one hour, 44 (6%) reported that symptoms lasted longer than four hours, and 28 (4%) reported that symptoms lasted longer than six hours. There were also four (1%) reported cases of spontaneously occurring flashbacks.\"\n\nMotion that is seen and felt\nWhen moving within a rotating reference frame such as in a centrifuge or environment where gravity is simulated with centrifugal force, the coriolis effect causes a sense of motion in the vestibular system that does not match the motion that is seen.\n\nPathophysiology\nThere are various hypotheses that attempt to explain the cause of the condition.\n\nSensory conflict theory\nContemporary sensory conflict theory, referring to \"a discontinuity between either visual, proprioceptive, and somatosensory input, or semicircular canal and otolith input\", is probably the most thoroughly studied. According to this theory, when the brain presents the mind with two incongruous states of motion, the result is often nausea and other symptoms of disorientation known as motion sickness. Such conditions happen when the vestibular system and the visual system do not present a synchronized and unified representation of one's body and surroundings.\nAccording to sensory conflict theory, the cause of terrestrial motion sickness is the opposite of the cause of space motion sickness. The former occurs when one perceives visually that one's surroundings are relatively immobile while the vestibular system reports that one's body is in motion relative to its surroundings. The latter can occur when the visual system perceives that one's surroundings are in motion while the vestibular system reports relative bodily immobility (as in zero gravity.)\n\nNeural mismatch\nA variation of the sensory conflict theory is known as neural mismatch, implying a mismatch occurring between ongoing sensory experience and long-term memory rather than between components of the vestibular and visual systems. This theory emphasizes \"the limbic system in the integration of sensory information and long-term memory, in the expression of the symptoms of motion sickness, and the impact of anti-motion-sickness drugs and stress hormones on limbic system function. The limbic system may be the neural mismatch center of the brain.\"\n\nDefense against poisoning\nIt has also been proposed that motion sickness could function as a defense mechanism against neurotoxins. The area postrema in the brain is responsible for inducing vomiting when poisons are detected, and for resolving conflicts between vision and balance. When feeling motion but not seeing it (for example, in the cabin of a ship with no portholes), the inner ear transmits to the brain that it senses motion, but the eyes tell the brain that everything is still. As a result of the incongruity, the brain concludes that the individual is hallucinating and further concludes that the hallucination is due to poison ingestion. The brain responds by inducing vomiting, to clear the supposed toxin. Treisman's indirect argument has recently been questioned via an alternative direct evolutionary hypothesis, as well as modified and extended via a direct poison hypothesis. The direct evolutionary hypothesis essentially argues that there are plausible means by which ancient real or apparent motion could have contributed directly to the evolution of aversive reactions, without the need for the co-opting of a poison response as posited by Treisman. Nevertheless, the direct poison hypothesis argues that there still are plausible ways in which the body's poison response system may have played a role in shaping the evolution of some of the signature symptoms that characterize motion sickness.\n\nNystagmus hypothesis\nYet another theory, known as the nystagmus hypothesis, has been proposed based on stimulation of the vagus nerve resulting from the stretching or traction of extra-ocular muscles co-occurring with eye movements caused by vestibular stimulation. There are three critical aspects to the theory: first is the close linkage between activity in the vestibular system, i.e., semicircular canals and otolith organs, and a change in tonus among various of each eye's six extra-ocular muscles. Thus, with the exception of voluntary eye movements, the vestibular and oculomotor systems are thoroughly linked. Second is the operation of Sherrington's Law describing reciprocal inhibition between agonist-antagonist muscle pairs, and by implication the stretching of extraocular muscle that must occur whenever Sherrington's Law is made to fail, thereby causing an unrelaxed (contracted) muscle to be stretched. Finally, there is the critical presence of afferent output to the Vagus nerves as a direct result of eye muscle stretch or traction. Thus, tenth nerve stimulation resulting from eye muscle stretch is proposed as the cause of motion sickness. The theory explains why labyrinthine-defective individuals are immune to motion sickness; why symptoms emerge when undergoing various body-head accelerations; why combinations of voluntary and reflexive eye movements may challenge the proper operation of Sherrington's Law, and why many drugs that suppress eye movements also serve to suppress motion sickness symptoms.\nA recent theory  argues that the main reason motion sickness occurs is due to an imbalance in vestibular outputs favoring the semicircular canals (nauseogenic) vs. otolith organs (anti-nauseogenic). This theory attempts to integrate previous theories of motion sickness. For example, there are many sensory conflicts that are associated with motion sickness and many that are not, but those in which canal stimulation occurs in the absence of normal otolith function (e.g., in free fall) are the most provocative. The vestibular imbalance theory is also tied to the different roles of the otoliths and canals in autonomic arousal (otolith output more sympathetic).\n\nDiagnosis\nThe diagnosis is based on symptoms. Other conditions that may present similarly include vestibular disorders such as benign paroxysmal positional vertigo and vestibular migraine and stroke.\n\nTreatment\nTreatment may include behavioral measures or medications.\n\nBehavioral measures\nBehavioral measures to decrease motion sickness include holding the head still and lying on the back. Focusing on the horizon may also be useful. Listening to music, mindful breathing, being the driver, and not reading while moving are other techniques.\nHabituation is the most effective technique but requires significant time. It is often used by the military for pilots. These techniques must be carried out at least every week to retain effectiveness.\nA head-worn, computer device with a transparent display can be used to mitigate the effects of motion sickness (and spatial disorientation) if visual indicators of the wearer's head position are shown. Such a device functions by providing the wearer with digital reference lines in their field of vision that indicate the horizon's position relative to the user's head. This is accomplished by combining readings from accelerometers and gyroscopes mounted in the device. This technology has been implemented in both standalone devices and Google Glass. One promising looking treatment is to wear LCD shutter glasses that create a stroboscopic vision of 4 Hz with a dwell of 10 milliseconds.\n\nMedication\nThree types of medications are sometimes prescribed to improve symptoms of motion sickness: antimuscarinics such as scopolamine, H1 antihistamines such as dimenhydrinate, and amphetamines such as dexamphetamine. Benefits are greater if used before the onset of symptoms or shortly after symptoms begin. Side effects, however, may limit the use of medications. A number of medications used for nausea such as ondansetron and metoclopramide are not effective in motion sickness.\n\nScopolamine (antimuscarinic)\nScopolamine is the most effective medication. Evidence is best for when it is used preventatively. It is available as a skin patch. Side effects may include blurry vision.\n\nAntihistamines\nAntihistamine medications are sometimes given to prevent or treat motion sickness. This class of medication is often effective at reducing the risk of getting motion sickness while in motion, however, the effectiveness of antihistamines at treating or stopping motion sickness once a person is already experiencing it has not been well studied. Effective first generation antihistamines include doxylamine, diphenhydramine, promethazine, meclizine, cyclizine, and cinnarizine. In pregnancy meclizine, dimenhydrinate and doxylamine are generally felt to be safe. Side effects include sleepiness. Second generation antihistamines have not been found to be useful.\n\nAmphetamines\nDextroamphetamine may be used together with an antihistamine or an antimuscarinic. Concerns include their addictive potential.\nThose involved in high-risk activities, such as SCUBA diving, should evaluate the risks versus the benefits of medications. Promethazine combined with ephedrine to counteract the sedation is known as \"the Coast Guard cocktail\".\n\nAlternative medicine\nAlternative treatments include acupuncture and ginger, although their effectiveness against motion sickness is variable. Providing smells does not appear to have a significant effect on the rate of motion sickness.\n\nEpidemiology\nRoughly one-third of people are highly susceptible to motion sickness, and most of the rest get motion sick under extreme conditions. Around 80% of the general population is susceptible to cases of medium to high motion sickness. The rates of space motion sickness have been estimated at between forty and eighty percent of those who enter weightless orbit. Several factors influence susceptibility to motion sickness, including sleep deprivation and the cubic footage allocated to each space traveler. Studies indicate that women are more likely to be affected than men, and that the risk decreases with advancing age. There is some evidence that people with Asian ancestry may develop motion sickness more frequently than people of European ancestry, and there are situational and behavioral factors, such as whether a passenger has a view of the road ahead, and diet and eating behaviors.\n\nSee also\nMal de debarquement - disembarkment syndrome, usually follows a cruise or other motion experience\n\nReferences\nExternal links\n\nDavis, Christopher J.; Lake-Bakaar, Gerry V.; Grahame-Smith, David G. (2012). Nausea and Vomiting: Mechanisms and Treatment. Springer Science & Business Media. p. 123. ISBN 978-3-642-70479-6.\nMotion Sickness from MedlinePlus","124":"Multiple sclerosis (MS) is an autoimmune disease in which the insulating covers of nerve cells in the brain and spinal cord are damaged. This damage disrupts the ability of parts of the nervous system to transmit signals, resulting in a range of signs and symptoms, including physical, mental, and sometimes psychiatric problems. Symptoms include double vision, vision loss, eye pain, muscle weakness, and loss of sensation or coordination. MS takes several forms, with new symptoms either occurring in isolated attacks (relapsing forms) or building up over time (progressive forms). In relapsing forms of MS, between attacks, symptoms may disappear completely, although some permanent neurological problems often remain, especially as the disease advances. In progressive forms of MS, bodily function slowly deteriorates once symptoms manifest and will steadily worsen if left untreated.\nWhile its cause is unclear, the underlying mechanism is thought to be either destruction by the immune system or failure of the myelin-producing cells. Proposed causes for this include immune dysregulation, genetics, and environmental factors, such as viral infections. MS is usually diagnosed based on the presenting signs and symptoms and the results of supporting medical tests.\nNo cure for multiple sclerosis is known. Current treatments are aimed at mitigating inflammation and resulting symptoms from acute flares and prevention of further attacks with disease-modifying medications. Physical therapy and occupational therapy, along with patient-centered symptom management, can help with people's ability to function. The long-term outcome is difficult to predict; better outcomes are more often seen in women, those who develop the disease early in life, those with a relapsing course, and those who initially experienced few attacks.\nMultiple sclerosis is the most common immune-mediated disorder affecting the central nervous system. Nearly one million people in the United States had MS in 2022, and in 2020, about 2.8 million people were affected globally, with rates varying widely in different regions and among different populations. The disease usually begins between the ages of 20 and 50 and is twice as common in women as in men. MS was first described in 1868 by French neurologist Jean-Martin Charcot.\nThe name \"multiple sclerosis\" is short for multiple cerebro-spinal sclerosis, which refers to the numerous glial scars (or sclerae \u2013 essentially plaques or lesions) that develop on the white matter of the brain and spinal cord.\n\nSigns and symptoms\nAs multiple sclerosis (MS) lesions can affect any part of the central nervous system, a person with MS can have almost any neurological symptom or sign referable to the central nervous system.\nFatigue is one of the most common symptoms of MS. Some 65% of people with MS experience fatigue symptomatology, and of these, some 15\u201340% report fatigue as their most disabling MS symptom.\nAutonomic, visual, motor, and sensory problems are also among the most common symptoms.\nThe specific symptoms are determined by the locations of the lesions within the nervous system, and may include focal loss of sensitivity or changes in sensation in the limbs, such as feeling tingling, pins and needles, or numbness; limb motor weakness\/pain, blurred vision, pronounced reflexes, muscle spasms, difficulty with ambulation (walking), difficulties with coordination and balance (ataxia); problems with speech or swallowing, visual problems (optic neuritis manifesting as eye pain & vision loss, or nystagmus manifesting as double vision), fatigue, and bladder and bowel difficulties (such as urinary or fecal incontinence or retention), among others. When multiple sclerosis is more advanced, walking difficulties can occur and the risk of falling increases.\nDifficulties thinking and emotional problems such as depression or unstable mood are also common. The primary deficit in cognitive function that people with MS experience is slowed information-processing speed, with memory also commonly affected, and executive function less commonly. Intelligence, language, and semantic memory are usually preserved, and the level of cognitive impairment varies considerably between people with MS.\nUhthoff's phenomenon, a worsening of symptoms due to exposure to higher-than-usual temperatures, and Lhermitte's sign, an electrical sensation that runs down the back when bending the neck, are particularly characteristic of MS, although may not always be present. Another presenting manifestation that is rare but highly suggestive of a demyelinating process such as MS is bilateral internuclear ophthalmoplegia, where the patient experiences double vision when attempting to move their gaze to the right & left.\nSome 60% or more of MS patients find their symptoms, particularly including fatigue, are affected by changes in body temperature.\n\nMeasures of disability\nThe main measure of disability and severity is the expanded disability status scale (EDSS), with other measures such as the multiple sclerosis functional composite being increasingly used in research. EDSS is also correlated with falls in people with MS. While it is a popular measure, EDSS has been criticized for some of its limitations, such as relying too much on walking.\n\nDisease course\nProdromal phase\nMS may have a prodromal phase in the years leading up to its manifestation, characterized by psychiatric issues, cognitive impairment, and increased use of healthcare.\n\nOnset\nThe condition begins in 85% of cases as a clinically isolated syndrome (CIS) over a number of days with 45% having motor or sensory problems, 20% having optic neuritis, and 10% having symptoms related to brainstem dysfunction, while the remaining 25% have more than one of the previous difficulties. Regarding optic neuritis as the most common presenting symptom, people with MS notice sub-acute loss of vision, often associated with pain worsening on eye movement, and reduced colour vision. Early diagnosis of MS-associated optic neuritis helps timely initiation of targeted treatments. However, it is crucial to adhere to established diagnostic criteria when treating optic neuritis due to the broad range of alternative causes, such as neuromyelitis optica spectrum disorder (NMOSD), and other autoimmune or infectious conditions. The course of symptoms occurs in two main patterns initially: either as episodes of sudden worsening that last a few days to months (called relapses, exacerbations, bouts, attacks, or flare-ups) followed by improvement (85% of cases) or as a gradual worsening over time without periods of recovery (10\u201315% of cases). A combination of these two patterns may also occur or people may start in a relapsing and remitting course that then becomes progressive later on.\n\nRelapses\nRelapses are usually unpredictable, occurring without warning. Exacerbations rarely occur more frequently than twice per year. Some relapses, however, are preceded by common triggers and they occur more frequently during spring and summer. Similarly, viral infections such as the common cold, influenza, or gastroenteritis increase their risk. Stress may also trigger an attack.\nMany events have been found not to affect rates of relapse requiring hospitalization including vaccination, breast feeding, physical trauma, and Uhthoff's phenomenon.\n\nPregnancy\nMany women with MS who become pregnant experience lower symptoms and fewer relapses. During the first months after delivery, the risk increases. Overall, pregnancy does not seem to influence long-term disability.\n\nCauses\nMultiple sclerosis is an autoimmune disease with a combination of genetic and environmental causes underlying it. Both T-cells and B-cells are involved, although T-cells are often considered to be the driving force of the disease. The causes of the disease are not fully understood. The Epstein-Barr Virus (EBV) has been shown to be directly present in the brain of most cases of MS and the virus is transcriptionally active in infected cells. EBV nuclear antigens are believed to be involved in the pathogenesis of multiple sclerosis, but not all people with MS have signs of EBV infection. Dozens of human peptides have been identified in different cases of the disease, and while some have plausible links to infectious organisms or known environmental factors, others do not.\n\nImmune dysregulation\nFailure of both central and peripheral nervous system clearance of autoreactive immune cells is implicated in the development of MS. The thymus is responsible for the immune system's central tolerance, where autoreactive T-cells are killed without being released into circulation. Via a similar mechanism, autoreactive B-cells in the bone marrow are killed. Some autoreactive T-cells & B-cells may escape these defense mechanisms, which is where peripheral immune tolerance defenses take action by preventing them from causing disease. However, these additional lines of defense can still fail. Further detail on immune dysregulation's contribution to MS risk is provided in the pathophysiology section of this article as well as the standalone article on the pathophysiology of MS.\n\nInfectious agents\nEarly evidence suggested the association between several viruses with human demyelinating encephalomyelitis, and the occurrence of demyelination in animals caused by some viral infections. One such virus, Epstein-Barr virus (EBV), can cause infectious mononucleosis and infects about 95% of adults, though only a small proportion of those infected later develop MS. A study of more than 10 million US military members compared 801 people who developed MS to 1,566 matched controls who did not develop MS. The study found a 32-fold increased risk of developing MS after infection with EBV. It did not find an increased risk after infection with other viruses, including the similar cytomegalovirus. These findings strongly suggest that EBV plays a role in MS onset, although EBV alone may be insufficient to cause it.\nThe nuclear antigen of EBV, which is the most consistent marker of EBV infection across all strains, has been identified as a direct source of autoreactivity in the human body. These antigens appear to be more likely to promote autoimmune responses in a person who also has a vitamin D deficiency. The exact nature of this relationship is poorly understood.\n\nGenetics\nMS is not considered a hereditary disease, but several genetic variations have been shown to increase its risk. Some of these genes appear to have higher expression levels in microglial cells than expected by chance. The probability of developing MS is higher in relatives of an affected person, with a greater risk among those more closely related. An identical twin of an affected individual has a 30% chance of developing MS, 5% for a nonidentical twin, 2.5% for a sibling, and an even lower chance for a half sibling. If both parents are affected, the risk in their children is 10 times that of the general population. MS is also more common in some ethnic groups than others.\nSpecific genes that have been linked with MS include differences in the human leukocyte antigen (HLA) system\u2014a group of genes on chromosome 6 that serves as the major histocompatibility complex (MHC). The contribution of HLA variants to MS susceptibility has been known since the 1980s, and this same region has also been implicated in the development of other autoimmune diseases, such as type 1 diabetes and systemic lupus erythematosus. The most consistent finding is the association between higher risk of developing multiple sclerosis and the MHC allele DR15, which is present in 30% of the U.S. and Northern European population. Other loci exhibit a protective effect, such as HLA-C554 and HLA-DRB1*11. HLA differences account for an estimated 20 to 60% of the genetic predisposition. Genome-wide association studies have revealed at least 200 variants outside the HLA locus that affect the risk of MS.\n\nGeography\nThe prevalence of MS from a geographic standpoint resembles a gradient, with MS being more common in people who live farther from the equator (e.g. those who live in northern regions of the world), although exceptions exist. The cause of this geographical pattern is not clear, although exposure to ultraviolet B (UVB) radiation and vitamin D levels have been proposed as potential explanations. As such, those who live in northern regions of the world are thought to have less exposure to UVB radiation and subsequently lower levels of vitamin D, which is a known risk factor for developing MS. Inversely, those who live in areas of relatively higher sun exposure and subsequently increased UVB radiation have a decreased risk of developing MS. While the north\u2013south gradient of incidence is decreasing, as of 2010, it is still present.\nMS is more common in regions with northern European populations, so the geographic variation may simply reflect the global distribution of these high-risk populations.\nA relationship between season of birth and MS lends support to this idea, with fewer people born in the Northern Hemisphere in November compared to May being affected later in life.\nEnvironmental factors may play a role during childhood, with several studies finding that people who move to a different region of the world before the age of 15 acquire the new region's risk of MS. If migration takes place after age 15, the persons retain the risk of their home country. Some evidence indicates that the effect of moving may still apply to people older than 15.\nThere are some exceptions to the above mentioned geographic pattern. These include ethnic groups that are at low risk and that live far from the equator such as the Sami, Amerindians, Canadian Hutterites, New Zealand M\u0101ori, and Canada's Inuit, as well as groups that have a relatively high risk and that live closer to the equator such as Sardinians, inland Sicilians, Palestinians, and Parsi.\n\nImpact of temperature\nMS symptoms may increase if body temperature is dysregulated. Fatigue is particularly effected.\n\nOther\nSmoking may be an independent risk factor for MS. Stress may also be a risk factor, although the evidence to support this is weak. Association with occupational exposures and toxins\u2014mainly organic solvents\u2014has been evaluated, but no clear conclusions have been reached. Vaccinations were studied as causal factors; most studies, though, show no association. Several other possible risk factors, such as diet and hormone intake, have been evaluated, but evidence on their relation with the disease is \"sparse and unpersuasive\". Gout occurs less than would be expected and lower levels of uric acid have been found in people with MS. This has led to the theory that uric acid is protective, although its exact importance remains unknown. Obesity during adolescence and young adulthood is a risk factor for MS.\n\nPathophysiology\nMultiple sclerosis is an autoimmune disease, primarily mediated by T-cells. The three main characteristics of MS are the formation of lesions in the central nervous system (also called plaques), inflammation, and the destruction of myelin sheaths of neurons. These features interact in a complex and not yet fully understood manner to produce the breakdown of nerve tissue, and in turn, the signs and symptoms of the disease. Damage is believed to be caused, at least in part, by attack on the nervous system by a person's own immune system.\n\nImmune dysregulation\nAs briefly detailed in the causes section of this article, MS is currently thought to stem from a failure of the body's immune system to kill off autoreactive T-cells & B-cells. Currently, the T-cell subpopulations that are thought to drive the development of MS are autoreactive CD8+ T-cells, CD4+ helper T-cells, and TH17 cells. These autoreactive T-cells produce substances called cytokines that induce an inflammatory immune response in the CNS, leading to the development of the disease. More recently, however, the role of autoreactive B-cells has been elucidated. Evidence of their contribution to the development of MS is implicated through the presence of oligoclonal IgG bands (antibodies produced by B-cells) in the CSF of patients with MS. The presence of these oligoclonal bands has been used as supportive evidence in clinching a diagnosis of MS. As similarly described before, B-cells can also produce cytokines that induce an inflammatory immune response via activation of autoreactive T-cells. As such, higher levels of these autoreactive B-cells is associated with increased number of lesions & neurodegeneration as well as worse disability.\nAnother cell population that is becoming increasingly implicated in MS are microglia. These cells are resident to & keep watch over the CNS, responding to pathogens by shifting between pro- & anti-inflammatory states. Microglia have been shown to be involved in the formation of MS lesions and have been shown to be involved in other diseases that primarily affect the CNS white matter. Although, because of their ability to switch between pro- & anti-inflammatory states, microglia have also been shown to be able to assist in remyelination & subsequent neuron repair. As such, microglia are thought to be participating in both acute & chronic MS lesions, with 40% of phagocytic cells in early active MS lesions being proinflammatory microglia.\n\nLesions\nThe name multiple sclerosis refers to the scars (sclerae \u2013 better known as plaques or lesions) that form in the nervous system. These lesions most commonly affect the white matter in the optic nerve, brain stem, basal ganglia, and spinal cord, or white matter tracts close to the lateral ventricles. The function of white matter cells is to carry signals between grey matter areas, where the processing is done, and the rest of the body. The peripheral nervous system is rarely involved.\n\nTo be specific, MS involves the loss of oligodendrocytes, the cells responsible for creating and maintaining a fatty layer\u2014known as the myelin sheath\u2014which helps the neurons carry electrical signals (action potentials). This results in a thinning or complete loss of myelin, and as the disease advances, the breakdown of the axons of neurons. When the myelin is lost, a neuron can no longer effectively conduct electrical signals. A repair process, called remyelination, takes place in early phases of the disease, but the oligodendrocytes are unable to completely rebuild the cell's myelin sheath. Repeated attacks lead to successively less effective remyelinations, until a scar-like plaque is built up around the damaged axons. These scars are the origin of the symptoms and during an attack magnetic resonance imaging (MRI) often shows more than 10 new plaques. This could indicate that some number of lesions exist, below which the brain is capable of repairing itself without producing noticeable consequences. Another process involved in the creation of lesions is an abnormal increase in the number of astrocytes due to the destruction of nearby neurons. A number of lesion patterns have been described.\n\nInflammation\nApart from demyelination, the other sign of the disease is inflammation. Fitting with an immunological explanation, the inflammatory process is caused by T cells, a kind of lymphocytes that plays an important role in the body's defenses. T cells gain entry into the brain as a result of disruptions in the blood\u2013brain barrier. The T cells recognize myelin as foreign and attack it, explaining why these cells are also called \"autoreactive lymphocytes\".\nThe attack on myelin starts inflammatory processes, which trigger other immune cells and the release of soluble factors like cytokines and antibodies. A further breakdown of the blood-brain barrier, in turn, causes a number of other damaging effects, such as swelling, activation of macrophages, and more activation of cytokines and other destructive proteins. Inflammation can potentially reduce transmission of information between neurons in at least three ways. The soluble factors released might stop neurotransmission by intact neurons. These factors could lead to or enhance the loss of myelin, or they may cause the axon to break down completely.\n\nBlood\u2013brain barrier\nThe blood\u2013brain barrier (BBB) is a part of the capillary system that prevents the entry of T cells into the central nervous system. It may become permeable to these types of cells secondary to an infection by a virus or bacteria. After it repairs itself, typically once the infection has cleared, T cells may remain trapped inside the brain. Gadolinium cannot cross a normal BBB, so gadolinium-enhanced MRI is used to show BBB breakdowns.\n\nMS fatigue\nThe pathophysiology and mechanisms causing MS fatigue are not well understood. MS fatigue can be affected by body heat, and this may differentiate MS fatigue from other primary fatigue. Fatigability (loss of strength) may increase perception of fatigue, but the two measures warrant independent assessment in clinical studies.\n\nDiagnosis\nMultiple sclerosis is typically diagnosed based on the presenting signs and symptoms, in combination with supporting medical imaging and laboratory testing. It can be difficult to confirm, especially early on, since the signs and symptoms may be similar to those of other medical problems.\n\nMcDonald criteria\nThe McDonald criteria, which focus on clinical, laboratory, and radiologic evidence of lesions at different times and in different areas, is the most commonly used method of diagnosis with the Schumacher and Poser criteria being of mostly historical significance. The McDonald criteria states that patients with multiple sclerosis should have lesions which are disseminated in time (DIT) and disseminated in space (DIS), i.e. lesions which have appeared in different areas in the brain and at different times. Below is an abbreviated outline of the 2017 McDonald Criteria for diagnosis of MS.\n\nAt least 2 clinical attacks with MRI showing 2 or more lesions characteristic of MS.\nAt least 2 clinical attacks with MRI showing 1 lesion characteristic of MS with clear historical evidence of a previous attack involving a lesion at a distinct location in the CNS.\nAt least 2 clinical attacks with MRI showing 1 lesion characteristic of MS, with DIT established by an additional clinical attack at a distinct CNS site or by MRI showing an old MS lesion.\n1 clinical attack with MRI showing at least 2 lesions characteristic of MS, with DIT established by an additional attack, by MRI showing old MS lesion(s), or presence of oligoclonal bands in CSF.\n1 clinical attack with MRI showing 1 lesion characteristic of MS, with DIS established by an additional attack at a different CNS site or by MRI showing old MS lesion(s), and DIT established by an additional attack, by MRI showing old MS lesion(s), or presence of oligoclonal bands in CSF.\nAs of 2017, no single test (including biopsy) can provide a definitive diagnosis.\n\nMRI\nMagnetic resonance imaging (MRI) of the brain and spine may show areas of demyelination (lesions or plaques). Gadolinium can be administered intravenously as a contrast agent to highlight active plaques, and by elimination, demonstrate the existence of historical lesions not associated with symptoms at the moment of the evaluation.\nCentral vein signs (CVSs) have been proposed as a good indicator of MS in comparison with other conditions causing white lesions. One small study found fewer CVSs in older and hypertensive people. Further research on CVS as a biomarker for MS is ongoing.\n\nCerebrospinal fluid (lumbar puncture)\nTesting of cerebrospinal fluid obtained from a lumbar puncture can provide evidence of chronic inflammation in the central nervous system. The cerebrospinal fluid is tested for oligoclonal bands of IgG on electrophoresis, which are inflammation markers found in 75\u201385% of people with MS.\n\nDifferential diagnosis\nSeveral diseases present similarly to MS. Medical professionals use a patient's specific presentation, history, and exam findings to make an individualized differential. Red flags are findings that suggest an alternate diagnosis, although they do not rule out MS. Red flags include a patient younger than 15 or older than 60, less than 24 hours of symptoms, involvement of multiple cranial nerves, involvement of organs outside of the nervous system, and atypical lab and exam findings.\nIn an emergency setting, it is important to rule out a stroke or bleeding in the brain. Intractable vomiting, severe optic neuritis, or bilateral optic neuritis raises suspicion for neuromyelitis optica spectrum disorder (NMOSD). Infectious diseases that may look similar to multiple sclerosis include HIV, Lyme disease, and syphilis. Autoimmune diseases include neurosarcoidosis, lupus, Guillain-Barr\u00e9 syndrome, acute disseminated encephalomyelitis, and Beh\u00e7et's disease. Psychiatric conditions such as anxiety or conversion disorder may also present in a similar way. Other rare diseases on the differential include CNS lymphoma, congenital leukodystrophies, and anti-MOG-associated myelitis.\n\nTypes and variants\nSeveral phenotypes (commonly termed \"types\"), or patterns of progression, have been described. Phenotypes use the past course of the disease in an attempt to predict the future course. They are important not only for prognosis, but also for treatment decisions.\nThe International Advisory Committee on Clinical Trials of MS describes four types of MS (revised in 2013) in what is known as the Lublin classification:\n\nClinically isolated syndrome (CIS)\nRelapsing-remitting MS (RRMS)\nPrimary progressive MS (PPMS)\nSecondary progressive MS (SPMS)\nCIS can be characterised as a single lesion seen on MRI which is associated with signs or symptoms found in MS. Due to the McDonald criteria, it does not completely fit the criteria to be diagnosed as MS, hence being named \"clinically isolated syndrome\". CIS can be seen as the first episode of demyelination in the central nervous system. To be classified as CIS, the attack must last at least 24 hours and be caused by inflammation or demyelination of the central nervous system. Patients who suffer from CIS may or may not go on to develop MS, but 30 to 70% of persons who experience CIS will later develop MS.\nRRMS is characterized by unpredictable relapses followed by periods of months to years of relative quiet (remission) with no new signs of disease activity. Deficits that occur during attacks may either resolve or leave problems, the latter in about 40% of attacks and being more common the longer a person has had the disease. This describes the initial course of 80% of individuals with MS.\nPPMS occurs in roughly 10\u201320% of individuals with the disease, with no remission after the initial symptoms. It is characterized by progression of disability from onset, with no, or only occasional and minor, remissions and improvements. The usual age of onset for the primary progressive subtype is later than of the relapsing-remitting subtype. It is similar to the age that secondary progressive usually begins in RRMS, around 40 years of age.\nSPMS occurs in around 65% of those with initial RRMS, who eventually have progressive neurologic decline between acute attacks without any definite periods of remission. Occasional relapses and minor remissions may appear. The most common length of time between disease onset and conversion from RRMS to SPMS is 19 years.\n\nSpecial courses\nIndependently of the types published by the MS associations, regulatory agencies such as the FDA often consider special courses, trying to reflect some clinical trials results on their approval documents. Some examples could be \"highly active MS\" (HAMS), \"active secondary MS\" (similar to the old progressive-relapsing) and \"rapidly progressing PPMS\".\nAlso, deficits always resolving between attacks is sometimes referred to as \"benign\" MS, although people still build up some degree of disability in the long term. On the other hand, the term malignant multiple sclerosis is used to describe people with MS having reached significant level of disability in a short period.\nAn international panel has published a standardized definition for the course HAMS.\n\nVariants\nAtypical variants of MS have been described; these include tumefactive multiple sclerosis, Balo concentric sclerosis, Schilder's diffuse sclerosis, and Marburg multiple sclerosis. Debate remains on whether they are MS variants or different diseases. Some diseases previously considered MS variants, such as Devic's disease, are now considered outside the MS spectrum.\n\nManagement\nAlthough no cure for multiple sclerosis has been found, several therapies have proven helpful. Several effective treatments can decrease the number of attacks and the rate of progression. The primary aims of therapy are returning function after an attack, preventing new attacks, and preventing disability. Starting medications is generally recommended in people after the first attack when more than two lesions are seen on MRI.\nThe first approved medications used to treat MS were modestly effective, though were poorly tolerated and had many adverse effects. Several treatment options with better safety and tolerability profiles have been introduced, improving the prognosis of MS.\nAs with any medical treatment, medications used in the management of MS have several adverse effects. Alternative treatments are pursued by some people, despite the shortage of supporting evidence of efficacy.\n\nInitial management of acute flare\nDuring symptomatic attacks, administration of high doses of intravenous corticosteroids, such as methylprednisolone, is the usual therapy, with oral corticosteroids seeming to have a similar efficacy and safety profile. Although effective in the short term for relieving symptoms, corticosteroid treatments do not appear to have a significant impact on long-term recovery. The long-term benefit is unclear in optic neuritis as of 2020. The consequences of severe attacks that do not respond to corticosteroids might be treatable by plasmapheresis.\n\nChronic management\nRelapsing remitting multiple sclerosis\nMultiple disease-modifying medications were approved by regulatory agencies for RRMS; they are modestly effective at decreasing the number of attacks. Interferons and glatiramer acetate are first-line treatments and are roughly equivalent, reducing relapses by approximately 30%. Early-initiated long-term therapy is safe and improves outcomes.\nTreatment of CIS with interferons decreases the chance of progressing to clinical MS. Efficacy of interferons and glatiramer acetate in children has been estimated to be roughly equivalent to that of adults. The role of some newer agents such as fingolimod, teriflunomide, and dimethyl fumarate, is not yet entirely clear. Making firm conclusions about the best treatment is difficult, especially regarding the long\u2010term benefit and safety of early treatment, given the lack of studies directly comparing disease-modifying therapies or long-term monitoring of patient outcomes.\nThe relative effectiveness of different treatments is unclear, as most have only been compared to placebo or a small number of other therapies. Direct comparisons of interferons and glatiramer acetate indicate similar effects or only small differences in effects on relapse rate, disease progression, and MRI measures. There is high confidence that natalizumab, cladribine, or alemtuzumab are decreasing relapses over a period of two years for people with RRMS. Natalizumab and interferon beta-1a (Rebif) may reduce relapses compared to both placebo and interferon beta-1a (Avonex) while Interferon beta-1b (Betaseron), glatiramer acetate, and mitoxantrone may also prevent relapses. Evidence on relative effectiveness in reducing disability progression is unclear. There is moderate confidence that a two-year treatment with natalizumab slows disability progression for people with RRMS. All medications are associated with adverse effects that may influence their risk to benefit profiles.\nUblituximab was approved for medical use in the United States in December 2022.\n\nMedications\nOverview of medications available for MS.\n\nProgressive multiple sclerosis\nIn 2011, mitoxantrone was the first medication approved for secondary progressive MS. In this population, tentative evidence supports mitoxantrone moderately slowing the progression of the disease and decreasing rates of relapses over two years.\nNew approved medications continue to emerge in modern medicine. In March 2017, the FDA approved ocrelizumab as a treatment for primary progressive MS in adults, the first drug to gain that approval, with requirements for several Phase IV clinical trials. It is also used for the treatment of relapsing forms of multiple sclerosis, to include clinically isolated syndrome, relapsing-remitting disease, and active secondary progressive disease in adults. According to a 2021 Cochrane review, ocrelizumab may reduce worsening of symptoms for primary progressive MS and probably increases unwanted effects but makes little or no difference to the number of serious unwanted effects.\nIn 2019, siponimod and cladribine were approved in the United States for the treatment of secondary progressive multiple sclerosis (SPMS). Subsequently, ozanimod was approved in 2020, and ponesimod was approved in 2021, which were both approved for management of CIS, relapsing MS, and SPMS in the U.S., and RRMS in Europe.\n\nAdverse effects\nThe disease-modifying treatments have several adverse effects. One of the most common is irritation at the injection site for glatiramer acetate and the interferons (up to 90% with subcutaneous injections and 33% with intramuscular injections). Over time, a visible dent at the injection site, due to the local destruction of fat tissue, known as lipoatrophy, may develop. Interferons may produce flu-like symptoms; some people taking glatiramer experience a post-injection reaction with flushing, chest tightness, heart palpitations, and anxiety, which usually lasts less than thirty minutes. More dangerous but much less common are liver damage from interferons, systolic dysfunction (12%), infertility, and acute myeloid leukemia (0.8%) from mitoxantrone, and progressive multifocal leukoencephalopathy occurring with natalizumab (occurring in 1 in 600 people treated).\nFingolimod may give rise to hypertension and slowed heart rate, macular edema, elevated liver enzymes, or a reduction in lymphocyte levels. Tentative evidence supports the short-term safety of teriflunomide, with common side effects including: headaches, fatigue, nausea, hair loss, and limb pain. There have also been reports of liver failure and PML with its use and it is dangerous for fetal development. Most common side effects of dimethyl fumarate are flushing and gastrointestinal problems. While dimethyl fumarate may lead to a reduction in the white blood cell count there were no reported cases of opportunistic infections during trials.\n\nAssociated symptoms\nBoth medications and neurorehabilitation have been shown to improve some symptoms, though neither changes the course of the disease. Some symptoms have a good response to medication, such as bladder spasticity, while others are little changed. Equipment such as catheters for neurogenic bladder dysfunction or mobility aids can be helpful in improving functional status.\nA multidisciplinary approach is important for improving quality of life; however, it is difficult to specify a 'core team' as many health services may be needed at different points in time. Multidisciplinary rehabilitation programs increase activity and participation of people with MS but do not influence impairment level. Studies investigating information provision in support of patient understanding and participation suggest that while interventions (written information, decision aids, coaching, educational programmes) may increase knowledge, the evidence of an effect on decision making and quality of life is mixed and low certainty. There is limited evidence for the overall efficacy of individual therapeutic disciplines, though there is good evidence that specific approaches, such as exercise, and psychological therapies are effective. Cognitive training, alone or combined with other neuropsychological interventions, may show positive effects for memory and attention though firm conclusions are not possible given small sample numbers, variable methodology, interventions and outcome measures. The effectiveness of palliative approaches in addition to standard care is uncertain, due to lack of evidence. The effectiveness of interventions, including exercise, specifically for the prevention of falls in people with MS is uncertain, while there is some evidence of an effect on balance function and mobility. Cognitive behavioral therapy has shown to be moderately effective for reducing MS fatigue. The evidence for the effectiveness of non-pharmacological interventions for chronic pain is insufficient to recommend such interventions alone, however their use in combination with medications may be reasonable.\n\nNon-pharmaceutical\nThere is some evidence that aquatic therapy is a beneficial intervention.\nThe spasticity associated with MS can be difficult to manage because of the progressive and fluctuating course of the disease. Although there is no firm conclusion on the efficacy in reducing spasticity, PT interventions can be a safe and beneficial option for patients with multiple sclerosis. Physical therapy including vibration interventions, electrical stimulation, exercise therapy, standing therapy, and radial shock wave therapy (RSWT), were beneficial for limiting spasticity, helping limit excitability, or increasing range of motion.\n\nAlternative treatments\nOver 50% of people with MS may use complementary and alternative medicine, although percentages vary depending on how alternative medicine is defined. Regarding the characteristics of users, they are more frequently women, have had MS for a longer time, tend to be more disabled and have lower levels of satisfaction with conventional healthcare. The evidence for the effectiveness for such treatments in most cases is weak or absent. Treatments of unproven benefit used by people with MS include dietary supplementation and regimens, vitamin D, relaxation techniques such as yoga, herbal medicine (including medical cannabis), hyperbaric oxygen therapy, self-infection with hookworms, reflexology, acupuncture, and mindfulness. Evidence suggests vitamin D supplementation, irrespective of the form and dose, provides no benefit for people with MS; this includes for measures such as relapse recurrence, disability, and MRI lesions while effects on health\u2010related quality of life and fatigue are unclear. There is insufficient evidence supporting high-dose biotin and some evidence for increased disease activity and higher risk of relapse with its use. A recent review of the effectiveness of Cannabis and Cannabinoids (2022) found that compared with placebo nabiximols probably reduce the severity of spasticity in the short term.\n\nPrognosis\nThe availability of treatments that modify the course of multiple sclerosis beginning in the 1990s, known as disease-modifying therapies (DMTs), has improved prognosis. These treatments can reduce relapses and slow progression, but there is no cure.\nThe prognosis of MS depends on the subtype of the disease, and there is considerable individual variation in the progression of the disease. In relapsing MS, the most common subtype, a 2016 cohort study found that after a median of 16.8 years from onset, one in ten needed a walking aid, and almost two in ten transitioned to secondary progressive MS, a form characterized by more progressive decline. With treatments available in the 2020s, relapses can be eliminated or substantially reduced. However, \"silent progression\" of the disease still occurs.\nIn addition to secondary progressive MS (SPMS), a small proportion of people with MS (10\u201315%) experience progressive decline from the onset, known as primary progressive MS (PPMS). Most treatments have been approved for use in relapsing MS; there are fewer treatments with lower efficacy for progressive forms of MS. The prognosis for progressive MS is worse, with faster accumulation of disability, though with considerable individual variation. In untreated PPMS, the median time from onset to requiring a walking aid is estimated as seven years. In SPMS, a 2014 cohort study reported that people required a walking aid after an average of five years from onset of SPMS, and were chair or bed-bound after an average of fifteen years.\nAfter diagnosis of MS, characteristics that predict a worse course are male sex, older age, and greater disability at the time of diagnosis; female sex is associated with a higher relapse rate. Currently, no biomarker can accurately predict disease progression in every patient. Spinal cord lesions, abnormalities on MRI, and more brain atrophy are predictive of a worse course, though brain atrophy as a predictor of disease course is experimental and not used in clinical practice. Early treatment leads to a better prognosis, but a higher relapse frequency when treated with DMTs is associated with a poorer prognosis. A 60-year longitudinal population study conducted in Norway found that those with MS had a life expectancy seven years shorter than the general population. Median life expectancy for RRMS patients was 77.8 years and 71.4 years for PPMS, compared to 81.8 years for the general population. Life expectancy for men was five years shorter than for women.\n\nEpidemiology\nMS is the most common autoimmune disorder of the central nervous system. The latest estimation of the total number of people with MS was 2.8 million globally, with a prevalence of 36 per 100,000 people. Moreover, prevalence varies widely in different regions around the world. In Africa, there are five people per 100,000 diagnosed with MS, compared to South East Asia where the prevalence is nine per 100,000, 112 per 100,000 in the Americas, and 133 per 100,000 in Europe.\nIncreasing rates of MS may be explained simply by better diagnosis. Studies on populational and geographical patterns have been common and have led to a number of theories about the cause.\nMS usually appears in adults in their late twenties or early thirties but it can rarely start in childhood and after 50 years of age. The primary progressive subtype is more common in people in their fifties. Similarly to many autoimmune disorders, the disease is more common in women, and the trend may be increasing. As of 2020, globally it is about two times more common in women than in men, and the ratio of women to men with MS is as high as 4:1 in some countries. In children, it is even more common in females than males, while in people over fifty, it affects males and females almost equally.\n\nHistory\nMedical discovery\nRobert Carswell (1793\u20131857), a British professor of pathology, and Jean Cruveilhier (1791\u20131873), a French professor of pathologic anatomy, described and illustrated many of the disease's clinical details, but did not identify it as a separate disease. Specifically, Carswell described the injuries he found as \"a remarkable lesion of the spinal cord accompanied with atrophy\". Under the microscope, Swiss pathologist Georg Eduard Rindfleisch (1836\u20131908) noted in 1863 that the inflammation-associated lesions were distributed around blood vessels.\nThe French neurologist Jean-Martin Charcot (1825\u20131893) was the first person to recognize multiple sclerosis as a distinct disease in 1868. Summarizing previous reports and adding his own clinical and pathological observations, Charcot called the disease sclerose en plaques.\n\nDiagnosis history\nThe first attempt to establish a set of diagnostic criteria was also due to Charcot in 1868. He published what now is known as the \"Charcot Triad\", consisting in nystagmus, intention tremor, and telegraphic speech (scanning speech). Charcot also observed cognition changes, describing his patients as having a \"marked enfeeblement of the memory\" and \"conceptions that formed slowly\".\nDiagnosis was based on Charcot triad and clinical observation until Schumacher made the first attempt to standardize criteria in 1965 by introducing some fundamental requirements: Dissemination of the lesions in time (DIT) and space (DIS), and that \"signs and symptoms cannot be explained better by another disease process\". The DIT and DIS requirement was later inherited by the Poser and McDonald criteria, whose 2017 revision is in use.\nDuring the 20th century, theories about the cause and pathogenesis were developed and effective treatments began to appear in the 1990s. Since the beginning of the 21st century, refinements of the concepts have taken place.  The 2010 revision of the McDonald criteria allowed for the diagnosis of MS with only one proved lesion (CIS).\nIn 1996, the US National Multiple Sclerosis Society (NMSS) (Advisory Committee on Clinical Trials) defined the first version of the clinical phenotypes that is in use. In this first version they provided standardized definitions for four MS clinical courses: relapsing-remitting (RR), secondary progressive (SP), primary progressive (PP), and progressive relapsing (PR). In 2010, PR was dropped and CIS was incorporated. Three years later, the 2013 revision of the \"phenotypes for the disease course\" were forced to consider CIS as one of the phenotypes of MS, making obsolete some expressions like \"conversion from CIS to MS\". Other organizations have proposed later new clinical phenotypes, like HAMS (Highly Active MS).\n\nHistorical cases\nThere are several historical accounts of people who probably had MS and lived before or shortly after the disease was described by Charcot.\nA young woman called Halldora who lived in Iceland around 1200 suddenly lost her vision and mobility but recovered them seven days after. Saint Lidwina of Schiedam (1380\u20131433), a Dutch nun, may be one of the first clearly identifiable people with MS. From the age of 16 until her death at 53, she had intermittent pain, weakness of the legs and vision loss: symptoms typical of MS. Both cases have led to the proposal of a \"Viking gene\" hypothesis for the dissemination of the disease.\nAugustus Frederick d'Este (1794\u20131848), son of Prince Augustus Frederick, Duke of Sussex and Lady Augusta Murray and a grandson of George III of the United Kingdom, almost certainly had MS. D'Este left a detailed diary describing his 22 years living with the disease. His diary began in 1822 and ended in 1846, although it remained unknown until 1948. His symptoms began at age 28 with a sudden transient visual loss (amaurosis fugax) after the funeral of a friend. During his disease, he developed weakness of the legs, clumsiness of the hands, numbness, dizziness, bladder disturbance and erectile dysfunction. In 1844, he began to use a wheelchair. Despite his illness, he kept an optimistic view of life. Another early account of MS was kept by the British diarist W. N. P. Barbellion, pen name of Bruce Frederick Cummings (1889\u20131919), who maintained a detailed log of his diagnosis and struggle. His diary was published in 1919 as The Journal of a Disappointed Man. Charles Dickens, a keen observer, described possible bilateral optic neuritis with reduced contrast vision and Uhthoff phenomenon in the main female character of Bleak House (1852\u20131853), Esther Summerville.\n\nResearch\nEpstein-Barr virus\nAs of 2022, the pathogenesis of MS as it relates to Epstein-Barr virus (EBV) is actively investigated, as are disease-modifying therapies; understanding of how risk factors combine with EBV to initiate MS is sought. Whether EBV is the only cause of MS might be better understood if an EBV vaccine is developed and shown to prevent MS as well.\nEven though a variety of studies showed the connection between an EBV infection and a later development of multiple sclerosis, the mechanisms behind this correlation are not completely clear, and several theories have been proposed to explain the relationship between the two diseases. It is thought that the involvement of EBV-infected B-cells (B lymphocytes) and the involvement of anti-EBNA antibodies, which appear to be significantly higher in multiple sclerosis patients, play a crucial role in the development of the disease. This is supported by the fact that treatment against B-cells, e.g. ocrelizumab, reduces the symptoms of multiple sclerosis: annual relapses appear less frequently and the disability progression is slower. A 2022 Stanford University study has shown that during an EBV infection, molecular mimicry can occur, where the immune system will produce antibodies against the EBNA1 protein, which at the same time is able to bind to GlialCAM in the myelin. Additionally, they observed a phenomenon which is uncommon in healthy individuals but often detected in multiple sclerosis patients \u2013 B-cells are trafficking to the brain and spinal cord, where they are producing oligoclonal antibody bands. A majority of these oligoclonal bands do have an affinity to the viral protein EBNA1, which is cross-reactive to GlialCAM. These antibodies are abundant in approximately 20\u201325% of multiple sclerosis patients and worsen the autoimmune demyelination which leads consequently to an pathophysiological exacerbation of the disease. Furthermore, the intrathecal oligoclonal expansion with a constant somatic hypermutation is unique in multiple sclerosis when compared to other neuroinflammatory diseases. In the study there was also the abundance of antibodies with IGHV 3\u20137 genes measured, which appears to be connected to the disease progress. Antibodies which are IGHV3\u20137-based are binding with a high affinity to EBNA1 and GlialCAM. This process is actively thriving the demyelination. It is probable that B-cells, expressing IGHV 3\u20137 genes entered the CSF and underwent affinity maturation after facing GlialCAM, which led consequently to the production of high affinity anti-GlialCAM antibodies. This was additionally shown in the EAE mouse model where immunization with EBNA1 lead to a strong B-cell response against GlialCAM, which worsened the EAE.\n\nHuman endogenous retroviruses\nTwo members of the human endogenous retroviruses-W (HERV-W) family, namely, ERVWE1 and MS-associated retrovirus (MSRV), may be co-factors in MS immunopathogenesis. HERVs constitute up to 8% of the human genome; most are epigenetically silent, but can be reactivated by exogenous viruses, proinflammatory conditions or oxidative stress.\n\nMedications\nMedications that influence voltage-gated sodium ion channels are under investigation as a potential neuroprotective strategy because of hypothesized role of sodium in the pathological process leading to axonal injury and accumulating disability. There is insufficient evidence of an effect of sodium channel blockers for people with MS.\n\nPathogenesis\nMS is a clinically defined entity with several atypical presentations. Some auto-antibodies have been found in atypical MS cases, giving birth to separate disease families and restricting the previously wider concept of MS.\nAnti-AQP4 autoantibodies were found in neuromyelitis optica (NMO), which was previously considered a MS variant. A spectrum of diseases named NMOSD (NMO spectrum diseases) or anti-AQP4 diseases has been accepted. Some cases of MS were presenting anti-MOG autoantibodies, mainly overlapping with the Marburg variant. Anti-MOG autoantibodies were found to be also present in ADEM, and a second spectrum of separated diseases is being considered. This spectrum is named inconsistently across different authors, but it is normally something similar to anti-MOG demyelinating diseases.\nA third kind of auto-antibodies is accepted. They are several anti-neurofascin auto-antibodies which damage the Ranvier nodes of the neurons. These antibodies are more related to the peripheral nervous demyelination, but they were also found in chronic progressive PPMS and combined central and peripheral demyelination (CCPD, which is considered another atypical MS presentation).\nIn addition to the significance of auto-antibodies in MS, four different patterns of demyelination have been reported, opening the door to consider MS as a heterogeneous disease.\n\nBiomarkers\nSince disease progression is the result of degeneration of neurons, the roles of proteins showing loss of nerve tissue such as neurofilaments, tau, and N-acetylaspartate are under investigation.\nImprovement in neuroimaging techniques such as positron emission tomography (PET) or MRI carry a promise for better diagnosis and prognosis predictions. Regarding MRI, there are several techniques that have already shown some usefulness in research settings and could be introduced into clinical practice, such as double-inversion recovery sequences, magnetization transfer, diffusion tensor, and functional magnetic resonance imaging. These techniques are more specific for the disease than existing ones, but still lack some standardization of acquisition protocols and the creation of normative values. This is particularly the case for proton magnetic resonance spectroscopy, for which a number of methodological variations observed in the literature may underlie continued inconsistencies in central nervous system metabolic abnormalities, particularly in N-acetyl aspartate, myoinositol, choline, glutamate, GABA, and GSH, observed for multiple sclerosis and its subtypes. There are other techniques under development that include contrast agents capable of measuring levels of peripheral macrophages, inflammation, or neuronal dysfunction, and techniques that measure iron deposition that could serve to determine the role of this feature in MS, or that of cerebral perfusion.\n\nCOVID-19\nThe hospitalization rate was found to be higher among individuals with MS and COVID-19 infection, at 10%, while the pooled infection rate is estimated at 4%. The pooled prevalence of death in hospitalized individuals with MS is estimated as 4%.\n\nMetformin\nA 2019 study on rats and a 2024 study on mice showed that a first-line medication for the treatment of type 2 diabetes, metformin, could promote remyelination. The promising drug is currently being researched on humans in the Octopus trials, a multi-arm, multi-stage trial, focussed on testing existing drugs for other conditions on patients with MS. Currently, clinical trials on humans are ongoing in Belgium, for patients with non-active progressive MS, in the U.K., in combination with clemastine for the treatment of relapsing remitting MS, and Canada, for MS patients up to 25 years old.\n\nOther emerging theories\nOne emerging hypothesis, referred to as the hygiene hypothesis, suggests that early-life exposure to infectious agents helps to develop the immune system and reduces susceptibility to allergies and autoimmune disorders. The hygiene hypothesis has been linked with MS and microbiome hypotheses.\nIt has also been proposed that certain bacteria found in the gut use molecular mimicry to infiltrate the brain via the gut\u2013brain axis, initiating an inflammatory response and increasing blood-brain barrier permeability. Vitamin D levels have also been correlated with MS; lower levels of vitamin D correspond to an increased risk of MS, suggesting a reduced prevalence in the tropics \u2013 an area with more Vitamin D-rich sunlight \u2013 strengthening the impact of geographical location on MS development. MS mechanisms begin when peripheral autoreactive effector CD4+ T cells get activated and move into the CNS. Antigen-presenting cells localize the reactivation of autoreactive effector CD4-T cells once they have entered the CNS, attracting more T cells and macrophages to form the inflammatory lesion. In MS patients, macrophages and microglia assemble at locations where demyelination and neurodegeneration are actively occurring, and microglial activation is more apparent in the normal-appearing white matter of MS patients. Astrocytes generate neurotoxic chemicals like nitric oxide and TNF\u03b1, attract neurotoxic inflammatory monocytes to the CNS, and are responsible for astrogliosis, the scarring that prevents the spread of neuroinflammation and kills neurons inside the scarred area.\nIn 2024, scientists shared research on their findings of ancient migration to northern Europe from the Yamnaya area of culture, tracing MS-risk gene variants dating back around 5,000 years. The MS-risk gene variants protected ancient cattle herders from animal diseases, but modern lifestyles, diets and better hygiene, have allowed the gene to develop, resulting in the higher risk of MS today.\n\nSee also\nList of multiple sclerosis organizations\nList of people with multiple sclerosis\n\nReferences\nExternal links\n\nMultiple sclerosis at Curlie","125":"Nausea is a diffuse sensation of unease and discomfort, sometimes perceived as an urge to vomit. While not painful, it can be a debilitating symptom if prolonged and has been described as placing discomfort on the chest, abdomen, or back of the throat.\nOver 30 definitions of nausea were proposed in a 2011 book on the topic.\nNausea is a non-specific symptom, which means that it has many possible causes. Some common causes of nausea are gastroenteritis and other gastrointestinal disorders, food poisoning, motion sickness, dizziness, migraine, fainting, low blood sugar, anxiety, hyperthermia, dehydration and lack of sleep. Nausea is a side effect of many medications including chemotherapy, or morning sickness in early pregnancy. Nausea may also be caused by disgust and depression.\nMedications taken to prevent and treat nausea and vomiting are called antiemetics. The most commonly prescribed antiemetics in the US are promethazine, metoclopramide, and the newer ondansetron. The word nausea is from Latin nausea, from Greek \u03bd\u03b1\u03c5\u03c3\u03af\u03b1 \u2013 nausia, \"\u03bd\u03b1\u03c5\u03c4\u03af\u03b1\" \u2013 nautia, motion sickness, \"feeling sick or queasy\".\n\nCauses\nGastrointestinal infections (37%) and food poisoning are the two most common causes of acute nausea and vomiting. Side effects from medications (3%) and pregnancy are also relatively frequent. There are many causes of chronic nausea. Nausea and vomiting remain undiagnosed in 10% of the cases. Aside from morning sickness, there are no sex differences in complaints of nausea. After childhood, doctor consultations decrease steadily with age. Only a fraction of one percent of doctor visits by those over 65 are due to nausea.\n\nGastrointestinal\nGastrointestinal infection is one of the most common causes of acute nausea and vomiting. Chronic nausea may be the presentation of many gastrointestinal disorders, occasionally as the major symptom, such as gastroesophageal reflux disease, functional dyspepsia, gastritis, biliary reflux, gastroparesis, peptic ulcer, celiac disease, non-celiac gluten sensitivity, Crohn's disease, hepatitis, upper gastrointestinal malignancy, and pancreatic cancer. Uncomplicated Helicobacter pylori infection does not cause chronic nausea.\n\nFood poisoning\nFood poisoning usually causes an abrupt onset of nausea and vomiting one to six hours after ingestion of contaminated food and lasts for one to two days. It is due to toxins produced by bacteria in food.\n\nMedications\nMany medications can potentially cause nausea. Some of the most frequently associated include cytotoxic chemotherapy regimens for cancer and other diseases, and general anaesthetic agents. An old cure for migraine, ergotamine, is well known to cause devastating nausea in some patients; a person using it for the first time will be prescribed an antiemetic for relief if needed.\n\nPregnancy\nNausea or \"morning sickness\" is common during early pregnancy but may occasionally continue into the second and third trimesters. In the first trimester nearly 80 % of women have some degree of nausea. Pregnancy should therefore be considered as a possible cause of nausea in any sexually active woman of child-bearing age. While usually it is mild and self-limiting, severe cases known as hyperemesis gravidarum may require treatment.\n\nDisequilibrium\nA number of conditions involving balance such as motion sickness and vertigo can lead to nausea and vomiting.\n\nGynecologic\nDysmenorrhea can cause nausea.\n\nPsychiatric\nNausea may be caused by depression, anxiety disorders and eating disorders.\n\nPotentially serious\nWhile most causes of nausea are not serious, some serious conditions are associated with nausea. These include pancreatitis, small bowel obstruction, appendicitis, cholecystitis, hepatitis, Addisonian crisis, diabetic ketoacidosis, increased intracranial pressure, spontaneous intracranial hypotension, brain tumors, meningitis, heart attack, rabies, carbon monoxide poisoning and many others.\n\nComprehensive list\nInside the abdomen\nObstructing disorders\n\nGastric outlet obstruction\nSmall bowel obstruction\nColonic obstruction\nSuperior mesenteric artery syndrome\nEnteric infections\n\nViral infection\nBacterial infection\nInflammatory diseases\n\nCeliac disease\nCholecystitis\nPancreatitis\nAppendicitis\nHepatitis\nSensorimotor dysfunction\n\nGastroparesis\nIntestinal pseudo-obstruction\nGastroesophageal reflux disease\nIrritable bowel syndrome\nCyclic vomiting syndrome\nOther\n\nNon-celiac gluten sensitivity\nBiliary colic\nKidney stone\nCirrhosis\nAbdominal irradiation\n\nOutside the abdomen\nCardiopulmonary\n\nCardiomyopathy\nMyocardial infarction (heart attack)\nParoxysmal cough\nInner-ear diseases\n\nMotion sickness\nLabyrinthitis\nMalignancy\nIntracerebral disorders\n\nMalignancy\nHemorrhage\nAbscess\nHydrocephalus\nMeningitis\nEncephalitis\nRabies\nPsychiatric illnesses\n\nAnorexia and bulimia nervosa\nDepression\nDrug withdrawal\nOther\n\nPost-operative vomiting\nNociception\nAltitude sickness\n\nMedications and metabolic disorders\nDrugs\n\nChemotherapy\nAntibiotics\nAntiarrhythmics\nDigoxin\nOral hypoglycemic medications\nOral contraceptives\nNorepinephrine reuptake inhibitors\nEndocrine\/metabolic disease\n\nPregnancy\nUremia\nKetoacidosis\nThyroid and parathyroid disease\nAdrenal insufficiency\nToxins\n\nLiver failure\nAlcohol\n\nPathophysiology\nResearch on nausea and vomiting has relied on using animal models to mimic the anatomy and neuropharmacologic features of the human body. The physiologic mechanism of nausea is a complex process that has yet to be fully elucidated. There are four general pathways that are activated by specific triggers in the human body that go on to create the sensation of nausea and vomiting.\n\nCentral nervous system (CNS): Stimuli can affect areas of the CNS including the cerebral cortex and the limbic system. These areas are activated by elevated intracranial pressure, irritation of the meninges (i.e. blood or infection), and extreme emotional triggers such as anxiety. The supratentorial region is also responsible for the sensation of nausea.\nChemoreceptor trigger zone (CTZ): The CTZ is located in the area postrema in the floor of the fourth ventricle within the brain. This area is outside the blood brain barrier, and is therefore readily exposed to substances circulating through the blood and cerebral spinal fluid. Common triggers of the CTZ include metabolic abnormalities, toxins, and medications. Activation of the CTZ is mediated by dopamine (D2) receptors, serotonin (5HT3) receptors, and neurokinin receptors (NK1).\nVestibular system: This system is activated by disturbances to the vestibular apparatus in the inner ear. These include movements that cause motion sickness and dizziness. This pathway is triggered via histamine (H1) receptors and acetylcholine (ACh) receptors.\nPeripheral Pathways: These pathways are triggered via chemoreceptors and mechanoreceptors in the gastrointestinal tract, as well as other organs such as the heart and kidneys. Common activators of these pathways include toxins present in the gastrointestinal lumen and distension of the gastrointestinal lumen from blockage or dysmotility of the bowels. Signals from these pathways travel via multiple neural tracts including the vagus, glossopharyngeal, splanchnic, and sympathetic nerves.\nSignals from any of these pathways then travel to the brainstem, activating several structures including the nucleus of the solitary tract, the dorsal motor nucleus of the vagus, and central pattern generator. These structures go on to signal various downstream effects of nausea and vomiting. The body's motor muscle responses involve halting the muscles of the gastrointestinal tract, and in fact causing reversed propulsion of gastric contents towards the mouth while increasing abdominal muscle contraction. Autonomic effects involve increased salivation and the sensation of feeling faint that often occurs with nausea and vomiting.\n\nPre-nausea pathophysiology\nIt has been described that alterations in heart rate can occur as well as the release of vasopressin from the posterior pituitary.\n\nDiagnosis\nPatient history\nTaking a thorough patient history may reveal important clues to the cause of nausea and vomiting. If the patient's symptoms have an acute onset, then drugs, toxins, and infections are likely. In contrast, a long-standing history of nausea will point towards a chronic illness as the culprit. The timing of nausea and vomiting after eating food is an important factor to pay attention to. Symptoms that occur within an hour of eating may indicate an obstruction proximal to the small intestine, such as gastroparesis or pyloric stenosis. An obstruction further down in the intestine or colon will cause delayed vomiting. An infectious cause of nausea and vomiting such as gastroenteritis may present several hours to days after the food was ingested. The contents of the emesis is a valuable clue towards determining the cause. Bits of fecal matter in the emesis indicate obstruction in the distal intestine or the colon. Emesis that is of a bilious nature (greenish in color) localizes the obstruction to a point past the stomach. Emesis of undigested food points to an obstruction prior to the gastric outlet, such as achalasia or Zenker's diverticulum. If patient experiences reduced abdominal pain after vomiting, then obstruction is a likely etiology. However, vomiting does not relieve the pain brought on by pancreatitis or cholecystitis.\n\nPhysical exam\nIt is important to watch out for signs of dehydration, such as orthostatic hypotension and loss of skin turgor. Auscultation of the abdomen can produce several clues to the cause of nausea and vomiting. A high-pitched tinkling sound indicates possible bowel obstruction, while a splashing \"succussion\" sound is more indicative of gastric outlet obstruction. Eliciting pain on the abdominal exam when pressing on the patient may indicate an inflammatory process. Signs such as papilledema, visual field losses, or focal neurological deficits are red flag signs for elevated intracranial pressure.\n\nDiagnostic testing\nWhen a history and physical exam are not enough to determine the cause of nausea and vomiting, certain diagnostic tests may prove useful. A chemistry panel would be useful for electrolyte and metabolic abnormalities. Liver function tests and lipase would identify pancreaticobiliary diseases. Abdominal X-rays showing air-fluid levels indicate bowel obstruction, while an X-ray showing air-filled bowel loops are more indicative of ileus. More advanced imaging and procedures may be necessary, such as a CT scan, upper endoscopy, colonoscopy, barium enema, or MRI. Abnormal GI motility can be assessed using specific tests like gastric scintigraphy, wireless motility capsules, and small-intestinal manometry.\n\nTreatment\nIf dehydration is present due to loss of fluids from severe vomiting, rehydration with oral electrolyte solutions is preferred. If this is not effective or possible, intravenous rehydration may be required. Medical care is recommended if: a person cannot keep any liquids down, has symptoms more than 2 days, is weak, has a fever, has stomach pain, vomits more than two times in a day or does not urinate for more than 8 hours.\n\nMedications\nNumerous pharmacologic medications are available for the treatment of nausea. There is no medication that is clearly superior to other medications for all cases of nausea. The choice of antiemetic medication may be based on the situation during which the person experiences nausea. For people with motion sickness and vertigo, antihistamines and anticholinergics such as meclizine and scopolamine are particularly effective. Nausea and vomiting associated with migraine headaches respond best to dopamine antagonists such as metoclopramide, prochlorperazine, and chlorpromazine. In cases of gastroenteritis, serotonin antagonists such as ondansetron were found to suppress nausea and vomiting, as well as reduce the need for IV fluid resuscitation. The combination of pyridoxine and doxylamine is the first line treatment for pregnancy-related nausea and vomiting. Dimenhydrinate is an inexpensive and effective over the counter medication for preventing postoperative nausea and vomiting. Other factors to consider when choosing an antiemetic medication include the person's preference, side-effect profile, and cost.\nNabilone is also indicated for this purpose.\n\nAlternative medicine\nIn certain people, cannabinoids may be effective in reducing chemotherapy associated nausea and vomiting. Several studies have demonstrated the therapeutic effects of cannabinoids for nausea and vomiting in the advanced stages of illnesses such as cancer and AIDS.\nIn hospital settings topical anti-nausea gels are not indicated because of lack of research backing their efficacy. Topical gels containing lorazepam, diphenhydramine, and haloperidol are sometimes used for nausea but are not equivalent to more established therapies.\nGinger has also been shown to be potentially effective in treating several types of nausea.\n\nPrognosis\nThe outlook depends on the cause. Most people recover within few hours or a day. While short-term nausea and vomiting are generally harmless, they may sometimes indicate a more serious condition. When associated with prolonged vomiting, it may lead to dehydration or dangerous electrolyte imbalances or both. Repeated intentional vomiting, characteristic of bulimia, can cause stomach acid to wear away at the enamel present on the teeth.\n\nEpidemiology\nNausea and or vomiting is the main complaint in 1.6% of visits to family physicians in Australia. However, only 25% of people with nausea visit their family physician. In Australia, nausea, as opposed to vomiting, occurs most frequently in persons aged 15\u201324 years, and is less common in other age groups.\n\nSee also\nCancer and nausea\nVasodilation\n\nReferences\nExternal links\n The dictionary definition of nausea at Wiktionary","126":"Functional neurologic disorder or functional neurological disorder (FND) is a condition in which patients experience neurological symptoms such as weakness, movement disorders, sensory symptoms, and blackouts. As a functional disorder, there is, by definition, no known disease process affecting the structure of the body, yet the person experiences symptoms relating to their body function. Symptoms of functional neurological disorders are clinically recognisable, but are not categorically associated with a definable organic disease. \nThe intended contrast is with an organic brain syndrome, where a pathology (disease process) which affects the body's physiology can be identified. Subsets of functional neurological disorders include functional neurological symptom disorder (FNsD), conversion disorder, functional movement disorder, and functional seizures. The diagnosis is made based on positive signs and symptoms in the history and examination during consultation of a neurologist.\nPhysiotherapy is particularly helpful for patients with motor symptoms (weakness, gait disorders, movement disorders) and tailored cognitive behavioural therapy has the best evidence in patients with dissociative (non-epileptic) attacks.\n\nHistory\nFrom the 18th century, there was a move from the idea of FND being caused by the nervous system. This led to an understanding that it could affect both sexes. Jean Martin Charcot argued that, what would be later called FND, was caused by \"a hereditary degeneration of the nervous system, namely a neurological disorder\".\nIn the 18th century, the illness was confirmed as a neurological disorder but a small number of doctors still believed in the previous definition. However, as early as 1874, doctors, including W.B. Carpenter and J.A. Omerod, began to speak out against this other term due to there being no evidence of its existence.\nAlthough the term \"conversion disorder\" has been used for many years, another term was still being used in the 20th century. However, by this point, it bore little resemblance to the original meaning. It referred instead to symptoms that could not be explained by a recognised organic pathology, and was therefore believed to be the result of stress, anxiety, trauma or depression. The term fell out of favour over time due to the negative connotations. Furthermore, critics pointed out that it can be challenging to find organic pathologies for all symptoms, and so the practice of diagnosing that patients who had such symptoms were imagining them led to the disorder being meaningless, vague and a sham-diagnosis, as it did not refer to any definable disease.\nThroughout its history, many patients have been misdiagnosed with conversion disorder when they had organic disorders such as tumours or epilepsy or vascular diseases. This has led to patient deaths, a lack of appropriate care and suffering for the patients. Eliot Slater, after studying the condition in the 1950s, was outspoken against the condition, as there has never been any evidence to prove that it exists. He stated that \"The diagnosis of 'hysteria' is a disguise for ignorance and a fertile source of clinical error. It is, in fact, not only a delusion but also a snare\".\nIn 1980, the DSM III added 'conversion disorder' to its list of conditions. The diagnostic criteria for this condition are nearly identical to those used for hysteria. The diagnostic criteria were:\nA. The predominant disturbance is a loss of or alteration in physical functioning suggesting a physical disorder. It is involuntary and medically unexplainable\nB. One of the following must also be present:\n\nA temporal relationship between symptom onset and some external event of psychological conflict.\nThe symptom allows the individual to avoid unpleasant activity.\nThe symptom provides opportunity for support which may not have been otherwise available.\nDuring the COVID-19 pandemic, neurologists noticed an increase in adolescents and young adults presenting with functional tic-like behaviors to clinics around the world. Researchers believe that social media content regarding Tourette Syndrome influenced the sudden increase in functional tic-like behaviors. The majority of the people who experienced functional tic-like behaviors were female, and neurologists have reported an overrepresentation of transgender and non-binary identities within this group of adolescents and young adults.\nToday, there is a growing understanding that symptoms are real and distressing, and are caused by an incorrect functioning of the brain rather than being imagined or made up.\n\nSigns and symptoms\nThere are a great number of symptoms experienced by those with a functional neurological disorder. While these symptoms are very real, their origin is complex, since it can be associated with severe psychological trauma (conversion disorder), and idiopathic neurological dysfunction. The core symptoms are those of motor or sensory dysfunction or episodes of altered awareness:\n\nLimb weakness or paralysis\nBlackouts (also called dissociative or non-epileptic seizures\/attacks) \u2013 these may look like epileptic seizures or faints\nMovement disorders including tremors, dystonia (spasms), myoclonus (jerky movements)\nVisual symptoms including loss of vision or double vision\nSpeech symptoms including dysphonia (whispering speech), slurred or stuttering speech\nSensory disturbance including hemisensory syndrome (altered sensation down one side of the body)\nNumbness or inability to sense touch\nDizziness and balance problems\nPain (including chronic migraines)\nExtreme slowness and fatigue\n\nCauses\nA systematic review found that stressful life events and childhood neglect were significantly more common in patients with FND than the general population, although some patients report no stressors.\nConverging evidence from several studies using different techniques and paradigms has now demonstrated distinctive brain activation patterns associated with functional deficits, unlike those seen in actors simulating similar deficits.  The new findings advance current understanding of the mechanisms involved in this disease, and offer the possibility of identifying markers of the condition and patients' prognosis.\nFND has been reported as a rare occurrence in the period following general anesthesia.\n\nWho is likely to get FND?\nAnyone can develop FND. It is more common in women and can affect both children and adults. Most people with functional movement disorders begin to have symptoms around their late 30s. Symptoms of functional seizures most often begin in a person\u2019s late 20s. The fundamental of FND involve biological and sociological factors. While risk factors in adults include exposure to psychological stressors and a history of childhood adversity, those factors are not seen in all people with FND. In children, risk factors can include family problems, bullying, perceived peer pressure, and abuse. It is common for people with FND to also have depression, anxiety, or post-traumatic stress disorder. Some studies suggest that genetic or environmental factors may affect a person\u2019s risk. [1]\n\nDiagnosis\nA diagnosis of a functional neurological disorder is dependent on positive features from the history and examination.\nPositive features of functional weakness on examination include Hoover's sign, when there is weakness of hip extension which normalizes with contralateral hip flexion. Signs of functional tremor include entrainment and distractibility. The patient with tremor should be asked to copy rhythmical movements with one hand or foot. If the tremor of the other hand entrains to the same rhythm, stops, or if the patient has trouble copying a simple movement this may indicate a functional tremor. Functional dystonia usually presents with an inverted ankle posture or clenched fist.\nPositive features of dissociative or non-epileptic seizures include prolonged motionless unresponsiveness, long duration episodes (>2minutes) and symptoms of dissociation prior to the attack. These signs can be usefully discussed with patients when the diagnosis is being made.\nPatients with functional movement disorders and limb weakness may experience symptom onset triggered by an episode of acute pain, a physical injury or physical trauma. They may also experience symptoms when faced with a psychological stressor, but this isn't the case for most patients. Patients with functional neurological disorders are more likely to have a history of another illness such as irritable bowel syndrome, chronic pelvic pain or fibromyalgia but this cannot be used to make a diagnosis.\nFND does not show up on blood tests or structural brain imaging such as MRI or CT scanning. However, this is also the case for many other neurological conditions so negative investigations should not be used alone to make the diagnosis. FND can occur alongside other neurological diseases and tests may show non-specific abnormalities which cause confusion for doctors and patients.\n\nDSM-5 diagnostic criteria\nThe Diagnostic and Statistical Manual of Mental Illness (DSM-5) lists the following diagnostic criteria for functional neurological symptoms (conversion disorder):\n\nOne or more symptoms of altered voluntary motor or sensory function.\nClinical findings can provide evidence of incompatibility between the symptom and recognized neurological or medical conditions.\nAnother medical or mental disorder does not better explain the symptom or deficit.\nThe symptom or deficit results in clinically significant distress or impairment in social, occupational, or other vital areas of functioning or warrants medical evaluation.\nThe presence of symptoms defines an acute episode of functional neurologic disorder for less than six months, and persistent functional neurologic disorder includes the presence of symptoms for greater than six months. Functional neurologic disorder can also have the specifier of with or without the psychological stressor.\n\nAssociated conditions\nEpidemiological studies and meta-analysis have shown higher rates of depression and anxiety in patients with FND compared to the general population, but rates are similar to patients with other neurological disorders such as epilepsy or Parkinson's disease.  This is often the case because of years of misdiagnosis and accusations of malingering.\n\nDifferential diagnoses\nMultiple sclerosis has some overlapping symptoms with FND, potentially a source of misdiagnosis.\n\nPrevalence\nDissociative (non-epileptic) seizures account for about 1 in 7 referrals to neurologists after an initial seizure, and functional weakness has a similar prevalence to multiple sclerosis.\n\nTreatment\nTreatment requires a firm and transparent diagnosis based on positive features which both health professionals and patients can feel confident about. It is essential that the health professional confirms that this is a common problem which is genuine, not imagined and not a diagnosis of exclusion.\nA multi-disciplinary approach to treating functional neurological disorder is recommended.\nTreatment options can include:\n\nMedication such as sleeping tablets, painkillers, anti-epileptic medications and anti-depressants (for patients with depression co-morbid or for pain relief)\nCognitive behavior therapy (CBT) can help a person modify their thought patterns to change emotions, mood, or behavior\nPhysiotherapy and occupational therapy\nPhysiotherapy with someone who understands functional disorders may be the initial treatment of choice for patients with motor symptoms such as weakness, gait (walking) disorder and movement disorders.  Nielsen et al. have reviewed the medical literature on physiotherapy for functional motor disorders up to 2012 and concluded that the available studies, although limited, mainly report positive results.\nFor many patients with FND, accessing treatment can be difficult.  Availability of expertise is limited and they may feel that they are being dismissed or told 'it's all in your head' especially if psychological input is part of the treatment plan. Some medical professionals are uncomfortable explaining and treating patients with functional symptoms.  Changes in the diagnostic criteria, increasing evidence, literature about how to make the diagnosis and how to explain it and changes in medical training is slowly changing this.\nPeople with functional or dissociative seizures should try to identify warning signs and learn techniques to avoid harm or injury during and after the seizure. Be aware that relapses and flare-ups often recur, despite treatment.\n\nControversy\nThere was historically much controversy surrounding the FND diagnosis. Many doctors continue to believe that all FND patients have unresolved traumatic events (often of a sexual nature) which are being expressed in a physical way. However, some doctors do not believe this to be the case. Wessely and White have argued that FND may merely be an unexplained somatic symptom disorder. FND remains a stigmatized condition in the healthcare setting.\n\nReferences\n\n\n== Further reading ==","127":"M\u00e9ni\u00e8re's disease (MD) is a disease of the inner ear that is characterized by potentially severe and incapacitating episodes of vertigo, tinnitus, hearing loss, and a feeling of fullness in the ear. Typically, only one ear is affected initially, but over time, both ears may become involved. Episodes generally last from 20 minutes to a few hours. The time between episodes varies. The hearing loss and ringing in the ears can become constant over time.\nThe cause of M\u00e9ni\u00e8re's disease is unclear, but likely involves both genetic and environmental factors. A number of theories exist for why it occurs, including constrictions in blood vessels, viral infections, and autoimmune reactions. About 10% of cases run in families. Symptoms are believed to occur as the result of increased fluid buildup in the labyrinth of the inner ear. Diagnosis is based on the symptoms and a hearing test. Other conditions that may produce similar symptoms include vestibular migraine and transient ischemic attack.\nNo cure is known. Attacks are often treated with medications to help with the nausea and anxiety. Measures to prevent attacks are overall poorly supported by the evidence. A low-salt diet, diuretics, and corticosteroids may be tried. Physical therapy may help with balance and counselling may help with anxiety. Injections into the ear or surgery may also be tried if other measures are not effective, but are associated with risks. The use of tympanostomy tubes (ventilation tubes) to improve vertigo and hearing in people with M\u00e9ni\u00e8re's disease is not supported by definitive evidence.\nM\u00e9ni\u00e8re's disease was identified in the early 1800s by Prosper Meni\u00e8re. It affects between 0.3 and 1.9 per 1,000 people. It typically starts in people 40 to 60 years old. Females are more commonly affected than males. After 5 to 15 years of symptoms, the episodes of the world spinning sometimes stop and the person is left with loss of balance, poor hearing in the affected ear, and ringing or other sounds in the affected ear or ears.\n\nSigns and symptoms\nM\u00e9ni\u00e8re's is characterized by recurrent episodes of vertigo, fluctuating hearing loss, and tinnitus; episodes may be preceded by a headache and a feeling of fullness in the ears. People may also experience additional symptoms related to irregular reactions of the autonomic nervous system. These symptoms are not symptoms of M\u00e9ni\u00e8re's disease per se, but rather are side effects resulting from failure of the organ of hearing and balance, and include nausea, vomiting, and sweating, which are typically symptoms of vertigo, and not of M\u00e9ni\u00e8re's. This includes a sensation of being pushed sharply to the floor from behind. Sudden falls without loss of consciousness (drop attacks) may be experienced by some people.\n\nCauses\nThe cause of M\u00e9ni\u00e8re's disease is unclear, but likely involves both genetic and environmental factors. A number of theories exist including constrictions in blood vessels, viral infections, and autoimmune reactions.\n\nMechanism\nThe initial triggers of M\u00e9ni\u00e8re's disease are not fully understood, with a variety of potential inflammatory causes that lead to endolymphatic hydrops, a distension of the endolymphatic spaces in the inner ear. Endolymphatic hydrops (EH) is strongly associated with developing M\u00e9ni\u00e8re's disease, but not everyone with EH develops M\u00e9ni\u00e8re's disease: \"The relationship between endolymphatic hydrops and Meniere's disease is not a simple, ideal correlation.\"\nAdditionally, in fully developed M\u00e9ni\u00e8re's disease, the balance system (vestibular system) and the hearing system (cochlea) of the inner ear are affected, but some cases occur where EH affects only one of the two systems enough to cause symptoms. The corresponding subtypes of the disease are called vestibular M\u00e9ni\u00e8re's disease, showing symptoms of vertigo, and cochlear M\u00e9ni\u00e8re's disease, showing symptoms of hearing loss and tinnitus.\nThe mechanism of M\u00e9ni\u00e8re's disease is not fully explained by EH, but fully developed EH may mechanically and chemically interfere with the sensory cells for balance and hearing, which can lead to temporary dysfunction and even to death of the sensory cells, which in turn can cause the typical symptoms of MD \u2013 vertigo, hearing loss, and tinnitus.\nAn estimated 30% of people with M\u00e9ni\u00e8re's disease have Eustachian tube dysfunction.\n\nDiagnosis\nThe diagnostic criteria as of 2015 define definite MD and probable MD as:\n\nDefinite\nTwo or more spontaneous episodes of vertigo, each lasting 20 minutes to 12 hours\nAudiometrically documented low- to medium-frequency sensorineural hearing loss in the affected ear on at least one occasion before, during, or after one of the episodes of vertigo\nFluctuating aural symptoms (hearing, tinnitus, or fullness) in the affected ear\nNot better accounted for by another vestibular diagnosis\nProbable\n\nTwo or more episodes of vertigo or dizziness, each lasting 20 minutes to 24 hours\nFluctuating aural symptoms (hearing, tinnitus, or fullness) in the reported ear\nNot better accounted for by another vestibular diagnosis\nA common and important symptom of MD is hypersensitivity to sounds. This hypersensitivity is easily diagnosed by measuring the loudness discomfort levels (LDLs).\nSymptoms of MD overlap with migraine-associated vertigo (MAV) in many ways, but when hearing loss develops in MAV, it is usually in both ears, and this is rare in MD, and hearing loss generally does not progress in MAV as it does in MD.\nPeople who have had a transient ischemic attack (TIA) or stroke can present with symptoms similar to MD, and in people at risk magnetic resonance imaging  should be conducted to exclude TIA or stroke.\nOther vestibular conditions that should be excluded include vestibular paroxysmia, recurrent unilateral vestibulopathy, vestibular schwannoma, or a tumor of the endolymphatic sac.\n\nManagement\nNo cure for M\u00e9ni\u00e8re's disease is known, but medications, diet, physical therapy, and counseling, and some surgical approaches can be used to manage it. More than 85% of patients with M\u00e9ni\u00e8re's disease get better from changes in lifestyle, medical treatment, or minimally invasive surgical procedures. Those procedures include intratympanic steroid therapy, intratympanic gentamicin therapy or endolymphatic sac surgery.\n\nMedications\nDuring MD episodes, medications to reduce nausea are used, as are drugs to reduce the anxiety caused by vertigo. For longer-term treatment to stop progression, the evidence base is weak for all treatments. Although a causal relation between allergy and M\u00e9ni\u00e8re's disease is uncertain, medication to control allergies may be helpful. To assist with vertigo and balance problems, glycopyrrolate has been found to be a useful vestibular suppressant in patients with M\u00e9ni\u00e8re's disease.\nDiuretics, such as the thiazide-like diuretic chlortalidone, are widely used to manage MD on the theory that it reduces fluid buildup (pressure) in the ear. Based on evidence from multiple but small clinical trials, diuretics appear to be useful for reducing the frequency of episodes of dizziness but do not seem to prevent hearing loss.\nIn cases where hearing loss and continuing severe episodes of vertigo occur, a chemical labyrinthectomy, in which a medication such as gentamicin is injected into the middle ear and kills parts of the vestibular apparatus, may be prescribed. This treatment has the risk of worsening hearing loss.\n\nDiet\nPeople with MD are often advised to reduce their sodium intake. Reducing salt intake, however, has not been well studied. Based on the assumption that MD is similar in nature to a migraine, some advise eliminating \"migraine triggers\" such as caffeine, but the evidence for this is weak. There is no high-quality evidence that changing diet by restricting salt, caffeine or alcohol improves symptoms.\n\nPhysical therapy\nWhile use of physical therapy early after the onset of MD is probably not useful due to the fluctuating disease course, physical therapy to help retraining of the balance system appears to be useful to reduce both subjective and objective deficits in balance over the longer term.\n\nCounseling\nThe psychological distress caused by the vertigo and hearing loss may worsen the condition in some people. Counseling may be useful to manage the distress, as may education and relaxation techniques.\n\nSurgery\nIf symptoms do not improve with less invasive approaches and for cases where the condition is uncontrolled or persistent and affecting both ears, surgery may be considered.\n\nEndolymphatic sac surgery\nSurgery to decompress the endolymphatic sac is one surgical approach that is sometimes suggested. Three methods of surgical endolymphatic sac decompression are sometimes suggested \u2013 simple decompression, insertion of a shunt, or removal of the sac. There is some very weak evidence that all three methods may be useful for reducing dizziness, but that the level of evidence supporting these surgical procedures is low with further higher quality investigations being suggested. There is a risk in these types of surgical procedures that the shunts used in these surgeries are at risk of becoming displaced or misplaced. For those with severe cases who are eligible for endolymphatic sac decompression, a 2014 systematic review reported that in at least 75% of people, EL sac decompression was effective at controlling vertigo in the short term (>1 year of follow-up) and long term (>24 months).\n\nVentilation tubes\nSurgical implantation of eustachian tubes (ventilation tubes) is not strongly supported by medical studies. There are some tentative evidence of benefit from tympanostomy tubes for improvement in the unsteadiness associated with the disease, conclusions about how effective this surgery is and the potential for side effects and harms is not clear.\n\nOther surgical interventions\nDestructive surgeries such as vestibular nerve labyrinthectomy are irreversible and involve removing entire functionality of most, if not all, of the affected ear; as of 2013, almost no evidence existed with which to judge whether these surgeries are effective. The inner ear itself can be surgically removed via labyrinthectomy, although hearing is always completely lost in the affected ear with this operation. The surgeon can also cut the nerve to the balance portion of the inner ear in a vestibular neurectomy. The hearing is often mostly preserved; however, the surgery involves cutting open into the lining of the brain, and a hospital stay of a few days for monitoring is required.\n\nPoorly supported\nAs of 2014, betahistine is often used as it is inexpensive and safe; but evidence does not justify its use in M\u00e9ni\u00e8re's disease.\nTranstympanic micropressure pulses were investigated in two systematic reviews. Neither found evidence to justify this technique.\nIntratympanic steroids were investigated in three systematic reviews. The data were found to be insufficient to decide if this therapy has positive effects.\nEvidence does not support the use of alternative medicine such as acupuncture or herbal supplements.\n\nPrognosis\nM\u00e9ni\u00e8re's disease usually starts confined to one ear; it extends to both ears in about 30% of cases. People may start out with only one symptom, but in M\u00e9ni\u00e8re's disease all three appear with time. Hearing loss usually fluctuates in the beginning stages and becomes more permanent in later stages. M\u00e9ni\u00e8re's disease has a course of 5\u201315 years, and people generally end up with mild disequilibrium, tinnitus, and moderate hearing loss in one ear.\nAs of 2020, there has been no recent major breakthrough in the pathogenesis research of M\u00e9ni\u00e8re's disease.\n\nEpidemiology\nFrom 3 to 11% of diagnosed dizziness in neuro-otological clinics are due to M\u00e9ni\u00e8re's disease. The annual incidence rate is estimated to be about 15 cases per 100,000 people and the prevalence rate is about 218 per 100,000, and around 15% of people with M\u00e9ni\u00e8re's disease are older than 65. In around 9% of cases, a relative also had M\u00e9ni\u00e8re's disease, indicating a genetic predisposition in some cases.\nThe odds of M\u00e9ni\u00e8re's disease are greater for people of white ethnicity, with severe obesity, and women. Several conditions are often comorbid with M\u00e9ni\u00e8re's disease, including arthritis, psoriasis, gastroesophageal reflux disease, irritable bowel syndrome, and migraine.\n\nHistory\nThe condition is named after the French physician Prosper Meni\u00e8re, who in an 1861 article described the main symptoms and was the first to suggest a single disorder for all of the symptoms, in the combined organ of balance and hearing in the inner ear.\nThe American Academy of Otolaryngology \u2013 Head and Neck Surgery Committee on Hearing and Equilibrium set criteria for diagnosing MD, as well as defining two subcategories \u2013 cochlear (without vertigo) and vestibular (without deafness).\nIn 1972, the academy defined criteria for diagnosing MD as:\n\nFluctuating, progressive, sensorineural deafness\nEpisodic, characteristic definitive spells of vertigo lasting 20 minutes to 24 hours with no unconsciousness, vestibular nystagmus always present.\nTinnitus (ringing in the ears, from mild to severe) is accompanied often by ear pain and a feeling of fullness in the affected ear; usually, the tinnitus is more severe before a spell of vertigo and lessens after the vertigo attack.\nAttacks are characterized by periods of remission and exacerbation.\nIn 1985, this list changed to alter wording, such as changing \"deafness\" to \"hearing loss associated with tinnitus, characteristically of low frequencies\" and requiring more than one attack of vertigo to diagnose. Finally in 1995, the list was again altered to allow for degrees of the disease:\n\nCertain \u2013 Definite disease with histopathological confirmation\nDefinite \u2013 Requires two or more definitive episodes of vertigo with hearing loss plus tinnitus and\/or aural fullness\nProbable \u2013 Only one definitive episode of vertigo and the other symptoms and signs\nPossible \u2013 Definitive vertigo with no associated hearing loss\nIn 2015, the International Classification for Vestibular Disorders Committee of the Barany Society published consensus diagnostic criteria in collaboration with the American Academy of Otolaryngology\u2013Head and Neck Surgery, the European Academy of Otology and Neurootology, the Japan Society for Equilibrium Research, and the Korean Balance Society.\n\nReferences\nExternal links\n\nBasura GJ, Adams ME, Monfared A, et al. (8 April 2020). \"Clinical Practice Guideline: M\u00e9ni\u00e8re's Disease\". Otolaryngology\u2013Head and Neck Surgery. 162 (2 suppl): S1\u2013S55. doi:10.1177\/0194599820909438. PMID 32267799.\nMeni\u00e8re's Disease, Stanford Ear Institute.","128":"A neurotransmitter is a signaling molecule secreted by a neuron to affect another cell across a synapse. The cell receiving the signal, or target cell, may be another neuron, but could also be a gland or muscle cell.\nNeurotransmitters are released from synaptic vesicles into the synaptic cleft where they are able to interact with neurotransmitter receptors on the target cell. The neurotransmitter's effect on the target cell is determined by the receptor it binds to. Many neurotransmitters are synthesized from simple and plentiful precursors such as amino acids, which are readily available and often require a small number of biosynthetic steps for conversion.\nNeurotransmitters are essential to the function of complex neural systems. The exact number of unique neurotransmitters in humans is unknown, but more than 100 have been identified. Common neurotransmitters include glutamate, GABA, acetylcholine, glycine and norepinephrine.\n\nMechanism and cycle\nSynthesis\nNeurotransmitters are generally synthesized in neurons and are made up of, or derived from, precursor molecules that are found abundantly in the cell. Classes of neurotransmitters include amino acids, monoamines, and peptides. Monoamines are synthesized by altering a single amino acid. For example, the precursor of serotonin is the amino acid tryptophan. Peptide transmitters, or neuropeptides, are protein transmitters that often are released together with other transmitters to have a modulatory effect. Purine neurotransmitters, like ATP, are derived from nucleic acids. Other neurotransmitters are made up of metabolic products like nitric oxide and carbon monoxide.\n\nStorage\nNeurotransmitters are generally stored in synaptic vesicles, clustered close to the cell membrane at the axon terminal of the presynaptic neuron. However, some neurotransmitters, like the metabolic gases carbon monoxide and nitric oxide, are synthesized and released immediately following an action potential without ever being stored in vesicles.\n\nRelease\nGenerally, a neurotransmitter is released at the presynaptic terminal in response to an electrical signal called an action potential in the presynaptic neuron. However, low level 'baseline' release also occurs without electrical stimulation. Neurotransmitters are released into and diffuse across the synaptic cleft, where they bind to specific receptors on the membrane of the postsynaptic neuron.\n\nReceptor interaction\nAfter being released into the synaptic cleft, neurotransmitters diffuse across the synapse where they are able to interact with receptors on the target cell. The effect of the neurotransmitter is dependent on the identity of the target cell's receptors present at the synapse. Depending on the receptor, binding of neurotransmitters may cause excitation, inhibition, or modulation of the postsynaptic neuron. See below for more information.\n\nElimination\nIn order to avoid continuous activation of receptors on the post-synaptic or target cell, neurotransmitters must be removed from the synaptic cleft.  Neurotransmitters are removed through one of three mechanisms:\n\nDiffusion \u2013 neurotransmitters drift out of the synaptic cleft, where they are absorbed by glial cells. These glial cells, usually astrocytes, absorb the excess neurotransmitters.\nAstrocytes, a type of glial cell in the brain, actively contribute to synaptic communication through astrocytic diffusion or gliotransmission. Neuronal activity triggers an increase in astrocytic calcium levels, prompting the release of gliotransmitters, such as glutamate, ATP, and D-serine.These gliotransmitters diffuse into the extracellular space, interacting with nearby neurons and influencing synaptic transmission. By regulating extracellular neurotransmitter levels, astrocytes help maintain proper synaptic function. This bidirectional communication between astrocytes and neurons add complexity to brain signaling, with implications for brain function and neurological disorders.\nEnzyme degradation \u2013 proteins called enzymes break the neurotransmitters down.\nReuptake \u2013 neurotransmitters are reabsorbed into the pre-synaptic neuron. Transporters, or membrane transport proteins, pump neurotransmitters from the synaptic cleft back into axon terminals (the presynaptic neuron) where they are stored for reuse.\nFor example, acetylcholine is eliminated by having its acetyl group cleaved by the enzyme acetylcholinesterase; the remaining choline is then taken in and recycled by the pre-synaptic neuron to synthesize more acetylcholine. Other neurotransmitters are able to diffuse away from their targeted synaptic junctions and are eliminated from the body via the kidneys, or destroyed in the liver. Each neurotransmitter has very specific degradation pathways at regulatory points, which may be targeted by the body's regulatory system or medication. Cocaine blocks a dopamine transporter responsible for the reuptake of dopamine. Without the transporter, dopamine diffuses much more slowly from the synaptic cleft and continues to activate the dopamine receptors on the target cell.\n\nDiscovery\nUntil the early 20th century, scientists assumed that the majority of synaptic communication in the brain was electrical. However, through histological examinations by Ram\u00f3n y Cajal, a 20 to 40 nm gap between neurons, known today as the synaptic cleft, was discovered. The presence of such a gap suggested communication via chemical messengers traversing the synaptic cleft, and in 1921 German pharmacologist Otto Loewi confirmed that neurons can communicate by releasing chemicals. Through a series of experiments involving the vagus nerves of frogs, Loewi was able to manually slow the heart rate of frogs by controlling the amount of saline solution present around the vagus nerve. Upon completion of this experiment, Loewi asserted that sympathetic regulation of cardiac function can be mediated through changes in chemical concentrations. Furthermore, Otto Loewi is credited with discovering acetylcholine (ACh) \u2013 the first known neurotransmitter.\n\nIdentification\nTo identify neurotransmitters, the following criteria are typically considered:\n\nSynthesis: The chemical must be produced within the neuron or be present in it as a precursor molecule.\nRelease and Response: When the neuron is activated, the chemical must be released and elicit a response in target cells or neurons.\nExperimental Response: Application of the chemical directly to the target cells should produce the same response observed when the chemical is naturally released from neurons.\nRemoval Mechanism: There must be a mechanism in place to remove the neurotransmitter from its site of action once its signaling role is complete.\nHowever, given advances in pharmacology, genetics, and chemical neuroanatomy, the term \"neurotransmitter\" can be applied to chemicals that:\n\nCarry messages between neurons via influence on the postsynaptic membrane.\nHave little or no effect on membrane voltage, but have a common carrying function such as changing the structure of the synapse.\nCommunicate by sending reverse-direction messages that affect the release or reuptake of transmitters.\nThe anatomical localization of neurotransmitters is typically determined using immunocytochemical techniques, which identify the location of either the transmitter substances themselves or of the enzymes that are involved in their synthesis. Immunocytochemical techniques have also revealed that many transmitters, particularly the neuropeptides, are co-localized, that is, a neuron may release more than one transmitter from its synaptic terminal. Various techniques and experiments such as staining, stimulating, and collecting can be used to identify neurotransmitters throughout the central nervous system.\n\nActions\nNeurons communicate with each other through synapses, specialized contact points where neurotransmitters transmit signals. When an action potential reaches the presynaptic terminal, the action potential can trigger the release of neurotransmitters into the synaptic cleft. These neurotransmitters then bind to receptors on the postsynaptic membrane, influencing the receiving neuron in either an inhibitory or excitatory manner. If the overall excitatory influences outweigh the inhibitory influences, the receiving neuron may generate its own action potential, continuing the transmission of information to the next neuron in the network. This process allows for the flow of information and the formation of complex neural networks.\n\nModulation\nA neurotransmitter may have an excitatory, inhibitory or modulatory effect on the target cell. The effect is determined by the receptors the neurotransmitter interacts with at the post-synaptic membrane. Neurotransmitter influences trans-membrane ion flow either to increase (excitatory) or to decrease (inhibitory) the probability that the cell with which it comes in contact will produce an action potential. Synapses containing receptors with excitatory effects are called Type I synapses, while Type II synapses contain receptors with inhibitory effects. Thus, despite the wide variety of synapses, they all convey messages of only these two types. The two types are different appearance and are primarily located on different parts of the neurons under its influence. Receptors with modulatory effects are spread throughout all synaptic membranes and binding of neurotransmitters sets in motion signaling cascades that help the cell regulate its function. Binding of neurotransmitters to receptors with modulatory effects can have many results. For example, it may result in an increase or decrease in sensitivity to future stimulus by recruiting more or less receptors to the synaptic membrane.\nType I (excitatory) synapses are typically located on the shafts or the spines of dendrites, whereas type II (inhibitory) synapses are typically located on a cell body. In addition, Type I synapses have round synaptic vesicles, whereas the vesicles of type II synapses are flattened. The material on the presynaptic and post-synaptic membranes is denser in a Type I synapse than it is in a Type II, and the Type I synaptic cleft is wider. Finally, the active zone on a Type I synapse is larger than that on a Type II synapse.\nThe different locations of Type I and Type II synapses divide a neuron into two zones: an excitatory dendritic tree and an inhibitory cell body. From an inhibitory perspective, excitation comes in over the dendrites and spreads to the axon hillock to trigger an action potential. If the message is to be stopped, it is best stopped by applying inhibition on the cell body, close to the axon hillock where the action potential originates. Another way to conceptualize excitatory\u2013inhibitory interaction is to picture excitation overcoming inhibition. If the cell body is normally in an inhibited state, the only way to generate an action potential at the axon hillock is to reduce the cell body's inhibition. In this \"open the gates\" strategy, the excitatory message is like a racehorse ready to run down the track, but first, the inhibitory starting gate must be removed.\n\nNeurotransmitter actions\nAs explained above, the only direct action of a neurotransmitter is to activate a receptor. Therefore, the effects of a neurotransmitter system depend on the connections of the neurons that use the transmitter, and the chemical properties of the receptors.\n\nGlutamate is used at the great majority of fast excitatory synapses in the brain and spinal cord. It is also used at most synapses that are \"modifiable\", i.e. capable of increasing or decreasing in strength. Modifiable synapses are thought to be the main memory-storage elements in the brain. Excessive glutamate release can overstimulate the brain and lead to excitotoxicity causing cell death resulting in seizures or strokes. Excitotoxicity has been implicated in certain chronic diseases including ischemic stroke, epilepsy, amyotrophic lateral sclerosis, Alzheimer's disease, Huntington disease, and Parkinson's disease.\nGABA is used at the great majority of fast inhibitory synapses in virtually every part of the brain. Many sedative\/tranquilizing drugs act by enhancing the effects of GABA. Correspondingly, glycine is the inhibitory transmitter in the spinal cord.\nAcetylcholine was the first neurotransmitter discovered in the peripheral and central nervous systems. It activates skeletal muscles in the somatic nervous system and may either excite or inhibit internal organs in the autonomic system. It is distinguished as the transmitter at the neuromuscular junction connecting motor nerves to muscles. The paralytic arrow-poison curare acts by blocking transmission at these synapses. Acetylcholine also operates in many regions of the brain, but using different types of receptors, including nicotinic and muscarinic receptors.\nDopamine has a number of important functions in the brain; this includes regulation of motor behavior, pleasures related to motivation and also emotional arousal. It plays a critical role in the reward system; Parkinson's disease has been linked to low levels of dopamine and schizophrenia has been linked to high levels of dopamine.\nSerotonin is a monoamine neurotransmitter. Most is produced by and found in the intestine (approximately 90%), and the remainder in central nervous system neurons. It functions to regulate appetite, sleep, memory and learning, temperature, mood, behaviour, muscle contraction, and function of the cardiovascular system and endocrine system. It is speculated to have a role in depression, as some depressed patients are seen to have lower concentrations of metabolites of serotonin in their cerebrospinal fluid and brain tissue.\nNorepinephrine is a member of the catecholamine classification of neurotransmitters. It is synthesized from the amino acid tyrosine. In the peripheral nervous system, one of the primary roles of norepinephrine is to stimulate the release of the stress hormone epinephrine (i.e. adrenaline) from the adrenal glands.\nEpinephrine, a neurotransmitter and hormone is synthesized from tyrosine. It is released from the adrenal glands and plays a role in the fight-or-flight response. Epinephrine has vasoconstrictive effects, which promote increased heart rate, blood pressure, energy mobilization. Vasoconstriction influences metabolism by promoting the breakdown of glucose released into the bloodstream. Epinephrine also has bronchodilation effects, which is the relaxing of airways.\n\nTypes\nThere are many different ways to classify neurotransmitters. Dividing them into amino acids, peptides, and monoamines is sufficient for some classification purposes.\nMajor neurotransmitters:\n\nAmino acids: glutamate, aspartate, D-serine, gamma-Aminobutyric acid (GABA), glycine\nGasotransmitters: nitric oxide (NO), carbon monoxide (CO), hydrogen sulfide (H2S)\nMonoamines:\nCatecholamines: dopamine (DA), norepinephrine (noradrenaline, NE), epinephrine (adrenaline)\nIndolamines: serotonin (5-HT, SER), melatonin\nhistamine\nTrace amines: phenethylamine, N-methylphenethylamine, tyramine, 3-iodothyronamine, octopamine, tryptamine, etc.\nPeptides: oxytocin, somatostatin, substance P, cocaine and amphetamine regulated transcript, opioid peptides\nPurines: adenosine triphosphate (ATP), adenosine\nOthers: acetylcholine (ACh), anandamide, etc.\nIn addition, over 100 neuroactive peptides have been found, and new ones are discovered regularly. Many of these are co-released along with a small-molecule transmitter. Nevertheless, in some cases, a peptide is the primary transmitter at a synapse. Beta-Endorphin is a relatively well-known example of a peptide neurotransmitter because it engages in highly specific interactions with opioid receptors in the central nervous system.\nSingle ions (such as synaptically released zinc) are also considered neurotransmitters by some, as well as some gaseous molecules such as nitric oxide (NO), carbon monoxide (CO), and hydrogen sulfide (H2S). The gases are produced in the neural cytoplasm and are immediately diffused through the cell membrane into the extracellular fluid and into nearby cells to stimulate production of second messengers. Soluble gas neurotransmitters are difficult to study, as they act rapidly and are immediately broken down, existing for only a few seconds.\nThe most prevalent transmitter is glutamate, which is excitatory at well over 90% of the synapses in the human brain. The next most prevalent is gamma-Aminobutyric Acid, or GABA, which is inhibitory at more than 90% of the synapses that do not use glutamate. Although other transmitters are used in fewer synapses, they may be very important functionally: the great majority of psychoactive drugs exert their effects by altering the actions of some neurotransmitter systems, often acting through transmitters other than glutamate or GABA. Addictive drugs such as cocaine and amphetamines exert their effects primarily on the dopamine system. The addictive opiate drugs exert their effects primarily as functional analogs of opioid peptides, which, in turn, regulate dopamine levels.\n\nList of neurotransmitters, peptides, and gaseous signaling molecules\nNeurotransmitter systems\nNeurons expressing certain types of neurotransmitters sometimes form distinct systems, where activation of the system affects large volumes of the brain, called volume transmission. Major neurotransmitter systems include the noradrenaline (norepinephrine) system, the dopamine system, the serotonin system, and the cholinergic system, among others. Trace amines have a modulatory effect on neurotransmission in monoamine pathways (i.e., dopamine, norepinephrine, and serotonin pathways) throughout the brain via signaling through trace amine-associated receptor 1. A brief comparison of these systems follows:\n\nDrug effects\nUnderstanding the effects of drugs on neurotransmitters comprises a significant portion of research initiatives in the field of neuroscience. Most neuroscientists involved in this field of research believe that such efforts may further advance our understanding of the circuits responsible for various neurological diseases and disorders, as well as ways to effectively treat and someday possibly prevent or cure such illnesses.\nDrugs can influence behavior by altering neurotransmitter activity. For instance, drugs can decrease the rate of synthesis of neurotransmitters by affecting the synthetic enzyme(s) for that neurotransmitter. When neurotransmitter syntheses are blocked, the amount of neurotransmitters available for release becomes substantially lower, resulting in a decrease in neurotransmitter activity. Some drugs block or stimulate the release of specific neurotransmitters.  Alternatively, drugs can prevent neurotransmitter storage in synaptic vesicles by causing the synaptic vesicle membranes to leak. Drugs that prevent a neurotransmitter from binding to its receptor are called receptor antagonists. For example, drugs used to treat patients with schizophrenia such as haloperidol, chlorpromazine, and clozapine are antagonists at receptors in the brain for dopamine. Other drugs act by binding to a receptor and mimicking the normal neurotransmitter. Such drugs are called receptor agonists. An example of a receptor agonist is morphine, an opiate that mimics effects of the endogenous neurotransmitter \u03b2-endorphin to relieve pain. Other drugs interfere with the deactivation of a neurotransmitter after it has been released, thereby prolonging the action of a neurotransmitter. This can be accomplished by blocking re-uptake or inhibiting degradative enzymes. Lastly, drugs can also prevent an action potential from occurring, blocking neuronal activity throughout the central and peripheral nervous system. Drugs such as tetrodotoxin that block neural activity are typically lethal.\nDrugs targeting the neurotransmitter of major systems affect the whole system, which can explain the complexity of action of some drugs. Cocaine, for example, blocks the re-uptake of dopamine back into the presynaptic neuron, leaving the neurotransmitter molecules in the synaptic gap for an extended period of time. Since the dopamine remains in the synapse longer, the neurotransmitter continues to bind to the receptors on the postsynaptic neuron, eliciting a pleasurable emotional response. Physical addiction to cocaine may result from prolonged exposure to excess dopamine in the synapses, which leads to the downregulation of some post-synaptic receptors. After the effects of the drug wear off, an individual can become depressed due to decreased probability of the neurotransmitter binding to a receptor. Fluoxetine is a selective serotonin re-uptake inhibitor (SSRI), which blocks re-uptake of serotonin by the presynaptic cell which increases the amount of serotonin present at the synapse and furthermore allows it to remain there longer, providing potential for the effect of naturally released serotonin. AMPT prevents the conversion of tyrosine to L-DOPA, the precursor to dopamine; reserpine prevents dopamine storage within vesicles; and deprenyl inhibits monoamine oxidase (MAO)-B and thus increases dopamine levels.\n\nAgonists\nAn agonist is a chemical capable of binding to a receptor, such as a neurotransmitter receptor, and initiating the same reaction typically produced by the binding of the endogenous substance. An agonist of a neurotransmitter will thus initiate the same receptor response as the transmitter. In neurons, an agonist drug may activate neurotransmitter receptors either directly or indirectly. Direct-binding agonists can be further characterized as full agonists, partial agonists, inverse agonists.\nDirect agonists act similar to a neurotransmitter by binding directly to its associated receptor site(s), which may be located on the presynaptic neuron or postsynaptic neuron, or both. Typically, neurotransmitter receptors are located on the postsynaptic neuron, while neurotransmitter autoreceptors are located on the presynaptic neuron, as is the case for monoamine neurotransmitters; in some cases, a neurotransmitter utilizes retrograde neurotransmission, a type of feedback signaling in neurons where the neurotransmitter is released postsynaptically and binds to target receptors located on the presynaptic neuron. Nicotine, a compound found in tobacco, is a direct agonist of most nicotinic acetylcholine receptors, mainly located in cholinergic neurons. Opiates, such as morphine, heroin, hydrocodone, oxycodone, codeine, and methadone, are \u03bc-opioid receptor agonists; this action mediates their euphoriant and pain relieving properties.\nIndirect agonists increase the binding of neurotransmitters at their target receptors by stimulating the release or preventing the reuptake of neurotransmitters. Some indirect agonists trigger neurotransmitter release and prevent neurotransmitter reuptake. Amphetamine, for example, is an indirect agonist of postsynaptic dopamine, norepinephrine, and serotonin receptors in each their respective neurons; it produces both neurotransmitter release into the presynaptic neuron and subsequently the synaptic cleft and prevents their reuptake from the synaptic cleft by activating TAAR1, a presynaptic G protein-coupled receptor, and binding to a site on VMAT2, a type of monoamine transporter located on synaptic vesicles within monoamine neurons.\n\nAntagonists\nAn antagonist is a chemical that acts within the body to reduce the physiological activity of another chemical substance (such as an opiate); especially one that opposes the action on the nervous system of a drug or a substance occurring naturally in the body by combining with and blocking its nervous receptor.\nThere are two main types of antagonist: direct-acting Antagonist and indirect-acting Antagonists:\n\nDirect-acting antagonist- which takes up space present on receptors which are otherwise taken up by neurotransmitters themselves. This results in neurotransmitters being blocked from binding to the receptors. An example of one of the most common is called Atropine.\nIndirect-acting antagonist- drugs that inhibit the release\/production of neurotransmitters (e.g., Reserpine).\n\nDrug antagonists\nAn antagonist drug is one that attaches (or binds) to a site called a receptor without activating that receptor to produce a biological response. It is therefore said to have no intrinsic activity. An antagonist may also be called a receptor \"blocker\" because they block the effect of an agonist at the site. The pharmacological effects of an antagonist, therefore, result in preventing the corresponding receptor site's agonists (e.g., drugs, hormones, neurotransmitters) from binding to and activating it. Antagonists may be \"competitive\" or \"irreversible\".\nA competitive antagonist competes with an agonist for binding to the receptor. As the concentration of antagonist increases, the binding of the agonist is progressively inhibited, resulting in a decrease in the physiological response. High concentration of an antagonist can completely inhibit the response. This inhibition can be reversed, however, by an increase of the concentration of the agonist, since the agonist and antagonist compete for binding to the receptor. Competitive antagonists, therefore, can be characterized as shifting the dose\u2013response relationship for the agonist to the right. In the presence of a competitive antagonist, it takes an increased concentration of the agonist to produce the same response observed in the absence of the antagonist.\nAn irreversible antagonist binds so strongly to the receptor as to render the receptor unavailable for binding to the agonist. Irreversible antagonists may even form covalent chemical bonds with the receptor. In either case, if the concentration of the irreversible antagonist is high enough, the number of unbound receptors remaining for agonist binding may be so low that even high concentrations of the agonist do not produce the maximum biological response.\n\nPrecursors\nWhile intake of neurotransmitter precursors does increase neurotransmitter synthesis, evidence is mixed as to whether neurotransmitter release and postsynaptic receptor firing is increased. Even with increased neurotransmitter release, it is unclear whether this will result in a long-term increase in neurotransmitter signal strength, since the nervous system can adapt to changes such as increased neurotransmitter synthesis and may therefore maintain constant firing. Some neurotransmitters may have a role in depression and there is some evidence to suggest that intake of precursors of these neurotransmitters may be useful in the treatment of mild and moderate depression.\n\nCatecholamine and trace amine precursors\nL-DOPA, a precursor of dopamine that crosses the blood\u2013brain barrier, is used in the treatment of Parkinson's disease. For depressed patients where low activity of the neurotransmitter norepinephrine is implicated, there is only little evidence for benefit of neurotransmitter precursor administration. L-phenylalanine and L-tyrosine are both precursors for dopamine, norepinephrine, and epinephrine. These conversions require vitamin B6, vitamin C, and S-adenosylmethionine. A few studies suggest potential antidepressant effects of L-phenylalanine and L-tyrosine, but there is much room for further research in this area.\n\nSerotonin precursors\nAdministration of L-tryptophan, a precursor for serotonin, is seen to double the production of serotonin in the brain. It is significantly more effective than a placebo in the treatment of mild and moderate depression. This conversion requires vitamin C. 5-hydroxytryptophan (5-HTP), also a precursor for serotonin, is more effective than a placebo.\n\nDiseases and disorders\nDiseases and disorders may also affect specific neurotransmitter systems. The following are disorders involved in either an increase, decrease, or imbalance of certain neurotransmitters.\nDopamine:\nFor example, problems in producing dopamine (mainly in the substantia nigra) can result in Parkinson's disease, a disorder that affects a person's ability to move as they want to, resulting in stiffness, tremors or shaking, and other symptoms. Some studies suggest that having too little or too much dopamine or problems using dopamine in the thinking and feeling regions of the brain may play a role in disorders like schizophrenia or attention deficit hyperactivity disorder (ADHD). Dopamine is also involved in addiction and drug use, as most recreational drugs cause an influx of dopamine in the brain (especially opioid and methamphetamines) that produces a pleasurable feeling, which is why users constantly crave drugs.\nSerotonin:\nSimilarly, after some research suggested that drugs that block the recycling, or reuptake, of serotonin seemed to help some people diagnosed with depression, it was theorized that people with depression might have lower-than-normal serotonin levels. Though widely popularized, this theory was not borne out in subsequent research. Therefore, selective serotonin reuptake inhibitors (SSRIs) are used to increase the amounts of serotonin in synapses.\nGlutamate:\nFurthermore, problems with producing or using glutamate have been suggestively and tentatively linked to many mental disorders, including autism, obsessive\u2013compulsive disorder (OCD), schizophrenia, and depression. Having too much glutamate has been linked to neurological diseases such as Parkinson's disease, multiple sclerosis, Alzheimer's disease, stroke, and ALS (amyotrophic lateral sclerosis).\n\nNeurotransmitter imbalance\nGenerally, there are no scientifically established \"norms\" for appropriate levels or \"balances\" of different neurotransmitters. It is in most cases pragmatically impossible to even measure levels of neurotransmitters in a brain or body at any distinct moments in time. Neurotransmitters regulate each other's release, and weak consistent imbalances in this mutual regulation were linked to temperament in healthy people. Strong imbalances or disruptions to neurotransmitter systems have been associated with many diseases and mental disorders. These include Parkinson's, depression, insomnia, Attention Deficit Hyperactivity Disorder (ADHD), anxiety, memory loss, dramatic changes in weight and addictions. Chronic physical or emotional stress can be a contributor to neurotransmitter system changes. Genetics also plays a role in neurotransmitter activities.\nApart from recreational use, medications that directly and indirectly interact with one or more transmitter or its receptor are commonly prescribed for psychiatric and psychological issues. Notably, drugs interacting with serotonin and norepinephrine are prescribed to patients with problems such as depression and anxiety\u2014though the notion that there is much solid medical evidence to support such interventions has been widely criticized. Studies shown that dopamine imbalance has an influence on multiple sclerosis and other neurological disorders.\n\nSee also\nNotes\nReferences\nExternal links\n\nPurves, Dale; Augustine, George J.; Fitzpatrick, David; Katz, Lawrence C.; LaMantia, Anthony-Samuel; McNamara, James O.; Williams, S. Mark (2001). \"Chapter 6. Neurotransmitters\". What Defines a Neurotransmitter? (2nd ed.). Sunderland (MA): Sinauer Associates. ISBN 0-87893-742-0. {{cite book}}: |journal= ignored (help)\nHolz, Ronald W.; Fisher, Stephen K. (1999). \"Chapter 10. Synaptic Transmission and Cellular Signaling: An Overview\". In Siegel, George J; Agranoff, Bernard W; Albers, R Wayne; Fisher, Stephen K; Uhler, Michael D (eds.). Synaptic Transmission (6th ed.). Philadelphia: Lippincott-Raven. ISBN 0-397-51820-X. {{cite book}}: |journal= ignored (help)\nNeurotransmitters and Neuroactive Peptides at Neuroscience for Kids website","129":"Nonsyndromic deafness is hearing loss that is not associated with other signs and symptoms. In contrast, syndromic deafness involves hearing loss that occurs with abnormalities in other parts of the body. Nonsyndromic deafness constitutes 75% of all hearing loss cases, and an estimated 100 genes are thought to be linked to this condition. About 80% are linked to autosomal recessive inheritance, 15% to autosomal dominant inheritance, 1-3% through the X chromosome, and 0.5-1% are associated with mitochondrial inheritance.\nGenetic changes are related to the following types of nonsyndromic deafness:\n\nDFNA: nonsyndromic deafness, autosomal dominant\nDFNB: nonsyndromic deafness, autosomal recessive\nDFNX: nonsyndromic deafness, X-linked\nnonsyndromic deafness, mitochondrial\nEach type is numbered in the order in which it was described. For example, DFNA1 was the first described autosomal dominant type of nonsyndromic deafness. Mitochondrial nonsyndromic deafness involves changes to the small amount of DNA found in mitochondria, the energy-producing centers within cells.\nMost forms of nonsyndromic deafness are associated with permanent hearing loss caused by damage to structures in the inner ear. The inner ear consists of three parts: a snail-shaped structure called the cochlea that helps process sound, nerves that send information from the cochlea to the brain, and structures involved with balance. Loss of hearing caused by changes in the inner ear is called sensorineural deafness. Hearing loss that results from changes in the middle ear is called conductive hearing loss. The middle ear contains three tiny bones that help transfer sound from the eardrum to the inner ear. Some forms of nonsyndromic deafness involve changes in both the inner ear and the middle ear; this combination is called mixed hearing loss.\nThe severity of hearing loss varies and can change over time. It can affect one ear (unilateral) or both ears (bilateral). Degrees of hearing loss range from mild (difficulty understanding soft speech) to profound (inability to hear even very loud noises). The loss may be stable, or it may progress as a person gets older. Particular types of nonsyndromic deafness often show distinctive patterns of hearing loss. For example, the loss may be more pronounced at high, middle, or low tones.\n\nClassification\nNonsyndromic deafness can occur at any age. Hearing loss that is present before a child learns to speak is classified as prelingual or congenital. Hearing loss that occurs after the development of speech is classified as postlingual.\n\nGenetics\nNonsyndromic deafness can have different patterns of inheritance. Between 75% and 80% of cases are inherited in an autosomal recessive pattern, which means two copies of the gene in each cell are altered. Usually, each parent of an individual with autosomal recessive deafness is a carrier of one copy of the altered gene. These carriers do not have hearing loss.\nAnother 20% to 25% of nonsyndromic deafness cases are autosomal dominant, which means one copy of the altered gene in each cell is sufficient to result in hearing loss. People with autosomal dominant deafness most often inherit an altered copy of the gene from a parent who has hearing loss.\nBetween 1% and 2% of cases show an X-linked pattern of inheritance, which means the mutated gene responsible for the condition is located on the X chromosome. Males with X-linked nonsyndromic deafness tend to develop more severe hearing loss earlier in life than females who inherit a copy of the same gene mutation. Fathers will not pass X-linked traits to their sons since they do not pass on the X chromosome to their male offspring.\nMitochondrial nonsyndromic deafness, which results from changes to the DNA in mitochondria, occurs in fewer than 1% of cases in the United States. The altered mitochondrial DNA is passed from a mother to her sons and daughters. This type of deafness is not inherited from fathers.\nLate onset progressive deafness is the most common neurological disability of the elderly. Although hearing loss of greater than 25 decibels is present in only 1% of young adults between the ages of 18 and 24 years of age, this increases to 10% in persons between 55 and 64 years of age and approximately 50% in octogenarians.\nThe relative contribution of heredity to age-related hearing impairment is not known, however the majority of inherited late-onset deafness is autosomal dominant and non-syndromic (Van Camp et al., 1997). Over forty genes associated with autosomal dominant non-syndromic hearing loss have been localized and of these fifteen have been cloned.\n\nGenes related to nonsyndromic deafness\nMutations in the ACTG1, CABP2, CDH23, CLDN14, COCH, COL11A2, DFNA5, ESPN, EYA4, GJB2, GJB6, KCNQ4, MYO15A, MYO6, MYO7A, OTOF, PCDH15, POU3F4, SLC26A4, STRC, TECTA, TMC1, TMIE, TMPRSS3, USH1C, and WFS1 genes cause nonsyndromic deafness, with weaker evidence currently implicating genes CCDC50, DIAPH1, DSPP, ESRRB, GJB3, GRHL2, GRXCR1, HGF, LHFPL5, LOXHD1, LRTOMT, MARVELD2, MIR96, MYH14, MYH9, MYO1A, MYO3A, OTOA, PJVK, POU4F3, PRPS1, PTPRQ, RDX, SERPINB6, SIX1, SLC17A8, TPRN, TRIOBP, SLC26A5, and WHRN.\nThe causes of nonsyndromic deafness can be complex.  Researchers have identified more than 30 genes that, when mutated, may cause nonsyndromic deafness; however, some of these genes have not been fully characterized.  Many genes related to deafness are involved in the development and function of the inner ear.  Gene mutations interfere with critical steps in processing sound, resulting in hearing loss.  Different mutations in the same gene can cause different types of hearing loss, and some genes are associated with both syndromic and nonsyndromic deafness.  In many families, the gene(s) involved have yet to be identified.\nDeafness can also result from environmental factors or a combination of genetic and environmental factors, including certain medications, peri-natal infections (infections occurring before or after birth), and exposure to loud noise over an extended period.\nTypes include:\n\nDiagnosis\nThe diagnosis of nonsyndromic deafness involves a comprehensive assessment to determine the cause of hearing loss in an individual without associated syndromic features. Key steps in the diagnosis may include:\n\nClinical evaluation: A detailed medical history will be obtained to identify factors that may contribute to hearing loss, such as exposure to loud noise, ototoxic medications, or a family history of hearing impairment. Additionally, a physical examination will be conducted to check for visible abnormalities or signs of underlying conditions.\nGenetic testing: Genetic testing may be recommended, especially if there is a family history of hearing loss. Nonsyndromic deafness can be caused by mutations in various genes associated with auditory function. Besides, high-throughput DNA sequencing methods can be employed to screen multiple genes simultaneously.\nAudiological testing: This may include different tests such as Pure-tone audiometry, Speech audiometry, Otoacoustic emissions, or Auditory brainstem response.\nIn some cases, other methods may be conducted, including imaging techniques such as CT or MRI, to examine the structures of the inner ear and identify any abnormalities in the cochlea or auditory nerve. Screening blood tests for metabolic conditions or infections that could contribute to hearing loss may also be recommended.\n\nTreatment\nTreatment is supportive and consists of management of- manifestations. Use of hearing aids and\/or cochlear implant, suitable educational programs can be offered. Periodic surveillance is also important.\n\nEpidemiology\nAbout 1 in 1,000 children in the United States is born with profound deafness. By age 9, about 3 in 1,000 children have hearing loss that affects the activities of daily living. More than half of these cases are caused by genetic factors. Most cases of genetic deafness (70% to 80%) are nonsyndromic; the remaining cases are caused by specific genetic syndromes. In adults, the chance of developing hearing loss increases with age; hearing loss affects half of all people older than 80 years.\n\nReferences\nFurther reading\n\n\n== External links ==","130":"Norepinephrine (NE), also called noradrenaline (NA) or noradrenalin, is an organic chemical in the catecholamine family that functions in the brain and body as a hormone, neurotransmitter and neuromodulator. The name \"noradrenaline\" (from Latin ad, \"near\", and ren, \"kidney\") is more commonly used in the United Kingdom, whereas \"norepinephrine\" (from Ancient Greek \u1f10\u03c0\u1fd0\u0301 (ep\u00ed), \"upon\", and \u03bd\u03b5\u03c6\u03c1\u03cc\u03c2 (nephr\u00f3s), \"kidney\") is usually preferred in the United States. \"Norepinephrine\" is also the international nonproprietary name given to the drug. Regardless of which name is used for the substance itself, parts of the body that produce or are affected by it are referred to as noradrenergic.\nThe general function of norepinephrine is to mobilize the brain and body for action. Norepinephrine release is lowest during sleep, rises during wakefulness, and reaches much higher levels during situations of stress or danger, in the so-called fight-or-flight response. In the brain, norepinephrine increases arousal and alertness, promotes vigilance, enhances formation and retrieval of memory, and focuses attention; it also increases restlessness and anxiety. In the rest of the body, norepinephrine increases heart rate and blood pressure, triggers the release of glucose from energy stores, increases blood flow to skeletal muscle, reduces blood flow to the gastrointestinal system, and inhibits voiding of the bladder and gastrointestinal motility.\nIn the brain, noradrenaline is produced in nuclei that are small yet exert powerful effects on other brain areas. The most important of these nuclei is the locus coeruleus, located in the pons. Outside the brain, norepinephrine is used as a neurotransmitter by sympathetic ganglia located near the spinal cord or in the abdomen, as well as Merkel cells located in the skin. It is also released directly into the bloodstream by the adrenal glands. Regardless of how and where it is released, norepinephrine acts on target cells by binding to and activating adrenergic receptors located on the cell surface.\nA variety of medically important drugs work by altering the actions of noradrenaline systems. Noradrenaline itself is widely used as an injectable drug for the treatment of critically low blood pressure. Stimulants often increase, enhance, or otherwise act as agonists of norepinephrine. Drugs such as cocaine and methylphenidate act as reuptake inhibitors of norepinephrine, as do some antidepressants, such as those in the SNRI class. One of the more notable drugs in the stimulant class is amphetamine, which acts as a dopamine and norepinephrine analog, reuptake inhibitor, as well as an agent that increases the amount of global catecholamine signaling throughout the nervous system by reversing transporters in the synapses. Beta blockers, which counter some of the effects of noradrenaline by blocking their receptors, are frequently used to treat glaucoma, migraine, and a range of cardiovascular problems. Alpha blockers, which counter a different set of noradrenaline effects, are used to treat several cardiovascular and psychiatric conditions. Alpha-2 agonists often have a sedating effect and are commonly used as anesthesia enhancers in surgery, as well as in treatment of drug or alcohol dependence. For reasons that are still unclear, some Alpha-2 drugs, such as guanfacine, have also been shown to be effective in the treatment of anxiety disorders and ADHD. Many important psychiatric drugs exert strong effects on noradrenaline systems in the brain, resulting in effects that may be helpful or harmful.\n\nStructure\nNorepinephrine is a catecholamine and a phenethylamine. Its structure differs from that of epinephrine only in that epinephrine has a methyl group attached to its nitrogen, whereas the methyl group is replaced by a hydrogen atom in norepinephrine. The prefix nor- is derived as an abbreviation of the word \"normal\", used to indicate a demethylated compound. Norepinephrine consists of a catechol moiety (a benzene ring with two adjoining hydroxyl groups in the meta-para position), and an ethylamine side chain consisting of a hydroxyl group bonded in the benzylic position.\n\nBiochemical mechanisms\nBiosynthesis\nNorepinephrine is synthesized from the amino acid tyrosine by a series of enzymatic steps in the adrenal medulla and postganglionic neurons of the sympathetic nervous system. While the conversion of tyrosine to dopamine occurs predominantly in the cytoplasm, the conversion of dopamine to norepinephrine by dopamine \u03b2-monooxygenase occurs predominantly inside neurotransmitter vesicles. The metabolic pathway is:\n\nPhenylalanine \u2192 Tyrosine \u2192 L-DOPA \u2192 Dopamine \u2192 Norepinephrine\nThus the direct precursor of norepinephrine is dopamine, which is synthesized indirectly from the essential amino acid phenylalanine or the non-essential amino acid tyrosine. These amino acids are found in nearly every protein and, as such, are provided by ingestion of protein-containing food, with tyrosine being the most common.\nPhenylalanine is converted into tyrosine by the enzyme phenylalanine hydroxylase, with molecular oxygen (O2) and tetrahydrobiopterin as cofactors.  Tyrosine is converted into L-DOPA by the enzyme tyrosine hydroxylase, with tetrahydrobiopterin, O2, and probably ferrous iron (Fe2+) as cofactors. Conversion of tyrosine to L-DOPA is inhibited by Metyrosine, a tyrosine analog. L-DOPA is converted into dopamine by the enzyme aromatic L-amino acid decarboxylase (also known as DOPA decarboxylase), with pyridoxal phosphate as a cofactor. Dopamine is then converted into norepinephrine by the enzyme dopamine \u03b2-monooxygenase (formerly known as dopamine \u03b2-hydroxylase), with O2 and ascorbic acid as cofactors.\nNorepinephrine itself can further be converted into epinephrine by the enzyme phenylethanolamine N-methyltransferase with S-adenosyl-L-methionine as cofactor.\n\nDegradation\nIn mammals, norepinephrine is rapidly degraded to various metabolites. The initial step in the breakdown can be catalyzed by either of the enzymes monoamine oxidase (mainly monoamine oxidase A) or COMT. From there, the breakdown can proceed by a variety of pathways. The principal end products are either Vanillylmandelic acid or a conjugated form of MHPG, both of which are thought to be biologically inactive and are excreted in the urine.\n\nFunctions\nCellular effects\nLike many other biologically active substances, norepinephrine exerts its effects by binding to and activating receptors located on the surface of cells. Two broad families of norepinephrine receptors have been identified, known as alpha and beta adrenergic receptors.  Alpha receptors are divided into subtypes \u03b11 and \u03b12; beta receptors into subtypes \u03b21, \u03b22, and \u03b23. All of these function as G protein-coupled receptors, meaning that they exert their effects via a complex second messenger system. Alpha-2 receptors usually have inhibitory effects, but many are located pre-synaptically (i.e., on the surface of the cells that release norepinephrine), so the net effect of alpha-2 activation is often a decrease in the amount of norepinephrine released.  Alpha-1 receptors and all three types of beta receptors usually have excitatory effects.\n\nStorage, release, and reuptake\nInside the brain norepinephrine functions as a neurotransmitter and neuromodulator, and is controlled by a set of mechanisms common to all monoamine neurotransmitters. After synthesis, norepinephrine is transported from the cytosol into synaptic vesicles by the vesicular monoamine transporter (VMAT). VMAT can be inhibited by Reserpine causing a decrease in neurotransmitter stores. Norepinephrine is stored in these vesicles until it is ejected into the synaptic cleft, typically after an action potential causes the vesicles to release their contents directly into the synaptic cleft through a process called exocytosis.\nOnce in the synapse, norepinephrine binds to and activates receptors.  After an action potential, the norepinephrine molecules quickly become unbound from their receptors. They are then absorbed back into the presynaptic cell, via reuptake mediated primarily by the norepinephrine transporter (NET). Once back in the cytosol, norepinephrine can either be broken down by monoamine oxidase or repackaged into vesicles by VMAT, making it available for future release.\n\nSympathetic nervous system\nNorepinephrine is the main neurotransmitter used by the sympathetic nervous system, which consists of about two dozen sympathetic chain ganglia located next to the spinal cord, plus a set of prevertebral ganglia located in the chest and abdomen. These sympathetic ganglia are connected to numerous organs, including the eyes, salivary glands, heart, lungs, liver, gallbladder, stomach, intestines, kidneys, urinary bladder, reproductive organs, muscles, skin, and adrenal glands. Sympathetic activation of the adrenal glands causes the part called the adrenal medulla to release norepinephrine (as well as epinephrine) into the bloodstream, from which, functioning as a hormone, it gains further access to a wide variety of tissues.\nBroadly speaking, the effect of norepinephrine on each target organ is to modify its state in a way that makes it more conducive to active body movement, often at a cost of increased energy use and increased wear and tear. This can be contrasted with the acetylcholine-mediated effects of the parasympathetic nervous system, which modifies most of the same organs into a state more conducive to rest, recovery, and digestion of food, and usually less costly in terms of energy expenditure.\nThe sympathetic effects of norepinephrine include:\n\nIn the eyes, an increase in production of tears, making the eyes more moist, and pupil dilation through contraction of the iris dilator.\nIn the heart, an increase in the amount of blood pumped.\nIn brown adipose tissue, an increase in calories burned to generate body heat (thermogenesis).\nMultiple effects on the immune system. The sympathetic nervous system is the primary path of interaction between the immune system and the brain, and several components receive sympathetic inputs, including the thymus, spleen, and lymph nodes.  However the effects are complex, with some immune processes activated while others are inhibited.\nIn the arteries, constriction of blood vessels, causing an increase in blood pressure.\nIn the kidneys, release of renin and retention of sodium in the bloodstream.\nIn the liver, an increase in production of glucose, either by glycogenolysis after a meal or by gluconeogenesis when food has not recently been consumed. Glucose is the body's main energy source in most conditions.\nIn the pancreas, increased release of glucagon, a hormone whose main effect is to increase the production of glucose by the liver.\nIn skeletal muscles, an increase in glucose uptake.\nIn adipose tissue (i.e., fat cells), an increase in lipolysis, that is, conversion of fat to substances that can be used directly as energy sources by muscles and other tissues.\nIn the stomach and intestines, a reduction in digestive activity.  This results from a generally inhibitory effect of norepinephrine on the enteric nervous system, causing decreases in gastrointestinal mobility, blood flow, and secretion of digestive substances.\nNoradrenaline and ATP are sympathetic co-transmitters. It is found that the endocannabinoid anandamide and the cannabinoid WIN 55,212-2 can modify the overall response to sympathetic nerve stimulation, which indicates that prejunctional CB1 receptors mediate the sympatho-inhibitory action. Thus cannabinoids can inhibit both the noradrenergic and purinergic components of sympathetic neurotransmission.\n\nCentral nervous system\nThe noradrenergic neurons in the brain form a neurotransmitter system, that, when activated, exerts effects on large areas of the brain. The effects are manifested in alertness, arousal, and readiness for action.\nNoradrenergic neurons (i.e., neurons whose primary neurotransmitter is norepinephrine) are comparatively few in number, and their cell bodies are confined to a few relatively small brain areas, but they send projections to many other brain areas and exert powerful effects on their targets.  These noradrenergic cell groups were first mapped in 1964 by Annica Dahlstr\u00f6m and Kjell Fuxe, who assigned them labels starting with the letter \"A\" (for \"aminergic\").  In their scheme, areas A1 through A7 contain the neurotransmitter norepinephrine (A8 through A14 contain dopamine).  Noradrenergic cell group A1 is located in the caudal ventrolateral part of the medulla, and plays a role in the control of body fluid metabolism. Noradrenergic cell group A2 is located in a brainstem area called the solitary nucleus; these cells have been implicated in a variety of responses, including control of food intake and responses to stress.  Cell groups A5 and A7 project mainly to the spinal cord.\nThe most important source of norepinephrine in the brain is the locus coeruleus, which contains noradrenergic cell group A6 and adjoins cell group A4.  The locus coeruleus is quite small in absolute terms\u2014in primates it is estimated to contain around 15,000 neurons, less than one-millionth of the neurons in the brain\u2014but it sends projections to every major part of the brain and also to the spinal cord.\nThe level of activity in the locus coeruleus correlates broadly with vigilance and speed of reaction.  LC activity is low during sleep and drops to virtually nothing during the REM (dreaming) state.  It runs at a baseline level during wakefulness, but increases temporarily when a person is presented with any sort of stimulus that draws attention.  Unpleasant stimuli such as pain, difficulty breathing, bladder distension, heat or cold generate larger increases.  Extremely unpleasant states such as intense fear or intense pain are associated with very high levels of LC activity.\nNorepinephrine released by the locus coeruleus affects brain function in a number of ways.  It enhances processing of sensory inputs, enhances attention, enhances formation and retrieval of both long term and working memory, and enhances the ability of the brain to respond to inputs by changing the activity pattern in the prefrontal cortex and other areas.  The control of arousal level is strong enough that drug-induced suppression of the LC has a powerful sedating effect.\nThere is great similarity between situations that activate the locus coeruleus in the brain and situations that activate the sympathetic nervous system in the periphery:  the LC essentially mobilizes the brain for action while the sympathetic system mobilizes the body.  It has been argued that this similarity arises because both are to a large degree controlled by the same brain structures, particularly a part of the brainstem called the nucleus gigantocellularis.\n\nSkin\nNorepinephrine is also produced by Merkel cells which are part of the somatosensory system. It activates the afferent sensory neuron.\n\nPharmacology\nA large number of important drugs exert their effects by interacting with norepinephrine systems in the brain or body.  Their uses include treatment of cardiovascular problems, shock, and a variety of psychiatric conditions. These drugs are divided into: sympathomimetic drugs which mimic or enhance at least some of the effects of norepinephrine released by the sympathetic nervous system; sympatholytic drugs, in contrast, block at least some of the effects.  Both of these are large groups with diverse uses, depending on exactly which effects are enhanced or blocked.\nNorepinephrine itself is classified as a sympathomimetic drug:  its effects when given by intravenous injection of increasing heart rate and force and constricting blood vessels make it very useful for treating medical emergencies that involve critically low blood pressure. Surviving Sepsis Campaign recommended norepinephrine as first line agent in treating septic shock which is unresponsive to fluid resuscitation, supplemented by vasopressin and epinephrine. Dopamine usage is restricted only to highly selected patients.\n\nAntagonists\nBeta blockers\nThese are sympatholytic drugs that block the effects of beta adrenergic receptors while having little or no effect on alpha receptors.  They are sometimes used to treat high blood pressure, atrial fibrillation and congestive heart failure, but recent reviews have concluded that other types of drugs are usually superior for those purposes.  Beta blockers may be a viable choice for other cardiovascular conditions, though, including angina and Marfan syndrome.  They are also widely used to treat glaucoma, most commonly in the form of eyedrops.  Because of their effects in reducing anxiety symptoms and tremor, they have sometimes been used by entertainers, public speakers and athletes to reduce performance anxiety, although they are not medically approved for that purpose and are banned by the International Olympic Committee.\nHowever, the usefulness of beta blockers is limited by a range of serious side effects, including slowing of heart rate, a drop in blood pressure, asthma, and reactive hypoglycemia.  The negative effects can be particularly severe in people with diabetes.\n\nAlpha blockers\nThese are sympatholytic drugs that block the effects of adrenergic alpha receptors while having little or no effect on beta receptors.  Drugs belonging to this group can have very different effects, however, depending on whether they primarily block alpha-1 receptors, alpha-2 receptors, or both.  Alpha-2 receptors, as described elsewhere in this article, are frequently located on norepinephrine-releasing neurons themselves and have inhibitory effects on them; consequently, blockage of alpha-2 receptors usually results in an increase in norepinephrine release.  Alpha-1 receptors are usually located on target cells and have excitatory effects on them; consequently, blockage of alpha-1 receptors usually results in blocking some of the effects of norepinephrine.  Drugs such as phentolamine that act on both types of receptors can produce a complex combination of both effects. In most cases when the term \"alpha blocker\" is used without qualification, it refers to a selective alpha-1 antagonist.\nSelective alpha-1 blockers have a variety of uses. Since one of their effects is to inhibit the contraction of the smooth muscle in the prostate, they are often used to treat symptoms of benign prostatic hyperplasia. Alpha-blockers also likely help people pass their kidney stones. Their effects on the central nervous system make them useful for treating generalized anxiety disorder, panic disorder, and posttraumatic stress disorder. They may, however, have significant side-effects, including a drop in blood pressure.\nSome antidepressants function partly as selective alpha-2 blockers, but the best-known drug in that class is yohimbine, which is extracted from the bark of the African yohimbe tree.  Yohimbine acts as a male potency enhancer, but its usefulness for that purpose is limited by serious side-effects including anxiety and insomnia.  Overdoses can cause a dangerous increase in blood pressure.  Yohimbine is banned in many countries, but in the United States, because it is extracted from a plant rather than chemically synthesized, it is sold over the counter as a nutritional supplement.\n\nAlpha-2 agonists\nThese are sympathomimetic drugs that activate alpha-2 receptors or enhance their effects. Because alpha-2 receptors are inhibitory and many are located presynaptically on norepinephrine-releasing cells, the net effect of these drugs is usually to reduce the amount of norepinephrine released.  Drugs in this group that are capable of entering the brain often have strong sedating effects, due to their inhibitory effects on the locus coeruleus.  clonidine and guanfacine, for example, are used for the treatment of anxiety disorders and insomnia, and also as a sedative premedication for patients about to undergo surgery.  Xylazine, another drug in this group, is also a powerful sedative and is often used in combination with ketamine as a general anaesthetic for veterinary surgery\u2014in the United States it has not been approved for use in humans.\n\nStimulants and antidepressants\nThese are drugs whose primary effects are thought to be mediated by different neurotransmitter systems (dopamine for stimulants, serotonin for antidepressants), but many also increase levels of norepinephrine in the brain. Amphetamine, for example, is a stimulant that increases release of norepinephrine as well as dopamine. Monoamine oxidase A inhibitors (MAO-A) are antidepressants that inhibit the metabolic degradation of norepinephrine as well as serotonin and dopamine. In some cases it is difficult to distinguish the norepinephrine-mediated effects from the effects related to other neurotransmitters.\n\nDiseases and disorders\nA number of important medical problems involve dysfunction of the norepinephrine system in the brain or body.\n\nSympathetic hyperactivation\nHyperactivation of the sympathetic nervous system is not a recognized condition in itself, but it is a component of a number of conditions, as well as a possible consequence of taking sympathomimetic drugs.  It causes a distinctive set of symptoms including aches and pains, rapid heartbeat, elevated blood pressure, sweating, palpitations, anxiety, headache, paleness, and a drop in blood glucose.  If sympathetic activity is elevated for an extended time, it can cause weight loss and other stress-related body changes.\nThe list of conditions that can cause sympathetic hyperactivation includes severe brain injury, spinal cord damage, heart failure, high blood pressure, kidney disease, and various types of stress.\n\nPheochromocytoma\nA pheochromocytoma is a rarely occurring tumor of the adrenal medulla, caused either by genetic factors or certain types of cancer.  The consequence is a massive increase in the amount of norepinephrine and epinephrine released into the bloodstream.  The most obvious symptoms are those of sympathetic hyperactivation, including particularly a rise in blood pressure that can reach fatal levels.  The most effective treatment is surgical removal of the tumor.\n\nStress\nStress, to a physiologist, means any situation that threatens the continued stability of the body and its functions. Stress affects a wide variety of body systems:  the two most consistently activated are the hypothalamic-pituitary-adrenal axis and the norepinephrine system, including both the sympathetic nervous system and the locus coeruleus-centered system in the brain.  Stressors of many types evoke increases in noradrenergic activity, which mobilizes the brain and body to meet the threat.  Chronic stress, if continued for a long time, can damage many parts of the body.  A significant part of the damage is due to the effects of sustained norepinephrine release, because of norepinephrine's general function of directing resources away from maintenance, regeneration, and reproduction, and toward systems that are required for active movement.  The consequences can include slowing of growth (in children), sleeplessness, loss of libido, gastrointestinal problems, impaired disease resistance, slower rates of injury healing, depression, and increased vulnerability to addiction.\n\nADHD\nAttention deficit hyperactivity disorder is a neurodevelopmental condition involving problems with attention, hyperactivity, and impulsiveness. It is most commonly treated using stimulant drugs such as methylphenidate (Ritalin), whose primary effect is to increase dopamine levels in the brain, but drugs in this group also generally increase brain levels of norepinephrine, and it has been difficult to determine whether these actions are involved in their clinical value.  There is also substantial evidence that many people with ADHD show biomarkers involving altered norepinephrine processing. Several drugs whose primary effects are on norepinephrine, including guanfacine, clonidine, and atomoxetine, have been tried as treatments for ADHD, and found to have effects comparable to those of stimulants.\n\nAutonomic failure\nSeveral conditions, including Parkinson's disease, diabetes and so-called pure autonomic failure, can cause a loss of norepinephrine-secreting neurons in the sympathetic nervous system.  The symptoms are widespread, the most serious being a reduction in heart rate and an extreme drop in resting blood pressure, making it impossible for severely affected people to stand for more than a few seconds without fainting.  Treatment can involve dietary changes or drugs.\n\nREM sleep deprivation\nNorepinephrine prevents REM sleep, and lack of REM sleep increases noradrenaline secretion as a result of the locus coeruleus not ceasing producing it. It causes neurodegeneration if its loss is sustained for several days.\n\nComparative biology and evolution\nNorepinephrine has been reported to exist in a wide variety of animal species, including protozoa, placozoa and cnidaria (jellyfish and related species), but not in ctenophores (comb jellies), whose nervous systems differ greatly from those of other animals.  It is generally present in deuterostomes (vertebrates, etc.), but in protostomes (arthropods, molluscs, flatworms, nematodes, annelids, etc.) it is replaced by octopamine, a closely related chemical with a closely related synthesis pathway.  In insects, octopamine has alerting and activating functions that correspond (at least roughly) with the functions of norepinephrine in vertebrates. It has been argued that octopamine evolved to replace norepinephrine rather than vice versa; however, the nervous system of amphioxus (a primitive chordate) has been reported to contain octopamine but not norepinephrine, which presents difficulties for that hypothesis.\n\nHistory\nEarly in the twentieth century Walter Cannon, who had popularized the idea of a sympathoadrenal system preparing the body for fight and flight, and his colleague Arturo Rosenblueth developed a theory of two sympathins, sympathin E (excitatory) and sympathin I (inhibitory), responsible for these actions. The Belgian pharmacologist Z\u00e9non Bacq as well as Canadian and U.S. pharmacologists between 1934 and 1938 suggested that noradrenaline might be a sympathetic transmitter. In 1939, Hermann Blaschko and Peter Holtz independently identified the biosynthetic mechanism for norepinephrine in the vertebrate body. In 1945 Ulf von Euler published the first of a series of papers that established the role of norepinephrine as a neurotransmitter.  He demonstrated the presence of norepinephrine in sympathetically innervated tissues and brain, and adduced evidence that it is the sympathin of Cannon and Rosenblueth.\nStanley Peart was the first to demonstrate the release of noradrenaline after the stimulation of sympathetic nerves.\n\nReferences\nExternal links\n Media related to Norepinephrine at Wikimedia Commons","131":"Nystagmus is a condition of involuntary (or voluntary, in some cases) eye movement. People can be born with it but more commonly acquire it in infancy or later in life. In many cases it may result in reduced or limited vision.\nIn normal eyesight, while the head rotates about an axis, distant visual images are sustained by rotating eyes in the opposite direction of the respective axis. The semicircular canals in the vestibule of the ear sense angular acceleration, and send signals to the nuclei for eye movement in the brain. From here, a signal is relayed to the extraocular muscles to allow one's gaze to fix on an object as the head moves. Nystagmus occurs when the semicircular canals are stimulated (e.g., by means of the caloric test, or by disease) while the head is stationary. The direction of ocular movement is related to the semicircular canal that is being stimulated.\nThere are two key forms of nystagmus: pathological and physiological, with variations within each type. Physiological nystagmus occurs under normal conditions in healthy subjects. Nystagmus may be caused by congenital disorder or sleep deprivation, acquired or central nervous system disorders, toxicity, pharmaceutical drugs, alcohol, or rotational movement. Previously considered untreatable, in recent years several drugs have been identified for treatment of nystagmus. Nystagmus is also occasionally associated with vertigo.\n\nCauses\nThe cause of pathological nystagmus may be congenital, idiopathic, or secondary to a pre-existing neurological disorder. It also may be induced temporarily by disorientation (such as on roller coaster rides or when a person has been spinning in circles) or by some drugs (alcohol, lidocaine, and other central nervous system depressants, inhalant drugs, stimulants, psychedelics, and dissociative drugs).\n\nEarly-onset nystagmus\nEarly-onset nystagmus occurs more frequently than acquired nystagmus. It can be insular or accompany other disorders (such as micro-ophthalmic anomalies or Down syndrome). Early-onset nystagmus itself is usually mild and non-progressive. The affected persons are usually unaware of their spontaneous eye movements, but vision can be impaired depending on the severity of the eye movements.\nTypes of early-onset nystagmus include the following, along with some of their causes:\n\nInfantile:\nAlbinism\nAniridia\nBilateral congenital cataract\nBilateral optic nerve hypoplasia\nIdiopathic\nLeber's congenital amaurosis\nOptic nerve or macular disease\nPersistent tunica vasculosa lentis\nRod monochromatism\nVisual-motor syndrome of functional monophthalmus\nLatent nystagmus\nNoonan syndrome\nNystagmus blockage syndrome\nX-linked infantile nystagmus is associated with mutations of the gene FRMD7, which is located on the X chromosome.\nInfantile nystagmus is also associated with two X-linked eye diseases known as complete congenital stationary night blindness (CSNB) and incomplete CSNB (iCSNB or CSNB-2), which are caused by mutations of one of two genes located on the X chromosome. In CSNB, mutations are found in NYX (nyctalopin). CSNB-2 involves mutations of CACNA1F, a voltage-gated calcium channel that, when mutated, does not conduct ions.\n\nAcquired nystagmus\nNystagmus that occurs later in childhood or in adulthood is called acquired nystagmus. The cause is often unknown, or idiopathic, and thus referred to as idiopathic nystagmus. Other common causes include diseases and disorders of the central nervous system, metabolic disorders and alcohol and drug toxicity. In the elderly, stroke is the most common cause.\n\nGeneral diseases and conditions\nSome of the diseases that present nystagmus as a pathological sign or symptom are as follows:\n\nAniridia\nBenign paroxysmal positional vertigo\n\nToxicity or intoxication, metabolic disorders and combination\nSources of toxicity that could lead to nystagmus:\n\nThiamine deficiency\nRisk factors for thiamine deficiency, or beriberi, in turn include a diet of mostly white rice, as well as alcoholism, dialysis, chronic diarrhea, and taking high doses of diuretics. Rarely it may be due to a genetic condition that results in difficulties absorbing thiamine found in food. Wernicke encephalopathy and Korsakoff syndrome are forms of dry beriberi.\n\nCentral nervous system (CNS) diseases and disorders\nCentral nervous system disorders such as with a cerebellar problem, the nystagmus can be in any direction including horizontal. Purely vertical nystagmus usually originates in the central nervous system, but it is also an adverse effect commonly seen in high phenytoin toxicity. Other causes of toxicity that may result in nystagmus include:\n\nOther causes\nNon-physiological\nTrochlear nerve malfunction\nVestibular pathology (M\u00e9ni\u00e8re's disease, SCDS (superior canal dehiscence syndrome), BPPV, vestibular neuritis)\nExposure to strong magnetic fields (as in MRI machines)\nLong-term exposure to low light conditions or darkness, called miner's nystagmus after 19th-century coal miners who developed nystagmus from working in the dark.\nA slightly different form of nystagmus may be produced voluntarily by some (8% of) people. Some can sustain it for up to 35 seconds, but most average around 5 seconds.\n\nDiagnosis\nNystagmus is highly noticeable but rarely recognized. Nystagmus can be clinically investigated by using a number of non-invasive standard tests. The simplest one is the caloric reflex test, in which one ear canal is irrigated with warm or cold water or air. The temperature gradient provokes the stimulation of the horizontal semicircular canal and the consequent nystagmus.\nNystagmus is often very commonly present with Chiari malformation.\nThe resulting movement of the eyes may be recorded and quantified by a special device called an electronystagmograph (ENG), a form of electrooculography (an electrical method of measuring eye movements using external electrodes), or an even less invasive device called a videonystagmograph (VNG), a form of video-oculography (VOG) (a video-based method of measuring eye movements using external small cameras built into head masks), administered by an audiologist. Special swinging chairs with electrical controls can be used to induce rotatory nystagmus.\nOver the past forty years, objective eye-movement-recording techniques have been applied to the study of nystagmus, and the results have led to greater accuracy of measurement and understanding of the condition.\nOrthoptists may also use an optokinetic drum, or electrooculography or Frenzel goggles to assess a patient's eye movements.\nNystagmus can be caused by subsequent foveation of moving objects, pathology, sustained rotation or substance use. Nystagmus is not to be confused with other superficially similar-appearing disorders of eye movements (saccadic oscillations) such as opsoclonus or ocular flutter that are composed purely of fast-phase (saccadic) eye movements, while nystagmus is characterized by the combination of a smooth pursuit, which usually acts to take the eye off the point of focus, interspersed with the saccadic movement that serves to bring the eye back on target. Without the use of objective recording techniques, it may be very difficult to distinguish among these conditions.\nIn medicine, the presence of nystagmus can be benign, or it can indicate an underlying visual or neurological problem.\n\nPathological nystagmus\nPathological nystagmus is characterized by \"excessive drifts of stationary retinal images that degrades vision and may produce illusory motion of the seen world: oscillopsia (an exception is congenital nystagmus)\".\nBechterew's phenomenon was discovered by Vladimir Bekhterev in 1883 in animal experiments. Specifically, if one side of the vestibular system is damaged, then due to the lack of vestibular signal from that side, the animal behaves with nystagmus and vertigo. After a while, due to vestibular compensation, nystagmus and vertigo stops. However, if then the other vestibular system is damaged, then nystagmus and vertigo occurs in the opposite direction. This is an early evidence of sensorimotor adaptation in the brain. It is rarely reported in humans.\nWhen nystagmus occurs without fulfilling its normal function, it is pathologic (deviating from the healthy or normal condition). Pathological nystagmus is the result of damage to one or more components of the vestibular system, including the semicircular canals, otolith organs, and the vestibulocerebellum.\nPathological nystagmus generally causes a degree of vision impairment, although the severity of such impairment varies widely. Also, many blind people have nystagmus, which is one reason that some wear dark glasses.\n\nVariations\nCentral nystagmus occurs as a result of either normal or abnormal processes not related to the vestibular organ. For example, lesions of the midbrain or cerebellum can result in up- and down-beat nystagmus.\nGaze induced nystagmus occurs or is exacerbated as a result of changing one's gaze toward or away from a particular side which has an affected central apparatus.\nPeripheral nystagmus occurs as a result of either normal or diseased functional states of the vestibular system and may combine a rotational component with vertical or horizontal eye movements and may be spontaneous, positional, or evoked.\nPositional nystagmus occurs when a person's head is in a specific position. An example of disease state in which this occurs is benign paroxysmal positional vertigo (BPPV).\nPost rotational nystagmus occurs after an imbalance is created between a normal side and a diseased side by stimulation of the vestibular system by rapid shaking or rotation of the head.\nSpontaneous nystagmus is nystagmus that occurs randomly, regardless of the position of the patient's head.\n\nPhysiological nystagmus\nPhysiological nystagmus is a form of involuntary eye movement that is part of the vestibulo-ocular reflex (VOR), characterized by alternating smooth pursuit in one direction and saccadic movement in the other direction.\n\nVariations\nThe direction of nystagmus is defined by the direction of its quick phase (e.g. a right-beating nystagmus is characterized by a rightward-moving quick phase, and a left-beating nystagmus by a leftward-moving quick phase). The oscillations may occur in the vertical, horizontal or torsional planes, or in any combination. The resulting nystagmus is often named as a gross description of the movement, e.g. downbeat nystagmus, upbeat nystagmus, seesaw nystagmus, periodic alternating nystagmus.\nThese descriptive names can be misleading, however, as many were assigned historically, solely on the basis of subjective clinical examination, which is not sufficient to determine the eyes' true trajectory.\n\nOptokinetic (syn.\u2009opticokinetic) nystagmus: a nystagmus induced by looking at moving visual stimuli, such as moving horizontal or vertical lines, and\/or stripes. For example, if one fixates on a stripe of a rotating drum with alternating black and white, the gaze retreats to fixate on a new stripe as the drum moves. This is first a rotation with the same angular velocity, then returns in a saccade in the opposite direction. The process proceeds indefinitely. This is optokinetic nystagmus, and is a source for understanding the fixation reflex.\nPostrotatory nystagmus: if one spins in a chair continuously and stops suddenly, the fast phase of nystagmus is in the opposite direction of rotation, known as the \"post-rotatory nystagmus\", while slow phase is in the direction of rotation.\n\nTreatment\nCongenital nystagmus has long been viewed as untreatable, but medications have been discovered that show promise in some patients. In 1980, researchers discovered that a drug called baclofen could stop periodic alternating nystagmus. Subsequently, gabapentin, an anticonvulsant, led to improvement in about half the patients who took it. Other drugs found to be effective against nystagmus in some patients include memantine, levetiracetam, 3,4-diaminopyridine (available in the US to eligible patients with downbeat nystagmus at no cost under an expanded access program), 4-aminopyridine, and acetazolamide. Several therapeutic approaches, such as contact lenses, drugs, surgery, and low vision rehabilitation have also been proposed. For example, it has been proposed that mini-telescopic eyeglasses suppress nystagmus.\nSurgical treatment of congenital nystagmus is aimed at improving head posture, simulating artificial divergence, or weakening the horizontal recti muscles. Clinical trials of a surgery to treat nystagmus (known as tenotomy) concluded in 2001. Tenotomy is now being performed regularly at numerous centres around the world. The surgery aims to reduce the eye oscillations, which in turn tends to improve visual acuity.\nAcupuncture tests have produced conflicting evidence on its beneficial effects on the symptoms of nystagmus. Benefits have been seen in treatments in which acupuncture points of the neck were used, specifically points on the sternocleidomastoid muscle. Benefits of acupuncture for treatment of nystagmus include a reduction in frequency and decreased slow phase velocities, which led to an increase in foveation duration periods both during and after treatment. By the standards of evidence-based medicine, the quality of these studies is poor (for example, Ishikawa's study had sample size of six subjects, was unblinded, and lacked proper controls), and given high quality studies showing that acupuncture has no effect beyond placebo, the results of these studies have to be considered clinically irrelevant until higher quality studies are performed.\nPhysical or occupational therapy is also used to treat nystagmus. Treatment consists of learning strategies to compensate for the impaired system.\nA Cochrane Review on interventions for eye movement disorders due to acquired brain injury, updated in June 2017, identified three studies of pharmacological interventions for acquired nystagmus but concluded that these studies provided insufficient evidence to guide treatment choices.\n\nEpidemiology\nNystagmus is a relatively common clinical condition, affecting one in several thousand people. A survey conducted in Oxfordshire, United Kingdom, found that by the age of two, one in every 670 children had manifested nystagmus. Authors of another study in the United Kingdom estimated an incidence of 24 in 10,000 (c. 0.240%), noting an apparently higher rate amongst white Europeans than in individuals of Asian origin.\n\nLaw enforcement\nIn the United States, testing for horizontal gaze nystagmus is one of a battery of field sobriety tests used by police officers to determine whether a suspect is driving under the influence of alcohol. Horizontal gaze nystagmus will show if a subject is under the influence of a central nervous system depressant, an inhalant, or a dissociative anesthetic. The test involves observation of the suspect's pupil as it follows a moving object, noting\n\nlack of smooth pursuit,\ndistinct and sustained nystagmus at maximum deviation, and\nthe onset of nystagmus prior to 45 degrees.\nThe horizontal gaze nystagmus test has been highly criticized and major errors in the testing methodology and analysis found. However, the validity of the horizontal gaze nystagmus test for use as a field sobriety test for persons with a blood alcohol level between 0.04 and 0.08 is supported by peer-reviewed studies and has been found to be a more accurate indication of blood alcohol content than other standard field sobriety tests.\n\nMedia\nMy Dancing Eyes, a documentary by filmmaker Matt Morris, had participants explain what it is like to live with the eye condition, and was released for free. It was featured on NBN News, and ABC Radio Newcastle, in Australia.\n\nSee also\nBruns nystagmus\nMyoclonus\nOscillopsia\nOpsoclonus\nOptokinetic nystagmus\n\n\n== References ==","132":"OCLC, Inc., doing business as OCLC, is an American nonprofit cooperative organization \"that provides shared technology services, original research, and community programs for its membership and the library community at large\". It was founded in 1967 as the Ohio College Library Center, then became the Online Computer Library Center as it expanded. In 2017, the name was formally changed to OCLC, Inc. OCLC and thousands of its member libraries cooperatively produce and maintain WorldCat, the largest online public access catalog in the world. OCLC is funded mainly by the fees that libraries pay (around $217.8 million annually in total as of 2021) for the many different services it offers. OCLC also maintains the Dewey Decimal Classification system.\n\nHistory\nOCLC began in 1967, as the Ohio College Library Center, through a collaboration of university presidents, vice presidents, and library directors who wanted to create a cooperative, computerized network for libraries in the state of Ohio. The group first met on July 5, 1967, on the campus of Ohio State University to sign the articles of incorporation for the nonprofit organization and hired Frederick G. Kilgour, a former Yale University medical school librarian, as first executive director.\nKilgour and Ralph H. Parker, who was the head of libraries at the University of Missouri, had proposed the shared cataloging system in a 1965 report as consultants to the Committee of Librarians of the Ohio College Association. Kilgour and Parker wished to merge the latest information storage and retrieval system of the time, the computer, with the oldest, the library. They were inspired in part by the earlier Columbia\u2013Harvard\u2013Yale Medical Libraries Computerization Project, an attempt at shared automated printing of catalog cards. The plan was to merge the catalogs of Ohio libraries electronically through a computer network and database to streamline operations, control costs, and increase efficiency in library management, bringing libraries together cooperatively to best serve researchers and scholars. The first library to do online cataloging through OCLC was the Alden Library at Ohio University on August 26, 1971. This was the first online cataloging by any library worldwide.\nBetween 1967 and 1977, OCLC membership was limited to institutions in Ohio, but in 1978, a new governance structure was established that allowed institutions from other states to join. In 2002, the governance structure was again modified to accommodate participation from outside the United States.\nAs OCLC expanded services in the United States outside Ohio, it relied on establishing strategic partnerships with \"networks\", organizations that provided training, support and marketing services. By 2008, there were 15 independent United States regional service providers. OCLC networks played a key role in OCLC governance, with networks electing delegates to serve on the OCLC Members Council. During 2008, OCLC commissioned two studies to look at distribution channels; at the same time, the council approved governance changes that had been recommended by the Board of Trustees severing the tie between the networks and governance. In early 2009, OCLC negotiated new contracts with the former networks and opened a centralized support center.\nIn July 2010, the company was sued by SkyRiver, a rival startup, in an antitrust suit. Library automation company Innovative Interfaces joined SkyRiver in the suit. The suit was dropped in March 2013, however, following the acquisition of SkyRiver by Innovative Interfaces.\nInnovative Interfaces was bought by ExLibris in 2020, therefore passing OCLC as the dominant supplier of ILS services in the US (over 70% market share for academic libraries and over 50% for public libraries for ExLibris, versus OCLC's 10% market share of both types of libraries in 2019).\nIn 2022, membership and governance expanded to include any institution with a subscription to one of many qualifying OCLC products (previously institutions qualified for membership by \"contributing intellectual content or participating in global resource or reference sharing\"), with the exception of for-profit organizations that are part of OCLC's partner program. This change reflected OCLC's expanding number of services due to its corporate acquisitions.\n\nPresidents\nThe following people served successively as president of OCLC:\n\n1967\u20131980: Frederick G. Kilgour\n1980\u20131989: Rowland C. W. Brown\n1989\u20131998: K. Wayne Smith\n1998\u20132013: Jay Jordan\n2013\u2013present: Skip Prichard\n\nServices\nOCLC provides bibliographic, abstract and full-text information to anyone.\nOCLC and its member libraries cooperatively produce and maintain WorldCat\u2014the OCLC Online Union Catalog, the largest online public access catalog (OPAC) in the world. WorldCat has holding records from public and private libraries worldwide.\nThe Online Computer Library Center acquired the trademark and copyrights associated with the Dewey Decimal Classification System when it bought Forest Press in 1988. A browser for books with their Dewey Decimal Classifications was available until July 2013; it was replaced by the Classify Service.\nUntil August 2009, when it was sold to Backstage Library Works, OCLC owned a preservation microfilm and digitization operation called the OCLC Preservation Service Center, with its principal office in Bethlehem, Pennsylvania.\nStarting in 1971, OCLC produced catalog cards for members alongside its shared online catalog; the company printed its last catalog cards on October 1, 2015.\nQuestionPoint, an around-the-clock reference service provided to users by a cooperative of participating global libraries, was acquired by Springshare from OCLC in 2019 and migrated to Springshare's LibAnswers platform.\n\nSoftware\nOCLC commercially sells software, such as:\n\nCONTENTdm for managing digital collections\nWise, an integrated library system and \"community engagement system\"\nWorldCat Discovery, a bibliographic discovery system that allows library patrons to use a single search interface to access an institution's catalog, ebooks, database subscriptions and more\nWorldShare Management Services, an electronic resource management system\n\nResearch\nOCLC has been conducting research for the library community for more than 30 years. In accordance with its mission, OCLC makes its research outcomes known through various publications. These publications, including journal articles, reports, newsletters, and presentations, are available through the organization's website.\n\nOCLC Publications \u2013 Research articles from various journals including The Code4Lib Journal, OCLC Research, Reference and User Services Quarterly, College & Research Libraries News, Art Libraries Journal, and National Education Association Newsletter. The most recent publications are displayed first, and all archived resources, starting in 1970, are also available.\nMembership Reports \u2013 A number of significant reports on topics ranging from virtual reference in libraries to perceptions about library funding.\nNewsletters \u2013 Current and archived newsletters for the library and archive community.\nPresentations \u2013 Presentations from both guest speakers and OCLC research from conferences, webcasts, and other events. The presentations are organized into five categories: Conference presentations, Dewey presentations, Distinguished Seminar Series, Guest presentations, and Research staff presentations.\nDuring the COVID-19 pandemic, OCLC participated in the REopening Archives, Libraries, and Museums (REALM) project funded by the IMLS to study the surface transmission risks of SARS-CoV-2 on common library and museum materials and surfaces, and published a series of reports.\n\nAdvocacy\nAdvocacy has been a part of OCLC's mission since its founding in 1967. OCLC staff members meet and work regularly with library leaders, information professionals, researchers, entrepreneurs, political leaders, trustees, students and patrons to advocate \"advancing research, scholarship, education, community development, information access, and global cooperation\".\nWebJunction, which provides training services to librarians, is a division of OCLC funded by grants from the Bill & Melinda Gates Foundation beginning in 2003.\nOCLC partnered with search engine providers in 2003 to advocate for libraries and share information across the Internet landscape. Google, Yahoo!, and Ask.com all collaborated with OCLC to make WorldCat records searchable through those search engines.\nOCLC's advocacy campaign \"Geek the Library\", started in 2009, highlights the role of public libraries. The campaign, funded by a grant from the Bill & Melinda Gates Foundation, uses a strategy based on the findings of the 2008 OCLC report, \"From Awareness to Funding: A study of library support in America\".\nOther past advocacy campaigns have focused on sharing the knowledge gained from library and information research. Such projects have included communities such as the Society of American Archivists, the Open Archives Initiative, the Institute for Museum and Library Services, the International Organization for Standardization, the National Information Standards Organization, the World Wide Web Consortium, the Internet Engineering Task Force, and Internet2. One of the most successful contributions to this effort was the Dublin Core Metadata Initiative, \"an open forum of libraries, archives, museums, technology organizations, and software companies who work together to develop interoperable online metadata standards that support a broad range of purposes and business models.\"\nOCLC has collaborated with the Wikimedia Foundation and the Wikimedia volunteer community, through integrating library metadata with Wikimedia projects, hosting a Wikipedian in residence, and doing a national training program through WebJunction called \"Wikipedia + Libraries: Better Together\".\n\nOnline database: WorldCat\nOCLC's WorldCat database is used by the general public and by librarians for cataloging and research. WorldCat is available to the public for searching via a subscription web-based service called FirstSearch, to which many libraries subscribe, as well as through the publicly available WorldCat.org.\n\nIdentifiers and linked data\nOCLC assigns a unique control number (referred to as an \"OCN\" for \"OCLC Control Number\") to each new bibliographic record in WorldCat. Numbers are assigned serially, and in mid-2013 over a billion OCNs had been created. In September 2013, the OCLC declared these numbers to be in the public domain, removing a perceived barrier to widespread use of OCNs outside OCLC itself. The control numbers link WorldCat's records to local library system records by providing a common reference key for a record across libraries.\nOCNs are particularly useful as identifiers for books and other bibliographic materials that do not have ISBNs (e.g., books published before 1970). OCNs are often used as identifiers for Wikipedia and Wikidata. In October 2013, it was reported that out of 29,673 instances of book infoboxes in Wikipedia, \"there were 23,304 ISBNs and 15,226 OCNs\", and regarding Wikidata: \"of around 14 million Wikidata items, 28,741 were books. 5403 Wikidata items have an ISBN associated with them, and 12,262 have OCNs.\"\nOCLC also runs the Virtual International Authority File (VIAF), an international name authority file, with oversight from the VIAF Council composed of representatives of institutions that contribute data to VIAF. VIAF numbers are broadly used as standard identifiers, including in Wikipedia.\n\nCompany acquisitions\nOCLC acquired NetLibrary, a provider of electronic books and textbooks, in 2002 and sold it in 2010 to EBSCO Industries. OCLC owns 100% of the shares of OCLC PICA, a library automation systems and services company which has its headquarters in Leiden in the Netherlands and which was renamed \"OCLC\" at the end of 2007. In July 2006, the Research Libraries Group (RLG) merged with OCLC.\nOn January 11, 2008, OCLC announced that it had purchased EZproxy. It has also acquired OAIster. The process started in January 2009 and from October 31, 2009, OAIster records are freely available via WorldCat.org.\nIn 2013, OCLC acquired the Dutch library automation company HKA and its integrated library system Wise, which OCLC calls a \"community engagement system\" that \"combines the power of customer relationship management, marketing, and analytics with ILS functions\". OCLC began offering Wise to libraries in the United States in 2019.\nIn January 2015, OCLC acquired Sustainable Collection Services (SCS). SCS offered consulting services based on analyzing library print collection data to help libraries manage and share materials. In 2017, OCLC acquired Relais International, a library interlibrary loan service provider based in Ottawa, Canada.\nA more complete list of mergers and acquisitions is available on the OCLC website.\n\nCriticism\nIn May 2008, OCLC was criticized by Jeffrey Beall for monopolistic practices, among other faults. Library blogger Rick Mason responded that although he thought Beall had some \"valid criticisms\" of OCLC, he demurred from some of Beall's statements and warned readers to \"beware the hyperbole and the personal nature of his criticism, for they strongly overshadow that which is worth stating\".\nIn November 2008, the Board of Directors of OCLC unilaterally issued a new Policy for Use and Transfer of WorldCat Records that would have required member libraries to include an OCLC policy note on their bibliographic records; the policy caused an uproar among librarian bloggers. Among those who protested the policy was the non-librarian activist Aaron Swartz, who believed the policy would threaten projects such as the Open Library, Zotero, and Wikipedia, and who started a petition to \"Stop the OCLC powergrab\". Swartz's petition garnered 858 signatures, but the details of his proposed actions went largely unheeded. Within a few months, the library community had forced OCLC to retract its policy and to create a Review Board to consult with member libraries more transparently. In August 2012, OCLC recommended that member libraries adopt the Open Data Commons Attribution (ODC-BY) license when sharing library catalog data, although some member libraries have explicit agreements with OCLC that they can publish catalog data using the CC0 Public Domain Dedication.\n\nSee also\nDynix (software)\nPublic library advocacy\n\nReferences\nFurther reading\nExternal links\n\nOfficial website \nHanging Together \u2013 the OCLC Research blog\nOCLC Annual Reports collection at the OCLC Archives","133":"Otitis externa, also called swimmer's ear, is inflammation of the ear canal. It often presents with ear pain, swelling of the ear canal, and occasionally decreased hearing. Typically there is pain with movement of the outer ear. A high fever is typically not present except in severe cases.\nOtitis externa may be acute (lasting less than six weeks) or chronic (lasting more than three months). Acute cases are typically due to bacterial infection, and chronic cases are often due to allergies and autoimmune disorders. The most common cause of otitis externa is bacterial. Risk factors for acute cases include swimming, minor trauma from cleaning, using hearing aids and ear plugs, and other skin problems, such as psoriasis and dermatitis. People with diabetes are at risk of a severe form of malignant otitis externa. Diagnosis is based on the signs and symptoms. Culturing the ear canal may be useful in chronic or severe cases.\nAcetic acid ear drops may be used as a preventive measure. Treatment of acute cases is typically with antibiotic drops, such as ofloxacin or acetic acid. Steroid drops may be used in addition to antibiotics. Pain medications such as ibuprofen may be used for the pain. Antibiotics by mouth are not recommended unless the person has poor immune function or there is infection of the skin around the ear. Typically, improvement occurs within a day of the start of treatment. Treatment of chronic cases depends on the cause.\nOtitis externa affects 1\u20133% of people a year; more than 95% of cases are acute. About 10% of people are affected at some point in their lives. It occurs most commonly among children between the ages of seven and twelve and among the elderly. It occurs with near equal frequency in males and females. Those who live in warm and wet climates are more often affected.\n\nSigns and symptoms\nTenderness of pinna is the predominant complaint and the only symptom directly related to the severity of acute external otitis. Unlike other forms of ear infections, we observe tenderness in outer ear i.e., the pain of acute external otitis is worsened when the outer ear is touched or pulled gently. Pushing the tragus, the tablike portion of the auricle that projects out just in front of the ear canal opening, also typically causes pain in this condition as to be diagnostic of external otitis on physical examination. People may also experience ear discharge and itchiness. When enough swelling and discharge in the ear canal is present to block the opening, external otitis may cause temporary conductive hearing loss.\nBecause the symptoms of external otitis lead many people to attempt to clean out the ear canal (or scratch it) with slim implements, self-cleaning attempts generally lead to additional traumas of the injured skin, so rapid worsening of the condition often occurs.\n\nCauses\nThe two factors that are required for external otitis to develop are (1) the presence of germs that can infect the skin and (2) impairments in the integrity of the skin of the ear canal that allow an infection to occur. If the skin is healthy and uninjured, only exposure to a high concentration of pathogens, such as submersion in a pond contaminated by sewage, is likely to set off an episode. However, if there are chronic skin conditions that affect the ear canal skin, such as atopic dermatitis, seborrheic dermatitis, psoriasis or abnormalities of keratin production, or if there has been a break in the skin from trauma, even the normal bacteria found in the ear canal may cause infection and full-blown symptoms of external otitis.\nFungal ear canal infections, also known as otomycosis, range from inconsequential to extremely severe. Fungi can be saprophytic, in which there are no symptoms and the fungus simply co-exists in the ear canal in a commensal relationship with the host, in which case the only physical finding is the presence of a fungus.  If the fungus begins active reproduction, the ear canal can fill with dense fungal debris, causing pressure and ever-increasing pain that is unrelenting until the fungus is removed from the canal and anti-fungal medication is used.  Most antibacterial ear drops also contain a steroid to hasten resolution of canal edema and pain.  Unfortunately, such drops make the fungal infection worse.  Prolonged use of them promotes the growth of fungus in the ear canal.  Antibacterial ear drops should be used for a maximum of one week, but 5 days is usually enough.  Otomycosis responds more than 95% of the time to a three-day course of the same over-the-counter anti-fungal solutions used\nfor athlete's foot.\n\nSwimming\nSwimming in polluted water is a common way to contract swimmer's ear, but it is also possible to contract swimmer's ear from water trapped in the ear canal after a shower, especially in a humid climate. Prolonged swimming can saturate the skin of the canal, compromising its barrier function and making it more susceptible to further damage if the ear is instrumented with cotton swabs after swimming.  Main symptoms of swimmer\u2019s ear are a feeling of fullness in the ear, itchiness, redness, and swelling in or around the ear canal, muffled hearing, pain in the external ear and ear canal and especially a smelly discharge from the ear.\nConstriction of the ear canal from bone growth (Surfer's ear) can trap debris leading to infection. Saturation divers have reported otitis externa during occupational exposure.\n\nObjects in ear\nEven without exposure to water, the use of objects such as cotton swabs or other small objects to clear the ear canal is enough to cause breaks in the skin, and allow the condition to develop. Once the skin of the ear canal is inflamed, external otitis can be drastically enhanced by either scratching the ear canal with an object or by allowing water to remain in the ear canal for any prolonged length of time.\n\nInfections\nThe majority of cases are due to Pseudomonas aeruginosa and Staphylococcus aureus, followed by a great number of other gram-positive and gram-negative species. Candida albicans and Aspergillus species are the most common fungal pathogens responsible for the condition.\n\nDiagnosis\nWhen the ear is inspected, the canal appears red and swollen in well-developed cases. The ear canal may also appear eczema-like, with scaly shedding of skin. Touching or moving the outer ear increases the pain, and this maneuver on physical exam is important in establishing the clinical diagnosis. It may be difficult to see the eardrum with an otoscope at the initial examination because of narrowing of the ear canal from inflammation and the presence of drainage and debris. Sometimes the diagnosis of external otitis is presumptive and return visits are required to fully examine the ear. The culture of the drainage may identify the bacteria or fungus causing infection, but is not part of the routine diagnostic evaluation. In severe cases of external otitis, there may be swelling of the lymph node(s) directly beneath the ear.\nThe diagnosis may be missed in most early cases because the examination of the ear, with the exception of pain with manipulation, is nearly normal. In some early cases, the most striking visual finding is the lack of earwax. As a moderate or severe case of external otitis resolves, weeks may be required before the ear canal again shows a normal amount of it.\n\nClassification\nIn contrast to the chronic otitis externa, acute otitis externa (AOE) is predominantly a bacterial infection, occurs suddenly, rapidly worsens, and becomes painful.  The ear canal has an abundant nerve supply, so the pain is often severe enough to interfere with sleep. Wax in the ear can combine with the swelling of the canal skin and the associated pus to block the canal and dampen hearing, creating a temporary conductive hearing loss. In more severe or untreated cases, the infection can spread to the soft tissues of the face that surround the adjacent parotid gland and the jaw joint, making chewing painful. In its mildest forms, otitis externa is so common that some ear nose and throat physicians have suggested that most people will have at least a brief episode at some point in life.\nThe skin of the bony ear canal is unique, in that it is not movable but is closely attached to the bone, and it is almost paper-thin. For these reasons, it is easily abraded or torn by even minimal physical force. Inflammation of the ear canal skin typically begins with a physical insult, most often from injury caused by attempts at self-cleaning or scratching with cotton swabs, pen caps, fingernails, hair pins, keys, or other small implements. Another causative factor for acute infection is prolonged water exposure in the forms of swimming or exposure to extreme humidity, which can compromise the protective barrier function of the canal skin, allowing bacteria to flourish, hence the name \"swimmer's ear\".\n\nPrevention\nThe strategies for preventing acute external otitis are similar to those for treatment.\n\nAvoid inserting anything into the ear canal: use of cotton buds or swabs is the most common event leading to acute otitis externa. Most normal ear canals have a self-cleaning and self-drying mechanism, the latter by simple evaporation.\nAfter prolonged swimming, a person prone to external otitis can dry the ears using a small battery-powered ear dryer, available at many retailers, especially shops catering to watersports enthusiasts.  Alternatively, drops containing dilute acetic acid (vinegar diluted 3:1) or Burow's solution may be used.  It is especially important not to instrument ears when the skin is saturated with water, as it is very susceptible to injury, which can lead to external otitis.\nAvoid swimming in polluted water.\nAvoid washing hair or swimming if very mild symptoms of acute external otitis begin.\nAlthough the use of earplugs when swimming and shampooing hair may help prevent external otitis, there are important details in the use of plugs. Hard and poorly fitting earplugs can scratch the ear canal skin and set off an episode. When earplugs are used during an acute episode, either disposable plugs are recommended, or used plugs must be cleaned and dried properly to avoid contaminating the healing ear canal with infected discharge.\nAccording to one source, the use of in-ear headphones during otherwise \"dry\" exercise in the summer has been associated with the development of swimmer's ear since the plugs can create a warm and moist environment inside the ears. The source claims that on-ear or over-ear headphones can be a better alternative for preventing swimmer's ear.\n\nTreatment\nMedications\nEffective solutions for the ear canal include acidifying and drying agents, used either singly or in combination. When the ear canal skin is inflamed from the acute otitis externa, the use of dilute acetic acid may be painful.\nBurow's solution is a very effective remedy against both bacterial and fungal external otitis. This is a buffered mixture of aluminium sulfate and acetic acid, and is available without prescription in the United States.\nEar drops are the mainstay of treatment for external otitis.  Some contain antibiotics, either antibacterial or antifungal, and others are simply designed to mildly acidify the ear canal environment to discourage bacterial growth. Some prescription drops also contain anti-inflammatory steroids, which help to resolve swelling and itching.  Although there is evidence that steroids are effective at reducing the length of treatment time required, fungal otitis externa (also called otomycosis) may be caused or aggravated by overly prolonged use of steroid-containing drops.\nAntibiotics by mouth should not be used to treat uncomplicated acute otitis externa. Antibiotics by mouth are not a sufficient response to bacteria which cause this condition and have significant side effects including increased risk of opportunistic infection. In contrast, topical products can treat this condition. Oral anti-pseudomonal antibiotics can be used in case of severe soft tissue swelling extending into the face and neck and may hasten recovery.\nAlthough the acute external otitis generally resolves in a few days with topical washes and antibiotics, complete return of hearing and cerumen gland function may take a few more days. Once healed completely, the ear canal is again self-cleaning. Until it recovers fully, it may be more prone to repeat infection from further physical or chemical insult.\nEffective medications include ear drops containing antibiotics to fight infection, and corticosteroids to reduce itching and inflammation. In painful cases, a topical solution of antibiotics such as aminoglycoside, polymyxin or fluoroquinolone is usually prescribed.  Antifungal solutions are used in the case of fungal infections.  External otitis is almost always predominantly bacterial or predominantly fungal so that only one type of medication is necessary and indicated.\n\nCleaning\nRemoval of debris (wax, shed skin, and pus) from the ear canal promotes direct contact of the prescribed medication with the infected skin and shortens recovery time. When canal swelling has progressed to the point where the ear canal is blocked, ear drops may not penetrate far enough into the ear canal to be effective.  The physician may need to carefully insert a wick of cotton or other commercially available, pre-fashioned, absorbent material called an ear wick and then saturate that with the medication.  The wick is kept saturated with medication until the canal opens enough that the drops will penetrate the canal without it.  Removal of the wick does not require a health professional. Antibiotic ear drops should be dosed in a quantity that allows coating of most of the ear canal and used for no more than 4 to 7 days.  The ear should be left open.  It is imperative that visualization of an intact tympanic membrane (eardrum) is noted.\nUse of certain medications with a ruptured tympanic membrane can cause tinnitus, vertigo, dizziness and hearing loss in some cases.\n\nPrognosis\nOtitis externa responds well to treatment, but complications may occur if it is not treated. Individuals with underlying diabetes, disorders of the immune system, or history of radiation therapy to the base of the skull are more likely to develop complications, including malignant otitis externa. In these individuals, rapid examination by an otolaryngologist (ear, nose, and throat physician) is very important.\n\nChronic otitis externa\nSpread of infection to other areas of the body\nNecrotizing external otitis\nOtitis externa haemorhagica\n\nNecrotizing external otitis\nNecrotizing external otitis (malignant otitis externa) is an uncommon form of external otitis that occurs mainly in elderly diabetics, being somewhat more likely and more severe when the diabetes is poorly controlled. Even less commonly, it can develop due to a severely compromised immune system. Beginning as infection of the external ear canal, there is an extension of the infection into the bony ear canal and the soft tissues deep to the bony canal. Unrecognized and untreated, it may result in death. The hallmark of malignant otitis externa (MOE) is unrelenting pain that interferes with sleep and persists even after swelling of the external ear canal may have resolved with topical antibiotic treatment.  It can also cause skull base osteomyelitis (SBO), manifested by multiple cranial nerve palsies, described below under the \"Treatment\" heading.\n\nNatural history\nMOE follows a much more chronic and indolent course than ordinary acute otitis externa. There may be granulation involving the floor of the external ear canal, most often at the bony-cartilaginous junction.  Paradoxically, the physical findings of MOE, at least in its early stages, are often much less dramatic than those of ordinary acute otitis externa. In later stages, there can be soft tissue swelling around the ear, even in the absence of significant canal swelling. While fever and leukocytosis might be expected in response to bacterial infection invading the skull region, MOE does not cause fever or elevation of white blood count.\n\nTreatment of MOE\nUnlike ordinary otitis externa, MOE requires oral or intravenous antibiotics for cure. Pseudomonas is the most common offending pathogen.  Diabetes control is also an essential part of treatment.  When MOE goes unrecognized and untreated, the infection continues to smolder and over weeks or months can spread deeper into the head and involve the bones of the skull base, constituting skull base osteomyelitis (SBO). Multiple cranial nerve palsies can result, including the facial nerve (causing facial palsy), the recurrent laryngeal nerve (causing vocal cord paralysis),  and the cochlear nerve (causing deafness).\nThe infecting organism is almost always pseudomonas aeruginosa, but it can instead be fungal (aspergillus or mucor). MOE and SBO are not amenable to surgery, but exploratory surgery may facilitate the culture of unusual organism(s) that are not responding to empirically used anti-pseudomonal antibiotics (ciprofloxacin being the drug of choice).  The usual surgical finding is diffuse cellulitis without localized abscess formation.  SBO can extend into the petrous apex of the temporal bone or more inferiorly into the opposite side of the skull base.\nThe use of hyperbaric oxygen therapy as an adjunct to antibiotic therapy remains controversial.\n\nComplications\nAs the skull base is progressively involved, the adjacent exiting cranial nerves and their branches, especially the facial nerve and the vagus nerve, may be affected, resulting in facial paralysis and hoarseness, respectively. If both of the recurrent laryngeal nerves are paralyzed, shortness of breath may develop and necessitate tracheotomy. Profound deafness can occur, usually later in the disease course due to relative resistance of the inner ear structures.  Gallium scans are sometimes used to document the extent of the infection but are not essential to disease management. Skull base osteomyelitis is a chronic disease that can require months of IV antibiotic treatment, tends to recur, and has a significant mortality rate.\n\nEpidemiology\nThe incidence of otitis externa is high. In the Netherlands, it has been estimated at 12\u201314 per 1000 population per year, and has been shown to affect more than 1% of a sample of the population in the United Kingdom over a 12-month period.\n\nHistory\nDuring the Tektite Project in 1969 there was a great deal of otitis externa.  The Diving Medical Officer devised a prophylaxis that came to be known as, \"Tektite Solution\", equal parts of 15% tannic acid, 15% acetic acid and 50% isopropyl alcohol or ethanol. During Tektite ethanol was used because it was available in the lab for pickling specimens.\n\nOther animals\nReferences\n\n\n== External links ==","134":"The semicircular canals are three semicircular interconnected tubes located in the innermost part of each ear, the inner ear. The three canals are the lateral, anterior and posterior semicircular canals. They are the part of the bony labyrinth, a periosteum-lined cavity on the petrous part of the temporal bone filled with perilymph. \n\nEach semicircular canal contains its respective semicircular duct, i.e. the lateral, anterior and posterior semicircular ducts, which provide the sensation of angular acceleration and are part of the membranous labyrinth\u2014therefore filled with endolymph.\n\nStructure\nThe semicircular canals are a component of the bony labyrinth that are at right angles from each other and contain their respective semicircular duct. At one end of each of the semicircular ducts is a dilated sac called an membranous ampulla, which is more than twice the diameter of the ducts. Each ampulla contains an ampullary crest, the crista ampullaris which consists of a thick gelatinous cap called a cupula and many hair cells. The superior and posterior semicircular ducts are oriented vertically at right angles to each other. The lateral semicircular duct is about a 30-degree angle from the horizontal plane. The orientations of the ducts cause a different duct to be stimulated by movement of the head in different planes, and more than one duct is stimulated at once if the movement is off those planes. The lateral semicircular duct detects angular acceleration of the head when the head is turned and the anterior and posterior semicircular ducts detect vertical head movements when the head is moved up or down. When the head changes position, the endolymph in the ducts lags behind due to inertia and this acts on the cupula which bends the cilia of the hair cells. The stimulation of the hair cells sends the message to the brain that acceleration is taking place. The semicircular canals open into the vestibule by five orifices, one of the apertures being common to two of them.\nAmong species of mammals, the size of the semicircular canals is correlated with their type of locomotion. Specifically, species that are agile and have fast, jerky locomotion have larger canals relative to their body size than those that move more cautiously.\n\nLateral semicircular canal\nThe lateral semicircular canal (also known as horizontal or external semicircular canal) is the shortest of the three canals. Movement of fluid within its duct corresponds to rotation of the head around a vertical axis (i.e. the neck), or in other words, rotation in the transverse plane. This occurs, for example, when one turns the head from side to side (yaw axis).\nIt measures from 12 to 15 mm (0.47 to 0.59 in), and its arch is directed horizontally backward and laterally; thus each semicircular canal stands at right angles to the other two. Its ampullated end corresponds to the upper and lateral angle of the vestibule, just above the oval window, where it opens close to the ampullated end of the anterior semicircular canal; its opposite end opens at the upper and back part of the vestibule. The lateral canal of one ear is very nearly in the same plane as that of the other.\n\nAnterior semicircular canal\nThe anterior semicircular canal (also known as superior semicircular canal) contains the part of the vestibular system that detects rotations of the head in around the lateral axis, that is, rotation in the sagittal plane. This occurs, for example, when nodding one's head (pitch axis).\nIt is 15 to 20 mm (0.59 to 0.79 in) in length, is vertical in direction, and is placed transversely to the long axis of the petrous part of the temporal bone, on the anterior surface of which its arch forms a round projection. It describes about two-thirds of a circle. Its lateral extremity is ampullated, and opens into the upper part of the vestibule; the opposite end joins with the upper part of the posterior semicircular canal to form the crus osseum commune, which opens into the upper and medial part of the vestibule.\n\nPosterior semicircular canal\nThe posterior semicircular canal contains the part of the vestibular system that detects rotation of the head around the antero-posterior (sagittal) axis, or in other words, rotation in the coronal plane. This occurs, for example, when one moves the head to touch the shoulders, or when doing a cartwheel (roll axis).\nIt is directed superiorly and posteriorly, as per its nomenclature, nearly parallel to the posterior surface of the petrous part of the temporal bone. The vestibular aqueduct is immediately medial to it. The posterior semicircular canal is part of the bony labyrinth and its duct is used by the vestibular system to detect rotations of the head in the coronal plane. It is the longest of the three semicircular canals, measuring from 18 to 22 mm (0.71 to 0.87 in). Its lower or ampullated end opens into the lower and back part of the vestibule, its upper into the crus osseum commune.\n\nDevelopment\nFindings from a 2009 study demonstrated a critical late role for bone morphogenetic protein 2 (BMP-2) in the morphogenesis of semicircular canals in the zebrafish inner ear. It is suspected that the role of BMP-2 in semicircular canal duct outgrowth is likely to be conserved between different vertebrate species.\nAdditionally, it has been found that the two semicircular canals found in the lamprey inner ear are developmentally similar to the superior and posterior canals found in humans, as the canals of both organisms arise from two depressions in the otic vesicle during early development. These depressions first form in lampreys between the 11 and 42 millimeter larval stages and form in zebrafish 57 hours post-fertilization\n\nFunction\nThe semicircular ducts provide sensory input for experiences of rotary movements. They are oriented along the pitch, roll, and yaw axes. The lateral semicircular canal is oriented in the yaw axis, the anterior semicircular canal is oriented in the pitch axis, and the posterior semicircular canal is oriented in the roll axis.\nEach duct is filled with a fluid called endolymph and contains motion sensors within the fluids. The base of each duct is enlarged, opening into the utricle, and has a dilated sac at one end called the membranous ampulla. Within the ampulla is a mound of hair cells and supporting cells called crista ampullaris. These hair cells have many cytoplasmic projections on the apical surface called stereocilia which are embedded in a gelatinous structure called the cupula. As the head rotates, the duct moves, but the endolymph lags behind owing to inertia. This deflects the cupula and bends the stereocilia within. The bending of these stereocilia alters an electric signal that is transmitted to the brain. Within approximately 10 seconds of achieving constant motion, the endolymph catches up with the movement of the duct and the cupula is no longer affected, stopping the sensation of acceleration. The specific gravity of the cupula is comparable to that of the surrounding endolymph. Consequently, the cupula is not displaced by gravity, unlike the otolithic membranes of the utricle and saccule. As with macular hair cells, hair cells of the crista ampullaris will depolarise when the stereocilia deflect towards the kinocilium. Deflection in the opposite direction results in hyperpolarisation and inhibition. In the lateral semicircular duct, ampullopetal flow is necessary for hair-cell stimulation, whereas ampullofugal flow is necessary for the anterior and posterior semicircular ducts.\nThis adjustment period is in part the cause of an illusion known as \"the leans\" often experienced by pilots. As a pilot enters a turn, hair cells in the semicircular ducts are stimulated, telling the brain that the aircraft, and the pilot, are no longer moving in a straight line but rather making a banked turn. If the pilot were to sustain a constant rate turn, the endolymph would eventually catch up with the ducts and cease to deflect the cupula. The pilot would no longer feel as if the aircraft was in a turn. As the pilot exits the turn, the semicircular ducts are stimulated to make the pilot think that they are now turning in the opposite direction rather than flying straight and level. In response to this, the pilot will often lean in the direction of the original turn in an attempt to compensate for this illusion. A more serious form of this is called a graveyard spiral. Rather than the pilot leaning in the direction of the original turn, they may actually re-enter the turn. As the endolymph stabilizes, the semicircular ducts stop registering the gradual turn and the aircraft slowly loses altitude until impact with the ground.\n\nHistory\nJean Pierre Flourens, by destroying the horizontal semicircular canal of pigeons, noted that they continue to fly in a circle, showing the purpose of the semicircular canals.\n\nSee also\nEar\nInner ear\n\nReferences\nThis article incorporates text in the public domain from page 1049 of the 20th edition of Gray's Anatomy (1918)\n\nAdditional images\nExternal links\n\n\"Inner Ear Detail Model\". Nervous System & Special Senses. Archived from the original on Jun 16, 2007.\nPurves, Dale; Augustine, George J.; Fitzpatrick, David; Katz, Lawrence C.; LaMantia, Anthony-Samuel; McNamara, James O.; Williams, S. Mark (2001). \"The Semicircular Canals\". Neuroscience (2nd ed.). Sinauer Associates. Archived from the original on Jun 6, 2023 \u2013 via National Center for Biotechnology Information.\n\"Human ear\". Encyclop\u00e6dia Britannica. Archived from the original on Dec 3, 2023.","135":"Otitis media is a group of inflammatory diseases of the middle ear. One of the two main types is acute otitis media (AOM), an infection of rapid onset that usually presents with ear pain. In young children this may result in pulling at the ear, increased crying, and poor sleep. Decreased eating and a fever may also be present. The other main type is otitis media with effusion (OME), typically not associated with symptoms, although  occasionally a feeling of fullness is described; it is defined as the presence of non-infectious fluid in the middle ear which may persist for weeks or months often after an episode of acute otitis media. Chronic suppurative otitis media (CSOM) is middle ear inflammation that results in a perforated tympanic membrane with discharge from the ear for more than six weeks. It may be a complication of acute otitis media. Pain is rarely present. All three types of otitis media may be associated with hearing loss. If children with hearing loss due to OME do not learn sign language, it may affect their ability to learn.\nThe cause of AOM is related to childhood anatomy and immune function. Either bacteria or viruses may be involved. Risk factors include exposure to smoke, use of pacifiers, and attending daycare. It occurs more commonly among indigenous Australians and those who have cleft lip and palate or Down syndrome. OME frequently occurs following AOM and may be related to viral upper respiratory infections, irritants such as smoke, or allergies. Looking at the eardrum is important for making the correct diagnosis. Signs of AOM include bulging or a lack of movement of the tympanic membrane from a puff of air. New discharge not related to otitis externa also indicates the diagnosis.\nA number of measures decrease the risk of otitis media including pneumococcal and influenza vaccination, breastfeeding, and avoiding tobacco smoke. The use of pain medications for AOM is important. This may include paracetamol (acetaminophen), ibuprofen, benzocaine ear drops, or opioids. In AOM, antibiotics may speed recovery but may result in side effects. Antibiotics are often recommended in those with severe disease or under two years old. In those with less severe disease they may only be recommended in those who do not improve after two or three days. The initial antibiotic of choice is typically amoxicillin. In those with frequent infections tympanostomy tubes may decrease recurrence. In children with otitis media with effusion antibiotics may increase resolution of symptoms, but may cause diarrhoea, vomiting and skin rash.\nWorldwide AOM affects about 11% of people a year (about 325 to 710 million cases). Half the cases involve children less than five years of age and it is more common among males. Of those affected about 4.8% or 31 million develop chronic suppurative otitis media. The total number of people with CSOM is estimated at 65\u2013330 million people. Before the age of ten OME affects about 80% of children at some point. Otitis media resulted in 3,200 deaths in 2015 \u2013 down from 4,900 deaths in 1990.\n\nSigns and symptoms\nThe primary symptom of acute otitis media is ear pain; other possible symptoms include fever, reduced hearing during periods of illness, tenderness on touch of the skin above the ear, purulent discharge from the ears, irritability, ear blocking sensation and diarrhea (in infants). Since an episode of otitis media is usually precipitated by an upper respiratory tract infection (URTI), there are often accompanying symptoms like a cough and nasal discharge.  One might also experience a feeling of fullness in the ear.\nDischarge from the ear can be caused by acute otitis media with perforation of the eardrum, chronic suppurative otitis media, tympanostomy tube otorrhea, or acute otitis externa. Trauma, such as a basilar skull fracture, can also lead to cerebrospinal fluid otorrhea (discharge of CSF from the ear) due to cerebral spinal drainage from the brain and its covering (meninges).\n\nCauses\nThe common cause of all forms of otitis media is dysfunction of the Eustachian tube. This is usually due to inflammation of the mucous membranes in the nasopharynx, which can be caused by a viral upper respiratory tract infection (URTI), strep throat, or possibly by allergies.\nBy reflux or aspiration of unwanted secretions from the nasopharynx into the normally sterile middle-ear space, the fluid may then become infected \u2013 usually with bacteria. The virus that caused the initial upper respiratory infection can itself be identified as the pathogen causing the infection.\n\nDiagnosis\nAs its typical symptoms overlap with other conditions, such as acute external otitis, symptoms alone are not sufficient to predict whether acute otitis media is present; it has to be complemented by visualization of the tympanic membrane. Examiners may use a pneumatic otoscope with a rubber bulb attached to assess the mobility of the tympanic membrane. Other methods to diagnose otitis media is with a tympanometry, reflectometry, or hearing test.\nIn more severe cases, such as those with associated hearing loss or high fever, audiometry, tympanogram, temporal bone CT and MRI can be used to assess for associated complications, such as mastoid effusion, subperiosteal abscess formation, bony destruction, venous thrombosis or meningitis. \nAcute otitis media in children with moderate to severe bulging of the tympanic membrane or new onset of otorrhea (drainage) is not due to external otitis. Also, the diagnosis may be made in children who have mild bulging of the ear drum and recent onset of ear pain (less than 48 hours) or intense erythema (redness) of the ear drum. To confirm the diagnosis, middle-ear effusion and inflammation of the eardrum (called myringitis or tympanitis) have to be identified; signs of these are fullness, bulging, cloudiness and redness of the eardrum. It is important to attempt to differentiate between acute otitis media and otitis media with effusion (OME), as antibiotics are not recommended for OME. It has been suggested that bulging of the tympanic membrane is the best sign to differentiate AOM from OME, with a bulging of the membrane suggesting AOM rather than OME.\nViral otitis may result in blisters on the external side of the tympanic membrane, which is called bullous myringitis (myringa being Latin for \"eardrum\"). However, sometimes even examination of the eardrum may not be able to confirm the diagnosis, especially if the canal is small. If wax in the ear canal obscures a clear view of the eardrum it should be removed using a blunt cerumen curette or a wire loop. Also, an upset young child's crying can cause the eardrum to look inflamed due to distension of the small blood vessels on it, mimicking the redness associated with otitis media.\n\nAcute otitis media\nThe most common bacteria isolated from the middle ear in AOM are Streptococcus pneumoniae, Haemophilus influenzae, Moraxella catarrhalis, and Staphylococcus aureus.\n\nOtitis media with effusion\nOtitis media with effusion (OME), also known as serous otitis media (SOM) or secretory otitis media (SOM), and colloquially referred to as 'glue ear,' is fluid accumulation that can occur in the middle ear and mastoid air cells due to negative pressure produced by dysfunction of the Eustachian tube. This can be associated with a viral upper respiratory infection (URI) or bacterial infection such as otitis media. An effusion can cause conductive hearing loss if it interferes with the transmission of vibrations of middle ear bones to the vestibulocochlear nerve complex that are created by sound waves.\nEarly-onset OME is associated with feeding of infants while lying down, early entry into group child care, parental smoking, lack or too short a period of breastfeeding, and greater amounts of time spent in group child care, particularly those with a large number of children. These risk factors increase the incidence and duration of OME during the first two years of life.\n\nChronic suppurative otitis media\nChronic suppurative otitis media (CSOM) is a long-term middle ear inflammation causing persistent ear discharge due to a perforated eardrum. It often follows an unresolved upper respiratory infection leading to acute otitis media. Prolonged inflammation leads to middle ear swelling, ulceration, perforation, and attempts at repair with granulation tissue and polyps. This can worsen discharge and inflammation, potentially developing into CSOM, often associated with cholesteatoma. Symptoms may include ear discharge or pus seen only on examination. Hearing loss is common. Risk factors include poor eustachian tube function, recurrent ear infections, crowded living, daycare attendance, and certain craniofacial malformations.\nWorldwide approximately 11% of the human population is affected by AOM every year, or 709 million cases. About 4.4% of the population develop CSOM.\nAccording to the World Health Organization, CSOM is a primary cause of hearing loss in children. Adults with recurrent episodes of CSOM have a higher risk of developing permanent conductive and sensorineural hearing loss.\nIn Britain, 0.9% of children and 0.5% of adults have CSOM, with no difference between the sexes. The incidence of CSOM across the world varies dramatically where high income countries have a relatively low prevalence while in low income countries the prevalence may be up to three times as great. Each year 21,000 people worldwide die due to complications of CSOM.\n\nAdhesive otitis media\nAdhesive otitis media occurs when a thin retracted ear drum becomes sucked into the middle-ear space and stuck (i.e., adherent) to the ossicles and other bones of the middle ear.\n\nPrevention\nAOM is far less common in breastfed infants than in formula-fed infants, and the greatest protection is associated with exclusive breastfeeding (no formula use) for the first six months of life. A longer duration of breastfeeding is correlated with a longer protective effect.\nPneumococcal conjugate vaccines (PCV) in early infancy decrease the risk of acute otitis media in healthy infants. PCV is recommended for all children, and, if implemented broadly, PCV would have a significant public health benefit. Influenza vaccination in children appears to reduce rates of AOM by 4% and the use of antibiotics by 11% over 6 months. However, the vaccine resulted in increased adverse-effects such as fever and runny nose. The small reduction in AOM may not justify the side effects and inconvenience of influenza vaccination every year for this purpose alone. PCV does not appear to decrease the risk of otitis media when given to high-risk infants or for older children who have previously experienced otitis media.\nRisk factors such as season, allergy predisposition and presence of older siblings are known to be determinants of recurrent otitis media and persistent middle-ear effusions (MEE). History of recurrence, environmental exposure to tobacco smoke, use of daycare, and lack of breastfeeding have all been associated with increased risk of development, recurrence, and persistent MEE. Pacifier use has been associated with more frequent episodes of AOM.\nLong-term antibiotics, while they decrease rates of infection during treatment, have an unknown effect on long-term outcomes such as hearing loss. This method of prevention has been associated with emergence of undesirable antibiotic-resistant otitic bacteria.\nThere is moderate evidence that the sugar substitute xylitol may reduce infection rates in healthy children who go to daycare.\nEvidence does not support zinc supplementation as an effort to reduce otitis rates except maybe in those with severe malnutrition such as marasmus.\nProbiotics do not show evidence of preventing acute otitis media in children.\n\nManagement\nOral and topical pain killers are the mainstay for the treatment of pain caused by otitis media. Oral agents include ibuprofen, paracetamol (acetaminophen), and opiates. A 2023 review found evidence for the effectiveness of single or combinations of oral pain relief in acute otitis media is lacking. Topical agents shown to be effective include antipyrine and benzocaine ear drops. Decongestants and antihistamines, either nasal or oral, are not recommended due to the lack of benefit and concerns regarding side effects. Half of cases of ear pain in children resolve without treatment in three days and 90% resolve in seven or eight days. The use of steroids is not supported by the evidence for acute otitis media.\n\nAntibiotics\nUse of antibiotics for acute otitis media has benefits and harms. As over 82% of acute episodes settle without treatment, about 20 children must be treated to prevent one case of ear pain, 33 children to prevent one perforation, and 11 children to prevent one opposite-side ear infection. For every 14 children treated with antibiotics, one child has an episode of vomiting, diarrhea or a rash. Analgesics may relieve pain, if present. For people requiring surgery to treat otitis media with effusion, preventative antibiotics may not help reduce the risk of post-surgical complications.\nFor bilateral acute otitis media in infants younger than 24 months, there is evidence that the benefits of antibiotics outweigh the harms. A 2015 Cochrane review concluded that watchful waiting is the preferred approach for children over six months with non severe acute otitis media.\n\nMost children older than 6 months of age who have acute otitis media do not benefit from treatment with antibiotics. If antibiotics are used, a narrow-spectrum antibiotic like amoxicillin is generally recommended, as broad-spectrum antibiotics may be associated with more adverse events. If there is resistance or use of amoxicillin in the last 30 days then amoxicillin-clavulanate or another penicillin derivative plus beta lactamase inhibitor is recommended. Taking amoxicillin once a day may be as effective as twice or three times a day. While less than 7 days of antibiotics have fewer side effects, more than seven days appear to be more effective. If there is no improvement after 2\u20133 days of treatment a change in therapy may be considered. Azithromycin appears to have less side effects than either high dose amoxicillin or amoxicillin\/clavulanate.\n\nTympanostomy tube\nTympanostomy tubes (also called \"grommets\") are recommended with three or more episodes of acute otitis media in 6 months or four or more in a year, with at least one episode or more attacks in the preceding 6 months. There is tentative evidence that children with recurrent acute otitis media (AOM) who receive tubes have a modest improvement in the number of further AOM episodes (around one fewer episode at six months and less of an improvement at 12 months following the tubes being inserted). Evidence does not support an effect on long-term hearing or language development. A common complication of having a tympanostomy tube is otorrhea, which is a discharge from the ear. The risk of persistent tympanic membrane perforation after children have grommets inserted may be low. It is still uncertain whether or not grommets are more effective than a course of antibiotics.\nOral antibiotics should not be used to treat uncomplicated acute tympanostomy tube otorrhea. They are not sufficient for the bacteria that cause this condition and have side effects including increased risk of opportunistic infection. In contrast, topical antibiotic eardrops are useful.\n\nOtitis media with effusion\nThe decision to treat is usually made after a combination of physical exam and laboratory diagnosis, with additional testing including audiometry, tympanogram, temporal bone CT and MRI. Decongestants, glucocorticoids, and topical antibiotics are generally not effective as treatment for non-infectious, or serous, causes of mastoid effusion. Moreover, it is recommended against using antihistamines and decongestants in children with OME. In less severe cases or those without significant hearing impairment, the effusion can resolve spontaneously or with more conservative measures such as autoinflation. In more severe cases, tympanostomy tubes can be inserted, possibly with adjuvant adenoidectomy as it shows a significant benefit as far as the resolution of middle ear effusion in children with OME is concerned.\n\nChronic suppurative otitis media\nTopical antibiotics are of uncertain benefit as of 2020. Some evidence suggests that topical antibiotics may be useful either alone or with antibiotics by mouth. Antiseptics are of unclear effect. Topical antibiotics (quinolones) are probably better at resolving ear discharge than antiseptics.\n\nAlternative medicine\nComplementary and alternative medicine is not recommended for otitis media with effusion because there is no evidence of benefit. Homeopathic treatments have not been proven to be effective for acute otitis media in a study with children.  An osteopathic manipulation technique called the Galbreath technique was evaluated in one randomized controlled clinical trial; one reviewer concluded that it was promising, but a 2010 evidence report found the evidence inconclusive.\n\nOutcomes\nComplications of acute otitis media consists of perforation of the ear drum, infection of the mastoid space behind the ear (mastoiditis), and more rarely intracranial complications can occur, such as bacterial meningitis, brain abscess, or dural sinus thrombosis. It is estimated that each year 21,000 people die due to complications of otitis media.\n\nMembrane rupture\nIn severe or untreated cases, the tympanic membrane may perforate, allowing the pus in the middle-ear space to drain into the ear canal. If there is enough, this drainage may be obvious. Even though the perforation of the tympanic membrane suggests a highly painful and traumatic process, it is almost always associated with a dramatic relief of pressure and pain. In a simple case of acute otitis media in an otherwise healthy person, the body's defenses are likely to resolve the infection and the ear drum nearly always heals.\nAn option for severe acute otitis media in which analgesics are not controlling ear pain is to perform a tympanocentesis, i.e., needle aspiration through the tympanic membrane to relieve the ear pain and to identify the causative organism(s).\n\nHearing loss\nChildren with recurrent episodes of acute otitis media and those with otitis media with effusion or chronic suppurative otitis media have higher risks of developing conductive and sensorineural hearing loss. Globally approximately 141 million people have mild hearing loss due to otitis media (2.1% of the population). This is more common in males (2.3%) than females (1.8%).\nThis hearing loss is mainly due to fluid in the middle ear or rupture of the tympanic membrane. Prolonged duration of otitis media is associated with ossicular complications and, together with persistent tympanic membrane perforation, contributes to the severity of the disease and hearing loss. When a cholesteatoma or granulation tissue is present in the middle ear, the degree of hearing loss and ossicular destruction is even greater.\nPeriods of conductive hearing loss from otitis media may have a detrimental effect on speech development in children. Some studies have linked otitis media to learning problems, attention disorders, and problems with social adaptation. Furthermore, it has been demonstrated that individuals with otitis media have more depression\/anxiety-related disorders compared to individuals with normal hearing. Once the infections resolve and hearing thresholds return to normal, childhood otitis media may still cause minor and irreversible damage to the middle ear and cochlea. More research on the importance of screening all children under 4 years old for otitis media with effusion needs to be performed.\n\nEpidemiology\nAcute otitis media is very common in childhood. It is the most common condition for which medical care is provided in children under five years of age in the US. Acute otitis media affects 11% of people each year (709 million cases) with half occurring in those below five years. Chronic suppurative otitis media affects about 5% or 31 million of these cases with 22.6% of cases occurring annually under the age of five years. Otitis media resulted in 2,400 deaths in 2013 \u2013 down from 4,900 deaths in 1990.\nAustralian Aboriginals experience a high level of conductive hearing loss largely due to the massive incidence of middle ear disease among the young in Aboriginal communities. Aboriginal children experience middle ear disease for two and a half years on average during childhood compared with three months for non indigenous children. If untreated it can leave a permanent legacy of hearing loss. The higher incidence of deafness in turn contributes to poor social, educational and emotional outcomes for the children concerned. Such children as they grow into adults are also more likely to experience employment difficulties and find themselves caught up in the criminal justice system. Research in 2012 revealed that nine out of ten Aboriginal prison inmates in the Northern Territory suffer from significant hearing loss.\nAndrew Butcher speculates that the lack of fricatives and the unusual segmental inventories of Australian languages may be due to the very high presence of otitis media ear infections and resulting hearing loss in their populations. People with hearing loss often have trouble distinguishing different vowels and hearing fricatives and voicing contrasts. Australian Aboriginal languages thus seem to show similarities to the speech of people with hearing loss, and avoid those sounds and distinctions which are difficult for people with early childhood hearing loss to perceive. At the same time, Australian languages make full use of those distinctions, namely place of articulation distinctions, which people with otitis media-caused hearing loss can perceive more easily. This hypothesis has been challenged on historical, comparative, statistical, and medical grounds.\n\nEtymology\nThe term otitis media is composed of otitis, Ancient Greek for \"inflammation of the ear\", and media, Latin for \"middle\".\n\nReferences\nButcher AR (2018). \"The special nature of Australian phonologies: Why auditory constraints on human language sound systems are not universal\". Proceedings of Meetings on Acoustics. 176th Meeting of Acoustical Society of America. 177th Meeting of the Acoustical Society of America. Vol. 35. Acoustical Society of America. p. 060004. doi:10.1121\/2.0001004.\n\nExternal links\nNeff MJ (June 2004). \"AAP, AAFP, AAO-HNS release guideline on diagnosis and management of otitis media with effusion\". American Family Physician. 69 (12): 2929\u20132931. PMID 15222658.","136":"An otoacoustic emission (OAE) is a sound that is generated from within the inner ear. Having been predicted by Austrian astrophysicist Thomas Gold in 1948, its existence was first demonstrated experimentally by British physicist David Kemp in 1978, and otoacoustic emissions have since been shown to arise through a number of different cellular and mechanical causes within the inner ear. Studies have shown that OAEs disappear after the inner ear has been damaged, so OAEs are often used in the laboratory and the clinic as a measure of inner ear health.\nBroadly speaking, there are two types of otoacoustic emissions: spontaneous otoacoustic emissions (SOAEs), which occur without external stimulation, and evoked otoacoustic emissions (EOAEs), which require an evoking stimulus.\n\nMechanism of occurrence\nOAEs are considered to be related to the amplification function of the cochlea. In the absence of external stimulation, the activity of the cochlear amplifier increases, leading to the production of sound. Several lines of evidence suggest that, in mammals, outer hair cells are the elements that enhance cochlear sensitivity and frequency selectivity and hence act as the energy sources for amplification.\n\nTypes\nSpontaneous\nSpontaneous otoacoustic emissions (SOAEs) are sounds that are emitted from the ear without external stimulation and are measurable with sensitive microphones in the external ear canal. At least one SOAE can be detected in approximately 35\u201350% of the population. The sounds are frequency-stable between 500 Hz and 4,500 Hz and have unstable volumes between -30 dB SPL and +10 dB SPL. The majority of those with SOAEs are unaware of them, however 1\u20139% perceive a SOAE as an annoying tinnitus. It has been suggested that \"The Hum\" phenomena are SOAEs.\n\nEvoked\nEvoked otoacoustic emissions are currently evoked using three different methodologies.\n\nStimulus-frequency OAEs (SFOAEs) are measured during the application of a pure-tone stimulus and are detected by the vectorial difference between the stimulus waveform and the recorded waveform (which consists of the sum of the stimulus and the OAE).\nTransient-evoked OAEs (TEOAEs or TrOAEs) are evoked using a click (broad frequency range) or toneburst (brief duration pure tone) stimulus. The evoked response from a click covers the frequency range up to around 4 kHz, while a toneburst will elicit a response from the region that has the same frequency as the pure tone.\nDistortion-product OAEs (DPOAEs) are evoked using a pair of primary tones \n  \n    \n      \n        \n          f\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle f_{1}}\n  \n and \n  \n    \n      \n        \n          f\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle f_{2}}\n  \n with particular intensity (usually either 65\u201355 dB SPL or 65 for both) and ratio (\n  \n    \n      \n        \n          f\n          \n            1\n          \n        \n        \n          \n             \n          \n        \n        :\n        \n          \n             \n          \n        \n        \n          f\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle f_{1}{\\mbox{ }}:{\\mbox{ }}f_{2}}\n  \n).\nThe evoked responses from these stimuli occur at frequencies (\n  \n    \n      \n        \n          f\n          \n            d\n            p\n          \n        \n      \n    \n    {\\displaystyle f_{dp}}\n  \n) mathematically related to the primary frequencies, with the two most prominent being \n  \n    \n      \n        \n          f\n          \n            d\n            p\n          \n        \n        =\n        2\n        \n          f\n          \n            1\n          \n        \n        \u2212\n        \n          f\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle f_{dp}=2f_{1}-f_{2}}\n  \n (the \"cubic\" distortion tone, most commonly used for hearing screening), because they produce the most robust emission, and \n  \n    \n      \n        \n          f\n          \n            d\n            p\n          \n        \n        =\n        \n          f\n          \n            2\n          \n        \n        \u2212\n        \n          f\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle f_{dp}=f_{2}-f_{1}}\n  \n (the \"quadratic\" distortion tone, or simple difference tone).\n\nClinical importance\nOtoacoustic emissions are clinically important because they are the basis of a simple, non-invasive test for cochlear hearing loss in newborn babies and in children or adults who are unable or unwilling to cooperate during conventional hearing tests. In addition, the OAEs are highly reliable making it suitable for diagnostic and screening applications. Many western countries now have national programmes for the universal hearing screening of newborn babies. Newborn hearing screening is state-mandated prior to hospital discharge in the United States. Periodic early childhood hearing screenings programs are also utilizing OAE technology. The Early Childhood Hearing Outreach Initiative at the National Center for Hearing Assessment and Management (NCHAM) at Utah State University has helped hundreds of Early Head Start programs across the United States implement OAE screening and follow-up practices in those early childhood educational settings. The primary screening tool is a test for the presence of a click-evoked OAE. Otoacoustic emissions also assist in differential diagnosis of cochlear and higher level hearing losses (e.g., auditory neuropathy).\nThe relationships between otoacoustic emissions and tinnitus have been explored. Several studies suggest that in about 6% to 12% of normal-hearing persons with tinnitus and SOAEs, the SOAEs are at least partly responsible for the tinnitus. Studies have found that some subjects with tinnitus display oscillating or ringing EOAEs, and in these cases, it is hypothesized that the oscillating EOAEs and tinnitus are related to a common underlying pathology rather than the emissions being the source of the tinnitus.\nIn conjunction with audiometric testing, OAE testing can be completed to determine changes in the responses. Studies have found that exposure to noise can cause a decline in OAE responses. OAEs are a measurement of the activity of outer hair cells in the cochlea, and noise-induced hearing loss occurs as a result of damage to the outer hair cells in the cochlea. Therefore, the damage or loss of some outer hair cells will likely show up on OAEs before showing up on the audiogram. Studies have shown that for some individuals with normal hearing that have been exposed to excessive sound levels, fewer, reduced, or no OAEs can be present. This could be an indication of noise-induced hearing loss before it is seen on an audiogram.\nIn one study, a group of subjects with noise exposure was compared to a group of subjects with normal audiograms and a history of noise exposure, as well as a group of military recruits with no history of noise exposure and a normal audiogram. They found that an increase in severity of the noise-induced hearing loss resulted in OAEs with a smaller range of emissions and reduced amplitude of the emissions. The loss of emissions due to noise exposure was found to occur mostly in higher frequencies, and it was more prominent in the groups that had noise exposure in comparison to the non-exposed group. It was found that OAEs were more sensitive to identifying noise-induced cochlear damage than pure tone audiometry. In conclusion, the study identified OAEs as a method for helping with detection of the early onset of noise-induced hearing loss.\nIt has been found that distortion-product otoacoustic emissions (DPOAE's) have provided the most information for detecting hearing loss in high frequencies when compared to transient-evoked otoacoustic emissions (TEOAE). This is an indication that DPOAE's can help with detecting an early onset of noise-induced hearing loss. A study measuring audiometric thresholds and DPOAEs among individuals in the military showed that there was a decrease in DPOAEs after noise exposure, but did not show a shift in audiometric threshold. This supports OAEs as predicting early signs of noise damage.\n\nBiometric importance\nIn 2009, Stephen Beeby of the University of Southampton led research into utilizing otoacoustic emissions for biometric identification. Devices equipped with a microphone could detect these subsonic emissions and potentially identify an individual, thereby providing access to the device, without the need of a traditional password. It is speculated, however, that colds, medication, trimming one's ear hair, or recording and playing back a signal to the microphone could subvert the identification process.\n\nMeasuring otoacoustic emissions on earphones\nHigh-end personalized headphone products (e.g., Nuraphone) are being designed to measure OAEs and determine the listener\u2019s sensitivity to different acoustic frequencies. This is then used to  personalize the audio signal for each listener.\nIn 2022, researchers at the University of Washington built a low-cost prototype that can reliably detect otoacoustic emissions using commodity earphones and microphones attached to a smartphone. The low-cost prototype sends two frequency tones through each of the headphone\u2019s earbuds, detects the distortion-product OAEs generated by the cochlea and recorded via the microphone. Such low-cost technologies may help larger efforts to achieve universal neonatal hearing screening across the world.\n\nSee also\nAuditory brainstem response\nEntoptic phenomenon\nMaryanne Amacher, a composer who used this phenomenon in her music\nPure tone audiometry\nThe Hum\n\nReferences\nFurther reading\nM.S. Robinette and T.J. Glattke (eds., 2007). Otoacoustic Emissions: Clinical Applications, third edition (Thieme).\nG.A. Manley, R.R. Fay, and A.N. Popper (eds., 2008). Active Processes and Otoacoustic Emissions (Springer Handbook of Auditory Research, vol. 30).\nS. Dhar and J.W. Hall, III (2011). Otoacoustic Emissions: Principles, Procedures, and Protocols (Plural Publishing).","137":"An otolith (Greek: \u1f60\u03c4\u03bf-, \u014dto- ear + \u03bb\u1fd0\u0301\u03b8\u03bf\u03c2, l\u00edthos, a stone), also called statoconium, otoconium or statolith, is a calcium carbonate structure in the saccule or utricle of the inner ear, specifically in the vestibular system of vertebrates. The saccule and utricle, in turn, together make the otolith organs. These organs are what allows an organism, including humans, to perceive linear acceleration, both horizontally and vertically (gravity). They have been identified in both extinct and extant vertebrates.\nCounting the annual growth rings on the otoliths is a common technique in estimating the age of fish.\n\nDescription\nEndolymphatic infillings such as otoliths are structures in the saccule and utricle of the inner ear, specifically in the vestibular labyrinth of all vertebrates (fish, amphibians, reptiles, mammals and birds). In vertebrates, the saccule and utricle together make the otolith organs. Both statoconia and otoliths are used as gravity, balance, movement, and directional indicators in all vertebrates and have a secondary function in sound detection in higher aquatic and terrestrial vertebrates. They are sensitive to gravity and linear acceleration. Because of their orientation in the head, the utricle is sensitive to a change in horizontal movement, and the saccule gives information about vertical acceleration (such as when in an elevator).\nSimilar balance receptors called statocysts can be found in many invertebrate groups but are not contained in the structure of an inner ear. Mollusk statocysts are of a similar morphology to the displacement-sensitive organs of vertebrates; however, the function of the mollusk statocyst is restricted to gravity detection and possibly some detection of angular momentum. These are analogous structures, with similar form and function but not descended from a common structure.\nStatoconia (also called otoconia) are numerous grains, often spherical in shape, between 1 and 50 \u03bcm; collectively. Statoconia are also sometimes termed a statocyst. Otoliths (also called statoliths) are agglutinated crystals or crystals precipitated around a nucleus, with well defined morphology and together all may be termed endolymphatic infillings.\n\nMechanism\nThe semicircular canals and sacs in all vertebrates are attached to endolymphatic ducts, which in some groups (such as sharks) end in small openings, called endolymphatic pores, on the dorsal surface of the head. Extrinsic grains may enter through these openings, typically less than a millimeter in diameter. The size of material that enters is limited to sand-sized particles and in the case of sharks is bound together with an endogenous organic matrix that the animal secretes.\nIn mammals, otoliths are small particles, consisting of a combination of a gelatinous matrix and calcium carbonate in the viscous fluid of the saccule and utricle. The weight and inertia of these small particles causes them to stimulate hair cells when the head moves. The hair cells are made up of 40 to 70 stereocilia and one kinocilium, which is connected to an afferent nerve. Hair cells send signals down sensory nerve fibers which are interpreted by the brain as motion. In addition to sensing acceleration of the head, the otoliths can help to sense the orientation via gravity's effect on them.  When the head is in a normal upright position, the otolith presses on the sensory hair cell receptors.  This pushes the hair cell processes down and prevents them from moving side to side. However, when the head is tilted, the pull of gravity on otoliths shifts the hair cell processes to the side, distorting them and sending a message to the central nervous system that the head is tilted.\nThere is evidence that the vestibular system of mammals has retained some of its ancestral acoustic sensitivity and that this sensitivity is mediated by the otolithic organs (most likely the sacculus, due to its anatomical location). In mice lacking the otoconia of the utricle and saccule, this retained acoustic sensitivity is lost. In humans vestibular evoked myogenic potentials occur in response to loud, low-frequency acoustic stimulation in patients with the sensorineural hearing loss. Vestibular sensitivity to ultrasonic sounds has also been hypothesized to be involved in the perception of speech presented at artificially high frequencies, above the range of the human cochlea (~18 kHz). In mice, sensation of acoustic information via the vestibular system has been demonstrated to have a behaviourally relevant effect; response to an elicited acoustic startle reflex is larger in the presence of loud, low frequency sounds that are below the threshold for the mouse cochlea (~4 Hz), raising the possibility that the acoustic sensitivity of the vestibular system may extend the hearing range of small mammals.\n\nPaleontology\nAfter the death and decomposition of a fish, otoliths may be preserved within the body of an organism or be dispersed before burial and fossilization. Dispersed otoliths are one of the many microfossils which can be found through a micropalaeontological analysis of a fine sediment. Their stratigraphic significance is minimal, but can still be used to characterize a level or interval. Fossil otoliths are rarely found in situ (on the remains of the animal), likely because they are not recognized separately from the surrounding rock matrix. In some cases, due to differences in colour, grain size, or a distinctive shape, they can be identified. These rare cases are of special significance, since the presence, composition, and morphology of the material can clarify the relationship of species and groups. In the case of primitive fish, various fossil material shows that endolymphatic infillings were similar in elemental composition to the rock matrix but were restricted to coarse grained material, which presumably is better for the detection of gravity, displacement, and sound. The presence of these extrinsic grains in osteostracans, chondrichthyans, and acanthodians indicates a common inner ear physiology and presence of open endolymphatic ducts.\nAn unclassified fossil named Gluteus minimus has been thought to be possible otoliths, but it is hitherto unknown to which animal they could belong to.\n\nEcology\nComposition\nThe composition of fish otoliths is also proving useful to fisheries scientists.  The calcium carbonate that the otolith is composed of is primarily derived from the water.  As the otolith grows, new calcium carbonate crystals form.  As with any crystal structure, lattice vacancies will exist during crystal formation allowing trace elements from the water to bind with the otolith.  Studying the trace elemental composition or isotopic signatures of trace elements within a fish otolith gives insight to the water bodies fish have previously occupied. Fish otoliths as old as 172 million years have been used to study the environment in which the fish lived. Robotic micromilling devices have also been used to recover very high resolution records of life history, including diet and temperatures throughout the life of the fish, as well as their natal origin.\nThe most studied trace and isotopic signatures are strontium due to the same charge and similar ionic radius to calcium; however, scientists can study multiple trace elements within an otolith to discriminate more specific signatures.  A common tool used to measure trace elements in an otolith is a laser ablation inductively coupled plasma mass spectrometer.  This tool can measure a variety of trace elements simultaneously.  A secondary ion mass spectrometer can also be used.  This instrument can allow for greater chemical resolution but can only measure one trace element at a time. The hope of this research is to provide scientists with valuable information on where fish have frequented.  Combined with otolith annuli, scientists can add how old fish were when they traveled through different bodies of water.  This information can be used to determine fish life cycles so that fisheries scientists can make better informed decisions about fish stocks.\n\nGrowth rate and age\nFinfish (class Osteichthyes) have three pairs of otoliths \u2013 the sagittae (singular sagitta), lapilli (singular lapillus), and asterisci (singular asteriscus). The sagittae are largest, found just behind the eyes and approximately level with them vertically. The lapilli and asterisci (smallest of the three) are located within the semicircular canals. The sagittae are normally composed of aragonite (although vaterite abnormalities can occur), as are the lapilli, while the asterisci are normally composed of vaterite.\nThe shapes and proportional sizes of the otoliths vary with fish species. In general, fish from highly structured habitats such as reefs or rocky bottoms (e.g. snappers, groupers, many drums and croakers) will have larger otoliths than fish that spend most of their time swimming at high speed in straight lines in the open ocean (e.g. tuna, mackerel, dolphinfish). Flying fish have unusually large otoliths, possibly due to their need for balance when launching themselves out of the water to \"fly\" in the air. Often, the fish species can be identified from distinct morphological characteristics of an isolated otolith.\nFish otoliths accrete layers of calcium carbonate and gelatinous matrix throughout their lives. The accretion rate varies with growth of the fish \u2013 often less growth in winter and more in summer \u2013 which results in the appearance of rings that resemble tree rings. By counting the rings, it is possible to determine the age of the fish in years. Typically the sagitta is used, as it is largest, but sometimes lapilli are used if they have a more convenient shape. The asteriscus, which is smallest of the three, is rarely used in age and growth studies.\nIn addition, in most species the accretion of calcium carbonate and gelatinous matrix alternates on a daily cycle. It is therefore also possible to determine fish age in days. This latter information is often obtained under a microscope, and provides significant data to early life history studies.\nBy measuring the thickness of individual rings, it has been assumed (at least in some species) to estimate fish growth because fish growth is directly proportional to otolith growth. However, some studies disprove a direct link between body growth and otolith growth.  At times of lower or zero body growth the otolith continues to accrete leading some researchers to believe the direct link is to metabolism, not growth per se. Otoliths, unlike scales, do not reabsorb during times of decreased energy making it even more useful tool to age a fish.  Fish never stop growing entirely, though growth rate in mature fish is reduced. Rings corresponding to later parts of the life cycle tend to be closer together as a result. Furthermore, a small percentage of otoliths in some species bear deformities over time.\nAge and growth studies of fish are important for understanding such things as timing and magnitude of spawning, recruitment and habitat use, larval and juvenile duration, and population age structure. Such knowledge is in turn important for designing appropriate fisheries management policies. Due to the amount of required human labour in otolith age reading, there is active research in automating that process.\n\nDiet research\nSince the compounds in fish otoliths are resistant to digestion, they are found in the digestive tracts and scats of seabirds and piscivorous marine mammals, such as dolphins, seals, sea lions and walruses. Many fish can be identified to genus and species by their otoliths. Otoliths can therefore, to some extent, be used to deduce and reconstruct the prey composition of marine mammal and seabird diets.\nOtoliths (sagittae) are bilaterally symmetrical, with each fish having one right and one left. Separating recovered otoliths into right and left, therefore, allows one to infer a minimum number of prey individuals ingested for a given fish species. Otolith size is also proportional to the length and weight of a fish. They can therefore be used to back-calculate prey size and biomass, useful when trying to estimate marine mammal prey consumption, and potential impacts on fish stocks.\nOtoliths cannot be used alone to reliably estimate cetacean or pinniped diets, however. They may suffer partial or complete erosion in the digestive tract, skewing measurements of prey number and biomass. Species with fragile, easily digested otoliths may be underestimated in the diet. To address these biases, otolith correction factors have been developed through captive feeding experiments, in which seals are fed fish of known size, and the degree of otolith erosion is quantified for different prey taxa.\nThe inclusion of fish vertebrae, jaw bones, teeth, and other informative skeletal elements improves prey identification and quantification over otolith analysis alone. This is especially true for fish species with fragile otoliths, but other distinctive bones, such as Atlantic mackerel (Scomber scombrus), and Atlantic herring (Clupea harengus).\n\nOtolith ornaments\n'Sea gems' ornaments from fish otoliths have been introduced in the market in India recently, with the efforts of a group of enthusiastic fisher women in Vizhinjam. Scientists from Central Marine Fisheries Research Institute (CMFRI) have trained these fisher-women. Ornaments from fish otoliths, known to the Romans and Egyptians as lucky stones, are continued to be used in countries like Brazil and the Faer\u00f6er, and are being collected and sold in an organized and sustainable manner in India.\n\nSee also\nOssicles\nOtolithic membrane\nOtolith microchemical analysis\nOrbiting Frog Otolith, 1970 space mission\n\nReferences\nExternal links\nOtolith Research Lab \u2013 Bedford Institute of Oceanography.","138":"Otomycosis  is a fungal ear infection, a superficial mycotic infection of the outer ear canal caused by micro-organisms called fungi which are related to yeast and mushrooms. It is more common in tropical or warm countries. The infection may be either subacute or acute and is characterized by itching in the ear, malodorous discharge, inflammation, pruritus, scaling, and severe discomfort or ear pain. The mycosis results in inflammation, superficial epithelial exfoliation, masses of debris containing hyphae, suppuration, and pain. Otomycosis can also cause hearing loss.\n\nSigns and symptoms\nOtomycosis does not usually cause as much canal skin edema as does acute bacterial external otitis.  While a severe pressure type of pain is a prominent feature of advanced cases, the ear is usually much less tender, if at all, to traction or tragal pressure.  Appearance of the fungus is variable, most commonly gray, white, or black, often intermixed with cerumen and clinging to the canal skin.  Gray concretions may be present.\n\nCause\nMost fungal ear infections are caused by Aspergillus niger, Aspergillus fumigatus, Penicillium and Candida albicans. Otomycosis commonly worsens from overuse of antibacterial ear drops, which should never be used for more than 7 days.  In such cases the fungus is an opportunist that results from antibacterial suppression of the normal bacterial flora, combined with the steroid the drops also contain.\n\nDiagnosis\nOtoscopy (exam of the ear) is best done with a binocular microscope that provides adequate lighting, depth perception, and the ability to instrument the ear to comfortably remove the fungus.  Findings range from scattered saprophytic fungal colonies of various colors, causing no symptoms, to densely packed fungal debris, often intermixed with cerumen (wax), filling the entire canal and involving the tympanic membrane (eardrum).  The fungus can cling to the skin and tympanic membrane, presumably because of invading hyphae, and can require significant time to accomplish complete removal.\n\nTreatment\nTreatment of otomycosis typically includes microscopic suction to remove fungal mass, topical antibiotics to be discontinued, and treatment with antifungal eardrops for three weeks. The antifungal medications can be administered in the form of creams or drops applied to the ears and the most commonly used medications are azoles, a heterocyclic class of antifungal agents. Evidence in the form of high quality clinical trials on treatment methods is very weak and it is not known how effective these treatments are at improving the infection, having adverse effects (serious or not serious) when comparing different medications prescribed.\nIf neglected and not treated on time, the infection can cause perforation of eardrum. The only resort in such a situation can be major ear surgery, as hearing loss can be permanent.\n\nReferences\n\n\n== External links ==","139":"Otorhinolaryngology ( oh-toh-RY-noh-LARR-in-GOL-\u0259-jee, abbreviated ORL and also known as otolaryngology, otolaryngology \u2013 head and neck surgery (ORL\u2013H&N or OHNS), or ear, nose, and throat (ENT)\u200a) is a surgical subspecialty within medicine that deals with the surgical and medical management of conditions of the head and neck. Doctors who specialize in this area are called otorhinolaryngologists, otolaryngologists, head and neck surgeons, or ENT surgeons or physicians. Patients seek treatment from an otorhinolaryngologist for diseases of the ear, nose, throat, base of the skull, head, and neck. These commonly include functional diseases that affect the senses and activities of eating, drinking, speaking, breathing, swallowing, and hearing. In addition, ENT surgery encompasses the surgical management of cancers and benign tumors and reconstruction of the head and neck as well as plastic surgery of the face, scalp, and neck.\n\nEtymology\nThe term is a combination of Neo-Latin combining forms (oto- + rhino- + laryngo- + -logy) derived from four Ancient Greek words: \u03bf\u1f56\u03c2 ous (gen.: \u1f60\u03c4\u03cc\u03c2 otos), \"ear\", \u1fe5\u03af\u03c2 rhis, \"nose\", \u03bb\u03ac\u03c1\u03c5\u03b3\u03be larynx, \"larynx\" and -\u03bb\u03bf\u03b3\u03af\u03b1 logia, \"study\" (cf. Greek \u03c9\u03c4\u03bf\u03c1\u03b9\u03bd\u03bf\u03bb\u03b1\u03c1\u03c5\u03b3\u03b3\u03bf\u03bb\u03cc\u03b3\u03bf\u03c2, \"otorhinolaryngologist\").\n\nTraining\nOtorhinolaryngologists are physicians (MD, DO, MBBS, MBChB, etc.) who complete both medical school and an average of five\u2013seven years of post-graduate surgical training in ORL-H&N. In the United States, trainees complete at least five years of surgical residency training. This comprises three to six months of general surgical training and four and a half years in ORL-H&N specialist surgery. In Canada and the United States, practitioners complete a five-year residency training after medical school.\nFollowing residency training, some otolaryngologist-head & neck surgeons complete an advanced sub-specialty fellowship, where training can be one to two years in duration. Fellowships include head and neck surgical oncology, facial plastic surgery, rhinology and sinus surgery, neuro-otology, pediatric otolaryngology, and laryngology. In the United States and Canada, otorhinolaryngology is one of the most competitive specialties in medicine in which to obtain a residency position following medical school.\nIn the United Kingdom, entrance to higher surgical training is competitive and involves a rigorous national selection process. The training programme consists of 6 years of higher surgical training after which trainees frequently undertake fellowships in a sub-speciality prior to becoming a consultant.\nThe typical total length of education, training and post-secondary school is 12\u201314 years. Otolaryngology is among the more highly compensated surgical specialties in the United States. In 2022, the average annual income was $469,000.\n\nSub-specialties\n(*Currently recognized by American Board of Medical Subspecialties)\n\nTopics by subspecialty\nHead and neck surgery\nHead and neck surgical oncology (field of surgery treating cancer\/malignancy of the head and neck)\nHead and neck mucosal malignancy (cancer of the pink lining of the upper aerodigestive tract)\nOral cancer (cancer of lips, gums, tongue, hard palate, cheek, floor of mouth)\nOropharyngeal cancer (cancer of oropharynx, soft palate, tonsil, base of tongue)\nLarynx cancer (voice box cancer)\nHypopharynx cancer (lower throat cancer)\nSinonasal cancer\nNasopharyngeal cancer\nSkin cancer of the head & neck\nThyroid cancer\nSalivary gland cancer\nHead and neck sarcoma\nEndocrine surgery of the head and neck\nThyroid surgery\nParathyroid surgery\nMicrovascular free flap reconstructive surgery\nSkull base surgery\n\nOtology and neurotology\nStudy of diseases of the outer ear, middle ear and mastoid, and inner ear, and surrounding structures (such as the facial nerve and lateral skull base)\n\nOuter ear diseases\nOtitis externa \u2013\nouter ear or ear canal inflammation\nExostoses or Surfer's ear are bony growths in the outer ear canal\nMiddle ear and mastoid diseases\nOtitis media \u2013 middle ear inflammation\nPerforated eardrum (hole in the eardrum due to infection, trauma, explosion or loud noise)\nMastoiditis\nInner ear diseases\nBPPV \u2013 benign paroxysmal positional vertigo\nLabyrinthitis\/Vestibular neuronitis\nM\u00e9ni\u00e8re's disease\/Endolymphatic hydrops\nPerilymphatic fistula\nAcoustic neuroma, vestibular schwannoma\nFacial nerve disease\nIdiopathic facial palsy (Bell's Palsy)\nFacial nerve tumors\nRamsay Hunt Syndrome\nSymptoms\nHearing loss\nTinnitus (subjective noise in the ear)\nAural fullness (sense of fullness in the ear)\nOtalgia (pain referring to the ear)\nOtorrhea (fluid draining from the ear)\nVertigo\nImbalance\n\nRhinology\nRhinology includes nasal dysfunction and sinus diseases.\n\nNasal obstruction\nInferior turbinate hypertrophy\nNasal septum deviation\nChronic sinusitis with nasal polyps\nSinusitis \u2013 acute, chronic\nEnvironmental allergies\nRhinitis\nPituitary tumor\nEmpty nose syndrome\nSevere or recurrent epistaxis\n\nPediatric otorhinolaryngology\nAdenoidectomy\nCaustic ingestion\nCricotracheal resection\nDecannulation\nLaryngomalacia\nLaryngotracheal reconstruction\nMyringotomy and tubes\nObstructive sleep apnea \u2013 pediatric\nTonsillectomy\n\nLaryngology\nDysphonia\/hoarseness\nLaryngitis\nReinke's edema\nVocal cord nodules and polyps\nSpasmodic dysphonia\nTracheostomy\nCancer of the larynx\nVocology \u2013 science and practice of voice habilitation\n\nFacial plastic and reconstructive surgery\nFacial plastic and reconstructive surgery is a one-year fellowship open to otorhinolaryngologists who wish to begin learning the aesthetic and reconstructive surgical principles of the head, face, and neck pioneered by the specialty of Plastic and Reconstructive Surgery.\n\nRhinoplasty and septoplasty\nFacelift (rhytidectomy)\nBrowlift\nBlepharoplasty\nOtoplasty\nGenioplasty\nInjectable cosmetic treatments\nTrauma to the face\nNasal bone fracture\nMandible fracture\nOrbital fracture\nFrontal sinus fracture\nComplex lacerations and soft tissue damage\nSkin cancer (e.g. Basal Cell Carcinoma)\n\nSleep surgery\nSleep surgery encompasses any surgery that helps alleviate obstructive sleep apnea and can anatomically include any part of the upper airway.\n\nNasal cavity \/ nasopharynx\nSeptoplasty\nAdenoidectomy (especially in pediatrics)\nOral cavity \/ oropharynx\nTonsillectomy (especially in pediatrics)\nUvulopalatopharyngoplasty\nTransoral midline glossectomy\nGenioglossus advancement\nOther\nHyoid suspension\nMaxillomandibular advancement\nHypoglossal nerve stimulator implant (Inspire)\n\nMicrovascular reconstruction repair\nMicrovascular reconstruction repair is a common operation that is done on patients who see an otorhinolaryngologist. It is a surgical procedure that involves moving a composite piece of tissue from the patient's body and to the head and\/or neck. Microvascular head-and-neck reconstruction is used to treat head-and-neck cancers, including those of the larynx and pharynx, oral cavity, salivary glands, jaws, calvarium, sinuses, tongue and skin. The tissue that is most commonly moved during this procedure is from the arms, legs, and back, and can come from the skin, bone, fat, and\/or muscle. When doing this procedure, the decision on which is moved is determined on the reconstructive needs. Transfer of the tissue to the head and neck allows surgeons to rebuild the patient's jaw, optimize tongue function, and reconstruct the throat. When the pieces of tissue are moved, they require their own blood supply for a chance of survival in their new location. After the surgery is completed, the blood vessels that feed the tissue transplant are reconnected to new blood vessels in the neck. These blood vessels are typically no more than 1 to 3 millimeters in diameter, which means that these connections need to be made with a microscope, which is why the procedure is called \"microvascular surgery\".\n\nSee also\nExternal links\n Media related to Otorhinolaryngology at Wikimedia Commons\n\n\n== References ==","140":"An otoscope or auriscope is a medical device used by healthcare professionals to examine the ear canal and eardrum. This may be done as part of routine physical examinations, or for evaluating specific ear complaints, such as earaches, sense of fullness in the ear, or hearing loss.\n\nUsage\nFunction\nAn otoscope enables viewing and examination of the ear canal and tympanic membrane (eardrum). As the eardrum is the border between the external ear canal and the middle ear, its characteristics can indicate various diseases of the middle ear space. Otoscopic examination can help diagnose conditions such as acute otitis media (infection of the middle ear), otitis externa (infection of the outer ear), traumatic perforation of the eardrum, and cholesteatoma.  \nThe presence of cerumen (earwax), shed skin, pus, canal skin edema, foreign bodies, and various ear diseases, can obscure the view of the eardrum and thus compromise the value of otoscopy done with a common otoscope, but can confirm the presence of obstructing symptoms.  \nOtoscopes can also be used to examine patients' noses (avoiding the need for a separate nasal speculum) and upper throats (by removing the speculum).\n\nMethod of use\nThe most common otoscopes consist of a handle and a head. The head contains a light source and a magnifying lens, typically around 8 diopters (3\u00d7 magnification), to help illuminate and enlarge ear structures. The distal (front) end of the otoscope has an attachment for disposable plastic ear specula.   \nThe examiner first pulls on the pinna (usually the earlobe, side or top) to straighten the ear canal, and then inserts the ear speculum side of the otoscope into the outer ear. It is important to brace the index or little finger of the hand holding the otoscope against the patient's head to avoid injuring the ear canal. The examiner then looks through the lens on the rear of the instrument to see inside the ear canal.  \nIn many models, the examiner can remove the lens and insert instruments like specialized suction tips through the otoscope into the ear canal, such as for removing earwax. Most models also have an insertion point for a bulb that pushes air through the speculum (pneumatic otoscopy) for testing eardrum mobility.\n\nTypes\nMany otoscopes for doctors' offices are wall-mounted, with an electrical cord providing power from an electric outlet. Portable otoscopes powered by batteries (usually rechargeable) in the handle are also available.  \nOtoscopes are often sold with ophthalmoscopes as a diagnostic set.\n\nMonocular and binocular\nMost otoscopes used in emergency rooms, pediatric offices, general practice, and by internists are monocular devices. These provide a two-dimensional view of the ear canal and its contents, and usually at least a portion of the eardrum.   \nAnother method of performing otoscopy (visualization of the ear) is by using a binocular (two-eyed) microscope in conjunction with a larger plastic or metal ear speculum, which provides a much larger field of view. The microscope is suspended from a stand, which frees up both of the examiner's hands; the patient is placed in a supine position and their head is tilted, which keeps the head stable and enables better lighting. The binocular view enables depth perception, which makes removal of earwax or other obstructing materials easier and less hazardous. The microscope also has up to 40\u00d7 magnification, allowing more detailed viewing of the entire ear canal, and of the entire eardrum (unless prevented by edema of the canal skin). Subtle changes in the anatomy can also be more easily detected and interpreted.  \nTraditionally, binocular microscopes are only used by otolaryngologists (ear, nose, and throat specialists) and otologists (subspecialty ear doctors). Their widespread adoption in general medicine is hindered by cost and lack of familiarity among pediatric and general medicine professors in physician training programs. Studies have shown that reliance on a monocular otoscope to diagnose ear disease results in a more than 50% chance of misdiagnosis, as compared to binocular microscopic otoscopy.\n\nPneumatic otoscope\nThe pneumatic otoscope is used to examine the eardrum for assessing the health of the middle ear. This is done by assessing the eardrum's contour (normal, retracted, full, or bulging), its color (gray, yellow, pink, amber, white, red, or blue), its translucency (translucent, semi-opaque, opaque), and its mobility (normal, increased, decreased, or absent). The pneumatic otoscope is the standard tool used in diagnosing otitis media (infection of the middle ear).\nThe pneumatic otoscope has a pneumatic (diagnostic) head, which contains a lens, an enclosed light source, and a nipple for attaching a rubber bulb and tubing. By gently squeezing and releasing the bulb in rapid succession, the degree of eardrum mobility in response to positive and negative pressure can be observed. The head is designed so that an airtight chamber is produced when a speculum is attached and fitted snugly into the patient's ear canal. Using a rubber-tipped speculum or adding a small sleeve of rubber tubing at the end of a plastic speculum, can help improve the airtight seal and also help avoid injuring the patient.\nBy replacing the pneumatic head with a surgical head, the pneumatic otoscope can also be used to clear earwax from the ear canal, and to perform diagnostic tympanocentesis (drainage of fluid from the middle ear) or myringotomy (creation of incision in the eardrum). The surgical head consists of an unenclosed light source and a lens that can swivel over a wide arc.\n\nSee also\nHead mirror \u2013 Diagnostic device\nIntraoral camera \u2013 device used to take photos of the inside of a patient's mouthPages displaying wikidata descriptions as a fallback\n\nReferences\nExternal links\n\nPhisick \u2013 Pictures and information about antique otoscopes","141":"The outer ear, external ear, or auris externa is the external part of the ear, which consists of the auricle (also pinna) and the ear canal. It gathers sound energy and focuses it on the eardrum (tympanic membrane).\n\nStructure\nAuricle\nThe visible part is called the auricle, also known as the pinna, especially in other animals. It is composed of a thin plate of yellow elastic cartilage, covered with integument, and connected to the surrounding parts by ligaments and muscles; and to the commencement of the ear canal by fibrous tissue. Many mammals can move the pinna (with the auriculares muscles) in order to focus their hearing in a certain direction in much the same way that they can turn their eyes. Most humans do not have this ability.\n\nEar canal\nFrom the pinna, the sound waves move into the ear canal (also known as the external acoustic meatus) a simple tube running through to the middle ear. This tube leads inward from the bottom of the auricula and conducts the vibrations to the tympanic cavity and amplifies frequencies in the range 3 kHz to 12 kHz.\n\nAuricular muscles\nIntrinsic muscles\nThe intrinsic auricular muscles are:\n\nThe helicis major is a narrow vertical band situated upon the anterior margin of the helix. It arises below, from the spina helicis, and is inserted into the anterior border of the helix, just where it is about to curve backward.\nThe helicis minor is an oblique fasciculus, covering the crus helicis.\nThe tragicus is a short, flattened vertical band on the lateral surface of the tragus. Also known as the mini lobe.\nThe antitragicus arises from the outer part of the antitragus, and is inserted into the cauda helicis and antihelix.\nThe transverse muscle is placed on the cranial surface of the pinna. It consists of scattered fibers, partly tendinous and partly muscular, extending from the eminentia conchae to the prominence corresponding with the scapha.\nThe oblique muscle also on the cranial surface, consists of a few fibers extending from the upper and back part of the concha to the convexity immediately above it.\nThe intrinsic muscles contribute to the topography of the auricle, while also function as a sphincter of the external auditory meatus. It has been suggested that during prenatal development in the womb, these muscles exert forces on the cartilage which in turn affects the shaping of the ear.\n\nExtrinsic muscles\nThe extrinsic auricular muscles are the three muscles surrounding the auricula or outer ear:\n\nanterior auricular muscle\nsuperior auricular muscle\nposterior auricular muscle\nThe superior muscle is the largest of the three, followed by the posterior and the anterior.\nIn some mammals these muscles can adjust the direction of the pinna. In humans these muscles possess very little action.\nThe auricularis anterior draws the auricula forward and upward, the auricularis superior slightly raises it, and the auricularis posterior draws it backward. The superior auricular muscle also acts as a stabilizer of the occipitofrontalis muscle and as a weak brow lifter. The presence of auriculomotor activity in the posterior auricular muscle causes the muscle to contract and cause the pinna to be pulled backwards and flatten when exposed to sudden, surprising sounds.\n\nFunction\nOne consequence of the configuration of the outer ear is selectively to boost the sound pressure 30- to 100-fold for frequencies around 3 kHz. This amplification makes humans most sensitive to frequencies in this range\u2014and also explains why they are particularly prone to acoustical injury and hearing loss near this frequency. Most human speech sounds are also distributed in the bandwidth around 3 kHz.\n\nClinical significance\nMalformations of the external ear can be a consequence of hereditary disease, or exposure to environmental factors such as radiation, infection. Such defects include:\n\nA preauricular fistula, which is a long narrow tube, usually near the tragus. This can be inherited as an autosomal recessive fashion and may suffer from chronic infection in later life.\nCosmetic defects, such as very large ears, small ears.\nMalformation that may lead to functional impairment, such as atresia of the external auditory meatus or aplasia of the pinna,\nGenetic syndromes, which include:\nKonigsmark syndrome, characterised by small ears and atresia of the external auditory canal, causing conductive hearing loss and inherited in an autosomal recessive manner.\nGoldenhar syndrome, a combination of developmental abnormalities affecting the ears, eyes, bones of the skull, and vertebrae, inherited in an autosomal dominant manner.\nTreacher Collins syndrome, characterised by dysplasia of the auricle, atresia of the bony part of the auditory canal, hypoplasia of the auditory ossicles and tympanic cavity, and 'mixed' deafness (both sensorineural and conductive), inherited in an autosomal dominant manner.\nCrouzon syndrome, characterised by bilateral atresia of the external auditory canal, inherited in an autosomal dominant manner.\n\nSurgery\nUsually, malformations are treated with surgery, although artificial prostheses are also sometimes used.\n\nPreauricular fistulas are generally not treated unless chronically inflamed.\nCosmetic defects without functional impairment are generally repaired after ages 6\u20137.\nIf malformations are accompanied by hearing loss amenable to correction, then the early use of hearing aids may prevent complete hearing loss.\n\nAdditional images\nReferences\nThis article incorporates text in the public domain from page 1033 of the 20th edition of Gray's Anatomy (1918)\n\nExternal links\n Media related to Outer ear at Wikimedia Commons","142":"Otosclerosis is a condition of the middle ear where portions of the dense enchondral layer of the bony labyrinth remodel into one or more lesions of irregularly-laid spongy bone. As the lesions reach the stapes the bone is resorbed, then hardened (sclerotized), which limits its movement and results in hearing loss, tinnitus, vertigo or a combination of these. The term otosclerosis is something of a misnomer: much of the clinical course is characterized by lucent rather than sclerotic bony changes, so the disease is also known as otospongiosis.\n\nEtymology\nThe word otosclerosis derives from Greek \u1f60\u03c4\u03cc\u03c2 (\u014dtos), genitive of \u03bf\u1f56\u03c2 (o\u00fbs) \"ear\" + \u03c3\u03ba\u03bb\u03ae\u03c1\u03c9\u03c3\u03b9\u03c2 (skl\u0113r\u014dsis), \"hardening\".\n\nPresentation\nThe primary form of hearing loss in otosclerosis is conductive hearing loss (CHL) whereby sounds reach the ear drum but are incompletely transferred via the ossicular chain in the middle ear, and thus partly fail to reach the inner ear (cochlea). This can affect one ear or both ears. On audiometry, the hearing loss is characteristically low-frequency, with higher frequencies being affected later.\nSensorineural hearing loss (SNHL) has also been noted in patients with otosclerosis; this is usually a high-frequency loss, and usually manifests late in the disease. The causal link between otosclerosis and SNHL remains controversial. Over the past century, leading otologists and neurotologic researchers have argued whether the finding of SNHL late in the course of otosclerosis is due to otosclerosis or simply to typical presbycusis.\nMost patients with otosclerosis notice tinnitus (head noise) to some degree. The amount of tinnitus is not necessarily related to the degree or type of hearing impairment.  Tinnitus develops due to irritation of the delicate nerve endings in the inner ear. Since the nerve carries sound, this irritation is manifested as ringing, roaring or buzzing. It is usually worse when the patient is fatigued, nervous or in a quiet environment.\n\nCauses\nOtosclerosis can be caused by both genetic and environmental factors, such as a viral infection (like measles).  Ribonucleic acid of the measles virus has been found in stapes footplate in most patients with otosclerosis. Populations that have been vaccinated against measles had a significant reduction in otosclerosis.  While the disease is considered to be hereditary, its penetrance and the degree of expression is so highly variable that it may be difficult to detect an inheritance pattern.  Most of the implicated genes are transmitted in an autosomal dominant fashion. One genome-wide analysis associates otosclerosis with variation in the RELN gene.\nLoci include:\n\nPathophysiology\nThe pathophysiology of otosclerosis is complex.  The key lesions of otosclerosis are multifocal areas of sclerosis within the endochondral temporal bone.  These lesions share some characteristics with Paget's Disease, but they are not thought to be otherwise related.  Histopathological studies have all been done on cadaveric temporal bones, so only inferences can be made about progression of the disease histologically.  It seems that the lesions go through an active \"spongiotic\" or hypervascular phase before developing into \"sclerotic\" phase lesions.  There have been many genes and proteins identified that, when mutated, may lead to these lesions.  Also there is mounting evidence that measles virus is present within the otosclerotic foci, implicating an infectious etiology (this has also been noted in Paget's Disease).\nConductive hearing loss (CHL) in otosclerosis is caused by two main sites of involvement of the sclerotic (or scar-like) lesions.  The best understood mechanism is fixation of the stapes footplate to the oval window of the cochlea.  This greatly impairs movement of the stapes and therefore transmission of sound into the inner ear (\"ossicular coupling\").  Additionally the cochlea's round window can also become sclerotic, and in a similar way impair movement of sound pressure waves through the inner ear (\"acoustic coupling\").\nCHL is usually concomitant with impingement of abnormal bone on the stapes footplate. This involvement of the oval window forms the basis of the name fenestral otosclerosis. The most common location of involvement of otosclerosis is the bone just anterior to the oval window at a small cleft known as the fissula ante fenestram. The fissula is a thin fold of connective tissue extending through the endochondral layer, approximately between the oval window and the cochleariform process, where the tensor tympani tendon turns laterally toward the malleus.\nThe mechanism of sensorineural hearing loss in otosclerosis is less well understood. It may result\nfrom direct injury to the cochlea and spiral ligament from the lytic process or from release of proteolytic enzymes into the cochlea. There are certainly a few well documented instances of sclerotic lesions directly obliterating sensory structures within the cochlea and spiral ligament, which have been photographed and reported post-mortem.  Other supporting data includes a consistent loss of cochlear hair cells in patients with otosclerosis; these cells being the chief sensory organs of sound reception.  A suggested mechanism for this is the release of hydrolytic enzymes into the inner ear structures by the spongiotic lesions.\n\nDiagnosis\nOtosclerosis is traditionally diagnosed by characteristic clinical findings, which include progressive conductive\nhearing loss, a normal tympanic membrane, and no evidence of middle ear inflammation. The cochlear promontory may have a faint pink tinge reflecting the vascularity of the lesion, referred to as the Schwartz sign.\nApproximately 0.5% of the population will eventually be diagnosed with otosclerosis. Post-mortem studies show that as many as 10% of people may have otosclerotic lesions of their temporal bone, but apparently never had symptoms warranting a diagnosis. Caucasians are the most affected race, with the prevalence in the Black and Asian populations being much lower. In clinical practice otosclerosis is encountered about twice as frequently in females as in males, but this does not reflect the true sex ratio. When families are investigated it is found that the condition is only slightly more common in women. Usually noticeable hearing loss begins at middle-age, but can start much sooner. The hearing loss was long believed to grow worse during pregnancy, but recent research does not support this belief.\n\nDifferential testing\nAudiometry\nFixation of the stapes within the oval window causes a conductive hearing loss. In pure-tone audiometry, this manifests as air-bone gaps on the audiogram (i.e. a difference of more than 10 dB between the air-conduction and bone-conduction thresholds at a given test frequency). However, medial fixation of the ossicular chain impairs both the inertial and osseotympanic modes of bone conduction, increasing the bone-conduction thresholds between 500 Hz and 4 kHz, and reducing the size of air-bone gaps. As 2 kHz is the resonant frequency of the ossicular chain, the largest increase in bone-conduction threshold (around 15 dB) occurs at this frequency \u2013 the resultant notch is called Carhart's notch and is a useful clinical marker for medial ossicular-chain fixation.\nTympanometry measures the peak pressure (TPP) and peak-compensated static admittance (Ytm) of the middle ear at the eardrum. As the stapes is ankylosed in otosclerosis, the lateral end of the ossicular chain may still be quite mobile. Therefore, otosclerosis may only slightly reduce the admittance, resulting in either a shallow tympanogram (type AS), or a normal tympanogram (type A). Otosclerosis increases the stiffness of the middle-ear system, raising its resonant frequency. This can be quantified using multi-frequency tympanometry. Thus, a high resonant-frequency pathology such as otosclerosis can be differentiated from low resonant-frequency pathologies such as ossicular discontinuity.\nIn the absence of a pathology, a loud sound (generally greater than 70 dB above threshold) causes the stapedius muscle to contract, reducing the admittance of the middle ear and softening the perceived loudness of the sound. If the mobility of the stapes is reduced due to otosclerosis, then stapedius muscle contraction does not significantly decrease the admittance. When acoustic reflex testing is conducted, the acoustic reflex thresholds (ART) cannot be determined when attempting to measure on the affected side. Also, a conductive pathology will attenuate the test stimuli, resulting in either elevated reflex thresholds or absent reflexes when the stimulus is presented in the affected ear and measured in the other ear.\n\nCT imaging\nImaging is usually not pursued in those with uncomplicated conductive hearing loss and characteristic clinical findings. Those with only conductive hearing loss are often treated medically or with surgery without imaging. The diagnosis may be unclear clinically in cases of sensorineural or mixed hearing loss and may become apparent only on imaging. Therefore, imaging is often performed when the hearing loss is sensorineural or mixed.\nA high-resolution CT shows very subtle bone findings. However, CT is usually not needed prior to surgery.\nOtosclerosis on CT can be graded using the grading system suggested by Symons and Fanning.\n\nGrade 1, solely fenestral;\nGrade 2, patchy localized cochlear disease (with or without fenestral involvement) to either the basal cochlear turn (grade 2A), or the middle\/apical turns (grade 2B), or both the basal turn and the middle\/apical turns (grade 2C); and\nGrade 3, diffuse confluent cochlear involvement (with or without fenestral involvement).\n\nTreatment\nSeveral approaches have been used in the treatment of otosclerosis, including medical, surgical and amplification.  Technological innovations in hearing aid technology and cochlear implants are now being used to substitute or complement other interventions.\n\nMedical\nEarlier workers suggested the use of calcium fluoride; now sodium fluoride is the preferred compound. Fluoride ions inhibit the rapid progression of disease. In the otosclerotic ear, there occurs formation of hydroxylapatite crystals which lead to stapes (or other) fixation. The administration of fluoride replaces the hydroxyl radical with fluoride leading to the formation of fluorapatite crystals. Hence, the progression of disease is considerably slowed down and active disease process is arrested.\nThis treatment cannot reverse conductive hearing loss, but may slow the progression of both the conductive and sensorineural components of the disease process. Otofluor, containing sodium fluoride, is one treatment. Recently, some success has been claimed with a second such treatment, bisphosphonate medications that inhibit bone destruction. However, these early reports are based on non-randomized case studies that do not meet standards of clinical trials. There are numerous side-effects to both pharmaceutical treatments, including occasional stomach upset, allergic itching, and increased joint pains which can lead to arthritis. In the worst case, bisphosphonates may lead to osteonecrosis of the auditory canal itself. Finally, neither approach has been proven to be beneficial after the commonly preferred method of surgery has been undertaken.\n\nSurgical\nThere are various methods to treat otosclerosis. However the method of choice is a procedure known as stapedectomy. Early attempts at hearing restoration via the simple freeing of the stapes from its sclerotic attachments to the oval window were met with temporary improvement in hearing, but the conductive hearing loss would almost always recur. A stapedectomy consists of removing a portion of the sclerotic stapes footplate and replacing it with a middle ear implant that is secured to the incus.  This procedure restores continuity of ossicular movement and allows transmission of sound waves from the eardrum to the inner ear. A modern variant of this surgery called a stapedotomy, is performed by drilling a small hole in the stapes footplate with a micro-drill or a laser, and the insertion of a piston-like prothesis. The success rate of either surgery depends greatly on the skill and the familiarity with the procedure of the surgeon. However, comparisons have shown stapedotomy to yield results at least as good as stapedectomy, with fewer complications, and thus stapedotomy is preferred in normal circumstances. Recently, Endoscopic stapedotomy has been gaining popularity since its first description by Professor Tarabichi in 1999. The endsocope provides much better view of the stapes footplate without removal of bone to access that structure.\n\nAmplification\nAlthough hearing aids cannot prevent, cure or inhibit the progression of otosclerosis, they can help treat the largest symptom, hearing loss.  Hearing aids can be tuned to specific frequency losses.  However, due to the progressive nature of this condition, use of a hearing aid is palliative at best.  Without eventual surgery, deafness is likely to result.\n\nSociety and culture\nNotable cases\nGerman composer Ludwig van Beethoven was theorized to suffer from otosclerosis, although this is controversial.\nVictorian journalist Harriet Martineau gradually lost her hearing during her young life, and later medical historians have diagnosed her with probably suffering from otosclerosis as well.\nTheresa Goell, pioneering American archaeologist, lost her hearing due to otosclerosis and relied on lip reading. She had a successful career excavating Mount Nemrut in Turkey, despite the challenges of being a deaf woman in a heavily male-dominated field.\nMargaret Sullavan, American stage and film actress, suffered from the congenital hearing defect otosclerosis that worsened as she aged, making her more and more hard of hearing.\nHoward Hughes the pioneering American aviator, engineer, industrialist, and film producer also suffered from otosclerosis.\nFrankie Valli, lead singer of The Four Seasons, suffered from it in the late 1960s and early 1970s, forcing him to \"sing from memory\" in the latter part of the 70s (surgery restored most of his hearing by 1980).\nPittsburgh Penguins forward Steve Downie suffers from otosclerosis.\nThe British queen Alexandra of Denmark suffered from it, leading to her social isolation; Queen Alexandra's biographer, Georgina Battiscombe, was able to have \"some understanding of Alexandra's predicament\" because she too had otosclerosis.\nAdam Savage, co-host of MythBusters, uses a hearing aid due to otosclerosis.\nSir John Cornforth, Australian-British Nobel Prize in Chemistry laureate\nZak Abel, English singer-songwriter and musician\n\nReferences\nExternal links\nNIH\/Medline","143":"PubMed Central (PMC) is a free digital repository that archives open access full-text scholarly articles that have been published in biomedical and life sciences journals. As one of the major research databases developed by the National Center for Biotechnology Information (NCBI), PubMed Central is more than a document repository. Submissions to PMC are indexed and formatted for enhanced metadata, medical ontology, and unique identifiers which enrich the XML structured data for each article. Content within PMC can be linked to other NCBI databases and accessed via Entrez search and retrieval systems, further enhancing the public's ability to discover, read and build upon its biomedical knowledge.\nPubMed Central is distinct from PubMed. PubMed Central is a free digital archive of full articles, accessible to anyone from anywhere via a web browser (with varying provisions for reuse). Conversely, although PubMed is a searchable database of biomedical citations and abstracts, the full-text article resides elsewhere (in print or online, free or behind a subscriber paywall).\nAs of December 2018, the PMC archive contained over 5.2 million articles, with contributions coming from publishers or authors depositing their manuscripts into the repository per the NIH Public Access Policy. Earlier data shows that from January 2013 to January 2014 author-initiated deposits exceeded 103,000 papers during a 12-month period. PMC identifies about 4,000 journals which participate in some capacity to deposit their published content into the PMC repository. Some publishers delay the release of their articles on PubMed Central for a set time after publication, referred to as an \"embargo period\", ranging from a few months to a few years depending on the journal.  (Embargoes of six to twelve months are the most common.) PubMed Central is a key example of \"systematic external distribution by a third party\", which is still prohibited by the contributor agreements of many publishers.\n\nHistory\nPubMed Central began as E-biomed, initially proposed in May 1999 by then-NIH director Harold Varmus. The idea came to him \"abruptly\" in December 1998, inspired by the early use of arXiv for preprints after a presentation from Pat Brown of Stanford and David Lipman, director of NCBI:\n\nBut my views broadened abruptly one morning in December of 1998 when I met Pat Brown for coffee, at the caf\u00e9 that was formerly the famed Tassajara Bakery, on the corner of Cole and Parnassus, during a visit to San Francisco. [...] A few weeks before our coffee, Pat had learned about the methods being used by the physicist Paul Ginsparg and his colleagues at Los Alamos to allow physicists and mathematicians to share their work with one another over the Internet. They were posting \"preprints\" (articles not yet submitted or accepted for publication) at a publicly accessible website (called LanX or arXiv) for anyone to read and critique. [...]\nThe more I thought about this, the more I was convinced that a radical restructuring of methods for publishing, transmitting, storing, and using biomedical research reports might be possible and beneficial. In a spirit of enthusiasm and political innocence, I wrote a lengthy manifesto, proposing the creation of an NIH-supported online system, called E-biomed.\nThe goal of E-biomed was to provide free access to all biomedical research. Papers submitted to E-biomed could take one of two routes: either immediately published as a preprint, or through a traditional peer review process. The peer review process was to resemble contemporary overlay journals, with an external editorial board retaining control over the process of reviewing, curating, and listing papers which would otherwise be freely accessible on the central E-biomed server. Varmus intended to realize the new possibilities presented by communicating scientific results digitally, imagining continuous conversation about published work, versioned documents, and enriched \"layered\" formats allowing for multiple levels of detail. \nThe proposal to create a central index of biomedical research was a radical departure from prevailing publishing norms. Prior to the internet, publication indexes operated largely like ISBNs: allocated by registration agencies to secondary publishers. The idea that anyone could own their own address space via a domain name and create their own indexing system was a wholly new idea. Major commercial publishers had begun experimenting with an indexing system for scientific papers shared across publishers as early as 1993, and were spurred to action following the E-biomed proposal. At the October 1999 STM Annual Frankfurt Conference, several publishers led by Springer-Verlag reached a hurried conference room consensus to launch their competitor prototype:\n\nAt the Board meeting of the STM association, held the afternoon of Monday, October 11, before the fair's Wednesday opening, discussion focused on an emerging U.S. National Library of Medicine (NLM) initiative called E-Biomed (later PubMed Central) that had been proposed by Harold Varmus of the National Institutes of Health in the spring of 1999. Varmus envisioned a digital archive of journals, accessible free of charge and with the added value of reference linking. \"Our consensus was that publishers should be the ones doing the linking,\" said Bob Campbell, who chaired the meeting. \"Since we were 'higher up the stream,' so to speak, we should be able to link our articles ahead of the NLM as part of the process of producing them. Stefan von Holtzbrinck then set the ball rolling by offering to link Nature publications with anyone else's. We decided to issue an announcement of a broad STM reference linking initiative. It was, of course, a strategic move only, since we had neither plan nor prototype.\"\nA small group led by Arnoud de Kemp of Springer-Verlag met in an adjacent room immediately following the Board meeting to draft the announcement, which was distributed to all attendees of the STM annual meeting the following day and published in an STM membership publication. [...] The potential benefit of the service that would become CrossRef was immediately apparent. Organizations such as AIP and IOP (Institute of Physics) had begun to link to each other's publications, and the impossibility of replicating such one-off arrangements across the industry was obvious. As Tim Ingoldsby later put it, \"All those linking agreements were going to kill us.\"\n\nUnder pressure from vigorous lobbying from commercial publishers and scientific societies who feared for lost profits, NIH officials announced a revised PubMed Central proposal in August 1999. PMC would receive submissions from publishers, rather than from authors as in E-biomed. Publications were allowed time-embargoed paywalls up to one year. PMC would only allow peer-reviewed work \u2014 no preprints. The then-unnamed publisher-led linking system shortly thereafter became CrossRef and the larger DOI system. Varmus, Brown, and others including Michael Eisen went on to found the Public Library of Science (PLoS) in 2001, reaching the conclusion \"that if we really want to change the publication of scientific research, we must do the publishing ourselves.\"\n\nAdoption\nLaunched in February 2000, the repository has grown rapidly as the NIH Public Access Policy is designed to make all research funded by the National Institutes of Health (NIH) freely accessible to anyone, and, in addition, many publishers are working cooperatively with the NIH to provide free access to their works. In late 2007, the Consolidated Appropriations Act of 2008 (H.R. 2764) was signed into law and included a provision requiring the NIH to modify its policies and require inclusion into PubMed Central complete electronic copies of their peer-reviewed research and findings from NIH-funded research. These articles are required to be included within 12 months of publication. This is the first time the US government has required an agency to provide open access to research and is an evolution from the 2005 policy, in which the NIH asked researchers to voluntarily add their research to PubMed Central.\nA UK version of the PubMed Central system, UK PubMed Central (UKPMC), has been developed by the Wellcome Trust and the British Library as part of a nine-strong group of UK research funders. This system went live in January 2007. On 1 November 2012, it became Europe PubMed Central. The Canadian member of the PubMed Central International network, PubMed Central Canada, was launched in October 2009.\nThe National Library of Medicine \"NLM Journal Publishing Tag Set\" journal article markup language is freely available. The Association of Learned and Professional Society Publishers comments that \"it is likely to become the standard for preparing scholarly content for both books and journals\". A related DTD is available for books. The Library of Congress and the British Library have announced support for the NLM DTD. It has also been popular with journal service providers.\nWith the release of public access plans for many agencies beyond NIH, PMC is in the process of becoming the repository for a wider variety of articles. This includes NASA content, with the interface branded as \"PubSpace\".\n\nTechnology\nArticles are sent to PubMed Central by publishers in XML or SGML, using a variety of article DTDs. Older and larger publishers may have their own established in-house DTDs, but many publishers use the NLM Journal Publishing DTD (see above).\nReceived articles are converted via XSLT to the very similar NLM Archiving and Interchange DTD. This process may reveal errors that are reported back to the publisher for correction. Graphics are also converted to standard formats and sizes. The original and converted forms are archived. The converted form is moved into a relational database, along with associated files for graphics, multimedia, or other associated data. Many publishers also provide PDF of their articles, and these are made available without change.\nBibliographic citations are parsed and automatically linked to the relevant abstracts in PubMed, articles in PubMed Central, and resources on publishers' Web sites. PubMed links also lead to PubMed Central. Unresolvable references, such as to journals or particular articles not yet available at one of these sources, are tracked in the database and automatically come \"live\" when the resources become available.\nAn in-house indexing system provides search capability, and is aware of biological and medical terminology, such as generic vs. proprietary drug names, and alternate names for organisms, diseases and anatomical parts.\nWhen a user accesses a journal issue, a table of contents is automatically generated by retrieving all articles, letters, editorials, etc. for that issue. When an actual item such as an article is reached, PubMed Central converts the NLM markup to HTML for delivery, and provides links to related data objects. This is feasible because the variety of incoming data has first been converted to standard DTDs and graphic formats.\nIn a separate submission stream, NIH-funded authors may deposit articles into PubMed Central using the NIH Manuscript Submission (NIHMS). Articles thus submitted typically go through XML markup in order to be converted to NLM DTD.\n\nReception\nReactions to PubMed Central among the scholarly publishing community range between a genuine enthusiasm by some, to cautious concern by others.\nWhile PMC is a welcome partner to open access publishers in its ability to augment the discovery and dissemination of biomedical knowledge, that same truth causes others to worry about traffic being diverted from the published version of record, the economic consequences of less readership, as well as the effect on maintaining a community of scholars within learned societies. A 2013 analysis found strong evidence that public repositories of published articles were responsible for \"drawing significant numbers of readers away from journal websites\" and that \"the effect of PMC is growing over time\".\nLibraries, universities, open access supporters, consumer health advocacy groups, and patient rights organizations have applauded PubMed Central, and hope to see similar public access repositories developed by other federal funding agencies so to freely share any research publications that were the result of taxpayer support.\nThe Antelman study of open access publishing found that in philosophy, political science, electrical and electronic engineering and mathematics, open access papers had a greater research impact. A randomised trial found an increase in content downloads of open access papers, with no citation advantage over subscription access one year after publication.\nThe NIH policy and open access repository work has inspired a 2013 presidential directive which has sparked action in other federal agencies as well.\nIn March 2020, PubMed Central accelerated its deposit procedures for the full text of publications on coronavirus. The NLM did so upon request from the White House Office of Science and Technology Policy and international scientists to improve access for scientists, healthcare providers, data mining innovators, AI healthcare researchers, and the general public.\n\nPMCID\nThe PMCID (PubMed Central identifier), also known as the PMC reference number, is a bibliographic identifier for the PubMed Central open access database, much like the PMID is the bibliographic identifier for the PubMed database. The two identifiers are distinct however. It consists of \"PMC\" followed by a string of numbers. The format is:\n\nPMCID: PMC1852221\nAuthors applying for NIH awards must include the PMCID in their application.\n\nSee also\nEurope PubMed Central\nJATS (technology)\nMEDLINE, an international literature database of life sciences and biomedical information\nPMID (PubMed Identifier)\nPubMed Central Canada\nRedalyc (similar project focused on Latin America)\nSciELO (similar service)\n\nReferences\nExternal links\n\nOfficial website","144":"Parkinsonism is a clinical syndrome characterized by tremor, bradykinesia (slowed movements), rigidity, and postural instability. Both hypokinetic (bradykinesia and akinesia) as well as hyperkinetic (cogwheel rigidity and tremors at rest) features are displayed by Parkinsonism. These are the four motor symptoms found in Parkinson's disease (PD) \u2013 after which it is named \u2013 dementia with Lewy bodies (DLB), Parkinson's disease dementia (PDD), and many other conditions. This set of symptoms occurs in a wide range of conditions and may have many causes, including neurodegenerative conditions, drugs, toxins, metabolic diseases, and neurological conditions other than PD.\n\nSigns and symptoms\nParkinsonism is a clinical syndrome characterized by the four motor symptoms found in Parkinson's disease: tremor, bradykinesia (slowed movements), rigidity, and postural instability.\nParkinsonism gait problems can lead to falls and serious physical injuries. Other common symptoms include:\n\nTremors, as rest tremor (when resting, mostly in the hands) and\/or postular tremor\nShort, shuffling gait\nSlow movements (bradykinesia)\nLoss of sound perception leading to soft speech, hypophonia\nDifficulty sleeping\nDry skin\nApathy\nLack of facial expressions\nBalance problems\nFrequent falls\nVery small handwriting\nRigid, stiff muscles\nCogwheeling (jerky feeling in arm or leg)\nUpgaze impairment\nPlastic, dead feeling resistance known as \"lead-pipe rigidity\".\n\nConditions\nParkinsonism occurs in many conditions.\n\nNeurological\nNeurodegenerative conditions and Parkinson-plus syndromes that can cause parkinsonism include:\n\nCorticobasal degeneration\nDementia with Lewy bodies\nThe relationship (if any) with essential tremor is not clear.\nFrontotemporal dementia (Pick's disease)\nGerstmann\u2013Str\u00e4ussler\u2013Scheinker syndrome\nHuntington's disease\nLytico-bodig disease (ALS complex of Guam)\nMultiple system atrophy (Shy\u2013Drager syndrome)\nNeuroacanthocytosis\nNeuronal ceroid lipofuscinosis\nOlivopontocerebellar atrophy\nPantothenate kinase-associated neurodegeneration, also known as neurodegeneration with brain iron accumulation\nParkin mutation causing hereditary juvenile dystonia\nParkinson's disease\nParkinson's disease dementia\nProgressive supranuclear palsy\nWilson's disease\nX-linked dystonia parkinsonism (Lubag syndrome)\n\nInfectious\nCreutzfeldt\u2013Jakob disease\nEncephalitis lethargica\nHIV infection and AIDS\n\nToxins\nEvidence exists to show a link between exposure to pesticides and herbicides and PD; a two-fold increase in risk was seen with paraquat or maneb\/mancozeb exposure.\nChronic manganese (Mn) exposure has been shown to produce a parkinsonism-like illness characterized by movement abnormalities. This condition is not responsive to typical therapies used in the treatment of PD, suggesting an alternative pathway than the typical dopaminergic loss within the substantia nigra. Manganese may accumulate in the basal ganglia, leading to the abnormal movements that characterize parkinsonism. A mutation of the SLC30A10 gene, a manganese efflux transporter necessary for decreasing intracellular Mn, has been linked with the development of this parkinsonism-like disease. The Lewy bodies typical to PD are not seen in Mn-induced parkinsonism.\nAgent Orange may be a cause of parkinsonism, although evidence is inconclusive and further research is needed.\nOther toxins that have been associated with parkinsonism are:\n\nAnnonaceae\nCarbon monoxide\nCarbon disulfide\nCyanide\nEthanol\nHexane\nMercury\nMethanol\nMPTP\nRotenone\nToluene (inhalant abuse: \"huffing\")\n\nVascular\nBinswanger's disease (subcortical leukoencephalopathy)\nVascular dementia (multi-infarct)\nDural arteriovenous fistula \/ dAVF (reversible parkinsonism through fistula treatment) \nDural arteriovenous malformation \/ dAVM (reversible through dAVM treatment)\n\nOther\nChronic traumatic encephalopathy (boxer's dementia or pugilistic encephalopathy)\nDamage to the brain stem (especially dopaminergic nuclei of the substantia nigra),basal ganglia (especially globus pallidus) and the thalamus.\nHypothyroidism\nOrthostatic tremor\nParaneoplastic syndrome: neurological symptoms caused by antibodies associated with cancers\nRapid onset dystonia parkinsonism\nAutosomal recessive juvenile parkinsonism\n\nDifferential diagnosis\nSecondary parkinsonism, including vascular parkinsonism and drug-induced parkinsonism.\n\nDrug-induced parkinsonism\nAbout 7% of people with parkinsonism developed symptoms as a result of side effects of medication, mainly neuroleptic antipsychotics especially the phenothiazines (such as perphenazine and chlorpromazine), thioxanthenes (such as flupentixol and zuclopenthixol) and butyrophenones (such as haloperidol), and rarely, antidepressants. Yet another drug that can induce parkinsonism is the antihistaminic medication cinnarizine, usually prescribed for motion sickness; this is because besides antagonizing histamine receptors this drug antagonizes the dopamine D2 receptors. The incidence of drug-induced parkinsonism increases with age. Drug-induced parkinsonism tends to remain at its presenting level and does not worsen like Parkinson's disease.\nImplicated medications include:\n\nAntipsychotics\nLithium\nMetoclopramide\nMDMA (addiction and frequent use)\nTetrabenazine\nCinnarizine\n\nSociety and culture\nIn the United States, the 2021 National Defense Authorization Act (NDAA) added parkinsonism to the list of presumptive conditions associated with Agent Orange exposure, enabling affected service members to receive Veterans Affairs disability benefits.\n\nReferences\nExternal links\nGeneReviews\/NIH\/NCBI\/UW entry on Perry syndrome\nGeneReviews\/NCBI\/NIH\/UW entry on X-Linked Dystonia-Parkinsonism","145":"PubMed is a free database including primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintains the database as part of the Entrez system of information retrieval.\nFrom 1971 to 1997, online access to the MEDLINE database had been primarily through institutional facilities, such as university libraries. PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching. The PubMed system was offered free to the public starting in June 1997.\n\nContent\nIn addition to MEDLINE, PubMed provides access to:\n\nolder references from the print version of Index Medicus, back to 1951 and earlier\nreferences to some journals before they were indexed in Index Medicus and MEDLINE, for instance Science, BMJ, and Annals of Surgery\nvery recent entries to records for an article before it is indexed with Medical Subject Headings (MeSH) and added to MEDLINE\na collection of books available full-text and other subsets of NLM records\nPMC citations\nNCBI Bookshelf\nMany PubMed records contain links to full text articles, some of which are freely available, often in PubMed Central and local mirrors, such as Europe PubMed Central.\nInformation about the journals indexed in MEDLINE, and available through PubMed, is found in the NLM Catalog.\nAs of 23 May 2023, PubMed has more than 35 million citations and abstracts dating back to 1966, selectively to the year 1865, and very selectively to 1809. As of the same date, 24.6 million of PubMed's records are listed with their abstracts, and 26.8 million records have links to full-text versions (of which 10.9 million articles are available, full-text for free). Over the last 10 years (ending 31 December 2019), an average of nearly one million new records were added each year. \nIn 2016, NLM changed the indexing system so that publishers are able to directly correct typos and errors in PubMed indexed articles.\nPubMed has been reported to include some articles published in predatory journals. MEDLINE and PubMed policies for the selection of journals for database inclusion are slightly different. Weaknesses in the criteria and procedures for indexing journals in PubMed Central may allow publications from predatory journals to leak into PubMed.\n\nCharacteristics\nWebsite design\nA new PubMed interface was launched in October 2009 and encouraged the use of such quick, Google-like search formulations; they have also been described as 'telegram' searches. By default the results are sorted by Most Recent, but this can be changed to Best Match, Publication Date, First Author, Last Author, Journal, or Title.\nThe PubMed website design and domain was updated in January 2020 and became default on 15 May 2020, with the updated and new features. There was a critical reaction from many researchers who frequently use the site.\n\nPubMed for handhelds\/mobiles\nPubMed\/MEDLINE can be accessed via handheld devices, using for instance the \"PICO\" option (for focused clinical questions) created by the NLM. A \"PubMed Mobile\" option, providing access to a mobile friendly, simplified PubMed version, is also available.\n\nSearch\nStandard search\nSimple searches on PubMed can be carried out by entering key aspects of a subject into PubMed's search window.\nPubMed translates this initial search formulation and automatically adds field names, relevant MeSH (Medical Subject Headings) terms, synonyms, Boolean operators, and 'nests' the resulting terms appropriately, enhancing the search formulation significantly, in particular by routinely combining (using the OR operator) textwords and MeSH terms.\nThe examples given in a PubMed tutorial demonstrate how this automatic process works:\n\nCauses Sleep Walking is translated as (\"etiology\"[Subheading] OR \"etiology\"[All Fields] OR \"causes\"[All Fields] OR \"causality\"[MeSH Terms] OR \"causality\"[All Fields]) AND (\"somnambulism\"[MeSH Terms] OR \"somnambulism\"[All Fields] OR (\"sleep\"[All Fields] AND \"walking\"[All Fields]) OR \"sleep walking\"[All Fields])\nLikewise,\n\nsoft\n Attack Aspirin Prevention is translated as (\"myocardial infarction\"[MeSH Terms] OR (\"myocardial\"[All Fields] AND \"infarction\"[All Fields]) OR \"myocardial infarction\"[All Fields] OR (\"heart\"[All Fields] AND \"attack\"[All Fields]) OR \"heart attack\"[All Fields]) AND (\"aspirin\"[MeSH Terms] OR \"aspirin\"[All Fields]) AND (\"prevention and control\"[Subheading] OR (\"prevention\"[All Fields] AND \"control\"[All Fields]) OR \"prevention and control\"[All Fields] OR \"prevention\"[All Fields])\n\nComprehensive search\nFor optimal searches in PubMed, it is necessary to understand its core component, MEDLINE, and especially of the MeSH (Medical Subject Headings) controlled vocabulary used to index MEDLINE articles. They may also require complex search strategies, use of field names (tags), proper use of limits and other features; reference librarians and search specialists offer search services.\nThe search into PubMed's search window is only recommended for the search of unequivocal topics or new interventions that do not yet have a MeSH heading created, as well as for the search for commercial brands of medicines and proper nouns. It is also useful when there is no suitable heading or the descriptor represents a partial aspect. The search using the thesaurus MeSH is more accurate and will give fewer irrelevant results. In addition, it saves the disadvantage of the free text search in which the spelling, singular\/plural or abbreviated differences have to be taken into consideration. On the other side, articles more recently incorporated into the database to which descriptors have not yet been assigned will not be found. Therefore, to guarantee an exhaustive search, a combination of controlled language headings and free text terms must be used.\n\nJournal article parameters\nWhen a journal article is indexed, numerous article parameters are extracted and stored as structured information. Such parameters are: Article Type (MeSH terms, e.g., \"Clinical Trial\"), Secondary identifiers, (MeSH terms), Language, Country of the Journal or publication history (e-publication date, print journal publication date).\n\nPublication Type: Clinical queries\/systematic reviews\nPublication type parameter allows searching by the type of publication, including reports of various kinds of clinical research.\n\nSecondary ID\nSince July 2005, the MEDLINE article indexing process extracts identifiers from the article abstract and puts those in a field called Secondary Identifier (SI). The secondary identifier field is to store accession numbers to various databases of molecular sequence data, gene expression or chemical compounds and clinical trial IDs. For clinical trials, PubMed extracts trial IDs for the two largest trial registries: ClinicalTrials.gov (NCT identifier) and the International Standard Randomized Controlled Trial Number Register (IRCTN identifier).\n\nSee also\nA reference which is judged particularly relevant can be marked and \"related articles\" can be identified. If relevant, several studies can be selected and related articles to all of them can be generated (on PubMed or any of the other NCBI Entrez databases) using the 'Find related data' option. The related articles are then listed in order of \"relatedness\". To create these lists of related articles, PubMed compares words from the title and abstract of each citation, as well as the MeSH headings assigned, using a powerful word-weighted algorithm. The 'related articles' function has been judged to be so precise that the authors of a paper suggested it can be used instead of a full search.\n\nMapping to MeSH\nPubMed automatically links to MeSH terms and subheadings. Examples would be: \"bad breath\" links to (and includes in the search) \"halitosis\", \"heart attack\" to \"myocardial infarction\", \"breast cancer\" to \"breast neoplasms\". Where appropriate, these MeSH terms are automatically \"expanded\", that is, include more specific terms. Terms like \"nursing\" are automatically linked to \"Nursing [MeSH]\" or \"Nursing [Subheading]\". This feature is called Auto Term Mapping and is enacted, by default, in free text searching but not exact phrase searching (i.e. enclosing the search query with double quotes). This feature makes PubMed searches more sensitive and avoids false-negative (missed) hits by compensating for the diversity of medical terminology.\nPubMed does not apply automatic mapping of the term in the following circumstances: by writing the quoted phrase (e.g., \"kidney allograft\"), when truncated on the asterisk (e.g., kidney allograft*), and when looking with field labels (e.g., Cancer [ti]).\n\nMy NCBI\nThe PubMed optional facility \"My NCBI\" (with free registration) provides tools for\n\nsaving searches\nfiltering search results\nsetting up automatic updates sent by e-mail\nsaving sets of references retrieved as part of a PubMed search\nconfiguring display formats or highlighting search terms\nand a wide range of other options. The \"My NCBI\" area can be accessed from any computer with web-access.\nAn earlier version of \"My NCBI\" was called \"PubMed Cubby\".\n\nLinkOut\nLinkOut is an NLM facility to link and make available full-text local journal holdings. Some 3,200 sites (mainly academic institutions) participate in this NLM facility (as of March 2010), from Aalborg University in Denmark to ZymoGenetics in Seattle. Users at these institutions see their institution's logo within the PubMed search result (if the journal is held at that institution) and can access the full-text. Link out is being consolidated with Outside Tool as of the major platform update coming in the Summer of 2019.\n\nPubMed Commons\nIn 2016, PubMed allows authors of articles to comment on articles indexed by PubMed. This feature was initially tested in a pilot mode (since 2013) and was made permanent in 2016. In February 2018, PubMed Commons was discontinued due to the fact that \"usage has remained minimal\".\n\naskMEDLINE\naskMEDLINE, a free-text, natural language query tool for MEDLINE\/PubMed, developed by the NLM, also suitable for handhelds.\n\nPubMed identifier\nA PMID (PubMed identifier or PubMed unique identifier) is a unique integer value, starting at 1, assigned to each PubMed record. A PMID is not the same as a PMCID (PubMed Central identifier) which is the identifier for all works published in the free-to-access PubMed Central.\nThe assignment of a PMID or PMCID to a publication tells the reader nothing about the type or quality of the content. PMIDs are assigned to letters to the editor, editorial opinions, op-ed columns, and any other piece that the editor chooses to include in the journal, as well as peer-reviewed papers. The existence of the identification number is also not proof that the papers have not been retracted for fraud, incompetence, or misconduct. The announcement about any corrections to original papers may be assigned a PMID.\nEach number that is entered in the PubMed search window is treated by default as if it were a PMID. Therefore, any reference in PubMed can be located using the PMID.\n\nAlternative interfaces\nThe National Library of Medicine leases the MEDLINE information to a number of private vendors such as Embase, Ovid, Dialog, EBSCO, Knowledge Finder and many other commercial, non-commercial, and academic providers. As of October 2008, more than 500 licenses had been issued, more than 200 of them to providers outside the United States. As licenses to use MEDLINE data are available for free, the NLM in effect provides a free testing ground for a wide range of alternative interfaces and 3rd party additions to PubMed, one of a very few large, professionally curated databases which offers this option.\nLu identifies a sample of 28 current and free Web-based PubMed versions, requiring no installation or registration, which are grouped into four categories:\n\nRanking search results, for instance: eTBLAST; MedlineRanker; MiSearch;\nClustering results by topics, authors, journals etc., for instance: Anne O'Tate; ClusterMed;\nEnhancing semantics and visualization, for instance: EBIMed; MedEvi.\nImproved search interface and retrieval experience, for instance, askMEDLINE BabelMeSH; and PubCrawler.\nAs most of these and other alternatives rely essentially on PubMed\/MEDLINE data leased under license from the NLM\/PubMed, the term \"PubMed derivatives\" has been suggested. Without the need to store about 90 GB of original PubMed Datasets, anybody can write PubMed applications using the eutils-application program interface as described in \"The E-utilities In-Depth: Parameters, Syntax and More\", by Eric Sayers, PhD. Various citation format generators, taking PMID numbers as input, are examples of web applications making use of the eutils-application program interface. Sample web pages include Citation Generator \u2013 Mick Schroeder, Pubmed Citation Generator \u2013 Ultrasound of the Week, PMID2cite, and Cite this for me.\n\nData mining of PubMed\nAlternative methods to mine the data in PubMed use programming environments such as Matlab, Python or R. In these cases, queries of PubMed are written as lines of code and passed to PubMed and the response is then processed directly in the programming environment.  Code can be automated to systematically query with different keywords such as disease, year, organs, etc. \nIn addition to its traditional role as a biomedical database, PubMed has become common resource for training biomedical language models. Recent advancements in this field include the development of models like PubMedGPT, a 2.7B parameter model trained on PubMed data by Stanford CRFM, and Microsoft's BiomedCLIP-PubMedBERT, which utilizes figure-caption pairs from PubMed Central for vision-language processing. These models demonstrate the significant potential of PubMed data in enhancing the capabilities of AI in medical research and healthcare applications. Such advancements underline the growing intersection between large-scale data mining and AI development in the biomedical field.\nThe data accessible by PubMed can be mirrored locally using an unofficial tool such as MEDOC.\nMillions of PubMed records augment various open data datasets about open access, like Unpaywall. Data analysis tools like Unpaywall Journals are used by libraries to assist with big deal cancellations: libraries can avoid subscriptions for materials already served by instant open access via open archives like PubMed Central.\n\nSee also\nEurope PubMed Central\nJournalReview.org\nList of academic databases and search engines\nPubMed Central\nPubMed Central Canada\n\nReferences\nExternal links\n\nOfficial website\nPubMed search tags & field qualifiers","146":"Pathophysiology (or physiopathology) is a branch of study, at the intersection of pathology and physiology, concerning disordered physiological processes that cause, result from, or are otherwise associated with a disease or injury. Pathology is the medical discipline that describes conditions typically observed during a disease state, whereas physiology is the biological discipline that describes processes or mechanisms operating within an organism. Pathology describes the abnormal or undesired condition (symptoms of a disease), whereas pathophysiology seeks to explain the functional changes that are occurring within an individual due to a disease or pathologic state.\n\nEtymology\nThe term pathophysiology comes from the Ancient Greek \u03c0\u03ac\u03b8\u03bf\u03c2 (pathos) and \u03c6\u03c5\u03c3\u03b9\u03bf\u03bb\u03bf\u03b3\u03af\u03b1 (phisiologia).\n\nHistory\nNineteenth century\nReductionism\nIn Germany in the 1830s, Johannes M\u00fcller led the establishment of physiology research autonomous from medical research.  In 1843, the Berlin Physical Society was founded in part to purge biology and medicine of vitalism, and in 1847 Hermann von Helmholtz, who joined the Society in 1845, published the paper \"On the conservation of energy\", highly influential to reduce physiology's research foundation to physical sciences.  In the late 1850s, German anatomical pathologist Rudolf Virchow, a former student of M\u00fcller, directed focus to the cell, establishing cytology as the focus of physiological research, while Julius Cohnheim pioneered experimental pathology in medical schools' scientific laboratories.\n\nGerm theory\nBy 1863, motivated by Louis Pasteur's report on fermentation to butyric acid, fellow Frenchman Casimir Davaine identified a microorganism as the crucial causal agent of the cattle disease anthrax, but its routinely vanishing from blood left other scientists inferring it a mere byproduct of putrefaction.  In 1876, upon Ferdinand Cohn's report of a tiny spore stage of a bacterial species, the fellow German Robert Koch isolated Davaine's bacterides in pure culture \u2014a pivotal step that would establish bacteriology as a distinct discipline\u2014 identified a spore stage, applied Jakob Henle's postulates, and confirmed Davaine's conclusion, a major feat for experimental pathology. Pasteur and colleagues followed up with ecological investigations confirming its role in the natural environment via spores in soil.\nAlso, as to sepsis, Davaine had injected rabbits with a highly diluted, tiny amount of putrid blood, duplicated disease, and used the term ferment of putrefaction, but it was unclear whether this referred as did Pasteur's term ferment to a microorganism or, as it did for many others, to a chemical.  In 1878, Koch published Aetiology of Traumatic Infective Diseases, unlike any previous work, where in 80 pages Koch, as noted by a historian, \"was able to show, in a manner practically conclusive, that a number of diseases, differing clinically, anatomically, and in aetiology, can be produced experimentally by the injection of putrid materials into animals.\"  Koch used bacteriology and the new staining methods with aniline dyes to identify particular microorganisms for each. Germ theory of disease crystallized the concept of cause\u2014presumably identifiable by scientific investigation.\n\nScientific medicine\nThe American physician William Welch trained in German pathology from 1876 to 1878, including under Cohnheim, and opened America's first scientific laboratory \u2014a pathology laboratory\u2014 at Bellevue Hospital in New York City in 1878.  Welch's course drew enrollment from students at other medical schools, which responded by opening their own pathology laboratories.  Once appointed by Daniel Coit Gilman, upon advice by John Shaw Billings, as founding dean of the medical school of the newly forming Johns Hopkins University that Gilman, as its first president, was planning, Welch traveled again to Germany for training in Koch's bacteriology in 1883.  Welch returned to America but moved to Baltimore, eager to overhaul American medicine, while blending Vichow's anatomical pathology, Cohnheim's experimental pathology, and Koch's bacteriology.  Hopkins medical school, led by the \"Four Horsemen\" \u2014Welch, William Osler, Howard Kelly, and William Halsted\u2014 opened at last in 1893 as America's first medical school devoted to teaching German scientific medicine, so called.\n\nTwentieth century\nBiomedicine\nThe first biomedical institutes, Pasteur Institute and Berlin Institute for Infectious Diseases, whose first directors were Pasteur and Koch, were founded in 1888 and 1891, respectively. America's first biomedical institute, The Rockefeller Institute for Medical Research, was founded in 1901 with Welch, nicknamed \"dean of American medicine\", as its scientific director, who appointed his former Hopkins student Simon Flexner as director of pathology and bacteriology laboratories.  By way of World War I and World War II, Rockefeller Institute became the globe's leader in biomedical research.\n\nMolecular paradigm\nThe 1918 pandemic triggered frenzied search for its cause, although most deaths were via lobar pneumonia, already attributed to pneumococcal invasion.  In London, pathologist with the Ministry of Health, Fred Griffith in 1928 reported pneumococcal transformation from virulent to avirulent and between antigenic types \u2014nearly a switch in species\u2014 challenging pneumonia's specific causation. The laboratory of Rockefeller Institute's Oswald Avery, America's leading pneumococcal expert, was so troubled by the report that they refused to attempt repetition.\nWhen Avery was away on summer vacation, Martin Dawson, British-Canadian, convinced that anything from England must be correct, repeated Griffith's results, then achieved transformation in vitro, too, opening it to precise investigation. Having returned, Avery kept a photo of Griffith on his desk while his researchers followed the trail.  In 1944, Avery, Colin MacLeod, and Maclyn McCarty reported the transformation factor as DNA, widely doubted amid estimations that something must act with it.  At the time of Griffith's report, it was unrecognized that bacteria even had genes.\nThe first genetics, Mendelian genetics, began at 1900, yet inheritance of Mendelian traits was localized to chromosomes by 1903, thus chromosomal genetics. Biochemistry emerged in the same decade. In the 1940s, most scientists viewed the cell as a \"sack of chemicals\" \u2014a membrane containing only loose molecules in chaotic motion\u2014 and the only especial cell structures as chromosomes, which bacteria lack as such.  Chromosomal DNA was presumed too simple, so genes were sought in chromosomal proteins.  Yet in 1953, American biologist James Watson, British physicist Francis Crick, and British chemist Rosalind Franklin inferred DNA's molecular structure \u2014a double helix\u2014 and conjectured it to spell a code.  In the early 1960s, Crick helped crack a genetic code in DNA, thus establishing molecular genetics.\nIn the late 1930s, Rockefeller Foundation had spearheaded and funded the molecular biology research program \u2014seeking fundamental explanation of organisms and life\u2014 led largely by physicist Max Delbr\u00fcck at Caltech and Vanderbilt University.  Yet the reality of organelles in cells was controversial amid unclear visualization with conventional light microscopy.  Around 1940, largely via cancer research at Rockefeller Institute, cell biology emerged as a new discipline filling the vast gap between cytology and biochemistry by applying new technology \u2014ultracentrifuge and electron microscope\u2014 to identify and deconstruct cell structures, functions, and mechanisms.  The two new sciences interlaced, cell and molecular biology.\nMindful of Griffith and Avery, Joshua Lederberg confirmed bacterial conjugation \u2014reported decades earlier but controversial\u2014 and was awarded the 1958 Nobel Prize in Physiology or Medicine.   At Cold Spring Harbor Laboratory in Long Island, New York, Delbr\u00fcck and Salvador Luria led the Phage Group \u2014hosting Watson\u2014 discovering details of cell physiology by tracking changes to bacteria upon infection with their viruses, the process transduction.  Lederberg led the opening of a genetics department at Stanford University's medical school, and facilitated greater communication between biologists and medical departments.\n\nDisease mechanisms\nIn the 1950s, researches on rheumatic fever, a complication of streptococcal infections, revealed it was mediated by the host's own immune response, stirring investigation by pathologist Lewis Thomas that led to identification of enzymes released by the innate immune cells macrophages and that degrade host tissue.  In the late 1970s, as president of Memorial Sloan\u2013Kettering Cancer Center, Thomas collaborated with Lederberg, soon to become president of Rockefeller University, to redirect the funding focus of the US National Institutes of Health toward basic research into the mechanisms operating during disease processes, which at the time medical scientists were all but wholly ignorant of, as biologists had scarcely taken interest in disease mechanisms.   Thomas became for American basic researchers a patron saint.\n\nExamples\nParkinson's disease\nThe pathophysiology of Parkinson's disease is death of dopaminergic neurons as a result of changes in biological activity in the brain with respect to Parkinson's disease (PD). There are several proposed mechanisms for neuronal death in PD; however, not all of them are well understood. Five proposed major mechanisms for neuronal death in Parkinson's Disease include protein aggregation in Lewy bodies, disruption of autophagy, changes in cell metabolism or mitochondrial function, neuroinflammation, and blood\u2013brain barrier (BBB) breakdown resulting in vascular leakiness.\n\nHeart failure\nThe pathophysiology of heart failure is a reduction in the efficiency of the heart muscle, through damage or overloading. As such, it can be caused by a wide number of conditions, including myocardial infarction (in which the heart muscle is starved of oxygen and dies), hypertension (which increases the force of contraction needed to pump blood) and amyloidosis (in which misfolded proteins are deposited in the heart muscle, causing it to stiffen). Over time these increases in workload will produce changes to the heart itself.\n\nMultiple sclerosis\nThe pathophysiology of multiple sclerosis is that of an inflammatory demyelinating disease of the CNS in which activated immune cells invade the central nervous system and cause inflammation, neurodegeneration and tissue damage. The underlying condition that produces this behaviour is currently unknown. Current research in neuropathology, neuroimmunology, neurobiology, and neuroimaging, together with clinical neurology provide support for the notion that MS is not a single disease but rather a spectrum\n\nHypertension\nThe pathophysiology of hypertension is that of a chronic disease characterized by elevation of blood pressure. Hypertension can be classified by cause as either essential (also known as primary or idiopathic) or secondary. About 90\u201395% of hypertension is essential hypertension.\n\nHIV\/AIDS\nThe pathophysiology of HIV\/AIDS involves, upon acquisition of the virus, that the virus replicates inside and kills T helper cells, which are required for almost all adaptive immune responses. There is an initial period of influenza-like illness, and then a latent, asymptomatic phase. When the CD4 lymphocyte count falls below 200 cells\/ml of blood, the HIV host has progressed to AIDS, a condition characterized by deficiency in cell-mediated immunity and the resulting increased susceptibility to opportunistic infections and certain forms of cancer.\n\nSpider bites\nThe pathophysiology of spider bites is due to the effect of its venom. A spider envenomation occurs whenever a spider injects venom into the skin. Not all spider bites inject venom \u2013 a dry bite, and the amount of venom injected can vary based on the type of spider and the circumstances of the encounter. The mechanical injury from a spider bite is not a serious concern for humans.\n\nObesity\nThe pathophysiology of obesity involves many possible pathophysiological mechanisms involved in its development and maintenance. \nThis field of research had been almost unapproached until the leptin gene was discovered in 1994 by J. M. Friedman's laboratory. These investigators postulated that leptin was a satiety factor. In the ob\/ob mouse, mutations in the leptin gene resulted in the obese phenotype opening the possibility of leptin therapy for human obesity. However, soon thereafter J. F. Caro's laboratory could not detect any mutations in the leptin gene in humans with obesity. On the contrary Leptin expression was increased proposing the possibility of Leptin-resistance in human obesity.\n\nSee also\nPathogenesis\n\n\n== References ==","147":"Nystagmus is a condition of involuntary (or voluntary, in some cases) eye movement. People can be born with it but more commonly acquire it in infancy or later in life. In many cases it may result in reduced or limited vision.\nIn normal eyesight, while the head rotates about an axis, distant visual images are sustained by rotating eyes in the opposite direction of the respective axis. The semicircular canals in the vestibule of the ear sense angular acceleration, and send signals to the nuclei for eye movement in the brain. From here, a signal is relayed to the extraocular muscles to allow one's gaze to fix on an object as the head moves. Nystagmus occurs when the semicircular canals are stimulated (e.g., by means of the caloric test, or by disease) while the head is stationary. The direction of ocular movement is related to the semicircular canal that is being stimulated.\nThere are two key forms of nystagmus: pathological and physiological, with variations within each type. Physiological nystagmus occurs under normal conditions in healthy subjects. Nystagmus may be caused by congenital disorder or sleep deprivation, acquired or central nervous system disorders, toxicity, pharmaceutical drugs, alcohol, or rotational movement. Previously considered untreatable, in recent years several drugs have been identified for treatment of nystagmus. Nystagmus is also occasionally associated with vertigo.\n\nCauses\nThe cause of pathological nystagmus may be congenital, idiopathic, or secondary to a pre-existing neurological disorder. It also may be induced temporarily by disorientation (such as on roller coaster rides or when a person has been spinning in circles) or by some drugs (alcohol, lidocaine, and other central nervous system depressants, inhalant drugs, stimulants, psychedelics, and dissociative drugs).\n\nEarly-onset nystagmus\nEarly-onset nystagmus occurs more frequently than acquired nystagmus. It can be insular or accompany other disorders (such as micro-ophthalmic anomalies or Down syndrome). Early-onset nystagmus itself is usually mild and non-progressive. The affected persons are usually unaware of their spontaneous eye movements, but vision can be impaired depending on the severity of the eye movements.\nTypes of early-onset nystagmus include the following, along with some of their causes:\n\nInfantile:\nAlbinism\nAniridia\nBilateral congenital cataract\nBilateral optic nerve hypoplasia\nIdiopathic\nLeber's congenital amaurosis\nOptic nerve or macular disease\nPersistent tunica vasculosa lentis\nRod monochromatism\nVisual-motor syndrome of functional monophthalmus\nLatent nystagmus\nNoonan syndrome\nNystagmus blockage syndrome\nX-linked infantile nystagmus is associated with mutations of the gene FRMD7, which is located on the X chromosome.\nInfantile nystagmus is also associated with two X-linked eye diseases known as complete congenital stationary night blindness (CSNB) and incomplete CSNB (iCSNB or CSNB-2), which are caused by mutations of one of two genes located on the X chromosome. In CSNB, mutations are found in NYX (nyctalopin). CSNB-2 involves mutations of CACNA1F, a voltage-gated calcium channel that, when mutated, does not conduct ions.\n\nAcquired nystagmus\nNystagmus that occurs later in childhood or in adulthood is called acquired nystagmus. The cause is often unknown, or idiopathic, and thus referred to as idiopathic nystagmus. Other common causes include diseases and disorders of the central nervous system, metabolic disorders and alcohol and drug toxicity. In the elderly, stroke is the most common cause.\n\nGeneral diseases and conditions\nSome of the diseases that present nystagmus as a pathological sign or symptom are as follows:\n\nAniridia\nBenign paroxysmal positional vertigo\n\nToxicity or intoxication, metabolic disorders and combination\nSources of toxicity that could lead to nystagmus:\n\nThiamine deficiency\nRisk factors for thiamine deficiency, or beriberi, in turn include a diet of mostly white rice, as well as alcoholism, dialysis, chronic diarrhea, and taking high doses of diuretics. Rarely it may be due to a genetic condition that results in difficulties absorbing thiamine found in food. Wernicke encephalopathy and Korsakoff syndrome are forms of dry beriberi.\n\nCentral nervous system (CNS) diseases and disorders\nCentral nervous system disorders such as with a cerebellar problem, the nystagmus can be in any direction including horizontal. Purely vertical nystagmus usually originates in the central nervous system, but it is also an adverse effect commonly seen in high phenytoin toxicity. Other causes of toxicity that may result in nystagmus include:\n\nOther causes\nNon-physiological\nTrochlear nerve malfunction\nVestibular pathology (M\u00e9ni\u00e8re's disease, SCDS (superior canal dehiscence syndrome), BPPV, vestibular neuritis)\nExposure to strong magnetic fields (as in MRI machines)\nLong-term exposure to low light conditions or darkness, called miner's nystagmus after 19th-century coal miners who developed nystagmus from working in the dark.\nA slightly different form of nystagmus may be produced voluntarily by some (8% of) people. Some can sustain it for up to 35 seconds, but most average around 5 seconds.\n\nDiagnosis\nNystagmus is highly noticeable but rarely recognized. Nystagmus can be clinically investigated by using a number of non-invasive standard tests. The simplest one is the caloric reflex test, in which one ear canal is irrigated with warm or cold water or air. The temperature gradient provokes the stimulation of the horizontal semicircular canal and the consequent nystagmus.\nNystagmus is often very commonly present with Chiari malformation.\nThe resulting movement of the eyes may be recorded and quantified by a special device called an electronystagmograph (ENG), a form of electrooculography (an electrical method of measuring eye movements using external electrodes), or an even less invasive device called a videonystagmograph (VNG), a form of video-oculography (VOG) (a video-based method of measuring eye movements using external small cameras built into head masks), administered by an audiologist. Special swinging chairs with electrical controls can be used to induce rotatory nystagmus.\nOver the past forty years, objective eye-movement-recording techniques have been applied to the study of nystagmus, and the results have led to greater accuracy of measurement and understanding of the condition.\nOrthoptists may also use an optokinetic drum, or electrooculography or Frenzel goggles to assess a patient's eye movements.\nNystagmus can be caused by subsequent foveation of moving objects, pathology, sustained rotation or substance use. Nystagmus is not to be confused with other superficially similar-appearing disorders of eye movements (saccadic oscillations) such as opsoclonus or ocular flutter that are composed purely of fast-phase (saccadic) eye movements, while nystagmus is characterized by the combination of a smooth pursuit, which usually acts to take the eye off the point of focus, interspersed with the saccadic movement that serves to bring the eye back on target. Without the use of objective recording techniques, it may be very difficult to distinguish among these conditions.\nIn medicine, the presence of nystagmus can be benign, or it can indicate an underlying visual or neurological problem.\n\nPathological nystagmus\nPathological nystagmus is characterized by \"excessive drifts of stationary retinal images that degrades vision and may produce illusory motion of the seen world: oscillopsia (an exception is congenital nystagmus)\".\nBechterew's phenomenon was discovered by Vladimir Bekhterev in 1883 in animal experiments. Specifically, if one side of the vestibular system is damaged, then due to the lack of vestibular signal from that side, the animal behaves with nystagmus and vertigo. After a while, due to vestibular compensation, nystagmus and vertigo stops. However, if then the other vestibular system is damaged, then nystagmus and vertigo occurs in the opposite direction. This is an early evidence of sensorimotor adaptation in the brain. It is rarely reported in humans.\nWhen nystagmus occurs without fulfilling its normal function, it is pathologic (deviating from the healthy or normal condition). Pathological nystagmus is the result of damage to one or more components of the vestibular system, including the semicircular canals, otolith organs, and the vestibulocerebellum.\nPathological nystagmus generally causes a degree of vision impairment, although the severity of such impairment varies widely. Also, many blind people have nystagmus, which is one reason that some wear dark glasses.\n\nVariations\nCentral nystagmus occurs as a result of either normal or abnormal processes not related to the vestibular organ. For example, lesions of the midbrain or cerebellum can result in up- and down-beat nystagmus.\nGaze induced nystagmus occurs or is exacerbated as a result of changing one's gaze toward or away from a particular side which has an affected central apparatus.\nPeripheral nystagmus occurs as a result of either normal or diseased functional states of the vestibular system and may combine a rotational component with vertical or horizontal eye movements and may be spontaneous, positional, or evoked.\nPositional nystagmus occurs when a person's head is in a specific position. An example of disease state in which this occurs is benign paroxysmal positional vertigo (BPPV).\nPost rotational nystagmus occurs after an imbalance is created between a normal side and a diseased side by stimulation of the vestibular system by rapid shaking or rotation of the head.\nSpontaneous nystagmus is nystagmus that occurs randomly, regardless of the position of the patient's head.\n\nPhysiological nystagmus\nPhysiological nystagmus is a form of involuntary eye movement that is part of the vestibulo-ocular reflex (VOR), characterized by alternating smooth pursuit in one direction and saccadic movement in the other direction.\n\nVariations\nThe direction of nystagmus is defined by the direction of its quick phase (e.g. a right-beating nystagmus is characterized by a rightward-moving quick phase, and a left-beating nystagmus by a leftward-moving quick phase). The oscillations may occur in the vertical, horizontal or torsional planes, or in any combination. The resulting nystagmus is often named as a gross description of the movement, e.g. downbeat nystagmus, upbeat nystagmus, seesaw nystagmus, periodic alternating nystagmus.\nThese descriptive names can be misleading, however, as many were assigned historically, solely on the basis of subjective clinical examination, which is not sufficient to determine the eyes' true trajectory.\n\nOptokinetic (syn.\u2009opticokinetic) nystagmus: a nystagmus induced by looking at moving visual stimuli, such as moving horizontal or vertical lines, and\/or stripes. For example, if one fixates on a stripe of a rotating drum with alternating black and white, the gaze retreats to fixate on a new stripe as the drum moves. This is first a rotation with the same angular velocity, then returns in a saccade in the opposite direction. The process proceeds indefinitely. This is optokinetic nystagmus, and is a source for understanding the fixation reflex.\nPostrotatory nystagmus: if one spins in a chair continuously and stops suddenly, the fast phase of nystagmus is in the opposite direction of rotation, known as the \"post-rotatory nystagmus\", while slow phase is in the direction of rotation.\n\nTreatment\nCongenital nystagmus has long been viewed as untreatable, but medications have been discovered that show promise in some patients. In 1980, researchers discovered that a drug called baclofen could stop periodic alternating nystagmus. Subsequently, gabapentin, an anticonvulsant, led to improvement in about half the patients who took it. Other drugs found to be effective against nystagmus in some patients include memantine, levetiracetam, 3,4-diaminopyridine (available in the US to eligible patients with downbeat nystagmus at no cost under an expanded access program), 4-aminopyridine, and acetazolamide. Several therapeutic approaches, such as contact lenses, drugs, surgery, and low vision rehabilitation have also been proposed. For example, it has been proposed that mini-telescopic eyeglasses suppress nystagmus.\nSurgical treatment of congenital nystagmus is aimed at improving head posture, simulating artificial divergence, or weakening the horizontal recti muscles. Clinical trials of a surgery to treat nystagmus (known as tenotomy) concluded in 2001. Tenotomy is now being performed regularly at numerous centres around the world. The surgery aims to reduce the eye oscillations, which in turn tends to improve visual acuity.\nAcupuncture tests have produced conflicting evidence on its beneficial effects on the symptoms of nystagmus. Benefits have been seen in treatments in which acupuncture points of the neck were used, specifically points on the sternocleidomastoid muscle. Benefits of acupuncture for treatment of nystagmus include a reduction in frequency and decreased slow phase velocities, which led to an increase in foveation duration periods both during and after treatment. By the standards of evidence-based medicine, the quality of these studies is poor (for example, Ishikawa's study had sample size of six subjects, was unblinded, and lacked proper controls), and given high quality studies showing that acupuncture has no effect beyond placebo, the results of these studies have to be considered clinically irrelevant until higher quality studies are performed.\nPhysical or occupational therapy is also used to treat nystagmus. Treatment consists of learning strategies to compensate for the impaired system.\nA Cochrane Review on interventions for eye movement disorders due to acquired brain injury, updated in June 2017, identified three studies of pharmacological interventions for acquired nystagmus but concluded that these studies provided insufficient evidence to guide treatment choices.\n\nEpidemiology\nNystagmus is a relatively common clinical condition, affecting one in several thousand people. A survey conducted in Oxfordshire, United Kingdom, found that by the age of two, one in every 670 children had manifested nystagmus. Authors of another study in the United Kingdom estimated an incidence of 24 in 10,000 (c. 0.240%), noting an apparently higher rate amongst white Europeans than in individuals of Asian origin.\n\nLaw enforcement\nIn the United States, testing for horizontal gaze nystagmus is one of a battery of field sobriety tests used by police officers to determine whether a suspect is driving under the influence of alcohol. Horizontal gaze nystagmus will show if a subject is under the influence of a central nervous system depressant, an inhalant, or a dissociative anesthetic. The test involves observation of the suspect's pupil as it follows a moving object, noting\n\nlack of smooth pursuit,\ndistinct and sustained nystagmus at maximum deviation, and\nthe onset of nystagmus prior to 45 degrees.\nThe horizontal gaze nystagmus test has been highly criticized and major errors in the testing methodology and analysis found. However, the validity of the horizontal gaze nystagmus test for use as a field sobriety test for persons with a blood alcohol level between 0.04 and 0.08 is supported by peer-reviewed studies and has been found to be a more accurate indication of blood alcohol content than other standard field sobriety tests.\n\nMedia\nMy Dancing Eyes, a documentary by filmmaker Matt Morris, had participants explain what it is like to live with the eye condition, and was released for free. It was featured on NBN News, and ABC Radio Newcastle, in Australia.\n\nSee also\nBruns nystagmus\nMyoclonus\nOscillopsia\nOpsoclonus\nOptokinetic nystagmus\n\n\n== References ==","148":"A perforated eardrum (tympanic membrane perforation) is a prick in the eardrum. It can be caused by infection (otitis media), trauma, overpressure (loud noise), inappropriate ear clearing, and changes in middle ear pressure. An otoscope can be used to view the eardrum to diagnose a perforation. Perforations may heal naturally or require surgery.\n\nPresentation\nA perforated eardrum leads to conductive hearing loss, which is usually temporary. Other symptoms may include tinnitus, ear pain, vertigo, or a discharge of mucus. Nausea and\/or vomiting secondary to vertigo may occur.\n\nCauses\nA perforated eardrum can have one of many causes, such as:\n\nInfection (otitis media). This infection may then spread through the middle ear and may reoccur.\nTrauma. This may be caused by trying to clean ear wax with sharp instruments. It may also occur due to surgical complications.\nOverpressure (loud noise or shockwave from an explosion).\nInappropriate ear clearing.\nFlying with a severe cold, due to changes in air pressure and blocked Eustachian tubes resulting from the cold. This is especially true on landing.\n\nDiagnosis\nAn otoscope can be used to look at the ear canal. This gives a view of the ear canal and eardrum, so that a perforated eardrum can be seen. Tympanometry may also be used.\n\nTreatment\nConservative management\nA perforated eardrum often heals naturally. It may heal in a few weeks or may take up to a few months.\n\nSurgery\nSome perforations require surgical intervention. This may take the form of a paper patch to promote healing (a simple procedure by an ear, nose and throat specialist), or surgery (tympanoplasty). However, in some cases, the perforation can last several years and will be unable to heal naturally. For patients with persistent perforation, surgery is usually undertaken to close the perforation. The objective of the surgery is to provide a platform of sort to support the regrowth and healing of the tympanic membrane in the two weeks post-surgery period. There are two ways of doing the surgery:\n\nTraditional tympanoplasty, usually using the microscope and performed through a 10 cm incision behind the ear lobe. This technique was introduced by Wullstien and Zollner and popularized by the Jim Sheehy at the House Ear Institute.\nEndoscopic tympanoplasty, usually using the endoscope through the ear canal without the need for incision. This technique was introduced and popularized by Professor Tarabichi of TSESI: Tarabichi Stammberger Ear and Sinus Institute.\nThe success of surgery is variable based on the cause of perforation and the technique being used. Predictors of success include traumatic perforation, dry ear, and central perforations. Predictors of failure includes young age and poor Eustachian tube function.  The use of minimally invasive endoscopic technique does not reduce the chance of successful outcome. Hearing is usually recovered fully, but chronic infection over a long period may lead to permanent hearing loss. Those with more severe ruptures may need to wear an ear plug to prevent water contact with the ear drum.\n\nReferences\n\n\n== External links ==","149":"Patulous Eustachian tube is the name of a physical disorder where the Eustachian tube, which is normally closed, instead stays intermittently open.  When this occurs, the person experiences autophony, the hearing of self-generated sounds. These sounds, such as one's own breathing, voice, and heartbeat, vibrate directly onto the ear drum and can create a \"bucket on the head\" effect, making it difficult for the patient to attend to environmental sounds. Patulous Eustachian tube is a form of Eustachian tube dysfunction, which is said to be present in about 1 percent of the general population.\n\nSigns and symptoms\nWith patulous Eustachian tube, variations in upper airway pressure associated with respiration are transmitted to the middle ear through the Eustachian tube.  This causes an unpleasant fullness feeling in the middle ear and alters the auditory perception.  Complaints seem to include muffled hearing and autophony. In addition, patulous Eustachian tube generally feels dry with no clogged feeling or sinus pressure.\nPatients hear their own voice or its echo from inside. They describe it as being amplified and unpleasant. Lying head down  may help since it increases venous blood pressure and congestion of the mucosa.\n\nCauses\nPatulous Eustachian tube is a physical disorder. The exact causes may vary depending on the person and are often unknown. Weight loss is a commonly cited cause of the disorder due to the nature of the Eustachian tube itself and is associated with approximately one-third of reported cases. Fatty tissues hold the tube closed most of the time in healthy individuals.  When circumstances cause overall body fat to diminish, the tissue surrounding the Eustachian tube shrinks and this function is disrupted.\nActivities and substances which dehydrate the body have the same effect and are also possible causes of patulous Eustachian tube.  Examples are stimulants (including caffeine) and exercise.  Exercise may have a more short-term effect than caffeine or weight loss in this regard.\nPregnancy can also be a cause of patulous Eustachian tube due to the effects of pregnancy hormones on surface tension and mucus in the respiratory system.\nGranulomatosis with polyangiitis can also be a cause of this disorder. It is yet unknown why.\nPatulous Eustachian tube can occur as a result of liquid residue in the Eustachian tube, after suffering a middle ear infection (otitis media).\n\nPotential causes list\nLongitudinal concave defect in the mucosal valve \nSudden weight loss\nRadiation therapy\nHigh levels of estrogen\nNasal decongestants\nStress\nNeurological disorders\nIt's worth noting that there hasn't been much comprehensive scientific research conducted to establish a clear correlation between most of the claimed potential causes and patulous eustachian tube disorder.\n\nDiagnosis\nUpon examination of a suspected case of patulous Eustachian tube, a doctor can directly view the tympanic membrane with a light and observe that it vibrates with every breath taken by the patient.  A tympanogram may also help with the diagnosis.  Patulous Eustachian tube is likely if brisk inspiration causes a significant pressure shift.\nPatulous Eustachian tube is frequently misdiagnosed as standard congestion due to the similarity in symptoms and rarity of the disorder.  Audiologists are more likely to recognize the disorder, usually with tympanometry or nasally delivered masking noise during a hearing assessment, which is highly sensitive to this condition.\nWhen misdiagnosis occurs, a decongestant medication is sometimes prescribed.  This type of medication aggravates the condition, as the Eustachian tube relies on sticky fluids to keep closed and the drying effect of a decongestant would make it even more likely to remain open and cause symptoms.\nIncidentally, patients who instead suffer from the even rarer condition of superior canal dehiscence are at risk for misdiagnosis of patulous Eustachian tube due to the similar autophony in both conditions.\n\nTreatment\nEstrogen nasal drops or saturated potassium iodide have been used to induce edema of the eustachian tube opening. Nasal medications containing diluted hydrochloric acid, chlorobutanol, and benzyl alcohol have been reported to be effective in some patients, with few side effects. Food and Drug Administration approval is still pending, however. Nasal sprays have also been a very effective temporary treatment for this disease, as well.\nIn extreme cases surgical intervention may attempt to restore the Eustachian tube tissues with fat, gel foam, or cartilage or scar it closed with cautery.  These methods are not always successful. For example, there is the case of the early attempts at surgical correction involving injections of tetrafluoroetheylene (Teflon) paste but although this treatment was able to give transient relief, it was discontinued due to several deaths that resulted from inadvertent intracarotid injections.\nAlthough a temporary solution, surgical ventilation tube placement in the ear drum has also proven to be an effective treatment option. This treatment is known as either a unilateral or bilateral myringotomy. 50% of patients reported relief of patulous Eustachian tube symptoms when given this treatment.\n\nReferences\n\n\n== External links ==","150":"Patient is a subsidiary of EMIS Health. First launching in 1996 as a directory of UK websites providing health related information, the company now provides digital healthcare products to the public in the form of Patient.info and Patient Access.\n\nPatient.info\nPatient.info is an online resource providing information on health, lifestyle, disease and other medical related topics. The website's aim is to provide members of the public with up-to-date information on health related topics in the form of comprehensive leaflets (which can be read online or printed), blogs, wellbeing advice and videos. Leaflets are compiled and reviewed by qualified medical practitioners with several years of experience in the medical profession. Users also have access to a symptom checker where they may attempt to self-diagnose any health condition they may have. As well as these resources, there is also a community forum to discuss any health topics with other users of the website, however healthcare professionals do not actively review this section.\nAnother section of the website contains articles and content geared towards medical professionals. These professional articles are typically written in a more technical manner, going into more specific detail and using more industry-specific language and jargon.\nThere is also a paid-for service called Patient Pro, where users may pay a monthly subscription fee in order to gain access to enhanced site capability.\nInformation on Patient.info is updated to keep abreast of the latest medical evidence, with each leaflet reviewed every two years or earlier when necessary (whichever comes first).\nIn 2013, the site appeared in a \"Top 50 websites\" feature published in The Times.\n\nPatient Access\nPatient Access is a service that enables users to connect with their GP online. It is available to users through a website, as well as a mobile app for iOS and Android.\nUsers are able to book appointments, order repeat prescriptions, send messages to their practice, and view their medical records remotely. Individual GP practices have control over which services they provide to their patients.\nAccording to Patient, 'more than half' of practices in the UK are connected to Patient Access.\n\nAwards\nPatient claims to have won several awards for its content over several years from different award bodies.\n\nWhich Joint Top Health Website - November 2010\nBMA Highly Commended Resource Award - BMA Patient Information Awards 2012\nThe Times Top Health Website -  January 2013\nWebsite of the Year Awards 2014 - Best Health Website - November 2014\nMedilink Yorkshire and Humber Healthcare Business Awards 2013 - Harrison Goddard Innovation Award for Patient Access app - March 2014\n\nReferences\nExternal links\npatient.info\nPatient Access","151":"Phonophobia, also called ligyrophobia or sonophobia, is a fear of or aversion to loud sounds (for example firecrackers)\u2014a type of specific phobia. It is a very rare phobia which is often the symptom of hyperacusis.  Sonophobia can refer to the hypersensitivity of a patient to sound and can be part of the diagnosis of a migraine.\nOccasionally it is called acousticophobia.\nThe term phonophobia comes from Greek \u03c6\u03c9\u03bd\u03ae - ph\u014dn\u0113, \"voice\" or \"sound\" and \u03c6\u03cc\u03b2\u03bf\u03c2 - phobos, \"fear\".\nLigyrophobics may be fearful of devices that can suddenly emit loud sounds, such as computer speakers or fire alarms. When operating a device such as a home theater system, computer, television, or CD player, they may wish to have the volume turned down all the way before doing anything that would cause the speakers to emit sound, so that once the command to produce sound is given, the user can raise the volume of the speakers to a comfortable listening level. They may avoid parades and carnivals due to the loud instruments such as drums. As festive occasions are accompanied by music of over 120 decibels, many phobics develop agoraphobia. Other ligyrophobics also steer clear of any events in which firecrackers are to be let off.\nAnother example is watching someone blow up a balloon beyond its normal capacity. This is often an unsettling, even disturbing thing for a person with ligyrophobia to observe, as they anticipate a loud sound when the balloon pops. When balloons pop, two types of reactions are heavy breathing and panic attacks. The sufferer becomes anxious to get away from the source of the loud sound and may get headaches. It may also be related to, caused by, or confused with hyperacusis, extreme sensitivity to loud sounds. Phonophobia also has been proposed to refer to an extreme form of misophonia.\n\nSee also\nAstraphobia \u2013 fear of thunder\nMisophonia \u2013 irrational 'hatred' or disgust expressed towards specific sounds\nGlobophobia - the fear of balloons, which is commonly linked to phonophobia\nList of phobias\n\nReferences\n\n\n== External links ==","152":"Perspiration, also known as sweat, is the fluid secreted by sweat glands in the skin of mammals.\nTwo types of sweat glands can be found in humans: eccrine glands and apocrine glands. The eccrine sweat glands are distributed over much of the body and are responsible for secreting the watery, brackish sweat most often triggered by excessive body temperature. Apocrine sweat glands are restricted to the armpits and a few other areas of the body and produce an odorless, oily, opaque secretion which then gains its characteristic odor from bacterial decomposition.\nIn humans, sweating is primarily a means of thermoregulation, which is achieved by the water-rich secretion of the eccrine glands. Maximum sweat rates of an adult can be up to 2\u20134 litres (0.53\u20131.06 US gal) per hour or 10\u201314 litres (2.6\u20133.7 US gal) per day, but is less in children prior to puberty. Evaporation of sweat from the skin surface has a cooling effect due to evaporative cooling. Hence, in hot weather, or when the individual's muscles heat up due to exertion, more sweat is produced. Animals with few sweat glands, such as dogs, accomplish similar temperature regulation results by panting, which evaporates water from the moist lining of the oral cavity and pharynx.\nAlthough sweating is found in a wide variety of mammals, relatively few (apart from humans, horses, some primates and some bovidae) produce sweat in order to cool down. In horses, such cooling sweat is created by apocrine glands and contains a wetting agent, the protein latherin which transfers from the skin to the surface of their coats.\n\nDefinitions\nThe words diaphoresis and hidrosis can both mean either perspiration (in which sense they are synonymous with sweating) or excessive perspiration (in which sense they can be either synonymous with hyperhidrosis or differentiable from it only by clinical criteria involved in narrow specialist senses of the words).\nHypohidrosis is decreased sweating from whatever cause.\nFocal hyperhidrosis is increased or excessive sweating in certain regions such as the underarm, palms, soles, face, or groin.\nHyperhidrosis is excessive sweating, usually secondary to an underlying condition (in which case it is called secondary hyperhidrosis) and usually involving the body as a whole (in which case it is called generalized hyperhidrosis).\nHidromeiosis is a reduction in sweating that is due to blockages of sweat glands in humid conditions.\nA substance or medicine that causes perspiration is a sudorific or sudatory.\n\nSigns and symptoms\nSweat contributes to body odor when it is metabolized by bacteria on the skin. Medications that are used for other treatments and diet also affect odor. Some medical conditions, such as kidney failure and diabetic ketoacidosis, can also affect sweat odor.\n\nCauses\nDiaphoresis is a non-specific symptom or sign, which means that it has many possible causes. Some causes of diaphoresis include physical exertion, menopause, fever, ingestion of toxins or irritants, and high environmental temperature. Strong emotions (anger, fear, anxiety) and recall of past trauma can also trigger sweating.\nThe vast majority of sweat glands in the body are innervated by sympathetic cholinergic neurons.  Sympathetic postganglionic neurons typically secrete norepinephrine and are named sympathetic adrenergic neurons;  however, the sympathetic postganglionic neurons that innervate sweat glands secrete acetylcholine and hence are termed sympathetic cholinergic neurons. Sweat glands, piloerector muscles, and some blood vessels are innervated by sympathetic cholinergic neurons.\n\nPathological sweating and symptoms\nDiaphoresis may be associated with some abnormal conditions, such as hyperthyroidism and shock. If it is accompanied by unexplained weight loss, fever\/chills, or by palpitations, shortness of breath, unconsciousness, fatigue, dizziness, muscle pain, nausea, vomiting, diarrhea, and chest discomfort, it suggests serious illness.\nDiaphoresis is also seen in an acute myocardial infarction (heart attack), from the increased firing of the sympathetic nervous system, and is frequent in serotonin syndrome, which can result in serious sickness or even death. Diaphoresis can also be caused by many types of infections, often accompanied by high fever and\/or chills which can trigger the result of hyperthermia. Most infections can cause some degree of diaphoresis and it is a very common symptom in some serious infections such as malaria and tuberculosis.  In addition, pneumothorax can cause diaphoresis with splinting of the chest wall.\nNeuroleptic malignant syndrome and other malignant diseases (e.g. leukemias) can also cause diaphoresis.\nDiabetics relying on insulin shots or oral medications may have low blood sugar (hypoglycemia), which can also cause diaphoresis.\nDrugs (including caffeine, morphine, alcohol, antidepressants and certain antipsychotics) may be causes, as well as withdrawal from alcohol, benzodiazepines, nonbenzodiazepines or narcotic painkiller dependencies. Sympathetic nervous system stimulants such as cocaine and amphetamines have also been associated with diaphoresis. Diaphoresis due to ectopic catecholamine is a classic symptom of a pheochromocytoma, a rare tumor of the adrenal gland. Acetylcholinesterase inhibitors (e.g. some insecticides) also cause contraction of sweat gland smooth muscle leading to diaphoresis.\nMercury is well known for its use as a diaphoretic, and was widely used in the 19th and early 20th century by physicians to \"purge\" the body of an illness. However, due to the high toxicity of mercury, secondary symptoms would manifest, which were erroneously attributed to the former disease that was being treated with mercurials.\nInfantile acrodynia (childhood mercury poisoning) is characterized by excessive perspiration.  A clinician should immediately consider acrodynia in an afebrile child who is sweating profusely.\nSome people can develop a sweat allergy. The allergy is not due to the sweat itself but instead to an allergy-producing protein secreted by bacteria found on the skin.:\u200a52\u200a Tannic-acid has been found to suppress the allergic response along with showering.\n\nHyperhidrosis\nMillions of people are affected by hyperhidrosis, but more than half never receive treatment due to embarrassment, lack of awareness, or lack of concern. While it most commonly affects the armpits, feet, and hands, it is possible for someone to experience this condition over their whole body. The face is another common area for hyperhidrosis to be an issue. Sweating uncontrollably is not always expected and may be embarrassing to people with the condition. It can cause both physiological and emotional problems in patients. It is generally inherited. It is not life-threatening, but it is threatening to a person's quality of life. Treatments for hyperhidrosis include antiperspirants, iontophoresis, and surgical removal of sweat glands. In severe cases, botulinum toxin injections or surgical cutting of nerves that stimulate the excessive sweating (endoscopic thoracic sympathectomy) may be an option.\n\nNight sweats\nNight sweats, also known as nocturnal hyperhidrosis, is the occurrence of excessive sweating during sleep. The person may or may not also perspire excessively while awake.\nOne of the most common causes of night sweats in women over 40 is the hormonal changes related to menopause and perimenopause. This is a very common occurrence during the menopausal transition years.\nWhile night sweats might be relatively harmless, it can also be a sign of a serious underlying disease. It is important to distinguish night sweats due to medical causes from those that occur simply because the sleep environment is too warm, either because the bedroom is unusually hot or because there are too many covers on the bed. Night sweats caused by a medical condition or infection can be described as \"severe hot flashes occurring at night that can drench sleepwear and sheets, which are not related to the environment\". Some of the underlying medical conditions and infections that cause these severe night sweats can be life-threatening and should promptly be investigated by a medical practitioner.\n\nMechanism\nSweating allows the body to regulate its temperature. Sweating is controlled from a center in the preoptic and anterior regions of the brain's hypothalamus, where thermosensitive neurons are located. The heat-regulatory function of the hypothalamus is also affected by inputs from temperature receptors in the skin. High skin temperature reduces the hypothalamic set point for sweating and increases the gain of the hypothalamic feedback system in response to variations in core temperature. Overall, however, the sweating response to a rise in hypothalamic ('core') temperature is much larger than the response to the same increase in average skin temperature.\nSweating causes a decrease in core temperature through evaporative cooling at the skin surface. As high energy molecules evaporate from the skin, releasing energy absorbed from the body, the skin and superficial vessels decrease in temperature. Cooled venous blood then returns to the body's core and counteracts rising core temperatures.\nThere are two situations in which the nerves will stimulate the sweat glands, causing perspiration: during physical heat and during emotional stress. In general, emotionally induced sweating is restricted to palms, soles, armpits, and sometimes the forehead, while physical heat-induced sweating occurs throughout the body.\nPeople have an average of two to four million sweat glands, but how much sweat is released by each gland is determined by many factors, including sex, genetics, environmental conditions, age and fitness level. Two of the major contributors to sweat rate are an individual's fitness level and weight. If an individual weighs more, sweat rate is likely to increase because the body must exert more energy to function and there is more body mass to cool down.  On the other hand, a fit person will start sweating earlier and more readily. As someone becomes fit, the body becomes more efficient at regulating the body's temperature and sweat glands adapt along with the body's other systems.\nHuman sweat is not pure water; though it contains no protein, it always contains a small amount (0.2\u20131%) of solute. When a person moves from a cold climate to a hot climate, adaptive changes occur in the sweating mechanisms of the person. This process is referred to as acclimatization: the maximum rate of sweating increases and its solute composition decreases. The volume of water lost in sweat daily is highly variable, ranging from 100 to 8,000 millilitres per day (0.041 to 3.259 imp fl oz\/ks). The solute loss can be as much as 350 mmol\/d (or 90 mmol\/d acclimatised) of sodium under the most extreme conditions. During average intensity exercise, sweat losses can average up to 2 litres (0.44 imp gal; 0.53 US gal) of water\/hour.  In a cool climate and in the absence of exercise, sodium loss can be very low (less than 5 mmol\/d). Sodium concentration in sweat is 30\u201365 mmol\/L, depending on the degree of acclimatisation.\nHorses have a thick, waterproofed, hairy coat that would normally block the rapid translocation of sweat water from the skin to the surface of the hair required for evaporative cooling. To solve this, horses have evolved a detergent-like protein, latherin, that they release at high concentrations in their sweat.  Their perspiration unlike humans is created by apocrine glands. This protein, by wetting the horses' coat hairs facilitate water flow for cooling evaporation. The presence of this protein can be seen in the lathering that often occurs on the coats of sweating horses, especially when rubbed. In hot conditions, horses during three hours of moderate-intensity exercise can lose 30 to 35 litres (6.6 to 7.7 imp gal; 7.9 to 9.2 US gal) of water and 100 grams (3.5 oz) of sodium, 198 grams (7.0 oz) of chloride and 45 grams (1.6 oz) of potassium.\n\nComposition\nSweat is mostly water. A microfluidic model of the eccrine sweat gland provides details on what solutes partition into sweat, their mechanisms of partitioning, and their fluidic transport to the skin surface.\nDissolved in the water are trace amounts of minerals, lactic acid, and urea. Although the mineral content varies, some measured concentrations are: sodium (0.9 gram\/litre), potassium (0.2 g\/L), calcium (0.015 g\/L), and magnesium (0.0013 g\/L).\nRelative to the plasma and extracellular fluid, the concentration of Na+ ions is much lower in sweat (~40 mM in sweat versus ~150 mM in plasma and extracellular fluid). Initially, within eccrine glands sweat has a high concentration of Na+ ions. In the sweat ducts, the Na+ ions are re-absorbed into tissue by epithelial sodium channels (ENaC) that are located on the apical membrane of epithelial cells that form the duct (see Fig. 9 of the reference).\nMany other trace elements are also excreted in sweat, again an indication of their concentration is (although measurements can vary fifteenfold) zinc (0.4 milligrams\/litre), copper (0.3\u20130.8 mg\/L), iron (1 mg\/L), chromium (0.1 mg\/L), nickel (0.05 mg\/L), and lead (0.05 mg\/L). Probably many other less-abundant trace minerals leave the body through sweating with correspondingly lower concentrations. Some exogenous organic compounds make their way into sweat as exemplified by an unidentified odiferous \"maple syrup\" scented compound in several of the species in the mushroom genus Lactarius. In humans, sweat is hypoosmotic relative to plasma (i.e. less concentrated). Sweat is found at moderately acidic to neutral pH levels, typically between 4.5 and 7.0.\nSweat contains many glycoproteins.\n\nOther functions\nAntimicrobial\nSweat may serve an antimicrobial function, like that of earwax or other secretory fluids (e.g., tears, saliva, and milk). It does this through a combination of glycoproteins that either bind directly to, or prevent the binding of microbes to, the skin and seem to form part of the innate immune system.\nIn 2001, researchers at Eberhard-Karls University in T\u00fcbingen, Germany, isolated a large protein called dermcidin from skin. This protein, which could be cleaved into other antimicrobial peptides, was shown to be effective at killing some species of bacteria and fungi that affect humans, including Escherichia coli, Enterococcus faecalis, Staphylococcus aureus, and Candida albicans. It was active at high salt concentrations and in the acidity range of human sweat, where it was present at concentrations of 1\u201310 mg\/ml.\n\nSociety and culture\nArtificial perspiration\nArtificial skin capable of sweating similar to natural sweat rates and with the surface texture and wetting properties of regular skin has been developed for research purposes. Artificial perspiration is also available for in-vitro testing, and contains 19 amino acids and the most abundant minerals and metabolites in sweat.\n\nDiagnostics\nThere is interest in its use in wearable technology. Sweat can be sampled and sensed non-invasively and continuously using electronic tattoos, bands, or patches. However, sweat as a diagnostic fluid presents numerous challenges as well, such as very small sample volumes and filtration (dilution) of larger-sized hydrophilic analytes. Currently the only major commercial application for sweat diagnostics is for infant cystic fibrosis testing based on sweat chloride concentrations.\n\nSee also\nReferences\nFurther reading\nFerner S, Koszmagk R, Lehmann A, Heilmann W (1990). \"[Reference values of Na(+) and Cl(-) concentrations in adult sweat]\". Zeitschrift f\u00fcr Erkrankungen der Atmungsorgane (in German). 175 (2): 70\u20135. PMID 2264363.\nNadel ER, Bullard RW, Stolwijk JA (July 1971). \"Importance of skin temperature in the regulation of sweating\". Journal of Applied Physiology. 31 (1): 80\u20137. doi:10.1152\/jappl.1971.31.1.80. PMID 5556967.\nSato K, Kang WH, Saga K, Sato KT (April 1989). \"Biology of sweat glands and their disorders. I. Normal sweat gland function\". Journal of the American Academy of Dermatology. 20 (4): 537\u201363. doi:10.1016\/S0190-9622(89)70063-3. PMID 2654204.\n\nExternal links\n Media related to Perspiration at Wikimedia Commons","153":"An otoscope or auriscope is a medical device used by healthcare professionals to examine the ear canal and eardrum. This may be done as part of routine physical examinations, or for evaluating specific ear complaints, such as earaches, sense of fullness in the ear, or hearing loss.\n\nUsage\nFunction\nAn otoscope enables viewing and examination of the ear canal and tympanic membrane (eardrum). As the eardrum is the border between the external ear canal and the middle ear, its characteristics can indicate various diseases of the middle ear space. Otoscopic examination can help diagnose conditions such as acute otitis media (infection of the middle ear), otitis externa (infection of the outer ear), traumatic perforation of the eardrum, and cholesteatoma.  \nThe presence of cerumen (earwax), shed skin, pus, canal skin edema, foreign bodies, and various ear diseases, can obscure the view of the eardrum and thus compromise the value of otoscopy done with a common otoscope, but can confirm the presence of obstructing symptoms.  \nOtoscopes can also be used to examine patients' noses (avoiding the need for a separate nasal speculum) and upper throats (by removing the speculum).\n\nMethod of use\nThe most common otoscopes consist of a handle and a head. The head contains a light source and a magnifying lens, typically around 8 diopters (3\u00d7 magnification), to help illuminate and enlarge ear structures. The distal (front) end of the otoscope has an attachment for disposable plastic ear specula.   \nThe examiner first pulls on the pinna (usually the earlobe, side or top) to straighten the ear canal, and then inserts the ear speculum side of the otoscope into the outer ear. It is important to brace the index or little finger of the hand holding the otoscope against the patient's head to avoid injuring the ear canal. The examiner then looks through the lens on the rear of the instrument to see inside the ear canal.  \nIn many models, the examiner can remove the lens and insert instruments like specialized suction tips through the otoscope into the ear canal, such as for removing earwax. Most models also have an insertion point for a bulb that pushes air through the speculum (pneumatic otoscopy) for testing eardrum mobility.\n\nTypes\nMany otoscopes for doctors' offices are wall-mounted, with an electrical cord providing power from an electric outlet. Portable otoscopes powered by batteries (usually rechargeable) in the handle are also available.  \nOtoscopes are often sold with ophthalmoscopes as a diagnostic set.\n\nMonocular and binocular\nMost otoscopes used in emergency rooms, pediatric offices, general practice, and by internists are monocular devices. These provide a two-dimensional view of the ear canal and its contents, and usually at least a portion of the eardrum.   \nAnother method of performing otoscopy (visualization of the ear) is by using a binocular (two-eyed) microscope in conjunction with a larger plastic or metal ear speculum, which provides a much larger field of view. The microscope is suspended from a stand, which frees up both of the examiner's hands; the patient is placed in a supine position and their head is tilted, which keeps the head stable and enables better lighting. The binocular view enables depth perception, which makes removal of earwax or other obstructing materials easier and less hazardous. The microscope also has up to 40\u00d7 magnification, allowing more detailed viewing of the entire ear canal, and of the entire eardrum (unless prevented by edema of the canal skin). Subtle changes in the anatomy can also be more easily detected and interpreted.  \nTraditionally, binocular microscopes are only used by otolaryngologists (ear, nose, and throat specialists) and otologists (subspecialty ear doctors). Their widespread adoption in general medicine is hindered by cost and lack of familiarity among pediatric and general medicine professors in physician training programs. Studies have shown that reliance on a monocular otoscope to diagnose ear disease results in a more than 50% chance of misdiagnosis, as compared to binocular microscopic otoscopy.\n\nPneumatic otoscope\nThe pneumatic otoscope is used to examine the eardrum for assessing the health of the middle ear. This is done by assessing the eardrum's contour (normal, retracted, full, or bulging), its color (gray, yellow, pink, amber, white, red, or blue), its translucency (translucent, semi-opaque, opaque), and its mobility (normal, increased, decreased, or absent). The pneumatic otoscope is the standard tool used in diagnosing otitis media (infection of the middle ear).\nThe pneumatic otoscope has a pneumatic (diagnostic) head, which contains a lens, an enclosed light source, and a nipple for attaching a rubber bulb and tubing. By gently squeezing and releasing the bulb in rapid succession, the degree of eardrum mobility in response to positive and negative pressure can be observed. The head is designed so that an airtight chamber is produced when a speculum is attached and fitted snugly into the patient's ear canal. Using a rubber-tipped speculum or adding a small sleeve of rubber tubing at the end of a plastic speculum, can help improve the airtight seal and also help avoid injuring the patient.\nBy replacing the pneumatic head with a surgical head, the pneumatic otoscope can also be used to clear earwax from the ear canal, and to perform diagnostic tympanocentesis (drainage of fluid from the middle ear) or myringotomy (creation of incision in the eardrum). The surgical head consists of an unenclosed light source and a lens that can swivel over a wide arc.\n\nSee also\nHead mirror \u2013 Diagnostic device\nIntraoral camera \u2013 device used to take photos of the inside of a patient's mouthPages displaying wikidata descriptions as a fallback\n\nReferences\nExternal links\n\nPhisick \u2013 Pictures and information about antique otoscopes","154":"Positional alcohol nystagmus (PAN) is nystagmus (visible jerkiness in eye movement) produced when the head is placed in a sideways position. PAN occurs when the specific gravity of the membrane space of the semicircular canals in the ear differs from the specific gravity of the fluid in the canals because of the presence of alcohol. Heavy water ingestion has the opposite effect.\n\nPAN I\nWhen a person consumes alcohol, the alcohol is carried by the bloodstream and diffused into the water compartments of the body. Normally, the specific gravity of a canal membrane is the same as the specific gravity of the surrounding fluid. Because of this, even though the Earth's gravity is a constant force of acceleration, the semicircular canals do not respond to it.  Alcohol has a lighter specific gravity than water. When alcohol enters the canal membrane via capillaries, the specific gravity of the membrane is lower than that of the surrounding fluid.  The alcohol does diffuse from the membrane to the fluid, but it does so very slowly. While the specific gravity of the membrane is lower than the specific gravity of the extracellular fluid, the hair cells on the membrane become responsive to the Earth's gravity. This is the condition of PAN I.\nPAN I is characterized by a nystagmus to the left when the left side of the head is down or\/right side when the right side of the head is down.  It is typically present during a rising and peak Blood Alcohol Concentration (BAC).\n\nPAN II\nAs soon as a person starts drinking, the body begins to process and eliminate the alcohol. The rate of elimination is fairly constant. Initially, the rate of absorption exceeds the rate of elimination, which results in a rising BAC. Sometime after a person stops drinking, the rate of absorption drops below the rate of elimination, and the BAC begins falling. As alcohol is eliminated from the body, it is removed from the membrane of the semicircular canal faster than the surrounding fluid. This creates the reverse situation of PAN I, as the specific gravity of the fluid is now lower than that of the membrane. This results in PAN II.\nPAN II is characterized by a nystagmus to the right when the left side of the head is down\/left when the right side of the head is down.\n\nRelationship between PAN and the effects of intoxication\nThe overstimulation of the semicircular canals during PAN I and PAN II is associated with the unsteadiness, nausea, and vertigo felt by intoxicated people. PAN I is more associated with postural problems (e.g. standing and walking) while PAN II has been more associated with the feelings of a hangover.\nThere is a brief period between PAN I and PAN II when the alcohol concentrations in the canal membrane and extracellular fluid are in equilibrium. During this time, neither PAN I nor PAN II is present.\n\nPAN versus testing for HGN in intoxicated individuals\nHorizontal Gaze Nystagmus (HGN) testing is a common practice used by law enforcement in the United States in the identification of persons who are intoxicated or under the influence of a controlled substance. The key difference between recognizing PAN and horizontal gaze nystagmus is the position of the subject's head in relation to the body.  PAN is identified when the head is tilted to one side or the other. In order for HGN to be properly identified, the head must be positioned in line with the spine. Because of this, if the head is tilted towards the side when an evaluation for HGN is given, PAN may be induced and give a \"false positive\" for HGN. Some defendants may claim or argue that the nystagmus observed by an officer was positional and not horizontal gaze.\n\nSee also\nOptokinetic reflex\nNystagmus\n\n\n== References ==","155":"The posterior cranial fossa is the part of the cranial cavity located between the foramen magnum, and tentorium cerebelli. It is formed by the  sphenoid bones, temporal bones, and occipital bone. It lodges the cerebellum, and parts of the brainstem.\n\nAnatomy\nThe posterior cranial fossa is formed by the sphenoid bones, temporal bones, and occipital bone. It is the most inferior of the fossae. It houses the cerebellum, medulla oblongata, and pons.\n\nBoundaries\nAnteriorly, the posterior cranial fossa is bounded by the dorsum sellae, posterior aspect of the body of sphenoid bone, and the basilar part of occipital bone\/clivus.\nLaterally, it is bounded by the petrous parts and mastoid parts of the temporal bones, and the lateral parts of occipital bone.\nPosteriorly, it is bounded by the squamous part of occipital bone.\n\nFeatures\nForamen magnum\nThe foramen magnum is a large opening of the floor of the posterior cranial fossa, its most conspicuous feature.\n\nInternal acoustic meatus\nLies in the anterior wall of the posterior cranial fossa. It transmits the facial (VII) and vestibulocochlear (VIII) cranial nerves into a canal in the petrous temporal bone.\n\nJugular foramen\nLies between the inferior edge of the petrous temporal bone and the adjacent occipital bone and transmits the internal jugular vein (actually begins here), the glossopharyngeal nerve (CN IX), vagus nerve (CN X), and accessory nerve (CN XI).\n\nHypoglossal canal\nLies at the anterolateral margins of the foramen magnum and transmits the hypoglossal nerve (CN XII).\n\nOther\nAlso visible in the posterior cranial fossa are depressions caused by the venous sinuses returning blood from the brain to the venous circulation:\nRight and left transverse sinuses which meet at the confluence of sinuses (marked by the internal occipital protuberance).\nThe transverse sinuses pass horizontally from the most posterior point of the occiput.\nWhere the apex of the petrous temporal meets the squamous temporal, the transverse sinuses lead into sigmoid (S-shaped) sinuses (one on each side).\nThese pass along the articulation between the posterior edge of the petrous temporal bone and the anterior edge of the occipital bones to the jugular foramen, where the sigmoid sinus becomes the internal jugular vein.\nNote that a superior petrosal sinus enters the junction of the transverse and sigmoid sinuses. Also an inferior petrosal sinus enters the sigmoid sinus near the jugular foramen.\nThe posterior cranial fossa is formed in the endocranium, and holds the most basal parts of the brain.\n\nClinical significance\nAn underdeveloped posterior cranial fossa can cause Arnold\u2013Chiari malformation.  These can be either acquired or congenital disorders.\n\nAdditional images\nSee also\nAnterior cranial fossa\nMiddle cranial fossa\n\nReferences\nExternal links\n\nAnatomy photo:22:os-0803 at the SUNY Downstate Medical Center","156":"Posturography is the technique used to quantify postural control in upright stance in either static or dynamic conditions. Among them, Computerized dynamic posturography (CDP), also called test of balance (TOB), is a non-invasive specialized clinical assessment technique used to quantify the central nervous system adaptive mechanisms (sensory, motor and central) involved in the control of posture and balance, both in normal (such as in physical education and sports training) and abnormal conditions (particularly in the diagnosis of balance disorders and in physical therapy and postural re-education). Due to the complex interactions among sensory, motor, and central processes involved in posture and balance, CDP requires different protocols in order to differentiate among the many defects and impairments which may affect the patient's posture control system. Thus, CDP challenges it by using several combinations of visual and support surface stimuli and parameters.\nClinical applications for CDP were first described by L.M. Nashner in 1982, and the first commercially available testing system was developed in 1986, when NeuroCom International, Inc., launched the EquiTest system.\n\nWorking\nStatic posturography is carried out by placing the patient in a standing posture on a fixed instrumented platform  (forceplate) connected to sensitive detectors (force and movement transducers), which are able to detect the tiny oscillations of the body.\nDynamic posturography differentiates from static posturography generally by using a special apparatus with a movable horizontal platform.  As the patient makes small movements, they transmit in real time to a computer. The computer is also used to command electric motors which can move the forceplate in the horizontal direction (translation) as well as to incline it (rotations). Thus, the posturography test protocols generate a sequence of standardized motions in the support platform in order to disequilibrate the patient's posture in an orderly and reproducible way. The platform is contained within an enclosure which can also be used to generate apparent visual surround motions. These stimuli are calibrated relative to the patient's height and weight. A special computer software integrates all this and produces detailed graphics and reports which can then be compared with normal ranges.\n\nComponents of balance\nCenter of gravity (COG) is an important component of balance and should be assessed when evaluating someone\u2019s posture. COG is often measured with COP (Center of pressure) because COG is hard to quantify. According to Lafage et al. (2008) the COG should be located at the midpoint of the base of support if an individual has ideal posture. COP excursion and velocity are indicators of control over COG and are key factors for identifying proper posture and the ability to maintain balance. COP excursion is defined by Collins & De Luca (1992) as the Euclidean*LINK* displacement in the anterior\/posterior and medial\/lateral directions within the base of support (perimeter around the feet).  With poor posture and \/ or exaggerated spinal curvatures it is possible that the COP excursion would increase which can cause instability as the COP shifts towards the perimeter of the base of support.\n\nTypes of tests\nThe test protocols usually include a Sensory Organization Test (SOT), Limits of Stability Test (LOS), a Motor Control Test (MCT) and an Adaptation Test (ADT). The SOT test was developed by Nashner and is a computerized system that is made up of dual movable force plates and movable visual screen (EquiTest).   During the test the patient is instructed to stand still and quietly with eyes open or closed depending on which of the six tests is being administered. The patient performs multiple trials per test; a description of these tests can be found in the table below.   The SOT test is based on the fact that there are three sensory systems mainly involved in maintaining balance (vision, vestibular, and proprioceptive). Minute spontaneous body sways are measured as well as reactions provoked by unexpected abrupt movements of the platform and the visual surroundings. Differences in these sways and reactions to system perturbations help to determine the patients ability to effectively use visual, vestibular, and proprioceptive input to maintain posture. Wrisley et al. (2007) found that there are learning effects associated with the SOT test and therefore it could be used clinically to assess, improve and track changes in balance.\n\nSOT results are subdivided in an Equilibrium Score, a Sensory Analysis, a Strategy Analysis and COG Alignment. \nThe sensory analysis calculates 4 different scores: somatosensory (SOM), visual (VIS), vestibular (VEST) and visual preference (PREF) (otherwise known as \"visual dependence\", an excessive reliance on visual information even when it is inappropriate). The scores are respectively calculated as ratios of the 6 different scores of the equilibrium score:\n\n  \n    \n      \n        \n          Somatosensory (SOM)\n        \n        =\n        \n          \n            condition 2\n            condition 1\n          \n        \n      \n    \n    {\\displaystyle {\\text{Somatosensory (SOM)}}={\\frac {\\text{condition 2}}{\\text{condition 1}}}}\n  \n\n  \n    \n      \n        \n          Visual (VIS)\n        \n        =\n        \n          \n            condition 4\n            condition 1\n          \n        \n      \n    \n    {\\displaystyle {\\text{Visual (VIS)}}={\\frac {\\text{condition 4}}{\\text{condition 1}}}}\n  \n\n  \n    \n      \n        \n          Vestibular (VEST)\n        \n        =\n        \n          \n            condition 5\n            condition 1\n          \n        \n      \n    \n    {\\displaystyle {\\text{Vestibular (VEST)}}={\\frac {\\text{condition 5}}{\\text{condition 1}}}}\n  \n\n  \n    \n      \n        \n          Visual Preference (PREF)\n        \n        =\n        \n          \n            conditions 3 + 6\n            conditions 2 + 5\n          \n        \n      \n    \n    {\\displaystyle {\\text{Visual Preference (PREF)}}={\\frac {\\text{conditions 3 + 6}}{\\text{conditions 2 + 5}}}}\n  \n\nMCT results include instead the Weight Symmetry, both for forward and for backward translations, Latency Scores for forward and backward translations, and Amplitude Scaling, which refers to the capacity of the participant to generate a response force adequate to the entity of the perturbation.\nThe limits of stability (LOS) is defined as the distance outside the base of support that can be traveled before a loss of balance occurs. The LOS test is frequently used to quantify this distance and has been suggested as a hybrid between static and dynamic balance assessment.   During this test the patient stands on the platform as directed above in the SOT test. The patient watches their movements on a screen so they can see each of the eight LOS targets. The patient begins with their COP directly in the center of the targets (displayed as a figure as a computerized person). At the onset of the test, the patient attempts to lean in the direction of the indicated perimeter target, without lifting their feet, and hold there until the test is complete.\nAccording to necessity of the diagnostic workup, CDP can be combined with other techniques, such as electronystagmography (ENG) and electromyography.\nThe main indications for CDP are dizziness and vertigo, and postural imbalances (balance disorders).\n\nSee also\nBalance disorders\nVestibular system\n\nSources\nNashner LM et al. Adaptation to altered support and visual conditions during stance: patients with vestibular deficits. J Neurosci. 1982 May;2(5):536-44. Medline abstract\nMonsell EM, et al. Technology assessment: computerized dynamic platform posturography\". Otolarynogol Head Neck Surg 1997, 117:394-398. Medline abstract\nGoebel, JA (Editor). Practical Management of the Dizzy Patient. Lippincott Williams & Wilkins Publ. 2000.\n\nReferences\nExternal links\nComputerized Dynamic Posturography","157":"Presbycusis (also spelled presbyacusis, from Greek \u03c0\u03c1\u03ad\u03c3\u03b2\u03c5\u03c2 presbys \"old\" + \u1f04\u03ba\u03bf\u03c5\u03c3\u03b9\u03c2 akousis \"hearing\"), or age-related hearing loss, is the cumulative effect of aging on hearing. It is a progressive and irreversible bilateral symmetrical age-related sensorineural hearing loss resulting from degeneration of the cochlea or associated structures of the inner ear or auditory nerves. The hearing loss is most marked at higher frequencies. Hearing loss that accumulates with age but is caused by factors other than normal aging (nosocusis and sociocusis) is not presbycusis, although differentiating the individual effects of distinct causes of hearing loss can be difficult.\nThe cause of presbycusis is a combination of genetics, cumulative environmental exposures and pathophysiological changes related to aging. At present there are no preventive measures known; treatment is by hearing aid or surgical implant.\nPresbycusis is the most common cause of hearing loss, affecting one out of three persons by age 65, and one out of two by age 75. Presbycusis is the second most common illness next to arthritis in aged people.\nMany vertebrates such as fish, birds and amphibians do not experience presbycusis in old age as they are able to regenerate their cochlear sensory cells, whereas mammals including humans have genetically lost this regenerative ability.\n\nPresentation\nPrimary symptoms:\n\nsounds or speech becoming dull, muffled or attenuated\nneed for increased volume on television, radio, music and other audio sources\ndifficulty using the telephone\nloss of directionality of sound\ndifficulty understanding speech, especially women and children\ndifficulty in speech discrimination against background noise (cocktail party effect)\nSecondary symptoms:\n\nhyperacusis, heightened sensitivity to certain volumes and frequencies of sound, resulting from \"recruitment\"\ntinnitus, ringing, buzzing, hissing or other sounds in the ear when no external sound is present\nUsually occurs after age 50, but deterioration in hearing has been found to start very early, from about the age of 18 years. The ISO standard 7029 shows expected threshold changes due purely to age for carefully screened populations (i.e. excluding those with ear disease, noise exposure etc.), based on a meta-analysis of published data. Age affects high frequencies more than low, and men more than women. One early consequence is that even young adults may lose the ability to hear very high frequency tones above 15 or 16 kHz. Despite this, age-related hearing loss may only become noticeable later in life. The effects of age can be exacerbated by exposure to environmental noise, whether at work or in leisure time (shooting, music, etc.). This is noise-induced hearing loss (NIHL) and is distinct from presbycusis. A second exacerbating factor is exposure to ototoxic drugs and chemicals.\nOver time, the detection of high-pitched sounds becomes more difficult, and speech perception is affected, particularly of sibilants and fricatives. Patients typically express a decreased ability to understand speech. Once the loss has progressed to the 2\u20134 kHz range, there is increased difficulty understanding consonants. Both ears tend to be affected. The impact of presbycusis on communication depends on both the severity of the condition and the communication partner.\nOlder adults with presbycusis often exhibit associated symptoms of social isolation, depression, anxiety, frailty and cognitive decline.\nThe risk of having cognitive impairment increased 7 percent for every 10 dB of hearing loss at baseline. No effect of hearing aids was seen in the Lin Baltimore study.\n\nCauses\nChanges in the inner ear, middle ear, and complex changes along the nerve pathways from the ear to the brain can affect hearing. Long-term exposure to noise and some medical conditions can also play a role. In addition, new research suggests that certain genes make some people more susceptible to hearing loss as they age. Other risk factors include preexisting noise-induced hearing loss and exposure to ototoxic medications.\n\nPathophysiology\nThere are four pathological phenotypes of presbycusis:\n\nSensory: characterised by degeneration of the organ of Corti, the sensory organ for hearing. Located within the scala media, it contains inner and outer hair cells with stereocilia. The outer hair cells play a significant role in the amplification of sound. Age-related hair cell degeneration is characterized by loss of stereocilia, shrinkage of hair cell soma, and reduction in outer hair cell mechanical properties, suggesting that functional decline in mechanotransduction and cochlear amplification precedes hair cell loss and contributes to age-related hearing loss.  At the molecular level, hair cell aging is associated with key molecular processes, including transcriptional regulation, DNA damage\/repair, autophagy, and inflammatory response, as well as those related to hair cell unique morphology and function. A 2020 study suggests that the main cause of presbycusis is the loss of inner ear sensory cellsand that the main cause of this loss is noise exposure. \n\nNeural: characterised by degeneration of cells of the spiral ganglion.\nStrial\/metabolic: characterised by atrophy of stria vascularis in all turns of cochlea. Located in the lateral wall of the cochlea, the stria vascularis contains sodium-potassium-ATPase pumps that are responsible for producing the endolymph resting potential. As individuals age, a loss of capillaries leads to the endolymphatic potential becoming harder to maintain, which brings a decrease in cochlear potential.\nCochlear conductive: due to stiffening of the basilar membrane thus affecting its movement.  This type of pathology has not been verified as contributing to presbycusis.\nIn addition there are two other types:\n\nMixed\nIndeterminate\nThe shape of the audiogram categorizes abrupt high-frequency loss (sensory phenotype) or flat loss (strial phenotype).\n\nClassically, audiograms in neural presbycusis show a moderate downward slope into higher frequencies with a gradual worsening over time. A severe loss in speech discrimination is often described, out of proportion to the threshold loss, making amplification difficult due to poor comprehension.\nThe audiogram associated with sensory presbycusis is thought to show a sharply sloping high-frequency loss extending beyond the speech frequency range, and clinical evaluation reveals a slow, symmetric, and bilateral progression of hearing loss.\n\nDiagnosis\nHearing loss is classified as mild, moderate, severe or profound. Pure-tone audiometry for air conduction thresholds at 250, 500, 1000, 2000, 4000, 6000 and 8000 Hz is traditionally used to classify the degree of hearing loss in each ear.  Normal hearing thresholds are considered to be 25 dB sensitivity, though it has been proposed that this threshold is too high, and that 15 dB (about half as loud) is more typical.  Mild hearing loss is thresholds of 25\u201345 dB; moderate hearing loss is thresholds of 45\u201365 dB; severe hearing loss is thresholds of 65\u201385 dB; and profound hearing loss thresholds are greater than 85 dB.\nTinnitus occurring in only one ear should prompt the clinician to initiate further evaluation for other etiologies. In addition, the presence of a pulse-synchronous rushing sound may require additional imaging to exclude vascular disorders.\n\nOtoscopy\nAn examination of the external ear canal and tympanic membrane performed by a medical doctor, otolaryngologist, or audiologist using an otoscope, a visual instrument inserted into the ear. This also allows some inspection of the middle ear through the translucent tympanic membrane.\n\nTympanometry\nA test administered by a medical doctor, otolaryngologist or audiologist of the tympanic membrane and middle ear function using a tympanometer, an air-pressure\/sound wave instrument inserted into the ear canal.  The result is a tympanogram showing ear canal volume, middle ear pressure and eardrum compliance. Normal middle ear function (Type A tympanogram) with a hearing loss may suggest presbycusis. Type B and Type C tympanograms indicate an abnormality inside the ear and therefore may have an additional effect on the hearing.\n\nLaboratory studies\nThis may include a blood or other sera test for inflammatory markers such as those for autoinflammatory diseases.\n\nAudiometry\nA hearing test administered by a medical doctor, otolaryngologist (ENT) or audiologist including pure tone audiometry and speech recognition may be used to determine the extent and nature of hearing loss, and distinguish presbycusis from other kinds of hearing loss. Otoacoustic emissions and evoked response testing may be used to test for audio neuropathy. The diagnosis of a sensorineural pattern hearing loss is made through audiometry, which shows a significant hearing loss without the \"air-bone gap\" that is characteristic of conductive hearing disturbances. In other words, air conduction is equal to bone conduction. Persons with cochlear deficits fail otoacoustic emissions testing, while persons with 8th cranial nerve (vestibulocochlear nerve) deficits fail auditory brainstem response testing.\n\nPresbycusis audiogram\nMagnetic resonance imaging (MRI)\nAs part of differential diagnosis, an MRI scan may be done to check for vascular anomalies, tumors, and structural problems like enlarged mastoids. MRI and other types of scan cannot directly detect or measure age-related hearing loss.\n\nTreatment\nAt present, presbycusis, being primarily sensorineural in nature, cannot be prevented, ameliorated or cured.  Treatment options fall into three categories: pharmacological, surgical and management.\n\nThere are no approved or recommended pharmaceutical treatments for presbycusis.\n\nCochlear implant\nIn cases of severe or profound hearing loss, a surgical cochlear implant is possible. This is an electronic device that replaces the cochlea of the inner ear.  Electrodes are typically inserted through the round window of the cochlea, into the fluid-filled scala tympani. They stimulate the peripheral axons of the primary auditory neurons, which then send information to the brain via the auditory nerve. The cochlea is tonotopically mapped in a spiral fashion, with lower frequencies localizing at the apex of the cochlea, and high frequencies at the base of the cochlea, near the oval and round windows. With age, comes a loss in distinction of frequencies, especially higher ones. The electrodes of the implant are designed to stimulate the array of nerve fibers that previously responded to different frequencies accurately. Due to spatial constraints, the cochlear implant may not be inserted all the way into the cochlear apex. It provides a different kind of sound spectrum than natural hearing, but may enable the recipient to recognize speech and environmental sounds.\n\nMiddle ear implants\nThese are surgically implanted hearing aids inserted onto the middle ear. These aids work by directly vibrating the ossicles, and are cosmetically favorable due to their hidden nature.\n\nManagement\nHearing aids help improve hearing of many elderly.  Hearing aids can now be tuned to specific frequency ranges of hearing loss.\nAural rehabilitation for the affected person and their communication partners may reduce the impact on communication. Techniques such as squarely facing the affected person, enunciating, ensuring adequate light, minimizing noise in the environment, and using contextual cues are used to improve comprehension.\n\nPopular culture\nAbilities of young people to hear high frequency tones inaudible to those over 25 or so has led to the development of technologies to disperse groups of young people around shops (The Mosquito), and development of a cell phone ringtone, Teen Buzz, for students to use in school, that older people cannot hear. In September 2006 this technique was used to make a dance track called 'Buzzin'. The track had two melodies, one that everyone could hear and one that only younger people could hear.\n\nAnimals\nMany vertebrates such as fish, birds and amphibians do not experience presbycusis in old age as they are able to regenerate their cochlear sensory cells, whereas mammals including humans have genetically lost this ability. A number of laboratories worldwide are conducting comparative studies of birds and mammals that aim to find the differences in regenerative capacity, with a view to developing new treatments for human hearing problems.\n\nSee also\nPresbyopia \u2013 age-related degeneration of the eyes\n\nReferences\n\n\n== External links ==","158":"Lightheadedness is a common and typically unpleasant sensation of dizziness or a feeling that one may faint. The sensation of lightheadedness can be short-lived, prolonged, or, rarely, recurring. In addition to dizziness, the individual may feel as though their head is weightless. The individual may also feel as though the room is \"spinning\" or moving (vertigo). Most causes of lightheadedness are not serious and either cure themselves quickly or are easily treated.\nKeeping a sense of balance requires the brain to process a variety of information received from the eyes, the nervous system, and the inner ears. If the brain is unable to process these signals, such as when the messages are contradictory, or if the sensory systems are improperly functioning, an individual may experience lightheadedness or dizziness.\nLightheadedness is very similar to pre-syncope. Pre-syncope is the immediate stage before syncope (fainting), particularly in cases of temporary visual field loss (i.e. vision getting \"dark\" or \"closing in\").\n\nCauses\nLightheadedness can be simply (and most commonly) an indication of a temporary shortage of blood or oxygen to the brain due to a drop in blood pressure, rapid dehydration from vomiting, diarrhea, or fever. Other causes are: altitude sickness, low blood sugar, hyperventilation, postural orthostatic tachycardia syndrome (increase in heart rate upon sitting up or standing), panic attacks, and anemia. It can also be a symptom of many other conditions, some of them serious, such as heart problems (including abnormal heart rhythm or heart attack), respiratory problems such as pulmonary hypertension or pulmonary embolism, and also stroke, bleeding, and shock. If any of these serious disorders are present, the individual will usually have additional symptoms such as chest pain, a feeling of a racing heart, loss of speech or a change in vision.\nMany people, especially as they age, experience lightheadedness if they arise too quickly from a lying or seated position. Lightheadedness often accompanies the flu, hypoglycaemia, common cold, or allergies.\nDizziness could be provoked by the use of antihistamine drugs, like levocetirizine, or by some antibiotics or SSRIs. Nicotine or tobacco products can cause lightheadedness for inexperienced users. Narcotic drugs, such as codeine, can also cause lightheadedness.\n\nTreatment\nTreatment for lightheadedness depends on the cause or underlying problem. Treatment may include drinking plenty of water or other fluids (unless the lightheadedness is the result of water intoxication in which case drinking water is quite dangerous). If a patient is unable to keep fluids down from nausea or vomiting, they may need intravenous fluids such as Ringer's lactate solution. They should try eating something sugary and lying down or sitting and reducing the elevation of the head relative to the body (for example, by positioning the head between the knees).\nOther simple remedies include avoiding sudden changes in posture when sitting or lying and avoiding bright lights.\nSeveral essential electrolytes are excreted when the body perspires. When people experience unusual or extreme heat for a long time, sweating excessively can cause a lack of some electrolytes, which in turn can cause lightheadedness.\n\nSee also\nOrthostatic hypotension\nDizziness\nVertigo\nVasovagal response\nClouding of consciousness\nGreyout\n\nReferences\nExternal links\n\nThe MedlinePlus Medical Encyclopedia","159":"Pure-tone audiometry is the main hearing test used to identify hearing threshold levels of an individual, enabling determination of the degree, type and configuration of a hearing loss and thus providing a basis for diagnosis and management. Pure-tone audiometry is a subjective, behavioural measurement of  a hearing threshold, as it relies on patient responses to pure tone stimuli. Therefore, pure-tone audiometry is only used on adults and children old enough to cooperate with the test procedure. As with most clinical tests, standardized calibration of the test environment, the equipment and the stimuli is needed before testing proceeds (in reference to ISO, ANSI, or other standardization body). Pure-tone audiometry only measures audibility thresholds, rather than other aspects of hearing such as sound localization and speech recognition. However, there are benefits to using pure-tone audiometry over other forms of hearing test, such as click auditory brainstem response (ABR). Pure-tone audiometry provides ear specific thresholds, and uses frequency specific pure tones to give place specific responses, so that the configuration of a hearing loss can be identified. As pure-tone audiometry uses both air and bone conduction audiometry, the type of loss can also be identified via the air-bone gap. Although pure-tone audiometry has many clinical benefits, it is not perfect at identifying all losses, such as \u2018dead regions\u2019 of the cochlea and neuropathies such as auditory processing disorder (APD). This raises the question of whether or not audiograms accurately predict someone's perceived degree of disability.\n\nPure-tone audiometry procedural standards\nThe current International Organization for Standardization (ISO) standard for pure-tone audiometry is ISO:8253-1, which was first published in 1983. The current American National Standards Institute (ANSI) standard for pure-tone audiometry is ANSI\/ASA S3.21-2004, prepared by the Acoustical Society of America.\nIn the United Kingdom, The British Society of Audiology (BSA) is responsible for publishing the recommended procedure for pure-tone audiometry, as well as many other audiological procedures. The British recommended procedure is based on international standards. Although there are some differences, the BSA-recommended procedures are in accordance with the ISO:8253-1 standard. The BSA-recommended procedures provide a \"best practice\" test protocol for professionals to follow, increasing validity and allowing standardisation of results across Britain.\nIn the United States, the American Speech\u2013Language\u2013Hearing Association (ASHA) published Guidelines for Manual Pure-Tone Threshold Audiometry in 2005.\n\nVariations\nThere are cases where conventional pure-tone audiometry is not an appropriate or effective method of threshold testing. Procedural changes to the conventional test method may be necessary with populations who are unable to cooperate with the test in order to obtain hearing thresholds. Sound field audiometry may be more suitable when patients are unable to wear earphones, as the stimuli are usually presented by loudspeaker.  A disadvantage of this method is that although thresholds can be obtained, results are not ear specific.  In addition, response to pure tone stimuli may be limited, because in a sound field pure tones create standing waves, which alter sound intensity within the sound field. Therefore, it may be necessary to use other stimuli, such as warble tones in sound field testing. There are variations of conventional audiometry testing that are designed specifically for young children and infants, such as behavioral observation audiometry, visual reinforcement audiometry and play audiometry.\nConventional audiometry tests frequencies between 250 hertz (Hz) and 8 kHz, whereas high frequency audiometry tests in the region of 8 kHz-16 kHz. Some environmental factors, such as ototoxic medication and noise exposure, appear to be more detrimental to high frequency sensitivity than to that of mid or low frequencies. Therefore, high frequency audiometry is an effective method of monitoring losses that are suspected to have been caused by these factors. It is also effective in detecting the auditory sensitivity changes that occur with aging.\n\nCross hearing and interaural attenuation\nWhen sound is applied to one ear the contralateral cochlea can also be stimulated to varying degrees, via vibrations through the bone of the skull. When the stimuli presented to the test ear stimulates the cochlea of the non-test ear, this is known as cross hearing. Whenever it is suspected that cross hearing has occurred it is best to use masking. This is done by temporarily elevating the threshold of the non-test ear, by presenting a masking noise at a predetermined level. This prevents the non-test ear from detecting the test signal presented to the test ear. The threshold of the test ear is measured at the same time as presenting the masking noise to the non-test ear. Thus, thresholds obtained when masking has been applied, provide an accurate representation of the true hearing threshold level of the test ear.\nA reduction or loss of energy occurs with cross hearing, which is referred to as interaural attenuation (IA) or transcranial transmission loss. IA varies with transducer type. It varies from 40 dB to 80 dB with supra-aural headphones. However, with insert earphones it is in the region of 55 dB. The use of insert earphones reduces the need for masking, due to the greater IA which occurs when they are used (See Figure 1).\nAir conduction results in isolation, give little information regarding the type of hearing loss. When the thresholds obtained via air conduction are examined alongside those achieved with bone conduction, the configuration of the hearing loss can be determined. However, with  bone conduction (performed by placing a vibrator on the mastoid bone behind the ear), both cochleas are stimulated. IA for bone conduction ranges from 0-20 dB (See Figure 2). Therefore, conventional audiometry is ear specific, with regards to both air and bone conduction audiometry, when masking is applied.\n\nPure-tone audiometry thresholds and hearing disability\nPure-tone audiometry is described as the gold standard for assessment of a hearing loss  but how accurate pure-tone audiometry is at classifying the hearing loss of an individual, in terms of hearing impairment and hearing disability is open to question. Hearing impairment is defined by the World Health Organization (WHO) as a hearing loss with thresholds higher than 25db in one or both ears. The degree of hearing loss is classified as mild, moderate, severe or profound. The results of pure-tone audiometry are however a very good indicator of hearing impairment.\nHearing disability is defined by the WHO as a reduction in the ability to hear sounds in both quiet and noisy environments (compared to people with normal hearing), which is caused by a hearing impairment. Several studies have investigated whether self-reported hearing problems (via questionnaires and interviews) were associated with the results from pure-tone audiometry. The findings of these studies indicate that in general, the results of pure-tone audiometry correspond to self-reported hearing problems (i.e. hearing disability). However, for some individuals this is not the case; the results of pure-tone audiometry only, should not  be used to ascertain an individual's hearing disability.\n\nHearing impairment (based on the audiogram) and auditory handicap (based on speech discrimination in noise) data was reviewed by Reinier Plomp . This led to the formulation of equations, which described the consequences of a hearing loss on speech intelligibility. The results of this review indicated that there were two factors of a hearing loss, which were involved in the effect on speech intelligibility. These factors were named Factor A and Factor D. Factor A affected speech intelligibility by attenuating the speech, whereas Factor D affected speech intelligibility by distorting the speech.\nSpeech recognition threshold (SRT) is defined as the sound pressure level at which 50% of the speech is identified correctly. For a person with a conductive hearing loss (CHL) in quiet, the SRT needs to be higher than for a person with normal hearing. The increase in SRT depends on the degree of hearing loss only, so Factor A reflects the audiogram of that person. In noise, the person with a CHL has the same problem as the person with normal hearing (See Figure 10).\nFor a person with a Sensorineural hearing loss (SNHL) in quiet, the SRT also needs to be higher than for a person with normal hearing. This is because the only factor that is important in quiet for a CHL and a SNHL is the audibility of the sound, which corresponds to Factor A. In noise, the person with a SNHL requires a better signal-to-noise ratio to achieve the same performance level, as the person with normal hearing and the person with a CHL. This shows that in noise, Factor A is not enough to explain the problems of a person with a SNHL. Therefore, there is another problem present, which is Factor D. At present, it is not known what causes Factor D. Thus, in noise the audiogram is irrelevant. It is the type of hearing loss that is important in this situation.\nThese findings have important implications for the design of hearing aids. As hearing aids at present can compensate for Factor A, but this is not the case for Factor D. This could be why hearing aids are not satisfactory for a lot of people.\n\nAudiograms and hearing loss\nThe shape of the audiogram resulting from pure-tone audiometry gives an indication of the type of hearing loss as well as possible causes.  Conductive hearing loss due to disorders of the middle ear shows as a flat increase in thresholds across the frequency range.  Sensorineural hearing loss will have a contoured shape depending on the cause.  Presbycusis or age-related hearing loss for example is characterized by a high frequency roll-off (increase in thresholds).  Noise-induced hearing loss has a characteristic notch at 4000 Hz.  Other contours may indicate other causes for the hearing loss.\n\nSee also\nHearing range\nAuditory masking\nAuditory filters\nAbsolute threshold of hearing\nEqual-loudness contours\nPure tone\n\nReferences\nExternal links\nEMedicine.com.\nPure Tone Audiometry: What is Pure Tone Average (PTA) Test?.\nSpringerlink.com.\nAudition Cochlea Promenade oreille ear organ Corti C.R.I.C Montpellier. This website provides excellent diagrams and animated pictures that aid understanding of the topics covered. There are a wide range of topics covered including sound, cochlea, Organ of Corti, hair cell pathology and audiometry.\nAudiology Resources. This site provides useful resources for people interested in audiology.\n[1] This site provides excellent information regarding the Audiometric Testing procedure\nWorld Hearing Day 2019- Check your Hearing materials and hearing test app","160":"Proprioception ( PROH-pree-oh-SEP-sh\u0259n, -\u2060\u0259-) is the sense of self-movement, force, and body position.\nProprioception is mediated by proprioceptors, sensory receptors, located within muscles, tendons, and joints. Most animals possess multiple subtypes of proprioceptors, which detect distinct kinesthetic parameters, such as joint position, movement, and load. Although all mobile animals possess proprioceptors, the structure of the sensory organs can vary across species.\nProprioceptive signals are transmitted to the central nervous system, where they are integrated with information from other sensory systems, such as the visual system and the vestibular system, to create an overall representation of body position, movement, and acceleration. In many animals, sensory feedback from proprioceptors is essential for stabilizing body posture and coordinating body movement.\n\nSystem overview\nIn vertebrates, limb movement and velocity (muscle length and the rate of change) are encoded by one group of sensory neurons (type Ia sensory fiber) and another type encode static muscle length (group II neurons). These two types of sensory neurons compose muscle spindles. There is a similar division of encoding in invertebrates; different subgroups of neurons of the chordotonal organ encode limb position and velocity.\nTo determine the load on a limb, vertebrates use sensory neurons in the Golgi tendon organs: type Ib afferents. These proprioceptors are activated at given muscle forces, which indicate the resistance that muscle is experiencing. Similarly, invertebrates have a mechanism to determine limb load: the campaniform sensilla. These proprioceptors are active when a limb experiences resistance.\nA third role for proprioceptors is to determine when a joint is at a specific position. In vertebrates, this is accomplished by Ruffini endings and Pacinian corpuscles. These proprioceptors are activated when the joint is at a threshold position, usually at the extremes of joint position. Invertebrates use hair plates to accomplish this; a field of bristles located within joints that detects the relative movement of limb segments through the deflection of the associated cuticular hairs.\n\nReflexes\nThe sense of proprioception is ubiquitous across mobile animals and is essential for the motor coordination of the body. Proprioceptors can form reflex circuits with motor neurons to provide rapid feedback about body and limb position. These mechanosensory circuits are important for flexibly maintaining posture and balance, especially during locomotion. For example, consider the stretch reflex, in which stretch across a muscle is detected by a sensory receptor (e.g., muscle spindle, chordotonal neurons), which activates a motor neuron to induce muscle contraction and oppose the stretch. During locomotion, sensory neurons can reverse their activity when stretched, to promote rather than oppose movement.\n\nConscious and nonconscious\nIn humans, a distinction is made between conscious proprioception and nonconscious proprioception:\n\nConscious proprioception is communicated by the dorsal column-medial lemniscus pathway to the cerebrum.\nNonconscious proprioception is communicated primarily via the dorsal spinocerebellar tract and ventral spinocerebellar tract, to the cerebellum.\nA nonconscious reaction is seen in the human proprioceptive reflex, or righting reflex\u2014in the event that the body tilts in any direction, the person will cock their head back to level the eyes against the horizon. This is seen even in infants as soon as they gain control of their neck muscles. This control comes from the cerebellum, the part of the brain affecting balance.\n\nMechanisms\nProprioception is mediated by mechanically sensitive proprioceptor neurons distributed throughout an animal's body. Most vertebrates possess three basic types of proprioceptors: muscle spindles, which are embedded in skeletal muscles, Golgi tendon organs, which lie at the interface of muscles and tendons, and joint receptors, which are low-threshold mechanoreceptors embedded in joint capsules. Many invertebrates, such as insects, also possess three basic proprioceptor types with analogous functional properties: chordotonal neurons, campaniform sensilla, and hair plates.\nThe initiation of proprioception is the activation of a proprioceptor in the periphery. The proprioceptive sense is believed to be composed of information from sensory neurons located in the inner ear (motion and orientation) and in the stretch receptors located in the muscles and the joint-supporting ligaments (stance). There are specific nerve receptors for this form of perception termed \"proprioceptors\", just as there are specific receptors for pressure, light, temperature, sound, and other sensory experiences. Proprioceptors are sometimes known as adequate stimuli receptors.\nMembers of the transient receptor potential family of ion channels have been found to be important for proprioception in fruit flies, nematode worms, African clawed frogs, and zebrafish. PIEZO2, a nonselective cation channel, has been shown to underlie the mechanosensitivity of proprioceptors in mice. Humans with loss-of-function mutations in the PIEZO2 gene exhibit specific deficits in joint proprioception, as well as vibration and touch discrimination, suggesting that the PIEZO2 channel is essential for mechanosensitivity in some proprioceptors and low-threshold mechanoreceptors.\nAlthough it was known that finger kinesthesia relies on skin sensation, recent research has found that kinesthesia-based haptic perception relies strongly on the forces experienced during touch. This research allows the creation of \"virtual\", illusory haptic shapes with different perceived qualities.\n\nAnatomy\nProprioception of the head stems from the muscles innervated by the trigeminal nerve, where the general somatic afferent fibers pass without synapsing in the trigeminal ganglion (first-order sensory neuron), reaching the mesencephalic tract and the mesencephalic nucleus of trigeminal nerve. Proprioception of limbs often occurs due to receptors in connective tissue near joints.\n\nFunction\nStability\nAn important role for proprioception is to allow an animal to stabilize itself against perturbations. For instance, for a person to walk or stand upright, they must continuously monitor their posture and adjust muscle activity as needed to provide balance. Similarly, when walking on unfamiliar terrain or even tripping, the person must adjust the output of their muscles quickly based on estimated limb position and velocity. Proprioceptor reflex circuits are thought to play an important role to allow fast and unconscious execution of these behaviors, To make control of these behaviors efficient, proprioceptors are also thought to regulate reciprocal inhibition in muscles, leading to agonist-antagonist muscle pairs.\n\nPlanning and refining movements\nWhen planning complex movements such as reaching or grooming, an animal must consider the current position and velocity of its limb and use that information to adjust dynamics to target a final position. If the animal's estimate of its limb's initial position is wrong, then a deficiency in the movement can result. Furthermore, proprioception is crucial in refining the movement if it deviates from the trajectory.\n\nDevelopment\nIn adult fruit flies, each proprioceptor class arises from a specific cell lineage (i.e. each chordotonal neuron is from the chordotonal neuron lineage, although multiple lineages give rise to sensory bristles).  After the last cell division, proprioceptors send out axons toward the central nervous system and are guided by hormonal gradients to reach stereotyped synapses.\nThe mechanisms underlying axon guidance are similar across invertebrates and vertebrates.\nIn mammals with longer gestation periods, muscle spindles are fully formed at birth.  Muscle spindles continue to grow throughout post-natal development as muscles grow.\n\nMathematical models\nProprioceptors transfer the mechanical state of the body into patterns of neural activity. This transfer can be modeled mathematically, for example to better understand the internal workings of a proprioceptor or to provide more realistic feedback in neuromechanical simulations.\nVarious proprioceptor models of complexity have been developed. They range from simple phenomenological models to complex structural models, in which the mathematical elements correspond to anatomical features of the proprioceptor. The focus has been on muscle spindles, but Golgi tendon organs and insects' hair plates have been modeled too.\n\nMuscle spindles\nPoppele and Bowman  used linear system theory to model mammalian muscle spindles Ia and II afferents. They obtained a set of de-afferented muscle spindles, measured their response to a series of sinusoidal and step function stretches, and fit a transfer function to the spike rate. They found that the following Laplace transfer function describes the firing rate responses of the primary sensory fibers for a change in length:\n\n  \n    \n      \n        H\n        (\n        s\n        )\n        =\n        \n          K\n          \n            1\n          \n        \n        \n          \n            \n              s\n              (\n              s\n              +\n              0.44\n              )\n              (\n              s\n              +\n              11.3\n              )\n              (\n              s\n              +\n              44\n              )\n            \n            \n              (\n              s\n              +\n              0.04\n              )\n              (\n              s\n              +\n              0.816\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle H(s)=K_{1}{\\frac {s(s+0.44)(s+11.3)(s+44)}{(s+0.04)(s+0.816)}}}\n  \n\nThe following equation describes the response of secondary sensory fibers:\n\n  \n    \n      \n        H\n        (\n        s\n        )\n        =\n        \n          K\n          \n            2\n          \n        \n        \n          \n            \n              (\n              s\n              +\n              0.44\n              )\n              (\n              s\n              +\n              11.3\n              )\n            \n            \n              s\n              +\n              0.816\n            \n          \n        \n      \n    \n    {\\displaystyle H(s)=K_{2}{\\frac {(s+0.44)(s+11.3)}{s+0.816}}}\n  \n\nMore recently, Blum et al. showed that the muscle spindle firing rate is modeled better as tracking the force of the muscle, rather than the length. Furthermore, muscle spindle firing rates show history dependence which cannot be modeled by a linear time-invariant system model.\n\nGolgi tendon organs\nHouk and Simon  provided one of the first mathematical models of a Golgi tendon organ receptor, modeling the firing rate of the receptor as a function of the muscle tension force. Just as for muscle spindles, they find that, as the receptors respond linearly to sine waves of different frequencies and has little variance in response over time to the same stimulus, Golgi tendon organ receptors may be modeled as linear time-invariant systems. Specifically, they find that the firing rate of a Golgi tendon organ receptor may be modeled as a sum of 3 decaying exponentials:\n\n  \n    \n      \n        r\n        (\n        t\n        )\n        =\n        K\n        [\n        1\n        +\n        A\n        exp\n        \u2061\n        (\n        \u2212\n        a\n        t\n        )\n        +\n        B\n        exp\n        \u2061\n        (\n        \u2212\n        b\n        t\n        )\n        +\n        C\n        exp\n        \u2061\n        (\n        \u2212\n        c\n        t\n        )\n        ]\n        u\n        (\n        t\n        )\n      \n    \n    {\\displaystyle r(t)=K[1+A\\exp(-at)+B\\exp(-bt)+C\\exp(-ct)]u(t)}\n  \n\nwhere \n  \n    \n      \n        r\n        (\n        t\n        )\n      \n    \n    {\\displaystyle r(t)}\n  \n is the firing rate and \n  \n    \n      \n        u\n        (\n        t\n        )\n      \n    \n    {\\displaystyle u(t)}\n  \n is a step function of force.\nThe corresponding Laplace transfer function for this system is:\n\n  \n    \n      \n        H\n        (\n        s\n        )\n        =\n        K\n        \n          (\n          \n            1\n            +\n            \n              \n                \n                  A\n                  s\n                \n                \n                  s\n                  +\n                  a\n                \n              \n            \n            +\n            \n              \n                \n                  B\n                  s\n                \n                \n                  s\n                  +\n                  b\n                \n              \n            \n            +\n            \n              \n                \n                  C\n                  s\n                \n                \n                  s\n                  +\n                  c\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle H(s)=K\\left(1+{\\frac {As}{s+a}}+{\\frac {Bs}{s+b}}+{\\frac {Cs}{s+c}}\\right)}\n  \n\nFor a soleus receptor, Houk and Simon obtain average values of K=57 pulses\/sec\/kg, A=0.31, a=0.22 sec\u22121, B=0.4, b=2.17 sec\u22121, C=2.5, c=36 sec\u22121 .\nWhen modeling a stretch reflex, Lin and Crago improved upon this model by adding a logarithmic nonlinearity before the Houk and Simon model and a threshold nonlinearity after.\n\nImpairment\nChronic\nProprioception, a sense vital for rapid and proper body coordination, can be permanently lost or impaired as a result of genetic conditions, disease, viral infections, and injuries. For instance, patients with joint hypermobility or Ehlers\u2013Danlos syndromes, genetic conditions that result in weak connective tissue throughout the body, have chronic impairments to proprioception. autism spectrum disorder, and Parkinson's disease. In regards to Parkinson's disease, it remains unclear whether the proprioceptive-related decline in motor function occurs due to disrupted proprioceptors in the periphery or signaling in the spinal cord or brain.\nIn rare cases, viral infections result in a loss of proprioception. Ian Waterman and Charles Freed are two such people that lost their sense of proprioception from the neck down from supposed viral infections (i.e. gastric flu and a rare viral infection). After losing their sense of proprioception, Ian and Charles could move their lower body, but could not coordinate their movements. However, both individuals regained some control of their limbs and body by consciously planning their movements and relying solely on visual feedback. Interestingly, both individuals can still sense pain and temperature, indicating that they specifically lost proprioceptive feedback, but not tactile and nociceptive feedback. The impact of losing the sense of proprioception on daily life is perfectly illustrated when Ian Waterman stated, \"What is an active brain without mobility\".\nProprioception is also permanently lost in people who lose a limb or body part through injury or amputation. After the removal of a limb, people may have a confused sense of that limb's existence on their body, known as phantom limb syndrome. Phantom sensations can occur as passive proprioceptive sensations of the limb's presence, or more active sensations such as perceived movement, pressure, pain, itching, or temperature. There are a variety of theories concerning the etiology of phantom limb sensations and experience. One is the concept of \"proprioceptive memory\", which argues that the brain retains a memory of specific limb positions and that after amputation there is a conflict between the visual system, which actually sees that the limb is missing, and the memory system which remembers the limb as a functioning part of the body. Phantom sensations and phantom pain may also occur after the removal of body parts other than the limbs, such as after amputation of the breast, extraction of a tooth (phantom tooth pain), or removal of an eye (phantom eye syndrome).\nThere is a decline in the sense of proprioception with ageing. This can often result in chronic lower back pain, and be the cause of falls in the elderly.\n\nAcute\nProprioception is occasionally impaired spontaneously, especially when one is tired. Similar effects can be felt during the hypnagogic state of consciousness, during the onset of sleep. One's body may feel too large or too small, or parts of the body may feel distorted in size. Similar effects can sometimes occur during epilepsy or migraine auras. These effects are presumed to arise from abnormal stimulation of the part of the parietal cortex of the brain involved with integrating information from different parts of the body. Proprioceptive illusions can also be induced, such as the Pinocchio illusion.\nTemporary impairment of proprioception has also been known to occur from an overdose of vitamin B6 (pyridoxine and pyridoxamine). This is due to a reversible neuropathy. Most of the impaired function returns to normal shortly after the amount of the vitamin in the body returns to a level that is closer to that of the physiological norm. Impairment can also be caused by cytotoxic factors such as chemotherapy.\nIt has been proposed that even common tinnitus and the attendant hearing frequency-gaps masked by the perceived sounds may cause erroneous proprioceptive information to the balance and comprehension centers of the brain, precipitating mild confusion.\nTemporary loss or impairment of proprioception may happen periodically during growth, mostly during adolescence. Growth that might also influence this would be large increases or drops in bodyweight\/size due to fluctuations of fat (liposuction, rapid fat loss or gain) and\/or muscle content (bodybuilding, anabolic steroids, catabolisis\/starvation). It can also occur in those that gain new levels of flexibility, stretching, and contortion. A limb's being in a new range of motion never experienced (or at least, not for a long time since youth perhaps) can disrupt one's sense of location of that limb. Possible experiences include suddenly feeling that feet or legs are missing from one's mental self-image; needing to look down at one's limbs to be sure they are still there; and falling down while walking, especially when attention is focused upon something other than the act of walking.\n\nDiagnosis\nImpaired proprioception may be diagnosed through a series of tests, each focusing on a different functional aspect of proprioception.\nThe Romberg's test is often used to assess balance. The subject must stand with feet together and eyes closed without support for 30 seconds. If the subject loses balance and falls, it is an indicator for impaired proprioception.\nFor evaluating proprioception's contribution to motor control, a common protocol is joint position matching. The patient is blindfolded while a joint is moved to a specific angle for a given period of time and then returned to neutral. The subject is then asked to move the joint back to the specified angle. Recent investigations have shown that hand dominance, participant age, active versus passive matching, and presentation time of the angle can all affect performance on joint position matching tasks.\nFor passive sensing of joint angles, recent studies have found that experiments to probe psychophysical thresholds produce more precise estimates of proprioceptive discrimination than the joint position matching task. In these experiments, the subject holds on to an object (such as an armrest) that moves and stops at different positions. The subject must discriminate whether one position is closer to the body than another. From the subject's choices, the tester may determine the subject's discrimination thresholds.\nProprioception is tested by American police officers using the field sobriety testing to check for alcohol intoxication. The subject is required to touch his or her nose with eyes closed; people with normal proprioception may make an error of no more than 20 mm (0.79 in), while people with impaired proprioception (a symptom of moderate to severe alcohol intoxication) fail this test due to difficulty locating their limbs in space relative to their noses.\n\nTraining\nProprioception is what allows someone to learn to walk in complete darkness without losing balance. During the learning of any new skill, sport, or art, it is usually necessary to become familiar with some proprioceptive tasks specific to that activity. Without the appropriate integration of proprioceptive input, an artist would not be able to brush paint onto a canvas without looking at the hand as it moved the brush over the canvas; it would be impossible to drive an automobile because a motorist would not be able to steer or use the pedals while looking at the road ahead; a person could not touch type or perform ballet; and people would not even be able to walk without watching where they put their feet.\nOliver Sacks reported the case of a young woman who lost her proprioception due to a viral infection of her spinal cord. At first she could not move properly at all or even control her tone of voice (as voice modulation is primarily proprioceptive). Later she relearned by using her sight (watching her feet) and inner ear only for movement while using hearing to judge voice modulation. She eventually acquired a stiff and slow movement and nearly normal speech, which is believed to be the best possible in the absence of this sense. She could not judge effort involved in picking up objects and would grip them painfully to be sure she did not drop them.\n\nThe proprioceptive sense can be sharpened through study of many disciplines. Juggling trains reaction time, spatial location, and efficient movement. Standing on a wobble board or balance board is often used to retrain or increase proprioceptive abilities, particularly as physical therapy for ankle or knee injuries. Slacklining is another method to increase proprioception.\nStanding on one leg (stork standing) and various other body-position challenges are also used in such disciplines as yoga, Wing Chun and tai chi. The vestibular system of the inner ear, vision and proprioception are the main three requirements for balance. Moreover, there are specific devices designed for proprioception training, such as the exercise ball, which works on balancing the abdominal and back muscles.\n\nHistory of study\nIn 1557, the position-movement sensation was described by Julius Caesar Scaliger as a \"sense of locomotion\".\nIn 1826, Charles Bell expounded the idea of a \"muscle sense\", which is credited as one of the first descriptions of physiologic feedback mechanisms. Bell's idea was that commands are carried from the brain to the muscles, and that reports on the muscle's condition would be sent in the reverse direction.\nIn 1847, the London neurologist Robert Todd highlighted important differences in the anterolateral and posterior columns of the spinal cord, and suggested that the latter were involved in the coordination of movement and balance.\nAt around the same time, Moritz Heinrich Romberg, a Berlin neurologist, was describing unsteadiness made worse by eye closure or darkness, now known as the eponymous Romberg's sign, once synonymous with tabes dorsalis, that became recognised as common to all proprioceptive disorders of the legs.\nIn 1880, Henry Charlton Bastian suggested \"kinaesthesia\" instead of \"muscle sense\" on the basis that some of the afferent information (back to the brain) comes from other structures, including tendons, joints, and skin.\nIn 1889, Alfred Goldscheider suggested a classification of kinaesthesia into three types: muscle, tendon, and articular sensitivity.\nIn 1906, the term proprio-ception (and also intero-ception and extero-ception) is attested in a publication by Charles Scott Sherrington involving receptors. He explains the terminology as follows:\n\nToday, the \"exteroceptors\" are the organs that provide information originating outside the body, such as the eyes, ears, mouth, and skin. The interoceptors provide information about the internal organs, and the \"proprioceptors\" provide information about movement derived from muscular, tendon, and articular sources. Using Sherrington's system, physiologists and anatomists search for specialised nerve endings that transmit mechanical data on joint capsule, tendon and muscle tension (such as Golgi tendon organs and muscle spindles), which play a large role in proprioception.\nPrimary endings of muscle spindles \"respond to the size of a muscle length change and its speed\" and \"contribute both to the sense of limb position and movement\". Secondary endings of muscle spindles detect changes in muscle length, and thus supply information regarding only the sense of position. Essentially, muscle spindles are stretch receptors. It has been accepted that cutaneous receptors also contribute directly to proprioception by providing \"accurate perceptual information about joint position and movement\", and this knowledge is combined with information from the muscle spindles.\n\nEtymology\nProprioception is from Latin proprius, meaning \"one's own\", \"individual\", and capio, capere, to take or grasp. Thus to grasp one's own position in space, including the position of the limbs in relation to each other and the body as a whole.\nThe word kinesthesia or kin\u00e6sthesia (kinesthetic sense) refers to  movement sense, but has been used inconsistently to refer either to proprioception alone or to the brain's integration of proprioceptive and vestibular inputs. Kinesthesia is a modern medical term composed of elements from Greek; kinein \"to set in motion; to move\" (from PIE root *keie- \"to set in motion\") + aisthesis \"perception, feeling\" (from PIE root *au- \"to perceive\").\n\nPlants and bacteria\nAlthough they lack neurons, systems responding to stimuli (analogous to the sensory system in animals with a nervous system, which includes the proprioception) have also been described in some plants (angiosperms). Terrestrial plants control the orientation of their primary growth through the sensing of several vectorial stimuli such as the light gradient or the gravitational acceleration. This control has been called tropism. A quantitative study of shoot gravitropism demonstrated that, when a plant is tilted, it cannot recover a steady erected posture under the sole driving of the sensing of its angular deflection versus gravity. An additional control through the continuous sensing of its curvature by the organ and the subsequent driving an active straightening process are required.  Being a sensing by the plant of the relative configuration of its parts, it has been called proprioception.  This dual sensing and control by gravisensing and proprioception has been formalized into a unifying mathematical model simulating the complete driving of the gravitropic movement. This model has been validated on 11 species sampling the phylogeny of land angiosperms, and on organs of very contrasted sizes, ranging from the small germination of wheat (coleoptile) to the trunk of poplar trees.\nFurther studies have shown that the cellular mechanism of proprioception in plants involves myosin and actin, and seems  to occur in specialized cells.  Proprioception was then found to be involved in other tropisms and to be central also to the control of nutation.\nThe discovery of proprioception in plants has generated an interest in the popular science and generalist media. This is because this discovery questions a long-lasting a priori that we have on plants. In some cases this has led to a shift between proprioception and self-awareness or self-consciousness. There is no scientific ground for such a semantic shift. Indeed, even in animals, proprioception can be unconscious;  so it is thought to be in plants.\nRecent studies suggest that bacteria have control systems that may resemble proprioception.\n\nSee also\nNotes\nReferences\nExternal links\nProprioception at the U.S. National Library of Medicine Medical Subject Headings (MeSH)","161":"Purkinje cells or Purkinje neurons, named for Czech physiologist Jan Evangelista Purkyn\u011b who identified them in 1837,  are a unique type of prominent large neurons located in the cerebellar cortex of the brain. With their flask-shaped cell bodies, many branching dendrites, and a single long axon, these cells are essential for controlling motor activity. Purkinje cells mainly release GABA (gamma-aminobutyric acid) neurotransmitter, which inhibits some neurons to reduce nerve impulse transmission. Purkinje cells efficiently control and coordinate the body's motor motions through these inhibitory actions.\n\nStructure\nThese cells are some of the largest neurons in the human brain (Betz cells being the largest), with an intricately elaborate dendritic arbor, characterized by a large number of dendritic spines. Purkinje cells are found within the Purkinje layer in the cerebellum. Purkinje cells are aligned like dominos stacked one in front of the other. Their large dendritic arbors form nearly two-dimensional layers through which parallel fibers from the deeper-layers pass. These parallel fibers make relatively weaker excitatory (glutamatergic) synapses to spines in the Purkinje cell dendrite, whereas climbing fibers originating from the inferior olivary nucleus in the medulla provide very powerful excitatory input to the proximal dendrites and cell soma. Parallel fibers pass orthogonally through the Purkinje neuron's dendritic arbor, with up to 200,000 parallel fibers forming a Granule-cell-Purkinje-cell synapse with a single Purkinje cell. \nCanonically, each adult Purkinje cell receives approximately 500 climbing fiber synapses, all originating from a single climbing fiber from the inferior olive. This has led to the notion that a \"highly conserved one-to-one relationship renders Purkinje dendrites into a single computational compartment\". However, multi-innervation has now been found that \"occurs\" in mice among the subset of Purkinje cells with multiple primary dendrites, a dendritic motif that is uncommon in rodents but \"predominant\" in humans. \nBoth basket and stellate cells (found in the cerebellar molecular layer) provide inhibitory (GABAergic) input to the Purkinje cell, with basket cells synapsing on the Purkinje cell axon initial segment and stellate cells onto the dendrites.\nPurkinje cells send inhibitory projections to the deep cerebellar nuclei, and constitute the sole output of all motor coordination in the cerebellar cortex.\n\nMolecular\nThe Purkinje layer of the cerebellum, which contains the cell bodies of the Purkinje cells and Bergmann glia, express a large number of unique genes. Purkinje-specific gene markers were also proposed by comparing the transcriptome of Purkinje-deficient mice with that of wild-type mice. One illustrative example is the Purkinje cell protein 4 (PCP4) in knockout mice, which exhibit impaired locomotor learning and markedly altered synaptic plasticity in Purkinje neurons. PCP4 accelerates both the association and dissociation of calcium (Ca2+) with calmodulin (CaM) in the cytoplasm of Purkinje cells, and its absence impairs the physiology of these neurons.\n\nDevelopment\nMammalian embryonic research has detailed the neurogenic origins of Purkinje cells. During early development Purkinje cells arise in the ventricular zone in the neural tube, the nervous system\u00b4s precursor in the embryo. All cerebellar neurons derive from germinal neuroepithelia from the ventricular zone. Purkinje cells are specifically generated from progenitors in the ventricular neuroepithelium of the embryonic cerebellar primordium. The first cells generated from the cerebellar primordium form a cap over a diamond-shaped cavity of the developing brain called the fourth ventricle forming the two cerebellar hemispheres. The Purkinje cells that develop later are those of the cerebellum's center-lying section called the vermis. They develop in the cerebellar primordium that covers the fourth ventricle and below a fissure-like region called the isthmus of the developing brain. Purkinje cells migrate toward the outer surface of the cerebellar cortex and form the Purkinje cell layer.\nPurkinje cells are born during the earliest stages of cerebellar neurogenesis. Neurogenin2, together with neurogenin1, are transiently expressed in restricted domains of the ventricular neuroepithelium during the time-window of Purkinje cell genesis. This spatio-temporal distribution pattern suggests that neurogenins are involved in the specification of phenotypically heterogeneous Purkinje cell subsets, ultimately responsible for constructing the framework of the cerebellar topography.\nThere is evidence in mice and humans that bone marrow cells either fuse with or generate cerebellar Purkinje cells, and it is possible that bone marrow cells, either by direct generation or by cell fusion, could play a role in repair of central nervous system damage. Further evidence points yet towards the possibility of a common stem cell ancestor among Purkinje neurons, B-lymphocytes and aldosterone-producing cells of the human adrenal cortex.\n\nFunction\nPurkinje cells show two distinct forms of electrophysiological activity:\n\nSimple spikes occur at rates of 17 \u2013 150 Hz (Raman and Bean, 1999), either spontaneously or when Purkinje cells are activated synaptically by the parallel fibers, the axons of the granule cells.\nComplex spikes are slow, 1\u20133 Hz spikes, characterized by an initial prolonged large-amplitude spike, followed by a high-frequency burst of smaller-amplitude action potentials. They are caused by climbing fiber activation and can involve the generation of calcium-mediated action potentials in the dendrites.  Following complex spike activity, simple spikes can be suppressed by the powerful complex spike input.\nPurkinje cells show spontaneous electrophysiological activity in the form of trains of spikes both sodium-dependent and calcium-dependent. This was initially shown by Rodolfo Llinas (Llinas and Hess (1977) and Llinas and Sugimori (1980)). P-type calcium channels were named after Purkinje cells, where they were initially encountered (Llinas et al. 1989), which are crucial in cerebellar function. Activation of the Purkinje cell by climbing fibers can shift its activity from a quiet state to a spontaneously active state and vice versa, serving as a kind of toggle switch. These findings have been challenged by a study suggesting that such toggling by climbing-fiber inputs occurs predominantly in anaesthetized animals and that Purkinje cells in awake behaving animals, in general, operate almost continuously in the upstate. But this latter study has itself been challenged and Purkinje cell toggling has since been observed in awake cats. A computational model of the Purkinje cell has shown intracellular calcium computations to be responsible for toggling.\nFindings have suggested that Purkinje cell dendrites release endocannabinoids that can transiently downregulate both excitatory and inhibitory synapses. The intrinsic activity mode of Purkinje cells is set and controlled by the sodium-potassium pump. This suggests that the pump might not be simply a homeostatic, \"housekeeping\" molecule for ionic gradients. Instead, it could be a computation element in the cerebellum and the brain. Indeed, a mutation in the Na+-K+ pump causes rapid onset dystonia parkinsonism; its symptoms indicate that it is a pathology of cerebellar computation. \nFurthermore, using the poison ouabain to block Na+-K+ pumps in the cerebellum of a live mouse induces ataxia and dystonia. Numerical modeling of experimental data suggests that, in vivo, the Na+-K+ pump produces long quiescent punctuations (>> 1 s) to Purkinje neuron firing; these may have a computational role. Alcohol inhibits Na+-K+ pumps in the cerebellum and this is likely how it corrupts cerebellar computation and body co-ordination.\n\nClinical significance\nIn humans, Purkinje cells can be harmed by a variety of causes: toxic exposure, e.g. to alcohol or lithium; autoimmune diseases; genetic mutations causing spinocerebellar ataxias, gluten ataxia, Unverricht-Lundborg disease, or autism; and neurodegenerative diseases that are not known to have a genetic basis, such as the cerebellar type of multiple system atrophy or sporadic ataxias.\nGluten ataxia is an autoimmune disease triggered by the ingestion of gluten. The death of Purkinje cells as a result of gluten exposure is irreversible. Early diagnosis and treatment with a gluten-free diet can improve ataxia and prevent its progression. Less than 10% of people with gluten ataxia present any gastrointestinal symptom, yet about 40% have intestinal damage. It accounts for 40% of ataxias of unknown origin and 15% of all ataxias.\nThe neurodegenerative disease spinocerebellar ataxia type 1 (SCA1) is caused by an unstable polyglutamine expansion within the Ataxin 1 protein. This defect in Ataxin 1 protein causes impairment of mitochondria in Purkinje cells, leading to premature degeneration of the Purkinje cells. As a consequence, motor coordination declines and eventually death ensues.\nSome domestic animals can develop a condition where the Purkinje cells begin to atrophy shortly after birth, called cerebellar abiotrophy. It can lead to symptoms such as ataxia, intention tremors, hyperreactivity, lack of menace reflex, stiff or high-stepping gait, apparent lack of awareness of foot position (sometimes standing or walking with a foot knuckled over), and a general inability to determine space and distance.  A similar condition known as cerebellar hypoplasia occurs when Purkinje cells fail to develop in utero or die off before birth.\nThe genetic conditions ataxia telangiectasia and Niemann Pick disease type C, as well as cerebellar essential tremor, involve the progressive loss of Purkinje cells.\nIn Alzheimer's disease, spinal pathology is sometimes seen, as well as loss of dendritic branches of the Purkinje cells. Purkinje cells can also be damaged by the rabies virus as it migrates from the site of infection in the periphery to the central nervous system.\n\nEtymology\nPurkinje cells are named after the Czech scientist Jan Evangelista Purkyn\u011b, who discovered them in 1839.\n\nSee also\nList of distinct cell types in the adult human body\n\nReferences\nFurther reading\nLlin\u00e1s R, Hess R (July 1976). \"Tetrodotoxin-resistant dendritic spikes in avian Purkinje cells\". Proc. Natl. Acad. Sci. U.S.A. 73 (7): 2520\u20133. Bibcode:1976PNAS...73.2520L. doi:10.1073\/pnas.73.7.2520. PMC 430632. PMID 1065905.\nLlin\u00e1s R, Sugimori M (August 1980). \"Electrophysiological properties of in vitro Purkinje cell somata in mammalian cerebellar slices\". J. Physiol. 305: 171\u201395. doi:10.1113\/jphysiol.1980.sp013357. PMC 1282966. PMID 7441552.\nLlin\u00e1s RR, Sugimori M, Cherksey B (1989). \"Voltage-dependent calcium conductances in mammalian neurons. The P channel\". Ann. N. Y. Acad. Sci. 560 (1 Calcium Chann): 103\u201311. doi:10.1111\/j.1749-6632.1989.tb24084.x. PMID 2545128. S2CID 84107834.\nForrest, Michael (October 2014). Biophysics and computations of the cerebellar Purkinje neuron. CreateSpace. ISBN 978-1502454546.\n\nExternal links\nCell Image Library - Purkinje\nDisorders of cerebellum Archived 2009-08-26 at the Wayback Machine\nNIF Search - Purkinje Cell Archived 2013-07-08 at the Wayback Machine via the Neuroscience Information Framework","162":"Quality of life (QOL) is defined by the World Health Organization as \"an individual's perception of their position in life in the context of the culture and value systems in which they live and in relation to their goals, expectations, standards and concerns\". \nStandard indicators of the quality of life include wealth, employment, the environment, physical and mental health, education, recreation and leisure time, social belonging, religious beliefs, safety, security and freedom. QOL has a wide range of contexts, including the fields of international development, healthcare, politics and employment. Health related QOL (HRQOL) is an evaluation of QOL and its relationship with health.\n\nEngaged theory\nOne approach, called the engaged theory, outlined in the journal of Applied Research in the Quality of Life, posits four domains in assessing quality of life: ecology, economics, politics and culture. In the domain of culture, for example, it includes the following subdomains of quality of life:\n\nBeliefs and ideas\nCreativity and recreation\nEnquiry and learning\nGender and generations\nIdentity and engagement\nMemory and projection\nWell-being and health\nUnder this conception, other frequently related concepts include freedom, human rights, and happiness. However, since happiness is subjective and difficult to measure, other measures are generally given priority. It has also been shown that happiness, as much as it can be measured, does not necessarily increase correspondingly with the comfort that results from increasing income. As a result, standard of living should not be taken to be a measure of happiness. Also, sometimes considered related is the concept of human security, though the latter may be considered at a more basic level and for all people.\n\nQuantitative measurement\nUnlike per capita GDP or standard of living, both of which can be measured in financial terms, it is harder to make objective or long-term measurements of the quality of life experienced by nations or other groups of people. Researchers have begun in recent times to distinguish two aspects of personal well-being: Emotional well-being, in which respondents are asked about the quality of their everyday emotional experiences \u2013 the frequency and intensity of their experiences of, for example, joy, stress, sadness, anger and affection \u2013 and life evaluation, in which respondents are asked to think about their life in general and evaluate it against a scale. Such and other systems and scales of measurement have been in use for some time. Research has attempted to examine the relationship between quality of life and productivity. \nThere are many different methods of measuring quality of life in terms of health care, wealth, and materialistic goods. However, it is much more difficult to measure meaningful expression of one's desires. One way to do so is to evaluate the scope of how individuals have fulfilled their own ideals. Quality of life can simply mean happiness, which is the subjective state of mind. By using that mentality, citizens of a developing country appreciate more since they are content with the basic necessities of health care, education and child protection.\n\nAccording to ecological economist Robert Costanza:While Quality of Life (QOL) has long been an explicit or implicit policy goal, adequate definition and measurement have been elusive. Diverse \"objective\" and \"subjective\" indicators across a range of disciplines and scales, and recent work on subjective well-being (SWB) surveys and the psychology of happiness have spurred renewed interest.\n\nHuman Development Index\nPerhaps the most commonly used international measure of development is the Human Development Index (HDI), which combines measures of life expectancy, education, and standard of living, in an attempt to quantify the options available to individuals within a given society. The HDI is used by the United Nations Development Programme in their Human Development Report. However, since 2010, The Human Development Report introduced an Inequality-adjusted Human Development Index (IHDI). While the original HDI remains useful, it stated that \"the IHDI is the actual level of human development (accounting for inequality), while the original HDI can be viewed as an index of 'potential' human development (or the maximum level of HDI) that could be achieved if there was no inequality.\"\n\nWorld Happiness Report\nThe World Happiness Report is a landmark survey on the state of global happiness. It ranks 156 countries by their happiness levels, reflecting growing global interest in using happiness and substantial well-being as an indicator of the quality of human development. Its growing purpose has allowed governments, communities and organizations to use appropriate data to record happiness in order to enable policies to provide better lives. The reports review the state of happiness in the world today and show how the science of happiness explains personal and national variations in happiness. \nDeveloped again by the United Nations and published recently along with the HDI, this report combines both objective and subjective measures to rank countries by happiness, which is deemed as the ultimate outcome of a high quality of life. It uses surveys from Gallup, real GDP per capita, healthy life expectancy, having someone to count on, perceived freedom to make life choices, freedom from corruption, and generosity to derive the final score. Happiness is already recognized as an important concept in global public policy. The World Happiness Report indicates that some regions have in recent years been experiencing progressive inequality of happiness.\n\nOther measures\nThe Physical Quality of Life Index (PQLI) is a measure developed by sociologist M. D. Morris in the 1970s, based on basic literacy, infant mortality, and life expectancy. Although not as complex as other measures, and now essentially replaced by the Human Development Index, the PQLI is notable for Morris's attempt to show a \"less fatalistic pessimistic picture\" by focusing on three areas where global quality of life was generally improving at the time, while ignoring gross national product and other possible indicators that were not improving.\nThe Happy Planet Index, introduced in 2006, is unique among quality of life measures in that, in addition to standard determinants of well-being, it uses each country's ecological footprint as an indicator. As a result, European and North American nations do not dominate this measure. The 2012 list is instead topped by Costa Rica, Vietnam and Colombia.\nIn 2010, Gallup researchers trying to find the world's happiest countries found Denmark to be at the top of the list. For the period 2014\u20132016, Norway surpasses Denmark to be at the top of the list.\nA 2010 study by two Princeton University professors looked at 1,000 randomly selected U.S. residents over an extended period. It concludes that their life evaluations \u2013 that is, their considered evaluations of their life against a stated scale of one to ten \u2013 rise steadily with income. On the other hand, their reported quality of emotional daily experiences (their reported experiences of joy, affection, stress, sadness, or anger) levels off after a certain income level (approximately $75,000 per year in 2010); income above $75,000 does not lead to more experiences of happiness nor to further relief of unhappiness or stress. Below this income level, respondents reported decreasing happiness and increasing sadness and stress, implying the pain of life's misfortunes, including disease, divorce, and being alone, is exacerbated by poverty.\nGross national happiness and other subjective measures of happiness are being used by the governments of Bhutan and the United Kingdom. The World Happiness report, issued by Columbia University is a meta-analysis of happiness globally and provides an overview of countries and grassroots activists using GNH. The OECD issued a guide for the use of subjective well-being metrics in 2013. In the U.S., cities and communities are using a GNH metric at a grassroots level.\nThe Social Progress Index measures the extent to which countries provide for the social and environmental needs of their citizens. Fifty-two indicators in the areas of basic human needs, foundations of wellbeing, and opportunity show the relative performance of nations. The index uses outcome measures when there is sufficient data available or the closest possible proxies.\nDay-Reconstruction Method was another way of measuring happiness, in which researchers asked their subjects to recall various things they did on the previous day and describe their mood during each activity. Being simple and approachable, this method required memory and the experiments have confirmed that the answers that people give are similar to those who repeatedly recalled each subject. The method eventually declined as it called for more effort and thoughtful responses, which often included interpretations and outcomes that do not occur to people who are asked to record every action in their daily lives.\nThe Digital Quality of Life Index - a yearly study on digital well-being across 121 countries created by Surfshark. It indexes each country according to five pillars that impact a population's digital quality of life: internet affordability, internet quality, electronic infrastructure, electronic security, and electronic government.\n\nLivability\nThe term quality of life is also used by politicians and economists to measure the livability of a given city or nation. Two widely known measures of livability are the Economist Intelligence Unit's Where-to-be-born Index and Mercer's Quality of Living Reports. These two measures calculate the livability of countries and cities around the world, respectively, through a combination of subjective life-satisfaction surveys and objective determinants of quality of life such as divorce rates, safety, and infrastructure. Such measures relate more broadly to the population of a city, state, or country, not to individual quality of life. Livability has a long history and tradition in urban design, and neighborhoods design standards such as LEED-ND are often used in an attempt to influence livability.\n\nCrimes\nSome crimes against property (e.g., graffiti and vandalism) and some \"victimless crimes\" have been referred to as \"quality-of-life crimes.\" American sociologist James Q. Wilson encapsulated this argument as the broken windows theory, which asserts that relatively minor problems left unattended (such as litter, graffiti, or public urination by homeless individuals) send a subliminal message that disorder, in general, is being tolerated, and as a result, more serious crimes will end up being committed (the analogy being that a broken window left broken shows an image of general dilapidation).\nWilson's theories have been used to justify the implementation of zero tolerance policies by many prominent American mayors, most notably Oscar Goodman in Las Vegas, Richard Riordan in Los Angeles, Rudolph Giuliani in New York City and Gavin Newsom in San Francisco. Such policies refuse to tolerate even minor crimes; proponents argue that this will improve the quality of life of local residents. However, critics of zero tolerance policies believe that such policies neglect investigation on a case-by-case basis and may lead to unreasonably harsh penalties for crimes.\n\nIn healthcare\nWithin the field of healthcare, quality of life is often regarded in terms of how a certain ailment affects a patient on an individual level. This may be a debilitating weakness that is not life-threatening; life-threatening illness that is not terminal; terminal illness; the predictable, natural decline in the health of an elder; an unforeseen mental\/physical decline of a loved one; or chronic, end-stage disease processes. Researchers at the University of Toronto's Quality of Life Research Unit define quality of life as \"The degree to which a person enjoys the important possibilities of his or her life\" (UofT). Their Quality of Life Model is based on the categories \"being\", \"belonging\", and \"becoming\"; respectively who one is, how one is connected to one's environment, and whether one achieves one's personal goals, hopes, and aspirations.\nExperience sampling studies show substantial between-person variability in within-person associations between somatic symptoms and quality of life. Hecht and Shiel measure quality of life as \"the patient's ability to enjoy normal life activities\" since life quality is strongly related to wellbeing without suffering from sickness and treatment. There are multiple assessments available that measure Health-Related Quality of Life, e.g., AQoL-8D, EQ5D \u2013 Euroqol, 15D, SF-36, SF-6D, HUI.\n\nIn international development\nQuality of life has been deemed an important concept in the field of international development because it allows development to be analyzed on a measure that is generally accepted as more comprehensive than standard of living. Within development theory, however, there are varying ideas concerning what constitutes desirable change for a particular society. The different ways that quality of life is defined by institutions, therefore, shape how these organizations work for its improvement as a whole.\nOrganisations such as the World Bank, for example, declare a goal of \"working for a world free of poverty\", with poverty defined as a lack of basic human needs, such as food, water, shelter, freedom, access to education, healthcare, or employment. In other words, poverty is defined as a low quality of life. Using this definition, the World Bank works towards improving quality of life through the stated goal of lowering poverty and helping people afford a better quality of life.\nOther organizations, however, may also work towards improved global quality of life using a slightly different definition and substantially different methods. Many NGOs do not focus at all on reducing poverty on a national or international scale, but rather attempt to improve the quality of life for individuals or communities. One example would be sponsorship programs that provide material aid for specific individuals. Although many organizations of this type may still talk about fighting poverty, the methods are significantly different.\nImproving quality of life involves action not only by NGOs but also by governments. Global health has the potential to achieve greater political presence if governments were to incorporate aspects of human security into foreign policy. Stressing individuals' basic rights to health, food, shelter, and freedom addresses prominent inter-sectoral problems negatively impacting today's society, and may lead to greater action and resources. Integration of global health concerns into foreign policy may be hampered by approaches that are shaped by the overarching roles of defense and diplomacy.\n\nSee also\nCanadian Index of Wellbeing\nCircles of Sustainability\nDepression (mood)\nEudaimonia\nEuthanasia\nFlourishing\nHappiness\nHuman security\nLivability\nMental health\nPositive psychology\nPoverty\nOccupational burnout\nQuality of working life\nRational expectations\nRehabilitation psychology\nSimple living\nSocial rejection\nSubjective well-being\nWell-being\n\nIndices\nJournals\nJournal of Business Ethics\nSocial Indicators Research\n\nReferences\nFurther reading\nEzechieli, Eric (2003). Beyond Sustainable Development: Education for Gross National Happiness in Bhutan. Stanford University.\n\nExternal links\n Learning materials related to What Matters at Wikiversity\nEthical Markets Quality of Life Indicators Archived 11 January 2022 at the Wayback Machine\nThe First European Quality of Life Survey 2003 Archived 7 December 2013 at the Wayback Machine\nQuality of Life in a Changing Europe, A research project on the quality of lives and work of European citizens\nEnsuring quality of life in Europe's cities and towns, European Environment Agency\nAQoL Instruments, Quality of Life Assessment Instruments \u2013 Centre for Health Economics, Monash University Australia\nThe Quality-of-Life-Recorder (Shareware\/Freeware) \u2013 An electronic questionnaire platform for MS Windows and Java with preconfigured adoptions of numerous important Quality-of-Life instruments (including SF-36, EORTC QLQ-C30) in multiple languages\nApplied Research in Quality of Life, the official journal of the International Society for Quality-of-Life Studies\nChild Indicators Research, the official journal of the International Society for Child Indicators\nQuality of Life Research, an international journal of quality of life aspects of treatment, care, and rehabilitation \u2013 official journal of the International Society of Quality of Life Research\nAfter 2015: '3D Human Wellbeing', policy briefing on the value of refocusing development on 3D human wellbeing for pro-poor policy change, from the Institute of Development Studies, UK.\nMercer Quality of Living survey \nBasic Guide to the World: Quality of Life Throughout the World\nFamily database, OECD","163":"The Rinne test ( RIN-\u0259) is used primarily to evaluate loss of hearing in one ear. It compares perception of sounds transmitted by air conduction to those transmitted by bone conduction through the mastoid. Thus, one can quickly screen for the presence of conductive hearing loss.\nA Rinne test should always be accompanied by a Weber test to also detect sensorineural hearing loss and thus confirm the nature of hearing loss.\nThe Rinne test was named after German otologist Heinrich Adolf Rinne (1819\u20131868); the Weber test was named after Ernst Heinrich Weber (1795\u20131878).\n\nProcedure\nThe Rinne test is performed by placing a 512 Hz vibrating tuning fork against the patient's mastoid bone and asking the patient to tell you when the sound is no longer heard. Once the patient signals they can't hear it, the still vibrating tuning fork is then placed 1\u20132 cm from the auditory canal. The patient is then asked again to indicate when they are no longer able to hear the tuning fork.\n\nResults\nNormal hearing\nAir conduction should be greater than bone conduction, so the patient should be able to hear the tuning fork next to the pinna (outer ear) after they can no longer hear it when held against the mastoid. This normal result is paradoxically called a positive Rinne test (as a positive medical test usually indicates an abnormality).\n\nAbnormal hearing\nIf the patient is not able to hear the tuning fork after it is moved from the mastoid to the pinna, it means that their bone conduction is greater than their air conduction. This indicates there is something inhibiting the passage of sound waves from the ear canal, through the middle ear apparatus and into the cochlea (i.e., there is a conductive hearing loss).\nIn sensorineural hearing loss the ability to sense the tuning fork by both bone and air conduction is equally diminished, implying they will hear the tuning fork by air conduction after they can no longer hear it through bone conduction. This pattern is the same to what is found in people with normal hearing, but patients with sensorineural hearing loss will indicate that the sound has stopped much earlier. This can be revealed by the person administering the test (with normal hearing) placing the fork close to their own ear after the patient indicates that the sound has subsided, noting that the sound from the fork is still noticeable to a normal ear.\nIn case of a severe sensorineural hearing loss caused due to a dead labyrinth, a false negative Rinne test may occur. It is caused by the fact that even though one ear is unable to respond to the test, the other ear can still be stimulated by the bone conduction test (via conducting sound through skull bones to the opposite ear), causing the patient to respond to the tuning fork on mastoid but not when it's placed near the affected ear's air canal.\n\nAir vs. bone conductive hearing loss\nAir conduction uses the apparatus of the middle ear (pinna, eardrum and ossicles) to amplify and direct the sound to the cochlea, whereas bone conduction bypasses some or all of these and allows the sound to be transmitted directly to the inner ear albeit at a reduced volume, or via the bones of the skull to the opposite ear.\n\nNote that the words positive and negative are used in a somewhat confusing fashion here, as compared to their typical use in medical tests. Positive or negative in this case means that a certain parameter that was evaluated was present or not. In this case, that parameter is whether air conduction is better than bone conduction. Thus, a \"positive\" result indicates the healthy state, in contrast to many other medical tests. Therefore, some prefer to avoid using the terms \"positive\" or \"negative\", and simply state if the test was normal or abnormal. For example: \"Rinne's test was abnormal in the right ear, with bone conduction greater than air conduction\".\n\nLimitations\nThis test and its complement, the Weber test, are quick screening tests and are not a replacement for formal audiometry. Recently, its value as a screening test has been questioned.\nThe Rinne test is not reliable in distinguishing sensorineural and conductive loss cases of severe unilateral or total sensorineural loss. In such cases, bone conduction to the contralateral normal ear will be better than air conduction, resulting in a false negative. In such a case, the Weber test will, however, show signs of lateralization, implying some kind of pathology. Formal audiometry testing would be required if any abnormal result is presented.\n\nReferences\nSee also\nInternal links\nWeber test\n\nExternal links\nTuning Fork Tests - Family Practice Notebook. Retrieved February 3, 2007.","164":"Romberg's test, Romberg's sign, or the Romberg maneuver is a test used in an exam of neurological function for balance. \nThe exam is based on the premise that a person requires at least two of the three following senses to maintain balance while standing: \n\nproprioception (the ability to know one's body position in space)\nvestibular function (the ability to know one's head position in space)\nvision (which can be used to monitor and adjust for changes in body position).\nA patient who has a problem with proprioception can still maintain balance by using vestibular function and vision. In the Romberg test, the standing patient is asked to close their eyes. An increased loss of balance is interpreted as a positive Romberg's test.\nThe Romberg test is a test of the body's sense of positioning (proprioception), which requires healthy functioning of the dorsal columns of the spinal cord.\nThe Romberg test is used to investigate the cause of loss of motor coordination (ataxia). A positive Romberg test suggests that the ataxia is sensory in nature, that is, depending on loss of proprioception. If a patient is ataxic and Romberg's test is not positive, it suggests that ataxia is cerebellar in nature, that is, depending on localized cerebellar dysfunction instead.\nIt is used as an indicator for possible alcohol or drug impaired driving and neurological decompression sickness.  When used to test impaired driving, the test is performed with the subject estimating 30 seconds in their head.  This is used to gauge the subject's internal clock and can be an indicator of stimulant or depressant use.\n\nProcedure\nAsk the subject to stand erect with feet together and eyes closed.  Stand close by as a precaution in order to stop the person from falling over.  Watch the movement of the body in relation to a perpendicular object behind the subject (corner of the room, door, window etc.).  A positive sign is noted when a swaying, sometimes irregular swaying and even toppling over occurs.  The essential feature is that the patient becomes more unsteady with eyes closed.\nThe essential features of the test are as follows:\n\nthe subject stands with feet together, eyes open and hands by the sides.\nthe subject closes the eyes while the examiner observes for a full minute.\nBecause the examiner is trying to elicit whether the patient falls when the eyes are closed, it is advisable to stand ready to catch the falling patient. For large subjects, a strong assistant is recommended.\nRomberg's test is positive if the patient falls while the eyes are closed. Swaying is not a positive sign as it shows proprioceptive correction.\nPatients with a positive result are said to demonstrate Romberg's sign or Rombergism. They can also be described as Romberg's positive. The basis of this test is that balance comes from the combination of several neurological systems, namely proprioception, vestibular input, and vision. If any two of these systems are working the person should be able to demonstrate a fair degree of balance. The key to the test is that vision is taken away by asking the patient to close their eyes. This leaves only two of the three systems remaining and if there is a vestibular disorder (labyrinthine) or a sensory disorder (proprioceptive dysfunction) the patient will become much more unbalanced.\n\nPhysiology\nMaintaining balance while standing in the stationary position relies on intact sensory pathways, sensorimotor integration centers and motor pathways.\nThe main sensory inputs are:\n\nJoint position sense (proprioception), carried in the dorsal columns of the spinal cord, the dorsal and ventral spinocerebellar tracts.\nVision\nVestibular apparatus\nCrucially, the brain can obtain sufficient information to maintain balance if any two of the three systems are intact.\nSensorimotor integration is carried out by the cerebellum and by the dorsal column-medial lemniscus tract.  The motor pathway is the corticospinal (pyramidal) tract and the medial and lateral vestibular tracts.\nThe first stage of the test (standing with the eyes open with hands on hips), demonstrates that at least two of the three sensory pathways are intact, and that sensorimotor integration and the motor pathway are functioning. The patient must stand unsupported with eyes open and hands on hips for 30 seconds. If the patient takes a step or removes a hand from the hip, the timer is stopped. The patient may make two attempts to complete the 30 seconds.\nSimilar to the sensory organization test, the visual pathway would then be removed by closing the eyes.  If the proprioceptive and vestibular pathways are intact, balance will be maintained.  But if proprioception is defective, two of the sensory inputs will be absent and the patient will sway then fall. Similar to the Romberg Test, the patient must stand unsupported with eyes closed and hands on hips for 30 seconds. The patient may make two attempts to complete the 30 seconds.\nA variation of the Romberg Test, the Sharpened Romberg Test, consists of narrowing the patient\u2019s base of support by placing feet in a heel to toe position. Nonetheless, test instructions do not specify which foot, preferred or non-preferred, should be placed in front of the other. The patient should be instructed to keep hands on hips for the whole 30 seconds. If the patient takes a step or removes hands from hips, the timer is stopped and the patient may attempt the test one more time.\nThe sharpened Romberg does have an early learning effect that will plateau between the third and fourth attempts.\n\nPositive Romberg\nRomberg's test is positive in conditions causing sensory ataxia such as:\n\nVitamin deficiencies such as Vitamin B12\nConditions affecting the dorsal columns of the spinal cord, such as tabes dorsalis (neurosyphilis), in which it was first described.\nConditions affecting the sensory nerves (sensory peripheral neuropathies), such as chronic inflammatory demyelinating polyradiculoneuropathy (CIDP).\nFriedreich's ataxia\nM\u00e9ni\u00e8re's disease\n\nRomberg and cerebellar function\nRomberg's test is not a test of cerebellar function, as it is commonly misconstrued.  Patients with severe cerebellar ataxia will generally be unable to balance even with their eyes open; therefore, the test cannot proceed beyond the first step and no patient with cerebellar ataxia can correctly be described as Romberg's positive.  Rather, Romberg's test is a test of the proprioception receptors and pathways function.\nA positive Romberg's test which will show wide base gait in patients with back pain has been shown to be 90 percent specific for lumbar spinal stenosis.\n\nHistory\nThe test was named after the German neurologist Moritz Heinrich Romberg (1795\u20131873), who also gave his name to Parry\u2013Romberg syndrome and Howship\u2013Romberg sign.\n\nSee also\nPosterior column\u2013medial lemniscus pathway\nSitting-rising test\nTimed Up and Go test\nTinetti test\n\n\n== References ==","165":"Semantic Scholar is a research tool for scientific literature powered by artificial intelligence. It is developed at the Allen Institute for AI and was publicly released in November 2015. Semantic Scholar uses modern techniques in natural language processing to support the research process, for example by providing automatically generated summaries of scholarly papers. The Semantic Scholar team is actively researching the use of artificial intelligence in natural language processing, machine learning, human\u2013computer interaction, and information retrieval.\nSemantic Scholar began as a database for the topics of computer science, geoscience, and neuroscience. In 2017, the system began including biomedical literature in its corpus. As of September 2022, it includes over 200 million publications from all fields of science.\n\nTechnology\nSemantic Scholar provides a one-sentence summary of scientific literature. One of its aims was to address the challenge of reading numerous titles and lengthy abstracts on mobile devices. It also seeks to ensure that the three million scientific papers published yearly reach readers, since it is estimated that only half of this literature is ever read.\nArtificial intelligence is used to capture the essence of a paper, generating it through an \"abstractive\" technique. The project uses a combination of machine learning, natural language processing, and machine vision to add a layer of semantic analysis to the traditional methods of citation analysis, and to extract relevant figures, tables, entities, and venues from papers.\nAnother key AI-powered feature is Research Feeds, an adaptive research recommender that uses AI to quickly learn what papers users care about reading and recommends the latest research to help scholars stay up to date. It uses a state-of-the-art paper embedding model trained using contrastive learning to find papers similar to those in each Library folder.\nSemantic Scholar also offers Semantic Reader, an augmented reader with the potential to revolutionize scientific reading by making it more accessible and richly contextual. Semantic Reader provides in-line citation cards that allow users to see citations with TLDR summaries as they read and skimming highlights that capture key points of a paper so users can digest faster.\nIn contrast with Google Scholar and PubMed, Semantic Scholar is designed to highlight the most important and influential elements of a paper. The AI technology is designed to identify hidden connections and links between research topics. Like the previously cited search engines, Semantic Scholar also exploits graph structures, which include the Microsoft Academic Knowledge Graph, Springer Nature's SciGraph, and the Semantic Scholar Corpus (originally a 45 million papers corpus in computer science, neuroscience and biomedicine).\n\nArticle identifier\nEach paper hosted by Semantic Scholar is assigned a unique identifier called the Semantic Scholar Corpus ID (abbreviated S2CID). The following entry is an example:\n\nLiu, Ying; Gayle, Albert A; Wilder-Smith, Annelies; Rockl\u00f6v, Joacim (March 2020). \"The reproductive number of COVID-19 is higher compared to SARS coronavirus\". Journal of Travel Medicine. 27 (2). doi:10.1093\/jtm\/taaa021. PMID 32052846. S2CID 211099356.\n\nIndexing\nSemantic Scholar is free to use and unlike similar search engines (i.e. Google Scholar) does not search for material that is behind a paywall.\nOne study compared the index scope of Semantic Scholar to Google Scholar, and found that for the papers cited by secondary studies in computer science, the two indices had comparable coverage, each only missing a handful of the papers.\n\nNumber of users and publications\nAs of January 2018, following a 2017 project that added biomedical papers and topic summaries, the Semantic Scholar corpus included more than 40 million papers from computer science and biomedicine. In March 2018, Doug Raymond, who developed machine learning initiatives for the Amazon Alexa platform, was hired to lead the Semantic Scholar project. As of August 2019, the number of included papers metadata (not the actual PDFs) had grown to more than 173 million after the addition of the Microsoft Academic Graph records. In 2020, a partnership between Semantic Scholar and the University of Chicago Press Journals made all articles published under the University of Chicago Press available in the Semantic Scholar corpus. At the end of 2020, Semantic Scholar had indexed 190 million papers.  In 2020, Semantic Scholar reached seven million users per month.\n\nSee also\nCitation analysis \u2013 Examination of the frequency, patterns, and graphs of citations in documents\nCitation index \u2013 Index of citations between publications\nKnowledge extraction \u2013 Creation of knowledge from structured and unstructured sources\nList of academic databases and search engines\nScientometrics \u2013 Quantitative study of scholarly literature\n\nReferences\nExternal links\n\nOfficial website","166":"SNOMED CT or SNOMED Clinical Terms is a systematically organized computer-processable collection of medical terms providing codes, terms, synonyms and definitions used in clinical documentation and reporting. SNOMED CT is considered to be the most comprehensive, multilingual clinical healthcare terminology in the world. The primary purpose of SNOMED CT is to encode the meanings that are used in health information and to support the effective clinical recording of data with the aim of improving patient care. SNOMED CT provides the core general terminology for electronic health records. SNOMED CT comprehensive coverage includes: clinical findings, symptoms, diagnoses, procedures, body structures, organisms and other etiologies, substances, pharmaceuticals, devices and specimens.\nSNOMED CT is maintained and distributed by SNOMED International, an international non-profit standards development organization, located in London, UK. SNOMED International is the trading name of the International Health Terminology Standards Development Organisation (IHTSDO), established in 2007.\nSNOMED CT provides for consistent information interchange and is fundamental to an interoperable electronic health record. It provides a consistent means to index, store, retrieve, and aggregate clinical data across specialties and sites of care. It also helps in organizing the content of electronic health records systems by reducing the variability in the way data are captured, encoded and used for clinical care of patients and research. SNOMED CT can be used to directly record clinical details of individuals in electronic patient records. It also provides the user with a number of linkages to clinical care pathways, shared care plans and other knowledge resources, in order to facilitate informed decision-making, and to support long-term patient care. The availability of free automatic coding tools and services, which can return a ranked list of SNOMED CT descriptors to encode any clinical report, could help healthcare professionals to navigate the terminology.\nSNOMED CT is a terminology that can cross-map to other international standards and classifications. Specific language editions are available which augment the international edition and can contain language translations, as well as additional national terms. For example, SNOMED CT-AU, released in December 2009 in Australia, is based on the international version of SNOMED CT, but encompasses words and ideas that are clinically and technically unique to Australia.\n\nHistory\nSNOMED started in 1965 as a Systematized Nomenclature of Pathology (SNOP) and was further developed into a logic-based health care terminology.\nSNOMED CT was created in 1999 by the merger, expansion and restructuring of two large-scale terminologies: SNOMED Reference Terminology (SNOMED RT), developed by the College of American Pathologists (CAP); and the Clinical Terms Version 3 (CTV3) (formerly known as the Read codes), developed by the National Health Service of the United Kingdom (NHS). The final product was released in January 2002. The International Health Terminology Standards Development Organisation now considers SNOMED CT to be a brand name rather than an acronym. Previously SNOMED was an acronym of Systematized Nomenclature Of Medicine, but it lost that meaning when SNOMED was combined with CTV3 (Clinical Terms Version 3) into the merged product called SNOMED Clinical Terms, which was shortened to SNOMED CT.\nThe historical strength of SNOMED was its coverage of medical specialties. SNOMED RT, with over 120,000 concepts, was designed to serve as a common reference terminology for the aggregation and retrieval of pathology health care data recorded by multiple organizations and individuals. The strength of CTV3 was its terminologies for general practice. CTV3, with 200,000 interrelated concepts, was used for storing structured information about primary care encounters in individual, patient-based records. The January 2020 release of the SNOMED CT International Edition included more than 350,000 concepts.\nIn July 2003, the National Library of Medicine (NLM), on behalf of the United States Department of Health and Human Services, entered into an agreement with the College of American Pathologists to make SNOMED CT available to U.S. users at no cost through the National Library of Medicine's Unified Medical Language System UMLS Metathesaurus. The NLM negotiation team was led by Betsy Humphreys, and the contract provided NLM with a perpetual license for the core SNOMED CT (in Spanish and English) and its ongoing updates.\nIn April 2007, SNOMED CT intellectual property rights were transferred from the CAP to the International Health Terminology Standards Development Organisation (IHTSDO) in order to promote international adoption and use of SNOMED CT. Now trading as SNOMED International, the organization is responsible for \"ongoing maintenance, development, quality assurance, and distribution of SNOMED CT\" internationally\nand its Membership consists of a number of the world's leading e-health countries and territories, including: Argentina, Australia, Belgium, Brunei, Canada, Czech Republic, Chile, Denmark, Estonia, Hong Kong, Iceland, India, Ireland, Israel, Lithuania, Malaysia, Malta, Netherlands, New Zealand, Norway, Poland, Portugal, Singapore, Slovak Republic, Republic of Slovenia, Spain, Sweden, Switzerland, United Kingdom, United States and Uruguay.\nSNOMED CT is a multinational and multilingual terminology, which can manage different languages and dialects. SNOMED CT is currently available in American English, British English, Spanish, Danish and Swedish, with other translations underway or nearly completed in French and Dutch.  SNOMED CT cross maps to other terminologies, such as: ICD-9-CM, ICD-10, ICD-O-3, ICD-10-AM, Laboratory LOINC and OPCS-4. It supports ANSI, DICOM, HL7, and ISO standards.\n\nStructure\nSNOMED CT consists of four primary core components:\n\nConcept Codes \u2013 numerical codes that identify clinical terms, primitive or defined, organized in hierarchies\nDescriptions \u2013 textual descriptions of Concept Codes\nRelationships \u2013 relationships between Concept Codes that have a related meaning\nReference Sets \u2013 used to group Concepts or Descriptions into sets, including reference sets and cross-maps to other classifications and standards.\nSNOMED CT \"Concepts\" are representational units that categorize all the things that characterize healthcare processes and need to be recorded therein. In 2011, SNOMED CT included more than 311,000 concepts, which are uniquely identified by a concept ID, e.g. the concept 22298006 refers to Myocardial infarction. All SNOMED CT concepts are organized into acyclic taxonomic (is-a) hierarchies; for example, Viral pneumonia IS-A Infectious pneumonia IS-A Pneumonia IS-A Lung disease.  Concepts may have multiple parents, for example Infectious pneumonia is also a child of Infectious disease. The taxonomic structure allows data to be recorded and later accessed at different levels of aggregation.\nSNOMED CT concepts are linked by approximately 1,360,000 links, called relationships.\nConcepts are further described by various clinical terms or phrases, called Descriptions, which are divided into Fully Specified Names (FSNs), Preferred Terms (PTs), and Synonyms.  Each Concept has exactly one FSN, which is unique across all of SNOMED CT. It has, in addition, exactly one PT, which has been decided by a group of clinicians to be the most common way of expressing the meaning of the concept. It may have zero to many Synonyms. Synonyms are additional terms and phrases used to refer to this concept. They do not have to be unique or unambiguous.\n\nSemantic tag\nSNOMED CT assigns each concept a semantic tag. It is present in parentheses in Fully Specified Name of each concept. There can be multiple semantic tags used within each SNOMED CT top level hierarchy. For example, top level hierarchy of Pharmaceutical\/biologic Product uses semantic tags of: product, medicinal product, medicinal product form and clinical drug. Only one semantic tag can be used for each concept.\n\nThe formal model underlying SNOMED CT\nSNOMED CT can be characterized as a multilingual thesaurus with an ontological foundation. Thesaurus-like features are concept\u2013term relations such as the synonymous descriptions \"Acute coryza\", \"Acute nasal catarrh\", \"Acute rhinitis\", \"Common cold\" (as well as Spanish \"resfr\u00edo com\u00fan\" and \"rinitis infecciosa\") for the concept 82272006.\nUnder ontological scrutiny, SNOMED-CT is a class hierarchy (with extensive overlap of classes in contrast to typical statistical classifications like ICD).\nThis means that the SNOMED CT concept 82272006 defines the class of all the individual disease instances that match the criteria for \"common cold\" (e.g., one patient may have \"head cold\" noted in their record, and another may have \"Acute coryza\"; both can be found as instances of \"common cold\").\nThe superclass (Is-A) Relation relates classes in terms of inclusion of their members. That is, all individual \"cold-processes\" are also included in all superclasses of the class Common Cold, such as Viral upper respiratory tract infection (Figure).\n\nSNOMED CT's relational statements are basically triplets of the form Concept1 \u2013 Relationx \u2013 Concept2, with Relationx being from a small number of relation types (called linkage concepts), e.g. finding site, due to, etc. The interpretation of these triplets is (implicitly) based on the semantics of a simple Description logic (DL). E.g., the triplet Common Cold \u2013 causative agent \u2013 Virus, corresponds to the first-order expression\n\nforall x:  instance-of (x, Common cold) ->  exists y: instance-of (y, Virus) and causative-agent (y, x)\n\nor the more intuitive DL expression\n\nCommon cold subClassOf causative-agent some Virus\n\nIn the Common cold example the concept description is \"primitive\", which means that necessary criteria are given that must be met for each instance, without being sufficient for classifying a disorder as an instance of Common Cold . In contrast, the example Viral upper respiratory tract infection depicts a fully described concept, which is represented in description logic as follows:\n\n Viral upper respiratory tract infection equivalentTo\n  Upper respiratory infection and Viral respiratory infection and\n   Causative-agent some Virus and\n   Finding-site some Upper respiratory tract structure and\n   Pathological-process some Infectious process\n\nThis means that each and every individual disorder for which all definitional criteria are met can be classified as an instance of Viral upper respiratory tract infection.\n\nDescription logics\nAs of 2021, SNOMED CT content limits itself to a subset of the EL++ formalism, restricting itself to the following operators:\n\nTop, bottom\nPrimitive roles and concepts with asserted parent(s) for each\nConcept definition and conjunction but NOT disjunction or negation\nRole hierarchy but not role composition\nDomain and range constraints\nExistential but not universal restriction\nA restricted form of role inclusion axiom (xRy ^ ySz => xRz)\nGeneral Concept Inclusion axioms (A \u2286 B).\nFor understanding the modelling, it is also important to look at the stated view of a concept versus the inferred view of the concept. In further considering the state view, SNOMED CT used in the past a modelling approach referred to as 'proximal parent' approach. After 2015, a superior approach called \"proximal primitive parent\" has been adopted.\n\nPrecoordination and postcoordination\nSNOMED CT provides a compositional syntax that can be used to create expressions that represent clinical ideas which are not explicitly represented by SNOMED CT concepts. This mechanism exists because it is challenging to create and maintain all possible concepts upfront (as precoordinated concepts).\nFor example, there is no explicit concept for a \"third degree burn of left index finger caused by hot water\". However, using the compositional syntax it can be represented as\n\n284196006 | burn of skin | :\n   116676008 | associated morphology | = 80247002 | third degree burn injury |\n , 272741003 | laterality | = 7771000 | left |\n , 246075003 | causative agent | = 47448006 | hot water |\n , 363698007 | finding site | = 83738005 | index finger structure\n\nSuch expressions are said to have been 'postcoordinated'.  Post-coordination avoids the need to create large numbers of defined Concepts within SNOMED CT.  However, many systems only allow for precoordinated representations. Reliable analysis and comparison of post-coordinated expressions is possible using appropriate algorithms machinery to efficiently process the expression taking account of the underlying description logic.\nMajor Electronic Health Record Systems (EHRS) have repeatedly complained to IHTSDO and other standards organizations about the \"complexity\" of post-coordinated expressions.\nFor example, the postcoordinated expression above can be transformed using a set of standard rules to the following \"normal form expression\" which enables comparison with similar concepts.\n\n64572001 | disease | :\n   246075003 | causative agent | = 47448006 | hot water |\n , 363698007 | finding site | = ( 83738005 | index finger structure | :\n          272741003 | laterality | = 7771000 | left | )\n , { 116676008 | associated morphology | = 80247002 | third degree burn injury |\n , 363698007 | finding site | = 39937001 | skin structure | }\n\nPostcoordination is an important desirable feature of a terminology. Prior 2020, International Classification of Diseases (ICD) did not allow post-coordination and SNOMED CT was the only terminology that supported postcoordination. Since 2020, a new version of  ICD-11 now also supports postcoordination.\n\nVeterinary content\nThe International Edition of SNOMED CT only includes human terms. In 2014, clearly veterinary concepts were moved into a SNOMED CT veterinary extension. This extension is managed by the Veterinary Terminology Services Lab at the Va-Md College of Veterinary Medicine at Virginia Tech.\n\nKnown deficiencies and mitigation strategies\nEarlier SNOMED versions had faceted structure ordered by semantic axes, requiring that more complex situations required to be coded by a coordination of different codes. This had two major shortcomings. On the one hand, the necessity of post-coordination was perceived as a user-unfriendly obstacle, which has certainly contributed to the rather low adoption of early SNOMED versions. On the other hand, uniform coding was difficult to obtain. E.g.,Acute appendicitis could be post-coordinated in three different ways with no means to compute semantic equivalences.\nSNOMED RT had addressed this problem by introducing description logic formula. With the addition of CTV3 a large number of concepts were redefined using formal expressions.  However, the fusion with CTV3, as a historically grown terminology with many close-to user descriptions, introduced some problems which still affect SNOMED CT. In addition to a confusing taxonomic web of many hierarchical levels with massive multiple inheritance (e.g. there are 36 taxonomic ancestors for Acute appendicitis), many ambiguous, context-dependent concepts have found their way into SNOMED CT. Pre-coordination was sometimes pushed to extremes, so there are, for example, 350 different concepts for burns found on the head.\nA further phenomenon which characterizes parts of SNOMED CT is the so-called epistemic intrusion.\nIn principle, the task of terminology (and even an ontology) should be limited to providing context-free term or class meanings. The contextualization of these representational units should be ideally the task of an information model.\nHuman language is misleading here, as we use syntactically similar expression to represent categorically distinct entities, e.g. Ectopic pregnancy vs. Suspected pregnancy.  The first one refers to a real pregnancy, the second one to a piece of (uncertain) information.  In SNOMED CT most (but not all) of these context-dependent concepts are concentrated in the subhierachy Situation with explicit context. A major reason for why such concepts cannot be dispensed with is that SNOMED CT takes on, in many cases, the functionality of information models, as the latter do not exist in a given implementation.\nWith the establishment of IHTSDO, SNOMED CT became more accessible to a wider audience. Criticism of the state of the terminology was sparked by numerous substantive weaknesses as well as on the lack of quality assurance measures. From the beginning IHTSDO was open regarding such (also academic) criticism. In the last few years considerable progress has been made regarding quality assurance and tooling.\nThe need for a more principled ontological foundation was gradually accepted, as well as a better understanding of description logic semantics. Redesign priorities were formulated regarding observables, disorders, findings, substances, organisms etc. Translation guidelines were elaborated as well as guidelines for content submission requests and a strategy for the inclusion of pre-coordinated content. There are still known deficiencies regarding the \"ontological commitment\" of SNOMED CT, e.g., the clarification of which kind of entity is an instance of a given SNOMED CT concept. The same term can be interpreted as a disorder or a patient with a disorder, for example Tumour might denote a process or a piece of tissue; Allergy may denote an allergic reaction or just an allergic disposition. A more recent strategy is the use of rigorously typed upper-level ontologies to disambiguate SNOMED CT content.\nThe increased take-up of SNOMED CT for research into applications in daily use across the world to support patient care is leading to a larger engaged community. This has led to an increase in the resource allocated to authoring SNOMED CT terms as well as to an increase in collaboration to take SNOMED CT into a robust industry used standard. This is leading to an increase in the number of software tools and development of materials that contribute to knowledge base to support implementation. A number of on-line communities that focus on particular aspects of SNOMED CT and its implementation are also developing.\nIn theory, description logic reasoning can be applied to any new candidate post-coordinated expressions in order to assess whether it is a parent or ancestor of, a child or other descendant of, or semantically equivalent to any existing concept from the existing pre-coordinated concepts. However, partly as the continuing fall-out from the merger with CTV3, SNOMED still contains undiscovered semantically duplicate primitive and defined concepts. Additionally, many concepts remain primitive whilst their semantics can also be legitimately defined in terms of other primitives and roles concurrently in the system. Because of these omissions and actual or possible redundancies of semantic content, real-world performance of algorithms to infer subsumption or semantic equivalence will be unpredictably imperfect.\n\nSNOMED CT validation\nUsing consistent rules is important for the quality of SNOMED CT. To that end, in 2009, a prototype Machine Readable Concept Model (MRCM) was created by the SNOMED CT team. In a follow-up work, this model is being revised to utilize SNOMED CT expression constraints.\n\nSNOMED CT and other terminologies\nSNOMED CT and ICD\nSNOMED CT is a clinical terminology designed to capture and represent patient data for clinical purposes. The International Statistical Classification of Diseases and Related Health Problems (ICD) is an internationally used medical classification system; which is used to assign diagnostic and, in some national modifications, procedural codes in order to produce coded data for statistical analysis, epidemiology, reimbursement and resource allocation. Both systems use standardized definitions and form a common medical language used within electronic health record (EHR) systems. SNOMED CT enables information input into an EHR system during the course of patient care, while ICD facilitates information retrieval, or output, for secondary data purposes. In 2010s, the advantage of SNOMED CT over ICD was the multiple parent hierarchy of SNOMED CT. Since 2020 release of ICD 11, this advantage is less important because ICD-11 foundational level allows an ICD 11 concept to have multiple parents.\n\nSNOMED CT and LOINC\nLOINC is a terminology that contains laboratory tests. Since 2017, SNOMED International started creating terms for LOINC components and created a set of SNOMED CT expressions that capture the meaning of many LOINC terms.\n\nSNOMED CT and MedDRA\nThere is overlap between MedDRA and SNOMED CT that is not beneficial for pharmaceutical industry. In 2021, two maps map between SNOMED CT and MedDRA were jointly published by both organizations (from SNOMED CT to MedDRA and from MedDRA to SNOMED CT).\n\nUse\nSNOMED CT is used in a number of different ways, some of which are:\n\nIt captures clinical information at the level of detail needed for the provision of healthcare\nThrough sharing data it can reduce the need to repeat health history at each new encounter with a healthcare professional\nInformation can be recorded by different people in different locations and combined into simple information views within the patient record\nUse of a common terminology decreases the potential for differing interpretation of information\nStructured Data Capture medical forms and questionnaires based on the FHIR standard\nElectronic recording in a common way reduces errors and can help to ensure completeness in recording all relevant data\nStandardised information makes analysis easier, supporting quality, cost effective practice, research and future clinical guideline development\nA clinical terminology allows a health care provider to identify patients based on specified coded information, and more effectively manage screening, treatment and follow up\n\nUse cases\nMore specifically, the following sample computer applications use SNOMED CT:\n\nElectronic Health Record Systems\nComputerized Provider Order Entry CPOE such as E-Prescribing or Laboratory Order Entry\nCatalogues of clinical services; e.g., for Diagnostic Imaging procedures\nKnowledge databases used in clinical decision support systems (CDSS)\nRemote Intensive Care Unit Monitoring\nLaboratory Reporting\nEmergency Room Charting\nCancer Reporting\nGenetic Databases\n\nAccess\nSNOMED CT is maintained and distributed by SNOMED International, an international non-profit standards development organization, located in London, UK.\nThe use of SNOMED CT in production systems requires a license. There are two types of license:\n\nCountry\/territory membership in SNOMED International (charged according to gross national product).\nAffiliate license (dependent on the number of end users). LDCs (least developed countries) can use SNOMED CT without charges.\nFor scientific research in medical informatics, for demonstrations or evaluation purposes SNOMED CT sources can be freely downloaded and used. The original SNOMED CT sources in tabular form are accessible by registered users of the Unified Medical Language System (UMLS) who have signed an agreement. Numerous online and offline browsers are available.\nThose wishing to obtain a license for its use and to download SNOMED CT should contact their National Release Centre, links to which are provided on the IHTSDO website.\n\nLicense free subsets\nTo facilitate adoption of SNOMED CT and use of SNOMED CT in other standards, there are license free subsets. For example, a set of 7 314 codes and descriptions is free for use by users of DICOM-compliant software (without restriction to IHTSDO member countries).\n\nGlobal Patient Set (GPS) subset\nGPS was released in Sep 2019 and contains 21 782 concepts.\n\nTop level concepts\nSNOMED CT concepts typically belong to a single hierarchy (with the exception of drug-device combined concepts). Some hierarchies, have a concept model defined (e.g., clinical findings). For other domains (e.g., Organism, Substance, Qualifier value), there is no concept model yet defined.\n\nProcedure\nConcept in this hierarchy represent procedures performed on a patient. There is a well established defined concept model for procedures. Procedure site (direct or indirect) specifies on what part of body the procedure is performed. A separate set of rules exist for evaluation procedures. Evaluation procedures are procedures where evidence is evaluated to  support the determination of a value, inference or conclusion. Evaluation procedures have additional attributes, such as 'Has specimen','Property' or 'Measurement method'.\n\nEvent\nAs of 2016, the Event hierarchy does not have a concept model defined. In 2006, some concepts from the 'Clinical Finding' hierarchy were moved to the Event hierarchy. Those concepts retained some of their attributes. (e.g., causative agent)\n\nObservable entities\nSNOMED International is working on creating a concept model for observable entities.\n\nBody Structure\nBody parts represent one of the largest  hierarchies within SNOMED CT. The modeling is based on Foundational Model of Anatomy but it differs from the model in some aspects (e.g., region is taken as 3D region and not a 2D region). Important attributes include: 'Laterality', several types of 'Part of' relationships, and 'Is a'.\n\nPharmaceutical \/ biologic product\nPharmaceutical and biologic products are modeled using constructs of active ingredient, presentation strength, and basis of strength. Since 2018, harmonization of SNOMED CT drug content with IDMP standard is an editorial goal. The following types of entities are present:\n\nMedicinal product\nA higher level term grouping drugs. For example, 398731002 | Product containing sulfamethoxazole and\ntrimethoprim (medicinal product) |\n\nClinical Drug\nConcept that represents a concrete drug product as used in clinical practice. For example, 317335000 | Product containing precisely esomeprazole 20milligram\/1 each conventional release oral tablet (clinical drug)|\n\nDose Form\nConcept representing how the product is delivered. For example, 385219001 | Conventional release solution for injection (dose form) |.\n\nAuthoring conventions\nA goal for SNOMED CT is consistency. Several mechanisms are employed to ensure this. Machine readable concept model is used to check for compliance with a set of rules. Rules for creating fully specified name for a concept define allowed and not allowed patterns. When defining a concept, a proximal primitive parent rule is used (in stated definition) to employ best description logic derived classification of concepts.\nSeparate conventions govern grouping of relationships. Ability to group related relationships is an important strength of SNOMED CT. Rules in Machine Readable Concept Model (MRCM) specify by domain which relationships are never grouped (e.g., 'Is a' or 'Laterality' attributes) and which relationships are always grouped (e.g., 'Finding site'). For correct subsumption inference, some relationships may be in a group but consist of a single relationship.\nAnother convention for SNOMED CT international edition is to avoid creating intermediate primitive concepts (unless medically necessary and impossible to define with existing concept model). An intermediate primitive (=not defined) concept is a non-defined concept that has children concepts and parent concepts. This convention is related to the use of description logic to facilitate terminology maintenance. Because primitive concepts can not be processed by the description logic classifier, the maintenance of such concepts relies solely on human editors. Adding new intermediate primitive concepts requires changes to all affected concepts and is demanding in terms of terminology maintenance.\n\nSee also\nCDISC\nClinical Care Classification System\nDOCLE\nEN 13606\nMEDCIN\nMedDRA\nOmaha System\nICD11\nFoundational Model of Anatomy\n\nReferences\nExternal links\n\nSNOMED International website\nSNOMED International's online browsers for SNOMED CT\nUS National Library of Medicine SNOMED CT resources\nNHS Digital SNOMED CT resources\nVeterinary Extension of SNOMED CT","167":"The saccule (Latin: sacculus) is a bed of sensory cells in the inner ear that detects linear acceleration and head tilting in the vertical plane, and converts these vibrations into electrical impulses to be interpreted by the brain. When the head moves vertically, the sensory cells of the saccule are moved due to a combination of inertia and gravity. In response, the neurons connected to the saccule transmit electrical impulses that represent this movement to the brain.  These impulses travel along the vestibular portion of the eighth cranial nerve to the vestibular nuclei in the brainstem.\nThe vestibular system is important for balance, or equilibrium.  It includes the saccule, utricle, and the three semicircular canals.  The vestibule is the name of the fluid-filled, membranous duct that contains these organs of balance and is in turn encased in the temporal bone of the skull as a part of the inner ear.\n\nStructure\nThe saccule, or sacculus, is the smaller of the two vestibular sacs. It is globular in form and lies in the recessus sph\u00e6ricus near the opening of the vestibular duct of the cochlea.  Its cavity does not directly communicate with that of the utricle. The anterior part of the saccule exhibits an oval thickening, the macula acustica sacculi, or macula, to which are distributed the saccular filaments of the vestibular branch of the vestibulocochlear nerve, also known as the statoacoustic nerve or cranial nerve VIII.\nWithin the macula are hair cells, each having a hair bundle on the apical aspect. The hair bundle is composed of a single kinocilium and many (at least 70) stereocilia.  Stereocilia are connected to mechanically gated ion channels in the hair cell plasma membrane via tip links.  Supporting cells interdigitate between hair cells and secrete the otolithic membrane, a thick, gelatinous layer of glycoprotein.  Covering the surface of the otolithic membrane are otoliths, which are crystals of calcium carbonate.  For this reason, the saccule is sometimes called an \"otolithic organ.\"\nFrom the posterior wall of the saccule is given off a canal, the ductus endolymphaticus (endolymphatic duct).  This duct is joined by the ductus utriculosaccularis, and then passes along the aqu\u00e6ductus vestibuli and ends in a blind pouch saccus endolymphaticus (endolymphatic sac) on the posterior surface of the petrous portion of the temporal bone, where it is in contact with the dura mater.\nFrom the lower part of the saccule a short tube, the canalis reuniens of Hensen, passes downward and opens into the ductus cochlearis near its vestibular extremity.\nBoth the utricle and the saccule provide information about acceleration. The difference between them is that the utricle is more sensitive to horizontal acceleration, whereas the saccule is more sensitive to vertical acceleration.\n\nFunction\nThe saccule gathers sensory information to orient the body in space. It primarily gathers information about linear movement in the vertical plane, including the force due to gravity. The saccule, like the utricle, provides information to the brain about head position when it is not moving. The structures that enable the saccule to gather this vestibular information are the hair cells. The 2 by 3 mm patch of hair cells and supporting cells are called a macula. Each hair cell of a macula has 40 to 70 stereocilia and one true cilium called a kinocilium. The stereocilia are oriented by the striola, a curved ridge that runs through the middle of the macula; in the saccule they are oriented away from the striola  The tips of the stereocilia and kinocilium are embedded in a gelatinous otolithic membrane. This membrane is weighted with protein-calcium carbonate granules called otoliths, which add to the weight and inertia of the membrane and enhance the sense of gravity and motion.\nNot much is known of how this organ is used in other species. Research has shown, like songbirds, females in some species of fish show seasonal variation in\nauditory processing and the sensitivity of the saccule of females peaks during the breeding season. This is due to an increase in the\ndensity of saccular hair cells, partly resulting from reduced apoptosis. The increase the hair cells make also increase the sensitivity to male mating calls. An example of this is seen in Porichthys notatus, or plainfin midshipman fish.\n\nClinical significance\nAssessment\nSaccular function can be assessed by the cervical vestibular evoked myogenic potential (cVEMP). This is a middle latency (P1 between 12 and 20 ms) waveform denoting inhibition of the sternocleidomastoid (SCM) muscle ipsilateral to the stimulus. While not truly a unilateral reflex (response waveforms can be detected in the SCM contralateral to the stimulus in approximately 40% of cases), cVEMPs are more unilateral than the closely related ocular vestibular evoked myogenic potential (oVEMP). The most reliable points on the cVEMP waveform are known as P1 and N1. Of all waveform characteristics, P1-N1 amplitude is the most reliable and clinically relevant. cVEMP amplitude is linearly dependent upon stimulus intensity and is most reliably elicited with a loud (generally at or above 95 dB nHL) click or tone burst. The cVEMP can also be said to be low-frequency tuned, with largest amplitudes in response to 500\u2013750 Hz tonebursts. This myogenic potential is felt to assess saccular function, because the response is present in completely deafened ears and because it is routed through the inferior vestibular nerve, which is known to dominantly innervate the saccule. .\n\nRole in evolution of the ear\nResearch suggests that in vertebrate evolution, sensory cells became specialized as gravistatic sensors after they became assembled to form the ear. After this aggregation, growth, including duplication and segregation of existing neurosensory epithelia, gave rise to new epithelia and can be appreciated by comparing sensory epithelia from the inner ears of different vertebrates and their innervation by different neuronal populations. Novel directions of differentiation were apparently further expanded by incorporating unique molecular modules in newly developed sensory epithelia. For example, the saccule gave rise to the auditory epithelium and corresponding neuronal population of tetrapods, starting possibly in an aquatic environment.\n\nSee also\nSaccular acoustic sensitivity\n\n\n== References ==","168":"The semicircular canals are three semicircular interconnected tubes located in the innermost part of each ear, the inner ear. The three canals are the lateral, anterior and posterior semicircular canals. They are the part of the bony labyrinth, a periosteum-lined cavity on the petrous part of the temporal bone filled with perilymph. \n\nEach semicircular canal contains its respective semicircular duct, i.e. the lateral, anterior and posterior semicircular ducts, which provide the sensation of angular acceleration and are part of the membranous labyrinth\u2014therefore filled with endolymph.\n\nStructure\nThe semicircular canals are a component of the bony labyrinth that are at right angles from each other and contain their respective semicircular duct. At one end of each of the semicircular ducts is a dilated sac called an membranous ampulla, which is more than twice the diameter of the ducts. Each ampulla contains an ampullary crest, the crista ampullaris which consists of a thick gelatinous cap called a cupula and many hair cells. The superior and posterior semicircular ducts are oriented vertically at right angles to each other. The lateral semicircular duct is about a 30-degree angle from the horizontal plane. The orientations of the ducts cause a different duct to be stimulated by movement of the head in different planes, and more than one duct is stimulated at once if the movement is off those planes. The lateral semicircular duct detects angular acceleration of the head when the head is turned and the anterior and posterior semicircular ducts detect vertical head movements when the head is moved up or down. When the head changes position, the endolymph in the ducts lags behind due to inertia and this acts on the cupula which bends the cilia of the hair cells. The stimulation of the hair cells sends the message to the brain that acceleration is taking place. The semicircular canals open into the vestibule by five orifices, one of the apertures being common to two of them.\nAmong species of mammals, the size of the semicircular canals is correlated with their type of locomotion. Specifically, species that are agile and have fast, jerky locomotion have larger canals relative to their body size than those that move more cautiously.\n\nLateral semicircular canal\nThe lateral semicircular canal (also known as horizontal or external semicircular canal) is the shortest of the three canals. Movement of fluid within its duct corresponds to rotation of the head around a vertical axis (i.e. the neck), or in other words, rotation in the transverse plane. This occurs, for example, when one turns the head from side to side (yaw axis).\nIt measures from 12 to 15 mm (0.47 to 0.59 in), and its arch is directed horizontally backward and laterally; thus each semicircular canal stands at right angles to the other two. Its ampullated end corresponds to the upper and lateral angle of the vestibule, just above the oval window, where it opens close to the ampullated end of the anterior semicircular canal; its opposite end opens at the upper and back part of the vestibule. The lateral canal of one ear is very nearly in the same plane as that of the other.\n\nAnterior semicircular canal\nThe anterior semicircular canal (also known as superior semicircular canal) contains the part of the vestibular system that detects rotations of the head in around the lateral axis, that is, rotation in the sagittal plane. This occurs, for example, when nodding one's head (pitch axis).\nIt is 15 to 20 mm (0.59 to 0.79 in) in length, is vertical in direction, and is placed transversely to the long axis of the petrous part of the temporal bone, on the anterior surface of which its arch forms a round projection. It describes about two-thirds of a circle. Its lateral extremity is ampullated, and opens into the upper part of the vestibule; the opposite end joins with the upper part of the posterior semicircular canal to form the crus osseum commune, which opens into the upper and medial part of the vestibule.\n\nPosterior semicircular canal\nThe posterior semicircular canal contains the part of the vestibular system that detects rotation of the head around the antero-posterior (sagittal) axis, or in other words, rotation in the coronal plane. This occurs, for example, when one moves the head to touch the shoulders, or when doing a cartwheel (roll axis).\nIt is directed superiorly and posteriorly, as per its nomenclature, nearly parallel to the posterior surface of the petrous part of the temporal bone. The vestibular aqueduct is immediately medial to it. The posterior semicircular canal is part of the bony labyrinth and its duct is used by the vestibular system to detect rotations of the head in the coronal plane. It is the longest of the three semicircular canals, measuring from 18 to 22 mm (0.71 to 0.87 in). Its lower or ampullated end opens into the lower and back part of the vestibule, its upper into the crus osseum commune.\n\nDevelopment\nFindings from a 2009 study demonstrated a critical late role for bone morphogenetic protein 2 (BMP-2) in the morphogenesis of semicircular canals in the zebrafish inner ear. It is suspected that the role of BMP-2 in semicircular canal duct outgrowth is likely to be conserved between different vertebrate species.\nAdditionally, it has been found that the two semicircular canals found in the lamprey inner ear are developmentally similar to the superior and posterior canals found in humans, as the canals of both organisms arise from two depressions in the otic vesicle during early development. These depressions first form in lampreys between the 11 and 42 millimeter larval stages and form in zebrafish 57 hours post-fertilization\n\nFunction\nThe semicircular ducts provide sensory input for experiences of rotary movements. They are oriented along the pitch, roll, and yaw axes. The lateral semicircular canal is oriented in the yaw axis, the anterior semicircular canal is oriented in the pitch axis, and the posterior semicircular canal is oriented in the roll axis.\nEach duct is filled with a fluid called endolymph and contains motion sensors within the fluids. The base of each duct is enlarged, opening into the utricle, and has a dilated sac at one end called the membranous ampulla. Within the ampulla is a mound of hair cells and supporting cells called crista ampullaris. These hair cells have many cytoplasmic projections on the apical surface called stereocilia which are embedded in a gelatinous structure called the cupula. As the head rotates, the duct moves, but the endolymph lags behind owing to inertia. This deflects the cupula and bends the stereocilia within. The bending of these stereocilia alters an electric signal that is transmitted to the brain. Within approximately 10 seconds of achieving constant motion, the endolymph catches up with the movement of the duct and the cupula is no longer affected, stopping the sensation of acceleration. The specific gravity of the cupula is comparable to that of the surrounding endolymph. Consequently, the cupula is not displaced by gravity, unlike the otolithic membranes of the utricle and saccule. As with macular hair cells, hair cells of the crista ampullaris will depolarise when the stereocilia deflect towards the kinocilium. Deflection in the opposite direction results in hyperpolarisation and inhibition. In the lateral semicircular duct, ampullopetal flow is necessary for hair-cell stimulation, whereas ampullofugal flow is necessary for the anterior and posterior semicircular ducts.\nThis adjustment period is in part the cause of an illusion known as \"the leans\" often experienced by pilots. As a pilot enters a turn, hair cells in the semicircular ducts are stimulated, telling the brain that the aircraft, and the pilot, are no longer moving in a straight line but rather making a banked turn. If the pilot were to sustain a constant rate turn, the endolymph would eventually catch up with the ducts and cease to deflect the cupula. The pilot would no longer feel as if the aircraft was in a turn. As the pilot exits the turn, the semicircular ducts are stimulated to make the pilot think that they are now turning in the opposite direction rather than flying straight and level. In response to this, the pilot will often lean in the direction of the original turn in an attempt to compensate for this illusion. A more serious form of this is called a graveyard spiral. Rather than the pilot leaning in the direction of the original turn, they may actually re-enter the turn. As the endolymph stabilizes, the semicircular ducts stop registering the gradual turn and the aircraft slowly loses altitude until impact with the ground.\n\nHistory\nJean Pierre Flourens, by destroying the horizontal semicircular canal of pigeons, noted that they continue to fly in a circle, showing the purpose of the semicircular canals.\n\nSee also\nEar\nInner ear\n\nReferences\nThis article incorporates text in the public domain from page 1049 of the 20th edition of Gray's Anatomy (1918)\n\nAdditional images\nExternal links\n\n\"Inner Ear Detail Model\". Nervous System & Special Senses. Archived from the original on Jun 16, 2007.\nPurves, Dale; Augustine, George J.; Fitzpatrick, David; Katz, Lawrence C.; LaMantia, Anthony-Samuel; McNamara, James O.; Williams, S. Mark (2001). \"The Semicircular Canals\". Neuroscience (2nd ed.). Sinauer Associates. Archived from the original on Jun 6, 2023 \u2013 via National Center for Biotechnology Information.\n\"Human ear\". Encyclop\u00e6dia Britannica. Archived from the original on Dec 3, 2023.","169":"The sense of balance or equilibrioception is the perception of balance and spatial orientation. It helps prevent humans and nonhuman animals from falling over when standing or moving. Equilibrioception is the result of a number of sensory systems working together; the eyes (visual system), the inner ears (vestibular system), and the body's sense of where it is in space (proprioception) ideally need to be intact.\nThe vestibular system, the region of the inner ear where three semicircular canals converge, works with the visual system to keep objects in focus when the head is moving. This is called the vestibulo-ocular reflex (VOR). The balance system works with the visual and skeletal systems (the muscles and joints and their sensors) to maintain orientation or balance. Visual signals sent to the brain about the body's position in relation to its surroundings are processed by the brain and compared to information from the vestibular and skeletal systems.\n\nVestibular system\nIn the vestibular system, equilibrioception is determined by the level of a fluid called endolymph in the labyrinth, a complex set of tubing in the inner ear.\n\nDysfunction\nWhen the sense of balance is interrupted it causes dizziness, disorientation and nausea. Balance can be upset by M\u00e9ni\u00e8re's disease, superior canal dehiscence syndrome, an inner ear infection, by a bad common cold affecting the head or a number of other medical conditions including but not limited to vertigo. It can also be temporarily disturbed by quick or prolonged acceleration, for example, riding on a merry-go-round. Blows can also affect equilibrioreception, especially those to the side of the head or directly to the ear.\nMost astronauts find that their sense of balance is impaired when in orbit because they are in a constant state of weightlessness. This causes a form of motion sickness called space adaptation syndrome.\n\nSystem overview\nThis overview also explains acceleration as its processes are interconnected with balance.\n\nMechanical\nThere are five sensory organs innervated by the vestibular nerve; three semicircular canals (Horizontal SCC, Superior SCC, Posterior SCC) and two otolith organs (saccule and utricle). Each semicircular canal (SSC) is a thin tube that doubles in thickness briefly at a point called osseous ampullae. At their center-base, each contains an ampullary cupula. The cupula is a gelatin bulb connected to the stereocilia of hair cells, affected by the relative movement of the endolymph it is bathed in.\nSince the cupula is part of the bony labyrinth, it rotates along with actual head movement, and by itself without the endolymph, it cannot be stimulated and therefore, could not detect movement. Endolymph follows the rotation of the canal; however, due to inertia its movement initially lags behind that of the bony labyrinth. The delayed movement of the endolymph bends and activates the cupula. When the cupula bends, the connected stereocilia bend along with it, activating chemical reactions in the hair cells surrounding crista ampullaris and eventually create action potentials carried by the vestibular nerve signaling to the body that it has moved in space.\nAfter any extended rotation, the endolymph catches up to the canal and the cupula returns to its upright position and resets. When extended rotation ceases, however, endolymph continues, (due to inertia) which bends and activates the cupula once again to signal a change in movement.\nPilots doing long banked turns begin to feel upright (no longer turning) as endolymph matches canal rotation; once the pilot exits the turn the cupula is once again stimulated, causing the feeling of turning the other way, rather than flying straight and level.\nThe horizontal SCC handles head rotations about a vertical axis (e.g. looking side to side), the superior SCC handles head movement about a lateral axis (e.g. head to shoulder), and the posterior SCC handles head rotation about a rostral-caudal axis (e.g. nodding). SCC sends adaptive signals, unlike the two otolith organs, the saccule and utricle, whose signals do not adapt over time.\nA shift in the otolithic membrane that stimulates the cilia is considered the state of the body until the cilia are once again stimulated. For example, lying down stimulates cilia and standing up stimulates cilia, however, for the time spent lying the signal that you are lying remains active, even though the membrane resets.\nOtolithic organs have a thick, heavy gelatin membrane that, due to inertia (like endolymph), lags behind and continues ahead past the macula it overlays, bending and activating the contained cilia.\nUtricle responds to linear accelerations and head-tilts in the horizontal plane (head to shoulder), whereas saccule responds to linear accelerations and head-tilts in the vertical plane (up and down). Otolithic organs update the brain on the head-location when not moving; SCC update during movement.\nKinocilium are the longest stereocilia and are positioned (one per 40-70 regular cilia) at the end of the bundle. If stereocilia go towards kinocilium, depolarization occurs, causing more neurotransmitters, and more vestibular nerve firings, as compared to when stereocilia tilt away from kinocilium (hyperpolarization, less neurotransmitter, less firing).\n\nNeural\nFirst order vestibular nuclei (VN) project to lateral vestibular nucleus (IVN), medial vestibular nucleus (MVN), and superior vestibular nucleus (SVN).\nThe inferior cerebellar peduncle is the largest center through which balance information passes. It is the area of integration between proprioceptive, and vestibular inputs, to aid in unconscious maintenance of balance and posture.\nThe inferior olivary nucleus aids in complex motor tasks by encoding coordinating timing sensory information; this is decoded and acted upon in the cerebellum.\nThe cerebellar vermis has three main parts. The vestibulocerebellum regulates eye movements by the integration of visual info provided by the superior colliculus and balance information. The spinocerebellum integrates visual, auditory, proprioceptive, and balance information to act out body and limb movements. It receives input from the trigeminal nerve, dorsal column (of the spinal cord), midbrain, thalamus, reticular formation and vestibular nuclei (medulla) outputs. Lastly, the cerebrocerebellum plans, times, and initiates movement after evaluating sensory input from, primarily, motor cortex areas, via pons and cerebellar dentate nucleus. It outputs to the thalamus, motor cortex areas, and red nucleus.\nThe flocculonodular lobe is a cerebellar lobe that helps maintain body equilibrium by modifying muscle tone (the continuous and passive muscle contractions).\nMVN and IVN are in the medulla, LVN and SVN are smaller and in pons. SVN, MVN, and IVN ascend within the medial longitudinal fasciculus. LVN descend the spinal cord within the lateral vestibulospinal tract and ends at the sacrum. MVN also descend the spinal cord, within the medial vestibulospinal tract, ending at lumbar 1.\nThe thalamic reticular nucleus distributes information to various other thalamic nuclei, regulating the flow of information. It is speculatively able to stop signals, ending transmission of unimportant info. The thalamus relays info between pons (cerebellum link), motor cortices, and insula.\nThe insula is also heavily connected to motor cortices; the insula is likely where balance is likely brought into perception.\nThe oculomotor nuclear complex refers to fibers going to tegmentum (eye movement), red nucleus (gait (natural limb movement)), substantia nigra (reward), and cerebral peduncle (motor relay). Nucleus of Cajal are one of the named oculomotor nuclei, they are involved in eye movements and reflex gaze coordination.\nThe abducens nerve solely innervates the lateral rectus muscle of the eye, moving the eye with the trochlear nerve. The trochlear solely innervates the superior oblique muscle of the eye. Together, trochlear and abducens contract and relax to simultaneously direct the pupil towards an angle and depress the globe on the opposite side of the eye (e.g. looking down directs the pupil down and depresses (towards the brain) the top of the globe). The pupil is not only directed, but often rotated, by these muscles. (See visual system)\n\nThe thalamus and superior colliculus are connected via the lateral geniculate nucleus. The superior colliculus (SC) is the topographical map for balance and quick orienting movements with primarily visual inputs. SC integrates multiple senses.\n\nOther animals\nSome animals have better equilibrioception than humans; for example, a cat uses its inner ear and tail to walk on a thin fence.\nEquilibrioception in many marine animals is done with an entirely different organ, the statocyst, which detects the position of tiny calcareous stones to determine which way is \"up\".\n\nIn plants\nPlants could be said to exhibit a form of equilibrioception, in that when rotated from their normal attitude the stems grow in the direction that is upward (away from gravity) while their roots grow downward (in the direction of gravity). This phenomenon is known as gravitropism and it has been shown that, for example, poplar stems can detect reorientation and inclination.\n\nSee also\nProprioception\nVertigo\n\nReferences\n\n\n== External links ==","170":"Sensorineural hearing loss (SNHL) is a type of hearing loss in which the root cause lies in the inner ear, sensory organ (cochlea and associated structures), or the vestibulocochlear nerve (cranial nerve VIII).  SNHL accounts for about 90% of reported hearing loss. SNHL is usually permanent and can be mild, moderate, severe, profound, or total. Various other descriptors can be used depending on the shape of the audiogram, such as high frequency, low frequency, U-shaped, notched, peaked, or flat.\nSensory hearing loss often occurs as a consequence of damaged or deficient cochlear hair cells. Hair cells may be abnormal at birth or damaged during the lifetime of an individual. There are both external causes of damage, including infection, and ototoxic drugs, as well as intrinsic causes, including genetic mutations. A common cause or exacerbating factor in SNHL is prolonged exposure to environmental noise, or noise-induced hearing loss. Exposure to a single very loud noise such as a gun shot or bomb blast can cause noise-induced hearing loss. Using headphones at high volume over time, or being in loud environments regularly, such as a loud workplace, sporting events, concerts, and using noisy machines can also be a risk for noise-induced hearing loss.\nNeural, or \"retrocochlear\", hearing loss occurs because of damage to the cochlear nerve (CVIII). This damage may affect the initiation of the nerve impulse in the cochlear nerve or the transmission of the nerve impulse along the nerve into the brainstem.\nMost cases of SNHL present with a gradual deterioration of hearing thresholds occurring over years to decades. In some, the loss may eventually affect large portions of the frequency range. It may be accompanied by other symptoms such as ringing in the ears (tinnitus) and dizziness or lightheadedness (vertigo). The most common kind of sensorineural hearing loss is age-related (presbycusis), followed by noise-induced hearing loss (NIHL).\nFrequent symptoms of SNHL are loss of acuity in distinguishing foreground voices against noisy backgrounds, difficulty understanding on the telephone, some kinds of sounds seeming excessively loud or shrill, difficulty understanding some parts of speech (fricatives and sibilants), loss of directionality of sound (especially with high frequency sounds), perception that people mumble when speaking, and difficulty understanding speech. Similar symptoms are also associated with other kinds of hearing loss; audiometry or other diagnostic tests are necessary to distinguish sensorineural hearing loss.\nIdentification of sensorineural hearing loss is usually made by performing a pure tone audiometry (an audiogram) in which bone conduction thresholds are measured. Tympanometry and speech audiometry may be helpful. Testing is performed by an audiologist.\nThere is no proven or recommended treatment or cure for SNHL; management of hearing loss is usually by hearing strategies and hearing aids. In cases of profound or total deafness, a cochlear implant is a specialised device that may restore a functional level of hearing. SNHL is at least partially preventable by avoiding environmental noise, ototoxic chemicals and drugs, and head trauma, and treating or inoculating against certain triggering diseases and conditions like meningitis.\n\nSigns and symptoms\nSince the inner ear is not directly accessible to instruments, identification is by patient report of the symptoms and audiometric testing. Of those who present to their doctor with sensorineural hearing loss, 90% report having diminished hearing, 57% report having a plugged feeling in ear, and 49% report having ringing in ear (tinnitus). About half report vestibular (vertigo) problems.\nFor a detailed exposition of symptoms useful for screening, a self-assessment questionnaire was developed by the American Academy of Otolaryngology, called the Hearing Handicap Inventory for Adults (HHIA). It is a 25-question survey of subjective symptoms.\n\nCauses\nSensorineural hearing loss may be genetic or acquired (i.e. as a consequence of disease, noise, trauma, etc.). People may have a hearing loss from birth (congenital) or the hearing loss may come on later. Many cases are related to old age (age-related).\n\nGenetic\nHearing loss can be inherited. More than 40 genes have been implicated in the cause of deafness.  There are 300 syndromes with related hearing loss, and each syndrome may have causative genes.\nRecessive, dominant, X-linked, or mitochondrial genetic mutations can affect the structure or metabolism of the inner ear. Some may be single point mutations, whereas others are due to chromosomal abnormalities. Some genetic causes give rise to a late onset hearing loss. Mitochondrial mutations can cause SNHL i.e. m.1555A>G, which makes the individual sensitive to the ototoxic effects of aminoglycoside antibiotics.\n\nThe most common cause of recessive genetic congenital hearing impairment in developed countries is DFNB1, also known as Connexin 26 deafness or GJB2-related deafness.\nThe most common syndromic forms of hearing impairment include (dominant) Stickler syndrome and Waardenburg syndrome, and (recessive) Pendred syndrome and Usher syndrome.\nMitochondrial mutations causing deafness are rare: MT-TL1 mutations cause MIDD (Maternally inherited deafness and diabetes) and other conditions which may include deafness as part of the picture.\nTMPRSS3 gene was identified by its association with both congenital and childhood onset autosomal recessive deafness. This gene is expressed in fetal cochleae and many other tissues, and is thought to be involved in the development and maintenance of the inner ear or the contents of the perilymph and endolymph. It was also identified as a tumor associated gene that is overexpressed in ovarian tumors.\nCharcot\u2013Marie\u2013Tooth disease an inherited neurological disorder with delayed onset that can affect the ears as well as other organs. The hearing loss in this condition is often ANSD (auditory neuropathy spectrum disorder) a neural cause of hearing loss.\nMuckle\u2013Wells syndrome, a rare inherited autoinflammatory disorder, can lead to hearing loss.\nAutoimmune disease: although probably rare, it is possible for autoimmune processes to target the cochlea specifically, without symptoms affecting other organs. Granulomatosis with polyangiitis, an autoimmune condition, may precipitate hearing loss.\n\nCongenital\nInfections:\nCongenital rubella syndrome, CRS, results from transplacental transmission of the rubella virus during pregnancy. CRS has been controlled by universal vaccination (MMR or MMRV vaccine).\nCytomegalovirus (CMV) infection is the most common cause of progressive sensorineural hearing loss in children. It is a common viral infection contracted by contact with infected bodily fluids such as saliva or urine and easily transmitted in nurseries and thus from toddlers to expectant mothers. CMV infection during pregnancy can affect the developing foetus and lead to learning difficulties as well as hearing loss.\nToxoplasmosis, a parasitic disease affecting 23% of the population in the U.S., can cause sensorineural deafness to the fetus in utero.\nHypoplastic auditory nerves or abnormalities of the cochlea. Abnormal development of the inner ear can occur in some genetic syndromes such as LAMM syndrome (labyrinthine aplasia, microtia and microdontia), Pendred syndrome, branchio-oto-renal syndrome, CHARGE syndrome\nGATA2 deficiency, a grouping of several disorders caused by common defect, viz., familial or sporadic inactivating mutations in one of the two parental GATA2 genes. These autosomal dominant mutations cause a reduction, i.e. a haploinsufficiency, in the cellular levels of the gene's product, GATA2. The GATA2 protein is a transcription factor critical for the embryonic development, maintenance, and functionality of blood-forming, lympathic-forming, and other tissue-forming stem cells. In consequence of these mutations, cellular levels of GATA2 are deficient and individuals develop over time hematological, immunological, lymphatic, and\/or other disorders. GATA2 deficiency-induced abnormalities in the lymphatic system are proposed to be responsible for a failure in generating the perilymphatic space around the inner ear's semicircular canals, which in turn underlies the development of sensorineural hearing loss.\n\nPresbycusis\nProgressive age-related loss of hearing acuity or sensitivity can start as early as age 18, primarily affecting the high frequencies, and men more than women. Such losses may not become apparent until much later in life. Presbycusis is by far the dominant cause of sensorineural hearing loss in industrialized societies. A study conducted in Sudan, with a population free from loud noise exposures, found significantly less cases of hearing loss when compared with age-matched cases from  an industrialized country.    Similar findings were reported by a study conducted of a population from Easter island, which reported worse hearing among those that spent time in industrialized countries when compared with those that never left the island.   Researchers have argued that factors other than differences in noise exposure, such as genetic make up, might also have contributed to the findings.   Hearing loss that worsens with age but is caused by factors other than normal aging, such as noise-induced hearing loss, is not presbycusis, although differentiating the individual effects of multiple causes of hearing loss can be difficult. One in three persons have significant hearing loss by age 65; by age 75, one in two. Age-related hearing loss is neither preventable nor reversible.\n\nNoise\nMost people living in modern society have some degree of progressive sensorineural (i.e. permanent) noise-induced hearing loss (NIHL) resulting from overloading and damaging the sensory or neural apparatus of hearing in the inner ear. NIHL is typically a drop-out or notch centered at 4000 Hz. Both intensity (SPL) and duration of exposure, and repetitive exposure to unsafe levels of noise contribute to cochlear damage that results in hearing loss. The louder the noise is, the shorter the safe amount of exposure is. NIHL can be either permanent or temporary, called a threshold shift. Unsafe levels of noise can be as little as 70 dB (about twice as loud as normal conversation) if there is prolonged (24-hour) or continuous exposure. 125 dB (a loud rock concert is ~120 dB) is the pain level; sounds above this level cause instant and permanent ear damage.\nNoise and ageing are the primary causes of presbycusis, or age-related hearing loss, the most common kind of hearing loss in industrial society. The dangers of environmental and occupational noise exposure are widely recognized. Numerous national and international organizations have established standards for safe levels of exposure to noise in industry, the environment, military, transportation, agriculture, mining and other areas. Sound intensity or sound pressure level (SPL) is measured in decibels (dB). For reference:\n\nAn increase of 6 dB represents a doubling of the SPL, or energy of the sound wave, and therefore its propensity to cause ear damage. Because human ears hear logarithmically, not linearly, it takes an increase of 10 dB to produce a sound that is perceived to be twice as loud. Ear damage due to noise is proportional to sound intensity, not perceived loudness, so it's misleading to rely on subjective perception of loudness as an indication of the risk to hearing, i.e. it can significantly underestimate the danger.\nWhile the standards differ moderately in levels of intensity and duration of exposure considered safe, some guidelines can be derived.\nThe safe amount of exposure is reduced by a factor of 2 for every exchange rate (3 dB for NIOSH standard or 5 dB for OSHA standard) increase in SPL. For example, the safe daily exposure amount at 85 dB (90 dB for OSHA) is 8 hours, while the safe exposure at 94 dB(A) (nightclub level) is only 1 hour. Noise trauma can also cause a reversible hearing loss, called a temporary threshold shift. This typically occurs in individuals who are exposed to gunfire or firecrackers, and hear ringing in their ears after the event (tinnitus).\n\nAmbient environmental noise: Populations living near airports, railyards and train stations, freeways and industrial areas are exposed to levels of noise typically in the 65 to 75 dBA range. If lifestyles include significant outdoor or open window conditions, these exposures over time can degrade hearing. U.S. Dept. of Housing and Urban Development sets standards for noise impact in residential and commercial construction zones. HUD's noise standards may be found in 24 CFR Part 51, Subpart B. Environmental noise above 65 dB defines a noise-impacted area.\nPersonal audio electronics: Personal audio equipment such as iPods (iPods often reach 115 decibels or higher), can produce powerful enough sound to cause significant NIHL.\nAcoustic trauma: Exposure to a single event of extremely loud noise (such as explosions) can also cause temporary or permanent hearing loss. A typical source of acoustic trauma is a too-loud music concert.\nWorkplace noise: The OSHA standards 1910.95 General Industry Occupational Noise Exposure and 1926.52 Construction Industry Occupational Noise Exposure identify the level of 90 dB(A) for 8 hour exposure as the level necessary to protect workers from hearing loss.\n\nDisease or disorder\nInflammatory\nSuppurative labyrinthitis or otitis interna (inflammation of the inner ear)\nDiabetes mellitus  A recent study found that hearing loss is twice as common in people with diabetes as it is in those who don't have the disease. Also, of the 86 million adults in the U.S. who have prediabetes, the rate of hearing loss is 30 percent higher than in those with normal blood glucose. It has not been established how diabetes is related to hearing loss. It is possible that the high blood glucose levels associated with diabetes cause damage to the small blood vessels in the inner ear, similar to the way in which diabetes can damage the eyes and the kidneys. Similar studies have shown a possible link between that hearing loss and neuropathy (nerve damage).\nTumor\nCerebellopontine angle tumour (junction of the pons and cerebellum) \u2013 The cerebellopontine angle is the exit site of both the facial nerve(CN7) and the vestibulocochlear nerve(CN8). Patients with these tumors often have signs and symptoms corresponding to compression of both nerves.\nAcoustic neuroma (vestibular schwannoma) \u2013 benign neoplasm of Schwann cells affecting the vestibulocochlear nerve\nMeningioma \u2013 benign tumour of the pia and arachnoid mater\nM\u00e9ni\u00e8re's disease \u2013 causes sensorineural hearing loss in the low frequency range (125 Hz to 1000 Hz). M\u00e9ni\u00e8re's disease is characterized by sudden attacks of vertigo, lasting minutes to hours preceded by tinnitus, aural fullness, and fluctuating hearing loss. It is relatively rare and commonly over diagnosed.\nBacterial meningitis e.g. pneumococcal, meningococcal, haemophilus influenzae may damage the cochlea \u2013 Hearing loss is one of the most common after-effects of bacterial meningitis. It has been estimated that 30% of bacterial meningitis cases result in mild to profound hearing loss. Children are most at risk: seventy percent of all bacterial meningitis occurs in young children under the age of five.\nViral\nAIDS and ARC patients frequently experience auditory system anomalies.\nMumps(epidemic parotitis) may result in profound sensorineural hearing loss (90 dB or more), unilaterally (one ear) or bilaterally (both ears).\nMeasles may result in auditory nerve damage but more commonly gives a mixed (sensorineural plus conductive) hearing loss, and can be bilaterally.\nRamsay Hunt syndrome type II (herpes zoster oticus)\nBacterial\nSyphilis is commonly transmitted from pregnant women to their fetuses, and about a third of the infected children will eventually become deaf.\n\nOtotoxic and neurotoxic drugs and chemicals\nSome over-the-counter as well as prescription drugs and certain industrial chemicals are ototoxic. Exposure to these can result in temporary or permanent hearing loss.\nSome medications cause irreversible damage to the ear, and are limited in their use for this reason. The most important group is the aminoglycosides (main member gentamicin). A rare mitochondrial mutation, m.1555A>G, can increase an individual's susceptibility to the ototoxic effect of aminoglycosides. Long term hydrocodone (Vicodin) abuse is known to cause rapidly progressing sensorineural hearing loss, usually without vestibular symptoms. Methotrexate, a chemotherapy agent, is also known to cause hearing loss. In most cases hearing loss does not recover when the drug is stopped. Paradoxically, methotrexate is also used in the treatment of autoimmune-induced inflammatory hearing loss.\nVarious other medications may reversibly degrade hearing. This includes loop diuretics, sildenafil (Viagra), high or sustained dosing of NSAIDs (aspirin, ibuprofen, naproxen, and various prescription drugs: celecoxib, etc.), quinine, and macrolide antibiotics (erythromycin, etc.). Cytotoxic agents such as carboplatinum, used to treat malignancies can give rise to a dose dependent SNHL, as can drugs such as desferrioxamine, used for haematological disorders such as thalassaemia; patients prescribed these drugs need to have hearing monitored.\nProlonged or repeated environmental or work-related exposure to ototoxic chemicals can also result in sensorineural hearing loss. Some of these chemicals are:\n\nbutyl nitrite \u2013 chemical used recreationally known as 'poppers'\ncarbon disulfide \u2013 a solvent used as a building block in many organic reactions\nstyrene, an industrial chemical precursor of polystyrene, a plastic\ncarbon monoxide, a poisonous gas resulting from incomplete combustion\nheavy metals: tin, lead, manganese, mercury\nhexane, an industrial solvent and one of the significant constituents of gasoline\nethylbenzene, an industrial solvent used in the production of styrene\ntoluene and xylene, highly poisonous petrochemical solvents. Toluene is a component of high-octane gasoline; xylene is used in the production of polyester fibers and resins.\ntrichloroethylene, an industrial degreasing solvent\nOrganophosphate pesticides\n\nHead trauma\nThere can be damage either to the ear itself or to the central auditory pathways that process the information conveyed by the ears. People who sustain head injury are susceptible to hearing loss or tinnitus, either temporary or permanent. Contact sports like football (U.S. NFL), hockey and cricket have a notable incidence of head injuries (concussions). In one survey of retired NFL players, all of whom reported one or more concussions during their playing careers, 25% had hearing loss and 50% had tinnitus.\n\nPerinatal conditions\nThese are much more common in premature babies, particularly those under 1500 g at birth. Premature birth can be associated with problems that result in sensorineural hearing loss such as anoxia or hypoxia (poor oxygen levels), jaundice, intracranial haemorrhages, meningitis. Fetal alcohol syndrome is reported to cause hearing loss in up to 64% of infants born to alcoholic mothers, from the ototoxic effect on the developing fetus, plus malnutrition during pregnancy from the excess alcohol intake.\n\nIodine deficiency \/ Hypothyroidism\nIodine deficiency and endemic hypothyroidism are associated with hearing loss. If a pregnant mother has insufficient iodine intake during pregnancy it affects the development of the inner ear in the foetus leading to sensorineural deafness. This occurs in certain areas of the world, such as the Himalayas, where iodine is deficient in the soil and thus the diet. In these areas there is a high incidence of endemic goitre. This cause of deafness is prevented by adding iodine to salt.\n\nBrain stroke\nBrain stroke in a region affecting auditory function such as a posterior circulation infarct has been associated with deafness.\n\nPathophysiology\nSensory hearing loss is caused by abnormal structure or function of the hair cells of the organ of Corti in the cochlea. Neural hearing impairments are consequent upon damage to the eighth cranial nerve (the vestibulocochlear nerve) or the auditory tracts of the brainstem. If higher levels of the auditory tract are affected this is known as central deafness. Central deafness may present as sensorineural deafness but should be distinguishable from the history and audiological testing.\n\nCochlear dead regions in sensory hearing loss\nHearing impairment may be associated with damage to the hair cells in the cochlea. Sometimes there may be complete loss of function of inner hair cells (IHCs) over a certain region of the cochlea; this is called a \"dead region\". The region can be defined in terms of the range of characteristic frequencies (CFs) of the IHCs and\/or neurons immediately adjacent to the dead region.\n\nCochlear hair cells\nOuter hair cells (OHCs) contribute to the structure of the Organ of Corti, which is situated between the basilar membrane and the tectorial membrane within the cochlea (See Figure 3). The tunnel of corti, which runs through the Organ of Corti, divides the OHCs and the inner hair cells (IHCs). OHCs are connected to the reticular laminar and the Deiters\u2019 cells. There are roughly twelve thousand OHCs in each human ear, and these are arranged in up to five rows. Each OHC has small tufts of 'hairs', or cilia, on their upper surface known as stereocilia, and these are also arranged into rows which are graded in height. There are approximately 140 stereocilia on each OHC.\nThe fundamental role of the OHCs and the IHCs is to function as sensory receptors. The main function of the IHCs is to transmit sound information via afferent neurons. They do this by transducing mechanical movements or signals into neural activity. When stimulated, the stereocilia on the IHCs move, causing a flow of electric current to pass through the hair cells. This electric current creates action potentials within the connected afferent neurons.\nOHCs are different in that they actually contribute to the active mechanism of the cochlea. They do this by receiving mechanical signals or vibrations along the basilar membrane, and transducing them into electrochemical signals. The stereocilia found on OHCs are in contact with the tectorial membrane. Therefore, when the basilar membrane moves due to vibrations, the stereocilia bend. The direction in which they bend, dictates the firing rate of the auditory neurons connected to the OHCs.\nThe bending of the stereocilia towards the basal body of the OHC causes excitation of the hair cell. Thus, an increase in firing rate of the auditory neurons connected to the hair cell occurs. On the other hand, the bending of the stereocilia away from the basal body of the OHC causes inhibition of the hair cell. Thus, a decrease in firing rate of the auditory neurons connected to the hair cell occurs. OHCs are unique in that they are able to contract and expand (electromotility). Therefore, in response to the electrical stimulations provided by the efferent nerve supply, they can alter in length, shape and stiffness. These changes influence the response of the basilar membrane to sound. It is therefore clear that the OHCs play a major role in the active processes of the cochlea. The main function of the active mechanism is to finely tune the basilar membrane, and provide it with a high sensitivity to quiet sounds. The active mechanism is dependent on the cochlea being in good physiological condition. However, the cochlea is very susceptible to damage.\n\nHair cell damage\nSNHL is most commonly caused by damage to the OHCs and the IHCs.  There are two methods by which they might become damaged. Firstly, the entire hair cell might die. Secondly, the stereocilia might become distorted or destroyed. Damage to the cochlea can occur in several ways, for example by viral infection, exposure to ototoxic chemicals, and intense noise exposure. Damage to the OHCs results in either a less effective active mechanism, or it may not function at all. OHCs contribute to providing a high sensitivity to quiet sounds at a specific range of frequencies (approximately 2\u20134 kHz). Thus, damage to the OHCs results in the reduction of sensitivity of the basilar membrane to weak sounds. Amplification to these sounds is therefore required, in order for the basilar membrane to respond efficiently. IHCs are less susceptible to damage in comparison to the OHCs. However, if they become damaged, this will result in an overall loss of sensitivity.\n\nNeural tuning curves\nFrequency selectivity\nThe traveling wave along the basilar membrane peaks at different places along it, depending on whether the sound is low or high frequency. Due to the mass and stiffness of the basilar membrane, low frequency waves peak in the apex, while high frequency sounds peak in the basal end of the cochlea. Therefore, each position along the basilar membrane is finely tuned to a particular frequency. These specifically tuned frequencies are referred to as characteristic frequencies (CF).\nIf a sound entering the ear is displaced from the characteristic frequency, then the strength of response from the basilar membrane will progressively lessen. The fine tuning of the basilar membrane is created by the input of two separate mechanisms. The first mechanism being a linear passive mechanism, which is dependent on the mechanical structure of the basilar membrane and its surrounding structures. The second mechanism is a non-linear active mechanism, which is primarily dependent on the functioning of the OHCs, and also the general physiological condition of the cochlea itself. The base and apex of the basilar membrane differ in stiffness and width, which cause the basilar membrane to respond to varying frequencies differently along its length. The base of the basilar membrane is narrow and stiff, resulting in it responding best to high frequency sounds. The apex of the basilar membrane is wider and much less stiff in comparison to the base, causing it to respond best to low frequencies.\nThis selectivity to certain frequencies can be illustrated by neural tuning curves. These demonstrate the frequencies a fiber responds to, by showing threshold levels (dB SPL) of auditory nerve fibers as a function of different frequencies. This demonstrates that auditory nerve fibers respond best, and hence have better thresholds at the fiber's characteristic frequency and frequencies immediately surrounding it. The basilar membrane is said to be \u2018sharply tuned\u2019 due to the sharp V-shaped curve, with its \u2018tip\u2019 centered at the auditory fibers characteristic frequency. This shape shows how few frequencies a fiber responds to. If it were a broader \u2018V\u2019 shape, it would be responding to more frequencies (See Figure 4).\n\nIHC vs OHC hearing loss\nA normal neural tuning curve is characterised by a broadly tuned low frequency \u2018tail\u2019, with a finely tuned middle frequency \u2018tip\u2019. However, where there is partial or complete damage to the OHCs, but with unharmed IHCs, the resulting tuning curve would show the elimination of sensitivity at the quiet sounds. I.e. where the neural tuning curve would normally be most sensitive (at the \u2018tip\u2019) (See Figure 5).\nWhere both the OHCs and the IHCs are damaged, the resulting neural tuning curve would show the elimination of sensitivity at the \u2018tip'. However, due to IHC damage, the whole tuning curve becomes raised, giving a loss of sensitivity across all frequencies (See Figure 6). It is only necessary for the first row of OHCs to be damaged for the elimination of the finely tuned \u2018tip\u2019 to occur. This supports the idea that the incidence of OHC damage and thus a loss of sensitivity to quiet sounds, occurs more than IHC loss.\nWhen the IHCs or part of the basilar membrane are damaged or destroyed, so that they no longer function as transducers, the result is a \u2018dead region\u2019. Dead regions can be defined in terms of the characteristic frequencies of the IHC, related to the specific place along the basilar membrane where the dead region occurs. Assuming that there has been no shift in the characteristic frequencies relating to certain regions of the basilar membrane, due to the damage of OHCs. This often occurs with IHC damage. Dead regions can also be defined by the anatomical place of the non-functioning IHC (such as an \u201capical dead region\u201d), or by the characteristic frequencies of the IHC adjacent to the dead region.\n\nDead region audiometry\nPure tone audiometry (PTA)\nDead regions affect audiometric results, but perhaps not in the way expected. For example, it may be expected that thresholds would not be obtained at the frequencies within the dead region, but would be obtained at frequencies adjacent to the dead region. Therefore, assuming normal hearing exists around the dead region, it would produce an audiogram that has a dramatically steep slope between the frequency where a threshold is obtained, and the frequency where a threshold cannot be obtained due to the dead region.\n\nHowever, it appears that this is not the case. Dead regions cannot be clearly found via PTA audiograms. This may be because although the neurons innervating the dead region, cannot react to vibration at their characteristic frequency. If the basilar membrane vibration is large enough, neurons tuned to different characteristic frequencies such as those adjacent to the dead region, will be stimulated due to the spread of excitation. Therefore, a response from the patient at the test frequency will be obtained. This is referred to as \u201coff-place listening\u201d, and is also known as \u2018off-frequency listening\u2019. This will lead to a false threshold being found. Thus, it appears a person has better hearing than they actually do, resulting in a dead region being missed. Therefore, using PTA alone, it is impossible to identify the extent of a dead region (See Figure 7 and 8).\nConsequently, how much is an audiometric threshold affected by a tone with its frequency within a dead region? This depends on the location of the dead region. Thresholds at low frequency dead regions, are more inaccurate than those at higher frequency dead regions. This has been attributed to the fact that excitation due to vibration of the basilar membrane spreads upwards from the apical regions of the basilar membrane, more than excitation spreads downwards from higher frequency basal regions of the cochlea. This pattern of the spread of excitation is similar to the \u2018upward spread of masking\u2019 phenomenon. If the tone is sufficiently loud to produce enough excitation at the normally functioning area of the cochlea, so that it is above that areas threshold. The tone will be detected, due to off-frequency listening which results in a misleading threshold.\nTo help to overcome the issue of PTA producing inaccurate thresholds within dead regions, masking of the area beyond the dead region that is being stimulated can be used. This means that the threshold of the responding area is sufficiently raised, so that it cannot detect the spread of excitation from the tone. This technique has led to the suggestion that a low frequency dead region may be related to a loss of 40-50 dB. However, as one of the aims of PTA is to determine whether or not there is a dead region, it may be difficult to assess which frequencies to mask without the use of other tests.\nBased on research it has been suggested that a low frequency dead region may produce a relatively flat loss, or a very gradually sloping loss towards the higher frequencies. As the dead region will be less detectable due to the upward spread of excitation. Whereas, there may be a more obvious steeply sloping loss at high frequencies for a high frequency dead region. Although it is likely that the slope represents the less pronounced downward spread of excitation, rather than accurate thresholds for those frequencies with non-functioning hair cells. Mid-frequency dead regions, with a small range, appear to have less effect on the patient's ability to hear in everyday life, and may produce a notch in the PTA thresholds. Although it is clear that PTA is not the best test to identify a dead region.\n\nPsychoacoustic tuning curves (PTC) and threshold equalizing noise (TEN) tests\nAlthough some debate continues regarding the reliability of such tests, it has been suggested that psychoacoustic tuning curves (PTCs) and threshold-equalising noise (TEN) results may be useful in detecting dead regions, rather than PTA. PTCs are similar to neural tuning curves. They illustrate the level of a masker (dB SPL) tone at threshold, as a function of deviation from center frequency (Hz). They are measured by presenting a fixed low intensity pure tone while also presenting a narrow-band masker, with a varying center frequency. The masker level is varied, so that the level of masker needed to just mask the test signal is found for the masker at each center frequency. The tip of the PTC is where the masker level needed to just mask the test signal is the lowest. For normal hearing people this is when the masker center frequency is closest to the frequency of the test signal (See Figure 9).\nIn the case of dead regions, when the test signal lies within the boundaries of a dead region, the tip of the PTC will be shifted to the edge of the dead region, to the area that is still functioning and detecting the spread of excitation from the signal. In the case of a low frequency dead region, the tip is shifted upwards indicating a low frequency dead region starting at the tip of the curve. For a high frequency dead region, the tip is shifted downwards from the signal frequency to the functioning area below the dead region. However, the traditional method of obtaining PTCs is not practical for clinical use, and it has been argued that TENs are not accurate enough. A fast method for finding PTCs has been developed and it may provide the solution. However, more research to validate this method is required, before it can be accepted clinically.\n\nPerceptual consequences of a dead region\nAudiogram configurations are not good indicators of how a dead region will affect a person functionally, mainly due to individual differences. For example, a sloping audiogram is often present with a dead region, due to the spread of excitation. However, the individual may well be affected differently from someone with a corresponding sloped audiogram caused by partial damage to hair cells rather than a dead region. They will perceive sounds differently, yet the audiogram suggests that they have the same degree of loss. Huss and Moore investigated how hearing impaired patients perceive pure tones, and found that they perceive tones as noisy and distorted, more (on average) than a person without a hearing impairment. However, they also found that the perception of tones as being like noise, was not directly related to frequencies within the dead regions, and was therefore not an indicator of a dead region. This therefore suggests that audiograms, and their poor representation of dead regions, are inaccurate predictors of a patient's perception of pure tone quality.\nResearch by Kluk and Moore  has shown that dead regions may also affect the patient's perception of frequencies beyond the dead regions. There is an enhancement in the ability to distinguish between tones that differ very slightly in frequency, in regions just beyond the dead regions compared to tones further away. An explanation for this may be that cortical re-mapping has occurred. Whereby, neurons which would normally be stimulated by the dead region, have been reassigned to respond to functioning areas near it. This leads to an over-representation of these areas, resulting in an increased perceptual sensitivity to small frequency differences in tones.\n\nVestibulocochlear nerve pathology\ncongenital deformity of the internal auditory canal,\nneoplastic and pseudo-neoplastic lesions, with special detailed emphasis on schwannoma of the eighth cranial nerve (acoustic neuroma),\nnon-neoplastic Internal Auditory Canal\/CerebelloPontine Angle pathology, including vascular loops,\n\nDiagnosis\nCase history\nBefore examination, a case history provides guidance about the context of the hearing loss.\n\nmajor concern\npregnancy and childbirth information\nmedical history\ndevelopment history\nfamily history\n\nOtoscopy\nDirect examination of the external canal and tympanic membrane (ear drum) with an otoscope, a medical device inserted into the ear canal that uses light to examine the condition of the external ear and tympanic membrane, and\nmiddle ear through the semi-translucent membrane.\n\nDifferential testing\nDifferential testing is most useful when there is unilateral hearing loss, and distinguishes conductive from sensorineural loss. These are conducted with a low frequency tuning fork, usually 512 Hz, and contrast measures of air and bone conducted sound transmission.\n\nWeber test, in which a tuning fork is touched to the midline of the forehead, localizes to the normal ear in people with unilateral sensorineural hearing loss.\nRinne test, which tests air conduction vs. bone conduction is positive, because both bone and air conduction are reduced equally.\nless common Bing and Schwabach variants of the Rinne test.\nabsolute bone conduction (ABC) test.\nTable 1. A table comparing sensorineural to conductive hearing loss\n\nOther, more complex, tests of auditory function are required to distinguish the different types of hearing loss. Bone conduction thresholds can differentiate sensorineural hearing loss from conductive hearing loss. Other tests, such as oto-acoustic emissions, acoustic stapedial reflexes, speech audiometry and evoked response audiometry are needed to distinguish sensory, neural and auditory processing hearing impairments.\n\nTympanometry\nA tympanogram is the result of a test with a tympanometer. It tests the function of the middle ear and mobility of the eardrum. It can help identify conductive hearing loss due to disease of the middle ear or eardrum from other kinds of hearing loss including SNHL.\n\nAudiometry\nAn audiogram is the result of a hearing test. The most common type of hearing test is pure tone audiometry (PTA). It charts the thresholds of hearing sensitivity at a selection of standard frequencies between 250 and 8000 Hz. There is also high frequency pure tone audiometry which tests frequencies from 8000 to 20,000 Hz. PTA can be used to differentiate between conductive hearing loss, sensorineural hearing loss and mixed hearing loss. A hearing loss can be described by its degree i.e. mild, moderate, severe or profound, or by its shape i.e. high frequency or sloping, low frequency or rising, notched, U-shaped or 'cookie-bite', peaked or flat.\nThere are also other kinds of audiometry designed to test hearing acuity rather than sensitivity (speech audiometry), or to test\nauditory neural pathway transmission (evoked response audiometry).\n\nMagnetic resonance imaging\nMRI scans can be used to identify gross structural causes of hearing loss. They are used for congenital hearing loss when changes to the shape of the inner ear or nerve of hearing may help diagnosis of the cause of the hearing loss. They are also useful in cases where a tumour is suspected or to determine the degree of damage in a hearing loss caused by bacterial infection or auto-immune disease. Scanning is of no value in age-related deafness.\n\nPrevention\nPresbycusis is the leading cause of SNHL and is progressive and nonpreventable, and at this time, we do not have either somatic or gene therapy to counter heredity-related SNHL. But other causes of acquired SNHL are largely preventable, especially nosocusis type causes. This would involve avoiding environmental noise, and traumatic noise such as rock concerts and nightclubs with loud music. Use of noise attenuation measures like ear plugs is an alternative, as well as learning about the noise levels one is exposed to. Currently, several accurate sound level measurement apps exist.  Reducing exposure time can also help manage risk from loud exposures.\n\nTreatment\nTreatment modalities fall into three categories: pharmacological, surgical, and management. As SNHL is a physiologic degradation and considered permanent, there are as of this time, no approved or recommended treatments.\nThere have been significant advances in identification of human deafness genes and elucidation of their cellular mechanisms as well as their physiological function in mice.  Nevertheless, pharmacological treatment options are very limited and clinically unproven. Such pharmaceutical treatments as are employed are palliative rather than curative, and addressed to the underlying cause if one can be identified, in order to avert progressive damage.\nProfound or total hearing loss may be amenable to management by cochlear implants, which stimulate cochlear nerve endings directly. A cochlear implant is surgical implantation of a battery powered electronic medical device in the inner ear. Unlike hearing aids, which make sounds louder, cochlear implants do the work of damaged parts of the inner ear (cochlea) to provide sound signals to the brain. These consist of both internal implanted electrodes and magnets and external components. The quality of sound is different than natural hearing but may enable the recipient to better recognize speech and environmental sounds.\nBecause of risk and expense, such surgery is reserved for cases of severe and disabling hearing impairment\nManagement of sensorineural hearing loss involves employing strategies to support existing hearing such as lip-reading, enhanced communication etc. and amplification using hearing aids. Hearing aids are specifically tuned to the individual hearing loss to give maximum benefit.\n\nResearch\nPharmaceuticals\nAntioxidant vitamins \u2013 Researchers at the University of Michigan report that a combination of high doses of vitamins A, C, and E, and Magnesium, taken one hour before noise exposure and continued as a once-daily treatment for five days, was very effective at preventing permanent noise-induced hearing loss in animals.\nTanakan \u2013 a brand name for an international prescription drug extract of Ginkgo biloba. It is classified as a vasodilator. Among its research uses is treatment of sensorineural deafness and tinnitus presumed to be of vascular origin.\nCoenzyme Q10 \u2013 a substance similar to a vitamin, with antioxidant properties. It is made in the body, but levels fall with age.\nEbselen, a synthetic drug molecule that mimics glutathione peroxidase (GPx), a critical enzyme in the inner ear that protects it from damage caused by loud sounds or noise\n\nStem cell and gene therapy\nHair cell regeneration using stem cell and gene therapy is years or decades away from being clinically feasible. However, studies are currently underway on the subject, with the first FDA-approved trial beginning in February 2012.\n\nSudden sensorineural hearing loss\nSudden sensorineural hearing loss (SSHL or SSNHL), commonly known as sudden deafness, occurs as an unexplained, rapid loss of hearing\u2014usually in one ear\u2014either at once or over several days. Nine out of ten people with SSHL lose hearing in only one ear. It should be considered a medical emergency. Delaying diagnosis and treatment may render treatment less effective or ineffective.\nExperts estimate that SSHL strikes one person per 100 every year, typically adults in their 40s and 50s. The actual number of new cases of SSHL each year could be much higher because the condition often goes undiagnosed.\n\nPresentation\nMany people notice that they have SSHL when they wake up in the morning. Others first notice it when they try to use the deafened ear, such as when they use a phone. Still others notice a loud, alarming \"pop\" just before their hearing disappears. People with sudden deafness often become dizzy, have ringing in their ears (tinnitus), or both.\n\nDiagnosis\nSSHL is diagnosed via pure tone audiometry. If the test shows a loss of at least 30 dB in three adjacent frequencies, the hearing loss is diagnosed as SSHL. For example, a hearing loss of 30 dB would make conversational speech sound more like a whisper.\n\nCauses\nOnly 10 to 15 percent of the cases diagnosed as SSHL have an identifiable cause. Most cases are classified as idiopathic, also called sudden idiopathic hearing loss (SIHL) and idiopathic sudden sensorineural hearing loss (ISSHL or ISSNHL) The majority of evidence points to some type of inflammation in the inner ear as the most common cause of SSNHL.\n\nInfection is believed to be the most common cause of SSNHL, accounting for approximately 13% of cases. Viruses that have been associated with SSNHL include cytomegalovirus, rubella, measles, mumps, human immunodeficiency virus (HIV), herpes simplex virus (HSV), varicella zoster virus (VZV), and West Nile virus. Patients with COVID-19 may also be at increased risk for developing SSNHL.\nVascular ischemia of the inner ear or cranial nerve VIII (CN8)\nPerilymph fistula, usually due to a rupture of the round or oval windows and the leakage of perilymph. The patient will usually also experience vertigo or imbalance. A history of trauma is usually present and changes to hearing or vertigo occur with alteration in intracranial pressure such as with straining; lifting, blowing etc.\nAutoimmune \u2013 can be due to an autoimmune illness such as systemic lupus erythematosus, granulomatosis with polyangiitis\n\nTreatment\nHearing loss completely recovers in around 35-39% of patients with SSNHL, usually within one to two weeks from onset. Eighty-five percent of those who receive treatment from an otolaryngologist (sometimes called an ENT surgeon) will recover some of their hearing.\n\nvitamins and antioxidants\nvasodilators\nbetahistine (Betaserc), an anti-vertigo drug\nhyperbaric oxygen\nrheologic agents that reduce blood viscosity (such as hydroxyethyl starch, dextran and pentoxifylline)\nanti-inflammatory agents, primarily oral corticosteroids (such as prednisone and dexamethasone)\nIntratympanic administration \u2013 Gel formulations are under investigation to provide more consistent drug delivery to the inner ear. Local drug delivery can be accomplished through intratympanic administration, a minimally invasive procedure where the ear drum is anesthetized and a drug is administered into the middle ear. From the middle ear, a drug can diffuse across the round window membrane into the inner ear. Intratympanic administration of steroids may be effective for sudden sensorineural hearing loss for some patients, but high quality clinical data has not been generated. Intratympanic administration of an anti-apoptotic peptide (JNK inhibitor) is currently being evaluated in late-stage clinical development.\n\nEpidemiology\nGeneral hearing loss affects close to 10% of the global population. In the United States alone, it is expected that 13.5 million Americans have sensorineural hearing loss. Of those with sensorineural hearing loss, approximately 50% are congenitally related. The other 50% are due to maternal or fetal infections, post-natal infections, viral infections due to rubella or cytomegalovirus, ototoxic drugs, exposure to loud sounds, severe head trauma, and premature births \nOf the genetically related sensorineural hearing loss cases, 75% are autosomal recessive, 15-20% autosomal dominant, and 1-3% sex-linked. While the specific gene and protein is still unknown, mutations in the connexin 26 gene near the DFNB1 locus of chromosome 13 are thought to account for most of the autosomal recessive genetic-related sensorineural hearing loss \nAt least 8.5 per 1000 children younger than age 18 have sensorineural hearing loss. General hearing loss is proportionally related to age. At least 314 per 1000 people older than age 65 have hearing loss. Several risk factors for sensorineural hearing loss have been studied over the past decade. Osteoporosis, stapedectomy surgery, pneumococcal vaccinations, mobile phone users, and hyperbilirubinemia at birth are among some of the known risk factors.\n\nSee also\nConductive hearing loss, hearing loss caused primarily by conditions in the middle ear\nCortical deafness, another kind of nerve deafness\nHearing loss\nInner ear, the innermost portion of the ear containing the sensorineural apparatus of hearing\nOtosclerosis, a sometimes associated or predecessor conductive hearing loss condition of the middle ear\nTinnitus, ringing in the ears, a common accompaniment of SNHL\n\nNotes\nReferences\n38.Ghazavi H,Kargoshaei A-A,Jamshidi-Koohsari M,\"Investigation of vitamin D levels in patients with Sudden Sensory-Neural Hearing Loss and its effect on treatment\",American journal of otolaryngology and head and neck medicine and surgery, November 2019\nhttps:\/\/doi.org\/10.1016\/j.amjoto.2019.102327\n\nExternal links\nHearing Loss Web","171":"Signs and symptoms are the observed or detectable signs, and experienced symptoms of an illness, injury, or condition.\nSigns are objective and externally observable; symptoms are a person's reported subjective experiences.\nA sign for example may be a higher or lower temperature than normal, raised or lowered blood pressure or an abnormality showing on a medical scan. A symptom is something out of the ordinary that is experienced by an individual such as feeling feverish, a headache or other pains in the body.\n\nSigns and symptoms\nSigns\nA medical sign is an objective observable indication of a disease, injury, or medical condition that may be detected during a physical examination. These signs may be visible, such as a rash or bruise, or otherwise detectable such as by using a stethoscope or taking blood pressure. Medical signs, along with symptoms, help in forming a diagnosis. Some examples of signs are nail clubbing of either the fingernails or toenails, an abnormal gait, and a limbal ring a darkened ring around the iris of the eye.\n\nIndications\nA sign is different from an \"indication\" \u2013 the activity of a condition 'pointing to' (thus \"indicating\") a remedy, not the reverse (viz., it is not a remedy 'pointing to' a condition) \u2013 which is a specific reason for using a particular treatment.\n\nSymptoms\nA symptom is something felt or experienced, such as pain or dizziness. Signs and symptoms are not mutually exclusive, for example a subjective feeling of fever can be noted as sign by using a thermometer that registers a high reading. The CDC lists various diseases by their signs and symptoms such as for measles which includes a high fever, conjunctivitis, and cough, followed a few days later by the measles rash.\n\nCardinal signs and symptoms\nCardinal signs and symptoms are specific even to the point of being pathognomonic. A cardinal sign or cardinal symptom can also refer to the major sign or symptom of a disease. Abnormal reflexes can indicate problems with the nervous system. Signs and symptoms are also applied to physiological states outside the context of disease, as for example when referring to the signs and symptoms of pregnancy, or the symptoms of dehydration. Sometimes a disease may be present without showing any signs or symptoms when it is known as being asymptomatic. The disorder may be discovered through tests including scans. An infection may be asymptomatic which may still be transmissible.\n\nSyndrome\nSigns and symptoms are often non-specific, but some combinations can be suggestive of certain diagnoses, helping to narrow down what may be wrong. A particular set of characteristic signs and symptoms that may be associated with a disorder is known as a syndrome. In cases where the underlying cause is known the syndrome is named as for example Down syndrome and Noonan syndrome. Other syndromes such as acute coronary syndrome may have a number of possible causes.\n\nTerms\nSymptomatic\nWhen a disease is evidenced by symptoms it is known as symptomatic. There are many conditions including subclinical infections that display no symptoms, and these are termed asymptomatic. \nSigns and symptoms may be mild or severe, brief or longer-lasting when they may become reduced (remission), or then recur (relapse or recrudescence) known as a flare-up. A flare-up may show more severe symptoms.\nThe term chief complaint, also \"presenting problem\", is used to describe the initial concern of an individual when seeking medical help, and once this is clearly noted a history of the present illness may be taken. The symptom that ultimately leads to a diagnosis is called a cardinal symptom. Some symptoms can be misleading as a result of referred pain, where for example a pain in the right shoulder may be due to an inflamed gallbladder and not to presumed muscle strain.\n\nProdrome\nMany diseases have an early prodromal stage where a few signs and symptoms may suggest the presence of a disorder before further specific symptoms may emerge. Measles for example has a prodromal presentation that includes a hacking cough, fever, and Koplik's spots in the mouth. Over half of migraine episodes have a prodromal phase. Schizophrenia has a notable prodromal stage, as has dementia.\n\nNonspecific symptoms\nSome symptoms are specific, that is, they are associated with a single, specific medical condition.\nNonspecific symptoms, sometimes also called equivocal symptoms, are not specific to a particular condition. They include unexplained weight loss, headache, pain, fatigue, loss of appetite, night sweats, and malaise. A group of three particular nonspecific symptoms \u2013 fever, night sweats, and weight loss \u2013 over a period of six months are termed B symptoms associated with lymphoma and indicate a poor prognosis.\nOther sub-types of symptoms include:\n\nconstitutional or general symptoms, which affect general well-being or the whole body, such as a fever;\nconcomitant symptoms, which are symptoms that occur at the same time as the primary symptom;\nprodromal symptoms, which are the first symptoms of an bigger set of problems;\ndelayed symptoms, which happen some time after the trigger; and\nobjective symptoms, which are symptoms whose existence can be observed and confirmed by a healthcare provider.\n\nVital signs\nVital signs are the four signs that can give an immediate measurement of the body's overall functioning and health status. They are temperature, heart rate, breathing rate, and blood pressure. The ranges of these measurements vary with age, weight, gender and with general health.\nA digital application has been developed for use in clinical settings that measures three of the vital signs (not temperature) using just a smartphone, and has been approved by NHS England. The application is registered as Lifelight First, and Lifelight Home is under development (2020) for monitoring-use by people at home using just the camera on their smartphone or tablet. This will additionally measure oxygen saturation and atrial fibrillation. Other devices are then not needed.\n\nSyndromes\nMany conditions are indicated by a group of known signs, or signs and symptoms. These can be a group of three known as a triad; a group of four (\"tetrad\"); or a group of five (\"pentad\").\nAn example of a triad is Meltzer's triad presenting purpura a rash, arthralgia painful joints, and myalgia painful and weak muscles. Meltzer's triad indicates the condition cryoglobulinemia. Huntington's disease is a neurodegenerative disease that is characterized by a triad of motor, cognitive, and psychiatric signs and symptoms. A large number of these groups that can be characteristic of a particular disease are known as a syndrome. Noonan syndrome for example, has a diagnostic set of unique facial and musculoskeletal features. Some syndromes such as nephrotic syndrome may have a number of underlying causes that are all related to diseases that affect the kidneys.\nSometimes a child or young adult may have symptoms suggestive of a genetic disorder that cannot be identified even after genetic testing. In such cases the term SWAN (syndrome without a name) may be used. Often a diagnosis may be made at some future point when other more specific symptoms emerge but many cases may remain undiagnosed. The inability to diagnose may be due to a unique combination of symptoms or an overlap of conditions, or to the symptoms being atypical of a known disorder, or to the disorder being extremely rare.\nIt is possible that a person with a particular syndrome might not display every single one of the signs and\/or symptoms that compose\/define a syndrome.\n\nPositive and negative\nSensory symptoms can also be described as positive symptoms, or as negative symptoms depending on whether the symptom is abnormally present such as tingling or itchiness, or abnormally absent such as loss of smell. The following terms are used for negative symptoms \u2013 hypoesthesia is a partial loss of sensitivity to moderate stimuli, such as pressure, touch, warmth, cold. Anesthesia is the complete loss of sensitivity to stronger stimuli, such as pinprick. Hypoalgesia  (analgesia) is loss of sensation to painful stimuli.\nSymptoms are also grouped in to negative and positive for some mental disorders such as schizophrenia. Positive symptoms are those that are present in the disorder and are not normally experienced by most individuals and reflects an excess or distortion of normal functions; examples are hallucinations, delusions, and bizarre behavior. Negative symptoms are functions that are normally found but that are diminished or absent,  such as apathy and anhedonia.\n\nDynamic and static\nDynamic symptoms are capable of change depending on circumstance, whereas static symptoms are fixed or unchanging regardless of circumstance. For example, the symptoms of exercise intolerance are dynamic as they are brought on by exercise, but alleviate during rest. Fixed muscle weakness is a static symptom as the muscle will be weak regardless of exercise or rest.\nA majority of patients with metabolic myopathies have dynamic rather than static findings, typically experiencing exercise intolerance, muscle pain, and cramps with exercise rather than fixed weakness. Those with the metabolic myopathy of McArdle's disease (GSD-V) and some individuals with phosphoglucomutase deficiency (CDG1T\/GSD-XIV), initially experience exercise intolerance during mild-moderate aerobic exercise, but the symptoms alleviate after 6\u201310 minutes in what is known as \"second wind\".\n\nNeuropsychiatric\nNeuropsychiatric symptoms are present in many degenerative disorders including dementia, and Parkinson's disease. Symptoms commonly include apathy, anxiety, and depression. Neurological and psychiatric symptoms are also present in some genetic disorders such as Wilson's disease. Symptoms of executive dysfunction are often found in many disorders including schizophrenia, and ADHD.\n\nRadiologic\nRadiologic signs are abnormal medical findings on imaging scanning. These include the Mickey Mouse sign and the Golden S sign. When using imaging to find the cause of a complaint, another unrelated finding may be found known as an incidental finding.\n\nCardinal\nCardinal signs and symptoms are those that may be diagnostic, and pathognomonic \u2013 of a certainty of diagnosis. Inflammation for example has a recognised group of cardinal signs and symptoms, as does exacerbations of chronic bronchitis, and Parkinson's disease.\nIn contrast to a pathognomonic cardinal sign, the absence of a sign or symptom can often rule out a condition. This is known by the Latin term sine qua non. For example, the absence of known genetic mutations specific for a hereditary disease would rule out that disease. Another example is where the vaginal pH is less than 4.5, a diagnosis of bacterial vaginosis would be excluded.\n\nReflexes\nA reflex is an automatic response in the body to a stimulus. Its absence, reduced (hypoactive), or exaggerated (hyperactive) response can be a sign of damage to the central nervous system or peripheral nervous system. In the patellar reflex (knee-jerk) for example, its reduction or absence is known as Westphal's sign and may indicate damage to lower motor neurons. When the response is exaggerated damage to the upper motor neurons may be indicated.\n\nFacies\nA number of medical conditions are associated with a distinctive facial expression or appearance known as a facies. An example is elfin facies which has facial features like those of the elf, and this may be associated with Williams syndrome, or Donohue syndrome. The most well-known facies is probably the Hippocratic facies that is seen on a person as they near death.\n\nAnamnestic signs\nAnamnestic signs (from anamn\u0113stik\u00f3s, \u1f00\u03bd\u03b1\u03bc\u03bd\u03b7\u03c3\u03c4\u03b9\u03ba\u03cc\u03c2, \"able to recall to mind\") are signs that indicate a past condition, for example paralysis in an arm may indicate a past stroke.:\u200a81\n\nAsymptomatic\nSome diseases including cancers, and infections may be present but show no signs or symptoms\nand these are known as asymptomatic. A gallstone may be asymptomatic and only discovered as an incidental finding. Easily spreadable viral infections such as COVID-19 may be asymptomatic but may still be transmissible.\n\nHistory\nSymptomatology\nA symptom (from Greek \u03c3\u03cd\u03bc\u03c0\u03c4\u03c9\u03bc\u03b1, \"accident, misfortune, that which befalls\", from \u03c3\u03c5\u03bc\u03c0\u03af\u03c0\u03c4\u03c9, \"I befall\", from \u03c3\u03c5\u03bd- \"together, with\" and \u03c0\u03af\u03c0\u03c4\u03c9, \"I fall\") is a departure from normal function or feeling. Symptomatology (also called semiology) is a branch of medicine dealing with the signs and symptoms of a disease. This study also includes the indications of a disease. It was first described as semiotics by Henry Stubbe in 1670 a term now used for the study of sign communication.\nPrior to the nineteenth century there was little difference in the powers of observation between physician and patient. Most medical practice was conducted as a co-operative interaction between the physician and patient; this was gradually replaced by a \"monolithic consensus of opinion imposed from within the community of medical investigators\". Whilst each noticed much the same things, the physician had a more informed interpretation of those things: \"the physicians knew what the findings meant and the layman did not\".:\u200a82\n\nDevelopment of medical testing\nA number of advances introduced mostly in the 19th century, allowed for more objective assessment by the physician in search of a diagnosis, and less need of input from the patient. During the 20th century the introduction of a wide range of imaging techniques and other testing methods such as genetic testing, clinical chemistry tests, molecular diagnostics and pathogenomics have made a huge impact on diagnostic capability.\n\nIn 1761 the percussion technique for diagnosing respiratory conditions was discovered by Leopold Auenbrugger. This method of tapping body cavities to note any abnormal sounds had already been in practice for a long time in cardiology. Percussion of the thorax became more widely known after 1808 with the translation of Auenbrugger's work from Latin into French by Jean-Nicolas Corvisart.\nIn 1819 the introduction of the stethoscope by Ren\u00e9 Laennec began to replace the centuries-old technique of immediate auscultation \u2013 listening to the heart by placing the ear directly on the chest, with mediate auscultation using the stethoscope to listen to the sounds of the heart and respiratory tract. Laennec's publication was translated into English, 1824, by John Forbes.\nThe 1846 introduction by surgeon John Hutchinson (1811\u20131861) of the spirometer, an apparatus for assessing the mechanical properties of the lungs via measurements of forced exhalation and forced inhalation. (The recorded lung volumes and air flow rates are used to distinguish between restrictive disease (in which the lung volumes are decreased: e.g., cystic fibrosis) and obstructive diseases (in which the lung volume is normal but the air flow rate is impeded; e.g., emphysema).)\nThe 1851 invention by Hermann von Helmholtz (1821\u20131894) of the ophthalmoscope, which allowed physicians to examine the inside of the human eye.\nThe (c.\u20091870) immediate widespread clinical use of Sir Thomas Clifford Allbutt's (1836\u20131925) six-inch (rather than twelve-inch) pocket clinical thermometer, which he had devised in 1867.\nThe 1882 introduction of bacterial cultures by Robert Koch, initially for tuberculosis, being the first laboratory test to confirm bacterial infections.\nThe 1895 clinical use of X-rays which began almost immediately after they had been discovered that year by Wilhelm Conrad R\u00f6ntgen (1845\u20131923).\nThe 1896 introduction of the sphygmomanometer, designed by Scipione Riva-Rocci (1863\u20131937), to measure blood pressure.\n\nDiagnosis\nThe recognition of signs, and noting of symptoms may lead to a diagnosis. Otherwise a physical examination may be carried out, and a medical history taken. Further diagnostic medical tests such as blood tests, scans, and biopsies, may be needed.  An X-ray for example would soon be diagnostic of a suspected bone fracture. A noted significance detected during an examination or from a medical test may be known as a medical finding.\n\nExamples\nSee also\n\nBiomarker (medicine)\nFocal neurologic signs\n\n\n== References ==","172":"Dysarthria is a speech sound disorder resulting from neurological injury of the motor component of the motor\u2013speech system and is characterized by poor articulation of phonemes. It is a condition in which problems effectively occur with the muscles that help produce speech, often making it very difficult to pronounce words. It is unrelated to problems with understanding language (that is, dysphasia or aphasia), although a person can have both. Any of the speech subsystems (respiration, phonation, resonance, prosody, and articulation) can be affected, leading to impairments in intelligibility, audibility, naturalness, and efficiency of vocal communication. Dysarthria that has progressed to a total loss of speech is referred to as anarthria. The term dysarthria was formed from the Greek components dys- \"dysfunctional, impaired\" and arthr- \"joint, vocal articulation\".\nNeurological injury due to damage in the central or peripheral nervous system may result in weakness, paralysis, or a lack of coordination of the motor\u2013speech system, producing dysarthria. These effects in turn hinder control over the tongue, throat, lips or lungs; for example, swallowing problems (dysphagia) are also often present in those with dysarthria. Cranial nerves that control the muscles relevant to dysarthria include the trigeminal nerve's motor branch (V), the facial nerve (VII), the glossopharyngeal nerve (IX), the vagus nerve (X), and the hypoglossal nerve (XII).\nDysarthria does not include speech disorders from structural abnormalities, such as cleft palate and must not be confused with apraxia of speech, which refers to problems in the planning and programming aspect of the motor\u2013speech system. Just as the term \"articulation\" can mean either \"speech\" or \"joint movement\", so is the combining form of arthr- the same in the terms \"dysarthria\", \"dysarthrosis\", and \"arthropathy\"; the term \"dysarthria\" is conventionally reserved for the speech problem and is not used to refer to arthropathy, whereas \"dysarthrosis\" has both senses but usually refers to arthropathy.\n\nCauses\nVarious neurological and motor disorders can give rise to dysarthria. The main causes can be classified as genetic, infectious, toxic, traumatic, vascular, neoplastic, demyelinating, degenerative, or other.\n\nGenetic: Wilson's disease, Tay\u2013Sachs disease, and Sensory ataxic neuropathy, dysarthria, and ophthalmoparesis (SANDO syndrome)\nInfectious: Lyme disease (borreliosis) and Creutzfeldt\u2013Jakob disease (CJD)\nToxic: Heavy metal poisoning and Alcohol\nTraumatic: Cerebral palsy (CP), Traumatic brain injury (TBI), Chronic traumatic encephalopathy (CTE)\nVascular: Stroke and Transient ischemic attack (TIA)\nNeoplastic: Brain tumors\nDemyelinating: Multiple sclerosis (MS) and Guillain\u2013Barr\u00e9 syndrome (GBS)\nDegenerative: Parkinson's disease (PD), Huntington's disease (HD), Amyotrophic lateral sclerosis (ALS), Niemann-Pick disease (NP disease), and Friedreich's ataxia (FRDA or FA)\nOther: Hypothermia, Hypoxic encephalopathy, Central pontine myelinolysis, and Idiopathic intracranial hypertension (IIH)\nThese result in lesions to key areas of the brain involved in planning, executing, or regulating motor operations in skeletal muscles (i.e. muscles of the limbs), including muscles of the head and neck (dysfunction of which characterises dysarthria). These can result in dysfunction, or failure of: the motor or somatosensory cortex of the brain, corticobulbar pathways, the cerebellum, basal nuclei (consisting of the putamen, globus pallidus, caudate nucleus, substantia nigra etc.), brainstem (from which the cranial nerves originate), or the neuromuscular junction (in diseases such as myasthenia gravis) which block the nervous system's ability to activate motor units and effect correct range and strength of movements.\n\nDiagnosis\nClassification\nDysarthrias are classified in multiple ways based on the presentation of symptoms. Specific dysarthrias include spastic (resulting from bilateral damage to the upper motor neuron), flaccid (resulting from bilateral or unilateral damage to the lower motor neuron), ataxic (resulting from damage to cerebellum), unilateral upper motor neuron (presenting milder symptoms than bilateral UMN damage), hyperkinetic and hypokinetic (resulting from damage to parts of the basal ganglia, such as in Huntington's disease or Parkinsonism), and the mixed dysarthrias (where symptoms of more than one type of dysarthria are present). The majority of dysarthric patients are diagnosed as having 'mixed' dysarthria, as neural damage resulting in dysarthria is rarely contained to one part of the nervous system\u2014for example, multiple strokes, traumatic brain injury, and some kinds of degenerative illnesses (such as amyotrophic lateral sclerosis) usually damage many different sectors of the nervous system.\nAtaxic dysarthria is an acquired neurological and sensorimotor speech deficit. It is a common diagnosis among the clinical spectrum of ataxic disorders. Since regulation of skilled movements is a primary function of the cerebellum, damage to the superior cerebellum and the superior cerebellar peduncle is believed to produce this form of dysarthria in ataxic patients. Growing evidence supports the likelihood of cerebellar involvement specifically affecting speech motor programming and execution pathways, producing the characteristic features associated with ataxic dysarthria. This link to speech motor control can explain the abnormalities in articulation and prosody, which are hallmarks of this disorder. Some of the most consistent abnormalities observed in patients with ataxia dysarthria are alterations of the normal timing pattern, with prolongation of certain segments and a tendency to equalize the duration of syllables when speaking. As the severity of the dysarthria increases, the patient may also lengthen more segments as well as increase the degree of lengthening of each individual segment.\nCommon clinical features of ataxic dysarthria include abnormalities in speech modulation, rate of speech, explosive or scanning speech, slurred speech, irregular stress patterns, and vocalic and consonantal misarticulations.\nAtaxic dysarthria is associated with damage to the left cerebellar hemisphere in right-handed patients.\nDysarthria may affect a single system; however, it is more commonly reflected in multiple motor\u2013speech systems. The etiology, degree of neuropathy, existence of co-morbidities, and the individual's response all play a role in the effect the disorder has on the individual's quality of life.  Severity ranges from occasional articulation difficulties to verbal speech that is completely unintelligible.\nIndividuals with dysarthria may experience challenges in the following:\n\nTiming\nVocal quality\nPitch\nVolume\nBreath control\nSpeed\nStrength\nSteadiness\nRange\nTone\nExamples of specific observations include a continuous breathy voice, irregular breakdown of articulation, monopitch, distorted vowels, word flow without pauses, and hypernasality.\n\nTreatment\nArticulation problems resulting from dysarthria are treated by speech language pathologists, using a variety of techniques. Techniques used depend on the effect the dysarthria has on control of the articulators. Traditional treatments target the correction of deficits in rate (of articulation), prosody (appropriate emphasis and inflection, affected e.g. by apraxia of speech, right hemisphere brain damage, etc.), intensity (loudness of the voice, affected e.g. in hypokinetic dysarthrias such as in Parkinson's), resonance (ability to alter the vocal tract and resonating spaces for correct speech sounds) and phonation (control of the vocal folds for appropriate voice quality and valving of the airway). These treatments have usually involved exercises to increase strength and control over articulator muscles (which may be flaccid and weak, or overly tight and difficult to move), and using alternate speaking techniques to increase speaker intelligibility (how well someone's speech is understood by peers). With the speech\u2013language pathologist, there are several skills that are important to learn; safe chewing and swallowing techniques, avoiding conversations when feeling tired, repeat words and syllables over and over in order to learn the proper mouth movements, and techniques to deal with the frustration while speaking.  Depending on the severity of the dysarthria, another possibility includes learning how to use a computer or flip cards in order to communicate more effectively.\nMore recent techniques based on the principles of motor learning (PML), such as LSVT (Lee Silverman voice treatment) speech therapy and specifically LSVT may improve voice and speech function in PD. For Parkinson's, aim to retrain speech skills through building new generalised motor programs, and attach great importance to regular practice, through peer\/partner support and self-management. Regularity of practice, and when to practice, are the main issues in PML treatments, as they may determine the likelihood of generalization of new motor skills, and therefore how effective a treatment is.\nAugmentative and alternative communication (AAC) devices that make coping with a dysarthria easier include speech synthesis and text-based telephones. These allow people who are unintelligible, or may be in the later stages of a progressive illness, to continue to be able to communicate without the need for fully intelligible speech.\n\nSee also\nLists of language disorders\n\nReferences\nFurther reading\nExternal links\n\nOnline Speech and Voice Disorder Support (VoiceMatters.net)\nAmerican Speech-Language-Hearing Association\nNews About Dysarthria","173":"Serotonin () or 5-hydroxytryptamine (5-HT) is a monoamine neurotransmitter. Its biological function is complex, touching on diverse functions including mood, cognition, reward, learning, memory, and numerous physiological processes such as vomiting and vasoconstriction.\nSerotonin is produced in the central nervous system (CNS), specifically in the brainstem's raphe nuclei, the skin's Merkel cells, pulmonary neuroendocrine cells and the tongue's taste receptor cells. Approximately 90% of the serotonin the human body produces is in the gastrointestinal tract's enterochromaffin cells, where it regulates intestinal movements. Additionally, it is stored in blood platelets and is released during agitation and vasoconstriction, where it then acts as an agonist to other platelets. About 8% is found in platelets and 1\u20132% in the CNS.\nThe serotonin is secreted luminally and basolaterally, which leads to increased serotonin uptake by circulating platelets and activation after stimulation, which gives increased stimulation of myenteric neurons and gastrointestinal motility. The remainder is synthesized in serotonergic neurons of the CNS, where it has various functions, including the regulation of mood, appetite, and sleep.\nSerotonin secreted from the enterochromaffin cells eventually finds its way out of tissues into the blood. There, it is actively taken up by blood platelets, which store it. When the platelets bind to a clot, they release serotonin, where it can serve as a vasoconstrictor or a vasodilator while regulating hemostasis and blood clotting. In high concentrations, serotonin acts as a vasoconstrictor by contracting endothelial smooth muscle directly or by potentiating the effects of other vasoconstrictors (e.g. angiotensin II and norepinephrine). The vasoconstrictive property is mostly seen in pathologic states affecting the endothelium \u2013 such as atherosclerosis or chronic hypertension. In normal physiologic states, vasodilation occurs through the serotonin mediated release of nitric oxide from endothelial cells, and the inhibition of release of norepinephrine from adrenergic nerves. Serotonin is also a growth factor for some types of cells, which may give it a role in wound healing. There are various serotonin receptors.\nBiochemically, the indoleamine molecule derives from the amino acid tryptophan. Serotonin is metabolized mainly to 5-hydroxyindoleacetic acid (5-HIAA), chiefly by the liver.\nSeveral classes of antidepressants, such as selective serotonin reuptake inhibitors (SSRIs) and serotonin\u2013norepinephrine reuptake inhibitors (SNRIs), interfere with the normal reabsorption of serotonin after it is done with the transmission of the signal, therefore augmenting the neurotransmitter levels in the synapses.\nBesides mammals, serotonin is found in all bilateral animals including worms and insects, as well as in fungi and in plants. Serotonin's presence in insect venoms and plant spines serves to cause pain, which is a side-effect of serotonin injection. Serotonin is produced by pathogenic amoebae, causing diarrhea in the human gut. Its widespread presence in many seeds and fruits may serve to stimulate the digestive tract into expelling the seeds.\n\nMolecular structure\nBiochemically, the indoleamine molecule derives from the amino acid tryptophan, via the (rate-limiting) hydroxylation of the 5 position on the ring (forming the intermediate 5-hydroxytryptophan), and then decarboxylation to produce serotonin. Preferable conformations are defined via ethylamine chain, resulting in six different conformations.\n\nCrystal structure\nSerotonin crystallizes in P212121 chiral space group forming different hydrogen-bonding interactions between serotonin molecules via N-H...O and O-H...N intermolecular bonds. Serotonin also forms several salts, including pharmaceutical formulation of serotonin adipate.\n\nBiological role\nSerotonin is involved in numerous physiological processes, including sleep, thermoregulation, learning and memory, pain, (social) behavior, sexual activity, feeding, motor activity, neural development, and biological rhythms. In less complex animals, such as some invertebrates, serotonin regulates feeding and other processes. In plants serotonin synthesis seems to be associated with stress signals. Despite its longstanding prominence in pharmaceutical advertising, the claim that low serotonin levels cause depression is not supported by scientific evidence.\n\nCellular effects\nSerotonin primarily acts through its receptors and its effects depend on which cells and tissues express these receptors.\nMetabolism involves first oxidation by monoamine oxidase to 5-hydroxyindoleacetaldehyde (5-HIAL). The rate-limiting step is hydride transfer from serotonin to the flavin cofactor. There follows oxidation by aldehyde dehydrogenase (ALDH) to 5-hydroxyindoleacetic acid (5-HIAA), the indole acetic-acid derivative. The latter is then excreted by the kidneys.\n\nReceptors\nThe 5-HT receptors, the receptors for serotonin, are located on the cell membrane of nerve cells and other cell types in animals, and mediate the effects of serotonin as the endogenous ligand and of a broad range of pharmaceutical and psychedelic drugs. Except for the 5-HT3 receptor, a ligand-gated ion channel, all other 5-HT receptors are G-protein-coupled receptors (also called seven-transmembrane, or heptahelical receptors) that activate an intracellular second messenger cascade.\n\nTermination\nSerotonergic action is terminated primarily via uptake of 5-HT from the synapse. This is accomplished through the specific monoamine transporter for 5-HT, SERT, on the presynaptic neuron. Various agents can inhibit 5-HT reuptake, including cocaine, dextromethorphan (an antitussive), tricyclic antidepressants and selective serotonin reuptake inhibitors (SSRIs). A 2006 study found that a significant portion of 5-HT's synaptic clearance is due to the selective activity of the plasma membrane monoamine transporter (PMAT) which actively transports the molecule across the membrane and back into the presynaptic cell.\nIn contrast to the high affinity of SERT, the PMAT has been identified as a low-affinity transporter, with an apparent Km of 114 micromoles\/l for serotonin, which is approximately 230 times higher than that of SERT. However, the PMAT, despite its relatively low serotonergic affinity, has a considerably higher transport \"capacity\" than SERT, \"resulting in roughly comparable uptake efficiencies to SERT ... in heterologous expression systems.\" The study also suggests that the administration of SSRIs such as fluoxetine and sertraline may be associated with an inhibitory effect on PMAT activity when used at higher than normal dosages (IC50 test values used in trials were 3\u20134 fold higher than typical prescriptive dosage).\n\nSerotonylation\nSerotonin can also signal through a nonreceptor mechanism called serotonylation, in which serotonin modifies proteins. This process underlies serotonin's effects upon platelet-forming cells (thrombocytes) in which it links to the modification of signaling enzymes called GTPases that then trigger the release of vesicle contents by exocytosis. A similar process underlies the pancreatic release of insulin.\nThe effects of serotonin upon vascular smooth muscle tone \u2013 the biological function after which serotonin was originally named \u2013 depend upon the serotonylation of proteins involved in the contractile apparatus of muscle cells.\n\nNervous system\nThe neurons of the raphe nuclei are the principal source of 5-HT release in the brain. There are nine raphe nuclei, designated B1\u2013B9, which contain the majority of serotonin-containing neurons (some scientists chose to group the nuclei raphes lineares into one nucleus), all of which are located along the midline of the brainstem, and centered on the reticular formation. Axons from the neurons of the raphe nuclei form a neurotransmitter system reaching almost every part of the central nervous system. Axons of neurons in the lower raphe nuclei terminate in the cerebellum and spinal cord, while the axons of the higher nuclei spread out in the entire brain.\n\nUltrastructure and function\nThe serotonin nuclei may also be divided into two main groups, the rostral and caudal containing three and four nuclei respectively. The rostral group consists of the caudal linear nuclei (B8), the dorsal raphe nuclei (B6 and B7) and the median raphe nuclei (B5, B8 and B9), that project into multiple cortical and subcortical structures. The caudal group consists of the nucleus raphe magnus (B3), raphe obscurus nucleus (B2), raphe pallidus nucleus (B1), and lateral medullary reticular formation, that project into the brainstem.\nThe serotonergic pathway is involved in sensorimotor function, with pathways projecting both into cortical (Dorsal and Median Raphe Nuclei), subcortical, and spinal areas involved in motor activity. Pharmacological manipulation suggests that serotonergic activity increases with motor activity while firing rates of serotonergic neurons increase with intense visual stimuli. Animal models suggest that kainate signaling negatively regulates serotonin actions in the retina, with possible implications for the control of the visual system. The descending projections form a pathway that inhibits pain called the \"descending inhibitory pathway\" that may be relevant to a disorder such as fibromyalgia, migraine, and other pain disorders, and the efficacy of antidepressants in them.\nSerotonergic projections from the caudal nuclei are involved in regulating mood and emotion, and hypo- or hyper-serotonergic states may be involved in depression and sickness behavior.\n\nMicroanatomy\nSerotonin is released into the synapse, or space between neurons, and diffuses over a relatively wide gap (>20 nm) to activate 5-HT receptors located on the dendrites, cell bodies, and presynaptic terminals of adjacent neurons.\nWhen humans smell food, dopamine is released to increase the appetite. But, unlike in worms, serotonin does not increase anticipatory behaviour in humans; instead, the serotonin released while consuming activates 5-HT2C receptors on dopamine-producing cells. This halts their dopamine release, and thereby serotonin decreases appetite. Drugs that block 5-HT2C receptors make the body unable to recognize when it is no longer hungry or otherwise in need of nutrients, and are associated with weight gain, especially in people with a low number of receptors. The expression of 5-HT2C receptors in the hippocampus follows a diurnal rhythm, just as the serotonin release in the ventromedial nucleus, which is characterised by a peak at morning when the motivation to eat is strongest.\nIn macaques, alpha males have twice the level of serotonin in the brain as subordinate males and females (measured by the concentration of 5-HIAA in the cerebrospinal fluid (CSF)). Dominance status and CSF serotonin levels appear to be positively correlated. When dominant males were removed from such groups, subordinate males begin competing for dominance. Once new dominance hierarchies were established, serotonin levels of the new dominant individuals also increased to double those in subordinate males and females. The reason why serotonin levels are only high in dominant males, but not dominant females has not yet been established.\nIn humans, levels of 5-HT1A receptor inhibition in the brain show negative correlation with aggression, and a mutation in the gene that codes for the 5-HT2A receptor may double the risk of suicide for those with that genotype. Serotonin in the brain is not usually degraded after use, but is collected by serotonergic neurons by serotonin transporters on their cell surfaces. Studies have revealed nearly 10% of total variance in anxiety-related personality depends on variations in the description of where, when and how many serotonin transporters the neurons should deploy.\n\nOutside the nervous system\nDigestive tract (emetic)\nSerotonin regulates gastrointestinal (GI) function. The gut is surrounded by enterochromaffin cells, which release serotonin in response to food in the lumen. This makes the gut contract around the food. Platelets in the veins draining the gut collect excess serotonin. There are often serotonin abnormalities in gastrointestinal disorders such as constipation and irritable bowel syndrome.\nIf irritants are present in the food, the enterochromaffin cells release more serotonin to make the gut move faster, i.e., to cause diarrhea, so the gut is emptied of the noxious substance. If serotonin is released in the blood faster than the platelets can absorb it, the level of free serotonin in the blood is increased. This activates 5-HT3 receptors in the chemoreceptor trigger zone that stimulate vomiting. Thus, drugs and toxins stimulate serotonin release from enterochromaffin cells in the gut wall. The enterochromaffin cells not only react to bad food but are also very sensitive to irradiation and cancer chemotherapy. Drugs that block 5HT3 are very effective in controlling the nausea and vomiting produced by cancer treatment, and are considered the gold standard for this purpose.\n\nLungs\nThe lung, including that of reptiles, contains specialized epithelial cells that occur as solitary cells or as clusters called neuroepithelial bodies or bronchial Kulchitsky cells or alternatively K cells. These are enterochromaffin cells that like those in the gut release serotonin. Their function is probably vasoconstriction during hypoxia.\n\nSkin\nSerotonin is also produced by Merkel cells which are part of the somatosensory system.\n\nBone metabolism\nIn mice and humans, alterations in serotonin levels and signalling have been shown to regulate bone mass. Mice that lack brain serotonin have osteopenia, while mice that lack gut serotonin have high bone density. In humans, increased blood serotonin levels have been shown to be a significant negative predictor of low bone density. Serotonin can also be synthesized, albeit at very low levels, in the bone cells. It mediates its actions on bone cells using three different receptors. Through 5-HT1B receptors, it negatively regulates bone mass, while it does so positively through 5-HT2B receptors and 5-HT2C receptors. There is very delicate balance between physiological role of gut serotonin and its pathology. Increase in the extracellular content of serotonin results in a complex relay of signals in the osteoblasts culminating in FoxO1\/ Creb and ATF4 dependent transcriptional events. Following the 2008 findings that gut serotonin regulates bone mass, the mechanistic investigations into what regulates serotonin synthesis from the gut in the regulation of bone mass have started. Piezo1 has been shown to sense RNA in the gut and relay this information through serotonin synthesis to the bone by acting as a sensor of single-stranded RNA (ssRNA) governing 5-HT production. Intestinal epithelium-specific deletion of mouse Piezo1 profoundly disturbed gut peristalsis, impeded experimental colitis, and suppressed serum 5-HT levels. Because of systemic 5-HT deficiency, conditional knockout of Piezo1 increased bone formation. Notably, fecal ssRNA was identified as a natural Piezo1 ligand, and ssRNA-stimulated 5-HT synthesis from the gut was evoked in a MyD88\/TRIF-independent manner. Colonic infusion of RNase A suppressed gut motility and increased bone mass. These findings suggest gut ssRNA as a master determinant of systemic 5-HT levels, indicating the ssRNA-Piezo1 axis as a potential prophylactic target for treatment of bone and gut disorders. Studies in 2008, 2010 and 2019 have opened the potential for serotonin research to treat bone mass disorders.\n\nOrgan development\nSince serotonin signals resource availability it is not surprising that it affects organ development. Many human and animal studies have shown that nutrition in early life can influence, in adulthood, such things as body fatness, blood lipids, blood pressure, atherosclerosis, behavior, learning, and longevity. Rodent experiment shows that neonatal exposure to SSRIs makes persistent changes in the serotonergic transmission of the brain resulting in behavioral changes, which are reversed by treatment with antidepressants. By treating normal and knockout mice lacking the serotonin transporter with fluoxetine scientists showed that normal emotional reactions in adulthood, like a short latency to escape foot shocks and inclination to explore new environments were dependent on active serotonin transporters during the neonatal period.\nHuman serotonin can also act as a growth factor directly. Liver damage increases cellular expression of 5-HT2A and 5-HT2B receptors, mediating liver compensatory regrowth (see Liver \u00a7 Regeneration and transplantation) Serotonin present in the blood then stimulates cellular growth to repair liver damage.\n5-HT2B receptors also activate osteocytes, which build up bone However, serotonin also inhibits osteoblasts, through 5-HT1B receptors.\n\nCardiovascular growth factor\nSerotonin, in addition, evokes endothelial nitric oxide synthase activation and stimulates, through a 5-HT1B receptor-mediated mechanism, the phosphorylation of p44\/p42 mitogen-activated protein kinase activation in bovine aortic endothelial cell cultures. In blood, serotonin is collected from plasma by platelets, which store it. It is thus active wherever platelets bind in damaged tissue, as a vasoconstrictor to stop bleeding, and also as a fibrocyte mitotic (growth factor), to aid healing.\n\nPharmacology\nSeveral classes of drugs target the serotonin system, including some antidepressants, antipsychotics, anxiolytics, antiemetics, and antimigraine drugs, as well as, the psychedelic drugs and entactogens.\n\nMechanism of action\nAt rest, serotonin is stored within the vesicles of presynaptic neurons. When stimulated by nerve impulses, serotonin is released as a neurotransmitter into the synapse, reversibly binding to the postsynaptic receptor to induce a nerve impulse on the postsynaptic neuron. Serotonin can also bind to auto-receptors on the presynaptic neuron to regulate the synthesis and release of serotonin. Normally serotonin is taken back into the presynaptic neuron to stop its action, then reused or broken down by monoamine oxidase.\n\nPsychedelic drugs\nThe serotonergic psychedelic drugs psilocin\/psilocybin, DMT, mescaline, psychedelic mushroom and LSD are agonists, primarily at 5-HT2A and 5-HT2C receptors. The empathogen-entactogen MDMA releases serotonin from synaptic vesicles of neurons.\n\nAntidepressants\nDrugs that alter serotonin levels are used in treating depression, generalized anxiety disorder, and social phobia. Monoamine oxidase inhibitors (MAOIs) prevent the breakdown of monoamine neurotransmitters (including serotonin), and therefore increase concentrations of the neurotransmitter in the brain. MAOI therapy is associated with many adverse drug reactions, and patients are at risk of hypertensive emergency triggered by foods with high tyramine content, and certain drugs. Some drugs inhibit the re-uptake of serotonin, making it stay in the synaptic cleft longer. The tricyclic antidepressants (TCAs) inhibit the reuptake of both serotonin and norepinephrine. The newer selective serotonin reuptake inhibitors (SSRIs) have fewer side-effects and fewer interactions with other drugs.\nCertain SSRI medications have been shown to lower serotonin levels below the baseline after chronic use, despite initial increases. The 5-HTTLPR gene codes for the number of serotonin transporters in the brain, with more serotonin transporters causing decreased duration and magnitude of serotonergic signaling. The 5-HTTLPR polymorphism (l\/l) causing more serotonin transporters to be formed is also found to be more resilient against depression and anxiety.\n\nSerotonin syndrome\nExtremely high levels of serotonin can cause a condition known as serotonin syndrome, with toxic and potentially fatal effects. In practice, such toxic levels are essentially impossible to reach through an overdose of a single antidepressant drug, but require a combination of serotonergic agents, such as an SSRI with a MAOI, which may occur in therapeutic doses. The intensity of the symptoms of serotonin syndrome vary over a wide spectrum, and the milder forms are seen even at nontoxic levels. It is estimated that 14% of patients experiencing serotonin syndrome overdose on SSRIs; meanwhile the fatality rate is between 2% and 12%.\n\nAntiemetics\nSome 5-HT3 antagonists, such as ondansetron, granisetron, and tropisetron, are important antiemetic agents. They are particularly important in treating the nausea and vomiting that occur during anticancer chemotherapy using cytotoxic drugs. Another application is in the treatment of postoperative nausea and vomiting.\n\nOther\nSome serotonergic agonist drugs cause fibrosis anywhere in the body, particularly the syndrome of retroperitoneal fibrosis, as well as cardiac valve fibrosis.\nIn the past, three groups of serotonergic drugs have been epidemiologically linked with these syndromes. These are the serotonergic vasoconstrictive antimigraine drugs (ergotamine and methysergide), the serotonergic appetite suppressant drugs (fenfluramine, chlorphentermine, and aminorex), and certain anti-Parkinsonian dopaminergic agonists, which also stimulate serotonergic 5-HT2B receptors. These include pergolide and cabergoline, but not the more dopamine-specific lisuride.\nAs with fenfluramine, some of these drugs have been withdrawn from the market after groups taking them showed a statistical increase of one or more of the side effects described. An example is pergolide. The drug was declining in use since it was reported in 2003 to be associated with cardiac fibrosis.\nTwo independent studies published in The New England Journal of Medicine in January 2007 implicated pergolide, along with cabergoline, in causing valvular heart disease. As a result of this, the FDA removed pergolide from the United States market in March 2007. (Since cabergoline is not approved in the United States for Parkinson's Disease, but for hyperprolactinemia, the drug remains on the market. Treatment for hyperprolactinemia requires lower doses than that for Parkinson's Disease, diminishing the risk of valvular heart disease).\n\nMethyl-tryptamines and hallucinogens\nSeveral plants contain serotonin together with a family of related tryptamines that are methylated at the amino (NH2) and (OH) groups, are N-oxides, or miss the OH group. These compounds do reach the brain, although some portion of them are metabolized by monoamine oxidase enzymes (mainly MAO-A) in the liver. Examples are plants from the genus Anadenanthera that are used in the hallucinogenic yopo snuff. These compounds are widely present in the leaves of many plants, and may serve as deterrents for animal ingestion. Serotonin occurs in several mushrooms of the genus Panaeolus.\n\nComparative biology and evolution\nUnicellular organisms\nSerotonin is used by a variety of single-cell organisms for various purposes. SSRIs have been found to be toxic to algae. The gastrointestinal parasite Entamoeba histolytica secretes serotonin, causing a sustained secretory diarrhea in some people. Patients infected with E. histolytica have been found to have highly elevated serum serotonin levels, which returned to normal following resolution of the infection. E. histolytica also responds to the presence of serotonin by becoming more virulent. This means serotonin secretion not only serves to increase the spread of entamoebas by giving the host diarrhea but also serves to coordinate their behaviour according to their population density, a phenomenon known as quorum sensing. Outside the gut of a host, there is nothing that the entamoebas provoke to release serotonin, hence the serotonin concentration is very low. Low serotonin signals to the entamoebas they are outside a host and they become less virulent to conserve energy. When they enter a new host, they multiply in the gut, and become more virulent as the enterochromaffine cells get provoked by them and the serotonin concentration increases.\n\nEdible plants and mushrooms\nIn drying seeds, serotonin production is a way to get rid of the buildup of poisonous ammonia. The ammonia is collected and placed in the indole part of L-tryptophan, which is then decarboxylated by tryptophan decarboxylase to give tryptamine, which is then hydroxylated by a cytochrome P450 monooxygenase, yielding serotonin.\nHowever, since serotonin is a major gastrointestinal tract modulator, it may be produced in the fruits of plants as a way of speeding the passage of seeds through the digestive tract, in the same way as many well-known seed and fruit associated laxatives. Serotonin is found in mushrooms, fruits, and vegetables. The highest values of 25\u2013400 mg\/kg have been found in nuts of the walnut (Juglans) and hickory (Carya) genera. Serotonin concentrations of 3\u201330 mg\/kg have been found in plantains, pineapples, banana, kiwifruit, plums, and tomatoes. Moderate levels from 0.1\u20133 mg\/kg have been found in a wide range of tested vegetables.\nSerotonin is one compound of the poison contained in stinging nettles (Urtica dioica), where it causes pain on injection in the same manner as its presence in insect venoms. It is also naturally found in Paramuricea clavata, or the Red Sea Fan.\nSerotonin and tryptophan have been found in chocolate with varying cocoa contents. The highest serotonin content (2.93 \u03bcg\/g) was found in chocolate with 85% cocoa, and the highest tryptophan content (13.27\u201313.34 \u03bcg\/g) was found in 70\u201385% cocoa. The intermediate in the synthesis from tryptophan to serotonin, 5-hydroxytryptophan, was not found.\nRoot development in Arabidopsis thaliana is stimulated and modulated by serotonin \u2013 in various ways at various concentrations.\nSerotonin serves as a plant defense chemical against fungi. When infected with Fusarium crown rot (Fusarium pseudograminearum), wheat (Triticum aestivum) greatly increases its production of tryptophan to synthesize new serotonin. The function of this is poorly understood but wheat also produces serotonin when infected by Stagonospora nodorum \u2013 in that case to retard spore production. The model cereal Brachypodium distachyon \u2013 used as a research substitute for wheat and other production cereals \u2013 also produces serotonin, coumaroyl-serotonin, and feruloyl-serotonin in response to F. graminearum. This produces a slight antimicrobial effect. B. distachyon produces more serotonin (and conjugates) in response to deoxynivalenol (DON)-producing F. graminearum than non-DON-producing. Solanum lycopersicum produces many AA conjugates \u2013 including several of serotonin \u2013 in its leaves, stems, and roots in response to Ralstonia solanacearum infection.\n\nInvertebrates\nSerotonin functions as a neurotransmitter in the nervous systems of most animals.\n\nNematodes\nFor example, in the roundworm Caenorhabditis elegans, which feeds on bacteria, serotonin is released as a signal in response to positive events, such as finding a new source of food or in male animals finding a female with which to mate. When a well-fed worm feels bacteria on its cuticle, dopamine is released, which slows it down; if it is starved, serotonin also is released, which slows the animal down further. This mechanism increases the amount of time animals spend in the presence of food. The released serotonin activates the muscles used for feeding, while octopamine suppresses them. Serotonin diffuses to serotonin-sensitive neurons, which control the animal's perception of nutrient availability.\n\nDecapods\nIf lobsters are injected with serotonin, they behave like dominant individuals whereas octopamine causes subordinate behavior. A crayfish that is frightened may flip its tail to flee, and the effect of serotonin on this behavior depends largely on the animal's social status. Serotonin inhibits the fleeing reaction in subordinates, but enhances it in socially dominant or isolated individuals. The reason for this is social experience alters the proportion between serotonin receptors (5-HT receptors) that have opposing effects on the fight-or-flight response. The effect of 5-HT1 receptors predominates in subordinate animals, while 5-HT2 receptors predominates in dominants.\n\nIn venoms\nSerotonin is a common component of invertebrate venoms, salivary glands, nervous tissues, and various other tissues, across molluscs, insects, crustaceans, scorpions, various kinds of worms, and jellyfish. Adult Rhodnius prolixus \u2013 hematophagous on vertebrates \u2013 secrete lipocalins into the wound during feeding. In 2003 these lipocalins were demonstrated to sequester serotonin to prevent vasoconstriction (and possibly coagulation) in the host.\n\nInsects\nSerotonin is evolutionarily conserved and appears across the animal kingdom. It is seen in insect processes in roles similar to in the human central nervous system, such as memory, appetite, sleep, and behavior. Some circuits in mushroom bodies are serotonergic. (See specific Drosophila example below, \u00a7Dipterans.)\n\nAcrididae\nLocust swarming is initiated but not maintained by serotonin, with release being triggered by tactile contact between individuals. This transforms social preference from aversion to a gregarious state that enables coherent groups. Learning in flies and honeybees is affected by the presence of serotonin.\n\nRole in insecticides\nInsect 5-HT receptors have similar sequences to the vertebrate versions, but pharmacological differences have been seen. Invertebrate drug response has been far less characterized than mammalian pharmacology and the potential for species selective insecticides has been discussed.\n\nHymenopterans\nWasps and hornets have serotonin in their venom, which causes pain and inflammation as do scorpions. Pheidole dentata takes on more and more tasks in the colony as it gets older, which requires it to respond to more and more olfactory cues in the course of performing them. This olfactory response broadening was demonstrated to go along with increased serotonin and dopamine, but not octopamine in 2006.\n\nDipterans\nIf flies are fed serotonin, they are more aggressive; flies depleted of serotonin still exhibit aggression, but they do so much less frequently. In their crops it plays a vital role in digestive motility produced by contraction. Serotonin that acts on the crop is exogenous to the crop itself and 2012 research suggested that it probably originated in the serotonin neural plexus in the thoracic-abdominal synganglion. In 2011 a Drosophila serotonergic mushroom body was found to work in concert with Amnesiac to form memories. In 2007 serotonin was found to promote aggression in Diptera, which was counteracted by neuropeptide F \u2013 a surprising find given that they both promote courtship, which is usually similar to aggression in most respects.\n\nVertebrates\nSerotonin, also referred to as 5-hydroxytryptamine (5-HT), is a neurotransmitter most known for its involvement in mood disorders in humans. It is also a widely present neuromodulator among vertebrates and invertebrates. Serotonin has been found having associations with many physiological systems such as cardiovascular, thermoregulation, and behavioral functions, including: circadian rhythm, appetite, aggressive and sexual behavior, sensorimotor reactivity and learning, and pain sensitivity. Serotonin's function in neurological systems along with specific behaviors among vertebrates found to be strongly associated with serotonin will be further discussed. Two relevant case studies are also mentioned regarding serotonin development involving teleost fish and mice.\nIn mammals, 5-HT is highly concentrated in the substantia nigra, ventral tegmental area and raphe nuclei. Lesser concentrated areas include other brain regions and the spinal cord. 5-HT neurons are also shown to be highly branched, indicating that they are structurally prominent for influencing multiple areas of the CNS at the same time, although this trend is exclusive solely to mammals.\n\n5-HT system in vertebrates\nVertebrates are multicellular organisms in the phylum Chordata that possess a backbone and a nervous system. This includes mammals, fish, reptiles, birds, etc. In humans, the nervous system is composed of the central and peripheral nervous system, with little known about the specific mechanisms of neurotransmitters in most other vertebrates. However, it is known that while serotonin is involved in stress and behavioral responses, it is also important in cognitive functions. Brain organization in most vertebrates includes 5-HT cells in the hindbrain. In addition to this, 5-HT is often found in other sections of the brain in non-placental vertebrates, including the basal forebrain and pretectum. Since location of serotonin receptors contribute to behavioral responses, this suggests serotonin is part of specific pathways in non-placental vertebrates that are not present in amniotic organisms. Teleost fish and mice are organisms most often used to study the connection between serotonin and vertebrate behavior. Both organisms show similarities in the effect of serotonin on behavior, but differ in the mechanism in which the responses occur.\n\nDogs \/ canine species\nThere are few studies of serotonin in dogs. One study reported serotonin values were higher at dawn than at dusk. In another study, serum 5-HT levels did not seem to be associated with dogs' behavioural response to a stressful situation.  Urinary serotonin\/creatinine ratio in bitches tended to be higher 4 weeks after surgery. In addition, serotonin was positively correlated with both cortisol and progesterone but not with testosterone after ovariohysterectomy.\n\nTeleost fish\nLike non-placental vertebrates, teleost fish also possess 5-HT cells in other sections of the brain, including the basal forebrain. Danio rerio (zebra fish) are a species of teleost fish often used for studying serotonin within the brain. Despite much being unknown about serotonergic systems in vertebrates, the importance in moderating stress and social interaction is known. It is hypothesized that AVT and CRF cooperate with serotonin in the hypothalamic-pituitary-interrenal axis. These neuropeptides influence the plasticity of the teleost, affecting its ability to change and respond to its environment. Subordinate fish in social settings show a drastic increase in 5-HT concentrations. High levels of 5-HT long term influence the inhibition of aggression in subordinate fish.\n\nMice\nResearchers at the Department of Pharmacology and Medical Chemistry used serotonergic drugs on male mice to study the effects of selected drugs on their behavior. Mice in isolation exhibit increased levels of agonistic behavior towards one another. Results found that serotonergic drugs reduce aggression in isolated mice while simultaneously increasing social interaction. Each of the treatments use a different mechanism for targeting aggression, but ultimately all have the same outcome. While the study shows that serotonergic drugs successfully target serotonin receptors, it does not show specifics of the mechanisms that affect behavior, as all types of drugs tended to reduce aggression in isolated male mice. Aggressive mice kept out of isolation may respond differently to changes in serotonin reuptake.\n\nBehavior\nLike in humans, serotonin is extremely involved in regulating behavior in most other vertebrates. This includes not only response and social behaviors, but also influencing mood. Defects in serotonin pathways can lead to intense variations in mood, as well as symptoms of mood disorders, which can be present in more than just humans.\n\nSocial interaction\nOne of the most researched aspects of social interaction in which serotonin is involved is aggression. Aggression is regulated by the 5-HT system, as serotonin levels can both induce or inhibit aggressive behaviors, as seen in mice (see section on Mice) and crabs.  While this is widely accepted, it is unknown if serotonin interacts directly or indirectly with parts of the brain influencing aggression and other behaviors. Studies of serotonin levels show that they drastically increase and decrease during social interactions, and they generally correlate with inhibiting or inciting aggressive behavior. The exact mechanism of serotonin influencing social behaviors is unknown, as pathways in the 5-HT system in various vertebrates can differ greatly.\n\nResponse to stimuli\nSerotonin is important in environmental response pathways, along with other neurotransmitters. Specifically, it has been found to be involved in auditory processing in social settings, as primary sensory systems are connected to social interactions. Serotonin is found in the IC structure of the midbrain, which processes specie specific and non-specific social interactions and vocalizations. It also receives acoustic projections that convey signals to auditory processing regions. Research has proposed that serotonin shapes the auditory information being received by the IC and therefore is influential in the responses to auditory stimuli. This can influence how an organism responds to the sounds of predatory or other impactful species in their environment, as serotonin uptake can influence aggression or social interaction.\n\nMood\nWe can describe mood not as specific to an emotional status, but as associated with a relatively long-lasting emotional state. Serotonin's association with mood is most known for various forms of depression and bipolar disorders in humans. Disorders caused by serotonergic activity potentially contribute to the many symptoms of major depression, such as overall mood, activity, suicidal thoughts and sexual and cognitive dysfunction. Selective serotonin reuptake inhibitors (SSRI's) are a class of drugs demonstrated to be an effective treatment in major depressive disorder and are the most prescribed class of antidepressants. SSRI's function is to block the reuptake of serotonin, making more serotonin available to absorb by the receiving neuron. Animals have been studied for decades in order to understand depressive behavior among species. One of the most familiar studies, the forced swimming test (FST), was performed to measure potential antidepressant activity. Rats were placed in an inescapable container of water, at which point time spent immobile and number of active behaviors (such as splashing or climbing) were compared before and after a panel of anti-depressant drugs were administered. Antidepressants that selectively inhibit NE reuptake were shown to reduce immobility and selectively increase climbing without affecting swimming. However, results of the SSRI's also show reduced immobility but increased swimming without affecting climbing. This study demonstrated the importance of behavioral tests for antidepressants, as they can detect drugs with an effect on core behavior along with behavioral components of species.\n\nGrowth and reproduction\nIn the nematode C. elegans, artificial depletion of serotonin or the increase of octopamine cues behavior typical of a low-food environment: C. elegans becomes more active, and mating and egg-laying are suppressed, while the opposite occurs if serotonin is increased or octopamine is decreased in this animal. Serotonin is necessary for normal nematode male mating behavior, and the inclination to leave food to search for a mate. The serotonergic signaling used to adapt the worm's behaviour to fast changes in the environment affects insulin-like signaling and the TGF beta signaling pathway, which control long-term adaption.\nIn the fruit fly insulin both regulates blood sugar as well as acting as a growth factor. Thus, in the fruit fly, serotonergic neurons regulate the adult body size by affecting insulin secretion. Serotonin has also been identified as the trigger for swarm behavior in locusts. In humans, though insulin regulates blood sugar and IGF regulates growth, serotonin controls the release of both hormones, modulating insulin release from the beta cells in the pancreas through serotonylation of GTPase signaling proteins. Exposure to SSRIs during pregnancy reduces fetal growth.\nGenetically altered C. elegans worms that lack serotonin have an increased reproductive lifespan, may become obese, and sometimes present with arrested development at a dormant larval state.\n\nAging and age-related phenotypes\nSerotonin is known to regulate aging, learning, and memory. The first evidence comes from the study of longevity in C. elegans. During early phase of aging, the level of serotonin increases, which alters locomotory behaviors and associative memory. The effect is restored by mutations and drugs (including mianserin and methiothepin) that inhibit serotonin receptors. The observation does not contradict with the notion that the serotonin level goes down in mammals and humans, which is typically seen in late but not early phase of aging.\n\nBiochemical mechanisms\nBiosynthesis\nIn animals and humans, serotonin is synthesized from the amino acid L-tryptophan by a short metabolic pathway consisting of two enzymes, tryptophan hydroxylase (TPH) and aromatic amino acid decarboxylase (DDC), and the coenzyme pyridoxal phosphate. The TPH-mediated reaction is the rate-limiting step in the pathway.\nTPH has been shown to exist in two forms: TPH1, found in several tissues, and TPH2, which is a neuron-specific isoform.\nSerotonin can be synthesized from tryptophan in the lab using Aspergillus niger and Psilocybe coprophila as catalysts. The first phase to 5-hydroxytryptophan would require letting tryptophan sit in ethanol and water for 7 days, then mixing in enough HCl (or other acid) to bring the pH to 3, and then adding NaOH to make a pH of 13 for 1 hour. Aspergillus niger would be the catalyst for this first phase. The second phase to synthesizing tryptophan itself from the 5-hydroxytryptophan intermediate would require adding ethanol and water, and letting sit for 30 days this time. The next two steps would be the same as the first phase: adding HCl to make the pH = 3, and then adding NaOH to make the pH very basic at 13 for 1 hour. This phase uses the Psilocybe coprophila as the catalyst for the reaction.\n\nSerotonin taken orally does not pass into the serotonergic pathways of the central nervous system, because it does not cross the blood\u2013brain barrier. However, tryptophan and its metabolite 5-hydroxytryptophan (5-HTP), from which serotonin is synthesized, do cross the blood\u2013brain barrier. These agents are available as dietary supplements and in various foods, and may be effective serotonergic agents.\nOne product of serotonin breakdown is 5-hydroxyindoleacetic acid (5-HIAA), which is excreted in the urine. Serotonin and 5-HIAA are sometimes produced in excess amounts by certain tumors or cancers, and levels of these substances may be measured in the urine to test for these tumors.\n\nAnalytical chemistry\nIndium tin oxide is recommended for the electrode material in electrochemical investigation of concentrations produced, detected, or consumed by microbes. A mass spectrometry technique was developed in 1994 to measure the molecular weight of both natural and synthetic serotonins.\n\nHistory and etymology\nIt had been known to physiologists for over a century that a vasoconstrictor material appears in serum when blood was allowed to clot. In 1935, Italian Vittorio Erspamer showed an extract from enterochromaffin cells made intestines contract. Some believed it contained adrenaline, but two years later, Erspamer was able to show it was a previously unknown amine, which he named \"enteramine\". In 1948, Maurice M. Rapport, Arda Green, and Irvine Page of the Cleveland Clinic discovered a vasoconstrictor substance in blood serum, and since it was a serum agent affecting vascular tone, they named it serotonin.\nIn 1952, enteramine was shown to be the same substance as serotonin, and as the broad range of physiological roles was elucidated, the abbreviation 5-HT of the proper chemical name 5-hydroxytryptamine became the preferred name in the pharmacological field. Synonyms of serotonin include: 5-hydroxytriptamine, enteramine, substance DS, and 3-(\u03b2-aminoethyl)-5-hydroxyindole. In 1953, Betty Twarog and Page discovered serotonin in the central nervous system. Page regarded Erspamer's work on Octopus vulgaris, Discoglossus pictus, Hexaplex trunculus, Bolinus brandaris, Sepia, Mytilus, and Ostrea as valid and fundamental to understanding this newly identified substance, but regarded his earlier results in various models \u2013 especially those from rat blood \u2013 to be too confounded by the presence of other bioactive chemicals, including some other vasoactives.\n\nNotes\nReferences\nFurther reading\nExternal links\n\n5-Hydroxytryptamine MS Spectrum\nSerotonin bound to proteins in the PDB\nPsychoTropicalResearch Extensive reviews on serotonergic drugs and Serotonin Syndrome.\nMolecule of the Month: Serotonin at University of Bristol\n60-Second Psych: No Fair! My Serotonin Level Is Low, Scientific American\nSerotonin Test Interpretation on ClinLab Navigator.","174":"Spatial disorientation is the inability to determine position or relative motion, commonly occurring during periods of challenging visibility, since vision is the dominant sense for orientation. The auditory system, vestibular system (within the inner ear), and proprioceptive system (sensory receptors located in the skin, muscles, tendons and joints) collectively work to coordinate movement with balance, and can also create illusory nonvisual sensations, resulting in spatial disorientation in the absence of strong visual cues.\nIn aviation, spatial disorientation can result in improper perception of the attitude of the aircraft, referring to the orientation of the aircraft relative to the horizon.  If a pilot relies on this improper perception, this can result in inadvertent turning, ascending or descending. For aviators, proper recognition of aircraft attitude is most critical at night or in poor weather, when there is no visible horizon; in these conditions, aviators may determine aircraft attitude by reference to an attitude indicator. Spatial disorientation can occur in other situations where visibility is reduced, such as diving operations.\n\nFlight safety, history, and statistics\nSpatial orientation in flight is difficult to achieve because numerous sensory stimuli (visual, vestibular, and proprioceptive) vary in magnitude, direction, and frequency. Any differences or discrepancies between visual, vestibular, and proprioceptive sensory inputs result in a sensory mismatch that can produce illusions and lead to spatial disorientation. The visual sense is considered to be the largest contributor to orientation.:\u200a4\u200a\nWhile testing an early turn and slip indicator devised by his friend Elmer Sperry in 1918, United States Army Air Corps pilot William Ocker entered a graveyard spiral while flying through clouds without visual references; the turn indicator showed he was in a turn, but his senses told him he was in level flight. Emerging from the clouds, Ocker was able to recover from the dive. In 1926, Ocker was subjected to a B\u00e1r\u00e1ny chair equilibrium test by Dr. David A. Myers at Crissy Field; the resulting duplication of the somatogyral illusion he had experienced and a subsequent re-test, which he passed using the turn indicator, led him to develop and champion instrumented flight. Sperry would go on to invent the gyrocompass and attitude indicator, both of which were being tested by 1930.:\u200a8\u200a With Lt. Carl Crane, Ocker published the instructional text Blind Flying in Theory and Practice in 1932. Influential advocates of instrumented flight training included Albert Hegenberger and Jimmy Doolittle.:\u200a8\u200a\nIn 1965, the Federal Aviation Agency of the United States issued Advisory Circular AC 60-4, warning pilots about the hazards of spatial disorientation, which may result from operation under visual flight rules in conditions of marginal visibility. A new version of the advisory was issued in 1983 as AC 60-4A, defining spatial disorientation as \"the inability to tell which way is 'up.'\"\nStatistics show that between 5% and 10% of all general aviation accidents can be attributed to spatial disorientation, 90% of which are fatal. Spatial-D and G-force induced loss of consciousness (g-LOC) are two of the most common causes of death from human factors in military aviation. A study on the prevalence of spatial disorientation incidents concluded that \"if a pilot flies long enough ... there is no chance that he\/she will escape experiencing at least one episode of [spatial disorientation]. Looked at another way, pilots can be considered to be in one of two groups; those who have been disorientated, and those who will be.\":\u200a2\n\nPhysiology\nThere are four physiologic systems that interact to allow humans to orient themselves in space. Vision is the dominant sense for orientation, but the vestibular system, proprioceptive system and auditory system also play a role.\nSpatial orientation (the inverse being spatial disorientation, aka spatial-D) is the ability to maintain body orientation and posture in relation to the surrounding environment (physical space) at rest and during motion. Humans have evolved to maintain spatial orientation on the ground. Good spatial orientation on the ground relies on the use of visual, auditory, vestibular, and proprioceptive sensory information. Changes in linear acceleration, angular acceleration, and gravity are detected by the vestibular system and the proprioceptive receptors, and then compared in the brain with visual information.\nThe three-dimensional environment of flight is unfamiliar to the human body, creating sensory conflicts and illusions that make spatial orientation difficult and sometimes impossible to achieve. The result of these various visual and nonvisual illusions is spatial disorientation. Various models have been developed to yield quantitative predictions of disorientation associated with known aircraft accelerations.\n\nThe vestibular system and sensory illusions\nThe vestibular system detects linear and angular (rotational) acceleration using specialized organs in the inner ear. Linear accelerations are detected by the otolith organs, while angular accelerations are detected by the semicircular canals.\n\nMisleading sensations\nWithout a visual reference or cues, such as a visible horizon, humans will rely on non-visual senses to establish their sense of motion and equilibrium. During the abnormal acceleratory environment of flight, the vestibular and proprioceptive systems can be misled, resulting in spatial disorientation. When an aircraft is maneuvering, inertial forces can be created by changes in vehicle speed (linear acceleration) and\/or changes in direction (rotational acceleration and centrifugal force), resulting in perceptual misjudgment of the vertical, as the combined forces of gravity and inertia do not align with what the vestibular system assumes is the vertical direction of gravity (towards the center of the Earth).\nUnder ideal conditions, visual cues will provide sufficient information to override illusory vestibular inputs, but at night or in poor weather, visual inputs can be overwhelmed by these illusory nonvisual sensations, resulting in spatial disorientation. Low visibility flight conditions include night, over water or other monotonous\/featureless terrain that blends into the sky, white-out weather, or inadvertent entry into instrument meteorological conditions after flying into fog or clouds.\n\nFor example, in an aircraft that is making a coordinated (banked) turn, no matter how steep, occupants will have little or no sensation of being tilted in the air unless the horizon is visible, as the combined forces of lift and gravity are felt as pressing the occupant into the seat without a lateral force sliding them to either side. Similarly, it is possible to gradually climb or descend without a noticeable change in pressure against the seat. In some aircraft, it is possible to execute a loop without pulling negative g-forces so that, without visual reference, the pilot could be upside down without being aware of it. A gradual change in any direction of movement may not be strong enough to activate the vestibular system, so the pilot may not realize that the aircraft is accelerating, decelerating, or banking.\n\nGyroscopic flight instruments such as the attitude indicator (artificial horizon) and the turn and slip indicator are designed to provide information to counteract misleading sensations from the non-visual senses.\n\nOtoliths and somatogravic illusions\nTwo otolith organs, the saccule and utricle, are located in each ear and are set at right angles to each other. The utricle detects changes in linear acceleration in the horizontal plane, while the saccule detects linear accelerations in the vertical plane; humans have evolved to assume the vertical acceleration is caused by gravity. However, the saccule and utricle can provide misleading sensory perception when gravity is not limited to the vertical plane, or when vehicle speeds and accelerations result in inertial forces comparable to the force of gravity, as the otoliths only detect acceleration, and cannot distinguish inertial forces from the force of gravity. Some examples of this include the inertial forces experienced during a vertical take-off in a helicopter or following the sudden opening of a parachute after a free fall.\nIllusions caused by the otolith organs are called somatogravic illusions and include the Inversion, Head-Up, and Head-Down Illusions. The Inversion Illusion results from a steep ascent followed by a sudden return to level flight; the resulting relative increase in forward speed produces an illusion the aircraft is inverted. The Head-Up and Head-Down illusions are similar, involving sudden linear acceleration (Head-Up) or deceleration (Head-Down), leading to a misperception the nose of the aircraft is pitching up (Head-Up) or down (Head-Down); the aviator could be fooled into pitching the nose down (Head-Up) or up (Head-Down) in response, leading to a crash or a stall, respectively.\nTypically, the Head-Up illusion occurs during take-off, as a strong linear acceleration is used to generate lift over the wing and flaps. Without a visual reference, the pilot may assume from the vestibular system the nose has pitched up and command a dive; if this occurs during take-off, the aircraft may not have sufficient altitude to recover before crashing into the ground.:\u200a7\n\nSemicircular canals and somatogyral illusions\nIn addition, the inner ear contains rotational accelerometers, known as the semicircular canals, which provide information to the lower brain on rotational accelerations in the pitch, roll and yaw axes. Changes in angular velocity are detected from the relative motion between the fluid in the canals and the canal itself, which is fixed to the head; because of inertia, the fluid in the canals tends to lag when the head moves, signaling a rotational acceleration. However, semicircular canal output ceases after prolonged rotation (beyond 15\u201320 s) as the fluid has now been entrained into motion through friction, matching the motion of the head. If the rotation is then stopped, the perceived motion signal from the inner ear indicates the aviator is now turning in the opposite direction from actual travel, as the fluid continues to move while the canal has stopped. In addition, the inertia of the fluid means the detection threshold of rotational acceleration is limited to approximately 2\u00b0\/sec2; angular accelerations below this value cannot be detected.:\u200a5\u200a Specific common somatogyral illusions induced by the semicircular canals are the Leans, Graveyard Spin, Graveyard Spiral, and Coriolis.\n\nIf the aircraft enters an unnoticed, prolonged turn gradually, then suddenly returns to level flight, the leans may result. The gradual turn sets the fluid into the semicircular canals into motion, and rotational acceleration of two degrees per second (or less) cannot be detected. Once the aircraft suddenly returns to level flight, the continued fluid motion gives the sensation the aircraft is banking in the opposite direction of the turn that just ended; the aviator may attempt to correct the misperception of the vertical by banking into the original turn. The leans is considered the most common form of spatial disorientation.:\u200a9\u200a\n\nThe graveyard spiral and graveyard spin are both caused by the acclimation of the semicircular canals to prolonged rotation; after a banked turn (in the case of the graveyard spiral) or spin (for the graveyard spin) of approximately 20 seconds, the fluid in the semicircular canals has been entrained into motion by friction, and the vestibular system no longer perceives a rotational acceleration. If the aviator then ends the turn or spin and returns to level flight, the continued motion of the fluid will cause a sensation the aircraft is turning or spinning in the opposite direction, and the pilot may re-enter the original turn or spin inadvertently; the aviator may not recognize the illusion before the aircraft loses too much altitude, resulting in a collision with terrain or the g-forces on the aircraft may exceed the structural strength of the airframe, resulting in catastrophic failure. One of the most infamous mishaps in aviation history involving the graveyard spiral is the crash involving John F. Kennedy Jr. in 1999.\nOnce an aircraft enters conditions under which the pilot cannot see a distinct visual horizon, the drift in the inner ear continues uncorrected.  Errors in the perceived rate of turn about any axis can build up at a rate of 0.2 to 0.3 degrees per second.  If the pilot is not proficient in the use of gyroscopic flight instruments, these errors will build up to a point that control of the aircraft is lost, usually in a steep, diving turn known as a graveyard spiral.  During the entire time, leading up to and well into the maneuver, the pilot remains unaware of the turning, believing that the aircraft is maintaining straight flight.:\u200a125\u200a\nIn a 1954 study (180 \u2013 Degree Turn Experiment), the University of Illinois Institute of Aviation found that 19 out of 20 non-instrument-rated subject pilots went into a graveyard spiral soon after entering simulated instrument conditions.  The 20th pilot also lost control of his aircraft, but in another maneuver. The average time between onset of instrument conditions and loss of control was 178 seconds.\n\nSpatial disorientation can also affect instrument-rated pilots in certain conditions. A powerful tumbling sensation (vertigo) can result if the pilot moves his or her head too much during instrument flight. This is called the Coriolis illusion. Because the semicircular canals are set in three different axes of rotation, if the aviator suddenly moves their head during a rotational acceleration, one canal may abruptly start to detect an angular acceleration while another ceases, resulting in a tumbling sensation.:\u200a9\n\nVisual illusions\nEven with good visibility, misleading visual inputs such as sloping cloud decks, unfamiliar runway grades, or false horizons can also form optical illusions, resulting in the pilot misjudging the vertical orientation, aircraft speed or altitude, and\/or distance and depth perception; these could even combine with nonvisual illusions from the vestibular and proprioceptive systems to produce an even more powerful illusion.\n\nExamples\nSee also\nBalance disorder \u2013 Physiological disturbance of perception\nB\u00e1r\u00e1ny chair \u2013 Device used for aerospace physiology training\nBroken escalator phenomenon \u2013 Illusion when stepping onto a broken escalator\nBrownout (aeronautics) \u2013 In-flight visual impairment by pilots\nDizziness \u2013 Neurological condition causing impairment in spatial perception and stability\nEquilibrioception \u2013 Physiological sense regarding posture\nIdeomotor phenomenon \u2013 Concept in hypnosis and psychological research\nIllusions of self-motion \u2013 Misperception of one's location or movement\nMotion sickness \u2013 Nausea caused by motion or perceived motion\nPilot error \u2013 Decision, action, or inaction by an aircraft pilot\nProprioception \u2013 Sense of self-movement, force, and body position\nSense of direction\nSensory illusions in aviation \u2013 Misjudgment of true orientation by pilots\nSituation awareness \u2013 Adequate perception of environmental elements and external events\nSpatial ability\nTopographical disorientation\n\nReferences\nExternal links\n\"Ashton Graybiel Spatial Orientation Laboratory \u2013 Brandeis University\". Retrieved 30 July 2016.","175":"Spatial hearing loss refers to a form of deafness that is an inability to use spatial cues about where a sound originates from in space. Poor sound localization in turn affects the ability to understand speech in the presence of background noise.\nPeople with spatial hearing loss have difficulty processing speech that arrives from one direction while simultaneously filtering out 'noise' arriving from other directions. Research has shown spatial hearing loss to be a leading cause of central auditory processing disorder (CAPD) in children. Children with spatial hearing loss commonly present with difficulties understanding speech in the classroom. Spatial hearing loss is found in most people over 70 years of age, and can sometimes be independent of other types of age related hearing loss. As with presbycusis, spatial hearing loss varies with age. Through childhood and into adulthood it can be viewed as spatial hearing gain (with it becoming easier to hear speech in noise), and then with middle age and beyond the spatial hearing loss begins (with it becoming harder again to hear speech in noise).\n\nLocalization mechanism\nSound streams arriving from the left or right (the horizontal plane) are localised primarily by the small time differences of the same sound arriving at the two ears. A sound straight in front of the head is heard at the same time by both ears. A sound to the side of the head is heard approximately 0.0005 seconds later by the ear furthest away. A sound halfway to one side is heard approximately 0.0003 seconds later. This is the interaural time difference (ITD) cue and is measured by signal processing in the two central auditory pathways that begin after the cochlea and pass through the brainstem and mid-brain. Some of those with spatial hearing loss are unable to process ITD (low frequency) cues.\nSound streams arriving from below the head, above the head, and over behind the head (the vertical plane) are localised again by signal processing in the central auditory pathways. The cues this time however are the notches\/peaks that are added to the sound arriving at the ears by the complex shapes of the pinna. Different notches\/peaks are added to sounds coming from below compared to sounds coming from above, and compared to sounds coming from behind. The most significant notches are added to sounds in the 4 kHz to 10 kHz range.  Some of those with spatial hearing loss are unable to process pinna related (high frequency) cues.\nBy the time sound stream representations reach the end of the auditory pathways brainstem inhibition processing ensures that the right pathway is solely responsible for the left ear sounds and the left pathway is solely responsible for the right ear sounds. It is then the responsibility of the auditory cortex (AC) of the right hemisphere (on its own) to map the whole auditory scene. Information about the right auditory hemifield joins with the information about the left hemifield once it has passed through the corpus callosum (CC) - the brain white matter that connects homologous regions of the left and right hemispheres. Some of those with spatial hearing loss are unable to integrate the auditory representations of the left and right hemifields, and consequently are unable to maintain any representation of auditory space.\nAn auditory space representation enables attention to be given (conscious top-down driven) to a single auditory stream. A gain mechanism can be employed involving the enhancement of the speech stream, and the suppression of any other speech streams and any noise streams. An inhibition mechanism can be employed involving the variable suppression of outputs from the two cochlea. Some of those with spatial hearing loss are unable to suppress unwanted cochlea output.\nThose individuals with spatial hearing loss are not able to accurately perceive the directions different sound streams are coming from and their hearing is no longer 3-dimensional (3D). Sound streams from the rear may appear to come from the front instead. Sound streams from the left or right may appear to come from the front. The gain mechanism can not be used to enhance the speech stream of interest from all other sound streams. Those with spatial hearing loss need target speech to be raised by typically more than 10 dB when listening to speech in a background noise compared to those with no spatial hearing loss.\nSpatial hearing ability normally begins to develop in early childhood, and then continues to develop into early adulthood. After the age of 50 years spatial hearing ability begins to decline. Both peripheral hearing and central auditory pathway problems can interfere with early development. With some individuals, for a range of different reasons, maturation of the two ear spatial hearing ability may simply never happen. For example, prolonged episodes of ear infections such as \u201cglue ear\u201d are likely to significantly hinder its development.\n\nCorpus callosum\nMany neuroscience studies have facilitated the development and refinement of a speech processing model. This model shows cooperation between the two hemispheres of the brain, with asymmetric inter-hemispheric and intrahemispheric connectivity consistent with the left hemisphere specialization for phonological processing. The right hemisphere is more specialized for sound localization, while auditory space representation in the brain requires the integration of information from both hemispheres.\nThe corpus callosum (CC) is the major route of communication between the two hemispheres. At maturity it is a large mass of white matter and consists of bundles of fibres linking the white matter of the two cerebral hemispheres. Its caudal and splenium portions contain fibres that originate from the primary and second auditory cortices, and from other auditory responsive areas. Transcallosal interhemispheric transfer of auditory information plays a significant role in spatial hearing functions that depend on binaural cues. Various studies have shown that despite normal audiograms, children with known auditory interhemispheric transfer deficits have particular difficulty localizing sound and understanding speech in noise.\nThe CC of the human brain is relatively slow to mature with its size continuing to increase until the fourth decade of life. From this point it then slowly begins to shrink. LiSN-S SRT scores show that the ability to understand speech in noisy environments develops with age, is beginning to be adult like by 18 years and starts to decline between 40 and 50 years of age.\n\nRoles of the SOC and the MOC\nThe medial olivocochlear bundle (MOC) is part of a collection of brainstem nuclei known as the superior olivary complex (SOC). The MOC innervates the outer hair cells of the cochlea and its activity is able to reduce basilar-membrane responses to sound by reducing the gain of cochlear amplification.\nIn a quiet environment when speech from a single talker is being listened to, then the MOC efferent pathways are essentially inactive. In this case the single speech stream enters both ears and its representation ascends the two auditory pathways. The stream arrives at both the right and left auditory cortices for eventual speech processing by the left hemisphere.\nIn a noisy environment the MOC efferent pathways are required to be active in two distinct ways. The first is an automatic response to the multiple sound streams arriving at the two ears, while the second is a top-down corticofugal attention driven response. The purpose of both is an attempt to enhance the signal to noise ratio between the speech stream being listened to and all other sound streams.\nThe automatic response involves the MOC efferents inhibiting the output of the cochlear of the left ear. The output of the right ear is therefore dominant and only the right hemispace streams (with their direct connection to the speech processing areas of the left hemisphere) travel up the auditory pathway. With children the underdeveloped Corpus Callosum (CC) is unable, in any case, to transfer auditory streams arriving (from the left ear) at the right hemisphere to the left hemisphere.\nWith adults with a mature CC, an attention driven (conscious) decision to attend to one particular sound stream is the trigger for further MOC activity. The 3D spatial representation of the multiple streams of the noisy environment (a function of the right hemisphere) enables a choice of the ear to be attended to. As a consequence, instruction may be given to the MOC efferents to inhibit the output of the right cochlear rather than the left cochlear. If the speech stream being attended to is from the left hemispace it will arrive at the right hemisphere and access speech processing via the CC.\n\nDiagnosis\nSpatial hearing loss can be diagnosed using the Listening in Spatialized Noise \u2013 Sentences test (LiSN-S), which was designed to assess the ability of children with central auditory processing disorder (CAPD) to understand speech in background noise. The LiSN-S allows audiologists to measure how well a person uses spatial (and pitch) information to understand speech in noise. Inability to use spatial information has been found to be a leading cause of CAPD in children.\nTest participants repeat a series of target sentences which are presented simultaneously with competing speech. The listener's speech reception threshold (SRT) for target sentences is calculated using an adaptive procedure. The targets are perceived as coming from in front of the listener whereas the distracters vary according to where they are perceived spatially (either directly in front or either side of the listener). The vocal identity of the distracters also varies (either the same as, or different from, the speaker of the target sentences).\nPerformance on the LISN-S is evaluated by comparing listeners' performances across four listening conditions, generating two SRT measures and three \"advantage\" measures. The advantage measures represent the benefit in dB gained when either talker, spatial, or both talker and spatial cues are available to the listener. The use of advantage measures minimizes the influence of higher order skills on test performance.  This serves to control for the inevitable differences that exist between individuals in functions such as language or memory.\nDichotic listening tests can be used to measure the efficacy of the attentional control of cochlear inhibition and the inter-hemispheric transfer of auditory information. Dichotic listening performance typically increases (and the right-ear advantage decreases) with the development of the Corpus Callosum (CC), peaking before the fourth decade. During middle age and older the auditory system ages, the CC reduces in size, and dichotic listening becomes worse, primarily in the left ear. Dichotic listening tests typically involve two different auditory stimuli (usually speech) presented simultaneously, one to each ear, using a set of headphones. Participants are asked to attend to one or (in a divided-attention test) both of the messages.\nThe activity of the medial olivocochlear bundle (MOC) and its inhibition of cochlear gain can be measured using a Distortion Product Otoacoustic Emission (DPOE) recording method. This involves the contralateral presentation of broadband noise and the measurement of both DPOAE amplitudes and the latency of onset of DPOAE suppression. DPOAE suppression is significantly affected by age and becomes difficult to detect by approximately 50 years of age.\n\nResearch\nResearch has shown that PC based spatial hearing training software can help some of the children identified as failing to develop their spatial hearing skills (perhaps because of frequent bouts of otitis media with effusion). Further research is needed to discover if a similar approach would help those over 60 to recover the loss of their spatial hearing. One such study showed that dichotic test scores for the left ear improved with daily training. Related research into the plasticity of white-matter (see L\u00f6vd\u00e9n et al. for example) suggests some recovery may be possible.\nMusic training leads to superior understanding of speech in noise across age groups and musical experience protects against age-related degradation in neural timing. Unlike speech (fast temporal information), music (pitch information) is primarily processed by areas of the brain in the right hemisphere. Given that it seems likely that the right ear advantage (REA) for speech is present from birth, it would follow that a left ear advantage for music is also present from birth and that MOC efferent inhibition (of the right ear) plays a similar role in creating this advantage. Does greater exposure to music increase conscious control of cochlear gain and inhibition? Further research is needed to explore the apparent ability of music to promote an enhanced capability of speech in noise recognition.\nBilateral digital hearing aids do not preserve localization cues (see, for example, Van den Bogaert et al., 2006) This means that audiologists when fitting hearing aids to patients (with a mild to moderate age related loss) risk negatively impacting their spatial hearing capability. With those patients who feel that their lack of understanding of speech in background noise is their primary hearing difficulty then hearing aids may simply make their problem even worse - their spatial hearing gain will be reduced by in the region of 10 dB. Although further research is needed, there is a growing number of studies which have shown that open-fit hearing aids are better able to preserve localisation cues (see, for example, Alworth 2011)\n\nSee also\nCocktail party effect\nCorpus callosum\nPresbycusis\nSpatial hearing\nUnilateral hearing loss\nSoundBite Hearing System\n\nReferences\nExternal links\nhttp:\/\/www.nal.gov.au","176":"The semicircular canal dehiscence (SCD) is a category of rare neurotological diseases\/disorders affecting the inner ears, which gathers the superior SCD, lateral SCD and posterior SCD. These SCDs induce SCD syndromes (SCDSs), which define specific sets of hearing and balance symptoms. This entry mainly deals with the superior SCDS.\nThe superior semicircular canal dehiscence syndrome (SSCDS) is a set of hearing and balance symptoms that a rare disease\/disorder of the inner ear's superior semicircular canal\/duct induces.  The symptoms are caused by a thinning or complete absence of the part of the temporal bone overlying the superior semicircular canal of the vestibular system. There is evidence that this rare defect, or susceptibility, is congenital. There are also numerous cases of symptoms arising after physical trauma to the head. It was first described in 1998 by Lloyd B. Minor of Johns Hopkins University in Baltimore.\n\nSymptoms\nThe superior canal dehiscence can affect both hearing and balance to different extents in different people.\nSymptoms of the SCDS include:\n\nAutophony \u2013 person's own speech or other self-generated noises (e.g. heartbeat, eye movements, creaking joints, chewing) are heard unusually loudly in the affected ear\nDizziness\/vertigo \u2013 chronic disequilibrium caused by the dysfunction of the superior semicircular canal\nTullio phenomenon \u2013 sound-induced vertigo, disequilibrium or dizziness, nystagmus and oscillopsia\nPulse-synchronous oscillopsia\nHyperacusis \u2013 the over-sensitivity to sound\nLow-frequency conductive hearing loss\nA feeling of fullness in the affected ear\nBrain fog\nFatigue\nHeadache\/migraine\nTinnitus \u2013 high pitched ringing in the ear\n\nSymptoms in detail\nSCDS-related autophony differs greatly in quality and range from the more common form which results from an open, or patulous Eustachian tube through which sufferers of this disorder hear the sound of their own voice and breathing. In contrast, patients with SCDS-related autophony report hearing their own voice as a disturbingly loud and distorted \"kazoo-like\" sound deep inside the head as if relayed through \"a cracked loudspeaker.\" Additionally they may hear the creaking and cracking of joints, the sound of their footsteps when walking or running, their heartbeat and the sound of chewing and other digestive noises. Some sufferers of this condition experience such a high level of conductive hyperacusis that a tuning fork placed on the ankle will be heard in the affected ear. The bizarre phenomenon of being able to hear the sound of the eyeballs moving in their sockets (e.g. when reading in a quiet room) \"like sandpaper on wood\" is one of the more distinctive features of this condition and is almost exclusively associated with SCDS.\nTullio phenomenon, another of the more identifiable symptoms leading to a positive SCD diagnosis is sound-induced loss of balance. Patients showing this symptom may experience a loss of equilibrium, a feeling of motion sickness or even actual nausea, triggered by normal everyday sounds. Although this is often associated with loud noises, volume is not necessarily a factor. Patients describe a wide range of sounds that affect balance: the 'rattle' of a plastic bag; a cashier tossing coins into the register; a telephone ringing; a knock at the door; music; the sound of children playing and even the patient's own voice are typical examples of sounds that can cause a loss of balance when this condition is present, although there are countless others. The presence of Tullio may also mean that involuntary eye movements (nystagmus), sometimes rotational, are set off by sound, giving the sufferer the impression that the world is tipping, clockwise or anticlockwise, depending on the site of the dehiscence. Some patients report this tilt as being as much as 15\u00b0. For such persons, a visit to the concert hall or to a noisy playground may seem like being at the epicenter of an earthquake. A change of pressure within the middle ear (for example when flying or nose-blowing) may equally set off a bout of disequilibrium or nystagmus.\nLow-frequency conductive hearing loss is present in many patients with SCDS and is explained by the dehiscence acting as a \"third window.\" Vibrations entering the ear canal and middle ear are then abnormally diverted through the superior semicircular canal and up into the intracranial space where they become absorbed instead of being registered as sound in the hearing center, the cochlea. Due to the difference in resistance between the normal round window and the pathological dehiscence window this hearing loss is more serious in the lower frequencies and may initially be mistaken for otosclerosis.  In some patients there is true enhancement of low frequency hearing via bone conducted sound.  A clinical sign of this phenomenon is the ability of the patient to hear (not feel) a tuning fork placed upon the ankle bone.\nPulsatile tinnitus is yet another of the typical symptoms of SCDS and is caused by the gap in the dehiscent bone allowing the normal pulse-related pressure changes within the cranial cavity to enter the inner ear abnormally. These pressure changes affect the sound of the tinnitus which will be perceived as containing a pulse-synchronized \"wave\" or \"blip\" which patients describe as a \"swooshing\" sound or as being like the chirrup of a cricket or grasshopper.\nBrain fog and fatigue are both common SCDS symptoms and are caused by the brain having to spend an unusual amount of its energy on the simple act of keeping the body in a state of equilibrium when it is constantly receiving confusing signals from the dysfunctional semicircular canal.\nHeadache and migraine are also often mentioned by patients showing other symptoms of SCDS due to the body overcompensating for poor hearing in the affected ear by tensing up nearby parts of the face, head, and neck and using them as almost a secondary eardrum.\n\nCauses\nAccording to current research, in approximately 2.5% of the general population the bones of the head develop to only 60\u201370% of their normal thickness in the months following birth. This genetic predisposition may explain why the section of temporal bone separating the superior semicircular canal from the cranial cavity, normally 0.8 mm thick, shows a thickness of only 0.5 mm, making it more fragile and susceptible to damage through physical head trauma or from slow erosion. An explanation for this erosion of the bone has not yet been found.\n\nDiagnosis\nThe presence of dehiscence can be detected by a high definition (0.6 mm or less) coronal CT scan of the temporal bone, currently the most reliable way to distinguish between superior canal dehiscence syndrome (SCDS) and other conditions of the inner ear involving similar symptoms such as M\u00e9ni\u00e8re's disease, perilymphatic fistula and cochlea-facial nerve dehiscence. Other diagnostic tools include the vestibular evoked myogenic potential or VEMP test, videonystagmography (VNG), electrocochleography (ECOG) and the rotational chair test. An accurate diagnosis is of great significance as unnecessary exploratory middle ear surgery may thus be avoided. Several of the symptoms typical to SCDS (e.g. vertigo and Tullio) may also be present singly or as part of M\u00e9ni\u00e8re's disease, sometimes causing the one illness to be confused with the other. There are reported cases of patients being affected by both M\u00e9ni\u00e8re's disease and SCDS concurrently.\nAs SCDS is a very rare and still a relatively unknown condition, obtaining an accurate diagnosis of this distressing (and even disabling) disease may take some time as many health care professionals are not yet aware of its existence and frequently dismiss symptoms as being mental health-related.\n\nTreatment\nOnce diagnosed, the gap in the temporal bone can be repaired by surgical resurfacing of the affected bone or plugging of the superior semicircular canal. These techniques are performed by accessing the site of the dehiscence either via a middle fossa craniotomy or via a canal drilled through the transmastoid bone behind the affected ear. Bone cement has been the material most often used, in spite of its tendency to slippage and resorption, and a consequent high failure rate; recently, soft tissue grafts have been substituted.\n\nEponym\nOccasionally this disorder has been referred to as Minor's syndrome, after its discoverer, Lloyd B. Minor. However, that eponym has also been given to an unrelated condition, the paralysis and anaesthesia following a spinal injury, which is named after the Russian neurologist, Lazar Minor (1855\u20131942). In the latter case this term is now nearly obsolete.\n\nKnown cases\nSean McDonough, ESPN sportscaster\n\nReferences\nExternal links\nWard, Bryan K.; Carey, John P.; Minor, Lloyd B. (28 April 2017). \"Superior Canal Dehiscence Syndrome: Lessons from the First 20 Years\". Frontiers in Neurology. 8: 177. doi:10.3389\/fneur.2017.00177. PMC 5408023. PMID 28503164.","177":"Hyperbaric medicine is medical treatment in which an increase in barometric pressure over ambient pressure is employed increasing the partial pressures of all gases present in the ambient atmosphere. The immediate effects include reducing the size of gas embolisms and raising the partial pressures of all gases present according to Henry's law.\nCurrently, there are two types of hyperbaric medicine depending on the gases compressed, hyperbaric air and hyperbaric oxygen.\nHyperbaric air (HBA), consists of compressed atmospheric air (79% nitrogen, 21% oxygen, and minor gases) and is FDA-approved for acute mountain sickness. The hyperbaric air environment is created by placing the patient in a portable hyperbaric air chamber and inflating that chamber up to 7.35 psi gauge (1.5 atmospheres absolute) using a foot-operated or electric air pump. Although the mechanisms of hyperbaric air are poorly understood it is thought that it relieves hypoxemia caused by the decreased partial pressure of oxygen resulting from high altitude by increasing the partial pressure of air (including oxygen and nitrogen) simulating a descent in altitude.  \nHyperbaric oxygen therapy (HBOT), the medical use of greater than 99% oxygen at an ambient pressure higher than atmospheric pressure, and therapeutic recompression for decompression illness, intended to reduce the injurious effects of systemic gas bubbles by physically reducing their size and providing improved conditions for elimination of bubbles and excess dissolved gas.\nThe equipment required for hyperbaric oxygen treatment consists of a pressure vessel for human occupancy, which may be of rigid or flexible construction, and a means of a controlled atmosphere supply. Operation is performed to a predetermined schedule by trained personnel who monitor the patient and may adjust the schedule as required. HBOT found early use in the treatment of decompression sickness, and has also shown great effectiveness in treating conditions such as gas gangrene and carbon monoxide poisoning. More recent research has examined the possibility that it may also have value for other conditions such as cerebral palsy and multiple sclerosis, but no significant evidence has been found.\nA pressure vessel for human occupancy (PVHO) is an enclosure that is intended to be occupied by one or more persons at a pressure which differs from ambient by at least 2 pounds per square inch (0.14 bar). All chambers used in the US made for hyperbaric medicine fall under the jurisdiction of the Federal Food and Drug Agency (FDA). The FDA requires hyperbaric chambers to comply with the American Society of Mechanical Engineers PVHO Codes and the National Fire Protection Association Standard 99, Health Care Facilities Code.  Similar conditions apply in most other countries.\nHyperbaric medicine poses some inherent hazards that are mitigated by FDA-compliant equipment and trained personnel.  Serious injury can occur at pressures as low as 2 psig (13.8 kPa) if a person in the PVHO is rapidly decompressed. If oxygen is used in the hyperbaric therapy, this can increase the fire hazard. This is why the FDA requires hyperbaric chambers to meet ASME PVHO and NFPA 99 standards or the local equivalent. All chambers that meet FDA standards must have an ASME data plate, and people seeking hyperbaric treatment should check to ensure the equipment and facilities are to proper standards.\nTherapeutic recompression is usually also provided in a hyperbaric chamber. It is the definitive treatment for decompression sickness and may also be used to treat arterial gas embolism caused by pulmonary barotrauma of ascent. In emergencies divers may sometimes be treated by in-water recompression (when a chamber is not available) if suitable diving equipment (to reasonably secure the airway) is available.\nA number of hyperbaric treatment schedules have been published over the years for both therapeutic recompression and hyperbaric oxygen therapy for other conditions. Some of these use breathing gases other than air or pure oxygen, when the partial pressure of oxygen must be limited but the pressure required is relatively high. Nitrox and Heliox treatment schedules are available for these cases. Treatment gas may be the ambient chamber gas, or delivered via a built-in breathing system.\n\nScope\nHyperbaric medicine includes hyperbaric oxygen treatment, which is the medical use of oxygen at greater than atmospheric pressure to increase the availability of oxygen in the body; and therapeutic recompression, which involves increasing the ambient pressure on a person, usually a diver, to treat decompression sickness or an air embolism by reducing the volume and more rapidly eliminating bubbles that have formed within the body.\n\nMedical uses\nIn the United States the Undersea and Hyperbaric Medical Society, known as UHMS, lists approvals for reimbursement for certain diagnoses in hospitals and clinics. The following indications have approved (for reimbursement) uses of hyperbaric oxygen therapy as defined by the UHMS Hyperbaric Oxygen Therapy Committee:\n\nAir or gas embolism;\nCarbon monoxide poisoning;\nCarbon monoxide poisoning complicated by cyanide poisoning;\nCentral retinal artery occlusion;\nClostridal myositis and myonecrosis (gas gangrene);\nCrush injury, compartment syndrome, and other acute traumatic ischemias;\nDecompression sickness;\nEnhancement of healing in selected problem wounds;\nDiabetically derived illness, such as short-term relief of diabetic foot, diabetic retinopathy, diabetic nephropathy;\nExceptional blood loss (anemia);\nIdiopathic sudden sensorineural hearing loss;\nIntracranial abscess;\nMucormycosis, especially rhinocerebral disease in the setting of diabetes mellitus;\nNecrotizing soft tissue infections (necrotizing fasciitis);\nOsteomyelitis (refractory);\nDelayed radiation injury (soft tissue and bony necrosis);\nSkin grafts and flaps (compromised);\nThermal burns.\nThere is no reliable evidence to support its use in autism, cancer, diabetes, HIV\/AIDS, Alzheimer's disease, asthma, Bell's palsy, cerebral palsy, depression, heart disease, migraines, multiple sclerosis, Parkinson's disease, spinal cord injury, sports injuries, or stroke. Furthermore, there is evidence that potential side effects of hyperbaric medicine pose an unjustified risk in such cases. A Cochrane review published in 2016 reviewed a small set of clinical trials attempting to treat autism spectrum disorders with hyperbaric oxygen therapy. They noted a small sample size and large \"confidence intervals\" did not provide much evidence. No links between improvements in social abilities or cognitive function were noted. There are also ethical issues with further trials, as  the eardrum can be damaged during hyperbaric therapy. Despite the lack of evidence, in 2015, the number of people utilizing this therapy has continued to rise.\nThere is also insufficient evidence to support its use in acute traumatic or surgical wounds.\n\nHearing issues\nThere is limited evidence that hyperbaric oxygen therapy improves hearing in patients with sudden sensorineural hearing loss who present within two weeks of hearing loss. There is some indication that HBOT might improve tinnitus presenting in the same time frame.\n\nChronic ulcers\nHBOT in diabetic foot ulcers increased the rate of early ulcer healing but does not appear to provide any benefit in wound healing at long-term follow-up. In particular, there was no difference in major amputation rate. For venous, arterial and pressure ulcers, no evidence was apparent that HBOT provides a long-term improvement over standard treatment.\n\nRadiation injury\nThere is some evidence that HBOT is effective for late radiation tissue injury of bone and soft tissues of the head and neck. Some people with radiation injuries of the head, neck or bowel show an improvement in quality of life. Importantly, no such effect has been found in neurological tissues. The use of HBOT may be justified to selected patients and tissues, but further research is required to establish the best people to treat and timing of any HBO therapy.\n\nNeuro-rehabilitation\nAs of 2012, there was no sufficient evidence to support using hyperbaric oxygen therapy to treat people who have traumatic brain injuries. In acute stroke, HBOT does not show benefit. Small clinical trials, however, have shown benefits from HBOT for stroke survivors between 6 months to 3 years after the acute phase.\nHBOT in multiple sclerosis has not shown benefit and routine use is not recommended.\nA 2007 review of HBOT in cerebral palsy found no difference compared to the control group. Neuropsychological tests also showed no difference between HBOT and room air and based on caregiver report, those who received room air had significantly better mobility and social functioning. Children receiving HBOT were reported to experience seizures and the need for tympanostomy tubes to equalize ear pressure, though the incidence was not clear.\n\nCancer\nIn alternative medicine, hyperbaric medicine has been promoted as a treatment for cancer. However, a 2011 study by the American Cancer Society reported no evidence it is effective for this purpose. A 2012 review article in the journal, Targeted Oncology, reports that \"there is no evidence indicating that HBO neither acts as a stimulator of tumor growth nor as an enhancer of recurrence. On the other hand, there is evidence that implies that HBO might have tumor-inhibitory effects in certain cancer subtypes, and we thus strongly believe that we need to expand our knowledge on the effect and the mechanisms behind tumor oxygenation.\"\n\nMigraines\nLow-quality evidence suggests that hyperbaric oxygen therapy may reduce the pain associated with an acute migraine headache in some cases. It is not known which people would benefit from this treatment, and there is no evidence that hyperbaric medicine can prevent future migraines. More research is necessary to confirm the effectiveness of hyperbaric oxygen therapy for treating migraines.\n\nRespiratory distress\nPatients who are having extreme difficulty breathing \u2013 acute respiratory distress syndrome \u2013 are commonly given oxygen and there have been limited trials of hyperbaric equipment in such cases. Examples include treatment of the Spanish flu and COVID-19.\n\nContraindications\nThe toxicology of the treatment has been reviewed by Ustundag et al. and its risk management is discussed by Christian R. Mortensen, in light of the fact that most hyperbaric facilities are managed by departments of anaesthesiology and some of their patients are critically ill.\nAn absolute contraindication to hyperbaric oxygen therapy is untreated pneumothorax. The reason is concern that it can progress to tension pneumothorax, especially during the decompression phase of therapy, although treatment on oxygen-based tables may avoid that progression. The COPD patient with a large bleb represents a relative contraindication for similar reasons. Also, the treatment may raise the issue of occupational health and safety (OHS), for chamber inside attendants, who should not be compressed if they are unable to equalise ears and sinuses.\nThe following are relative contraindications \u2013 meaning that special consideration must be made by specialist physicians before HBO treatments begin:\n\nCardiac disease\nCOPD with air trapping \u2013 can lead to pneumothorax during treatment.\nUpper respiratory infections \u2013 These conditions can make it difficult for the patient to equalise their ears or sinuses, which can result in what is termed ear or sinus squeeze.\nHigh fevers \u2013 In most cases the fever should be lowered before HBO treatment begins. Fevers may predispose to convulsions.\nEmphysema with CO2 retention \u2013 This condition can lead to pneumothorax during HBO treatment due to rupture of an emphysematous bulla during decompression. This risk can be evaluated by x-ray.\nHistory of thoracic (chest) surgery \u2013 This is rarely a problem and usually not considered a contraindication. However, there is concern that air may be trapped in lesions that were created by surgical scarring. These conditions need to be evaluated prior to considering HBO therapy.\nMalignant disease: Cancers thrive in blood-rich environments but may be suppressed by high oxygen levels. HBO treatment of individuals who have cancer presents a problem, since HBO both increases blood flow via angiogenesis and also raises oxygen levels. Taking an anti-angiogenic supplement may provide a solution. A study by Feldemier, et al. and NIH funded study on Stem Cells by Thom, et al., indicate that HBO is actually beneficial in producing stem\/progenitor cells and the malignant process is not accelerated.\nMiddle ear barotrauma is always a consideration in treating both children and adults in a hyperbaric environment because of the necessity to equalise pressure in the ears.\nPregnancy is not a relative contraindication to hyperbaric oxygen treatments, although it may be for underwater diving. In cases where a pregnant woman has carbon monoxide poisoning there is evidence that lower pressure (2.0 ATA) HBOT treatments are not harmful to the fetus, and that the risk involved is outweighed by the greater risk of the untreated effects of CO on the fetus (neurologic abnormalities or death.) In pregnant patients, HBO therapy has been shown to be safe for the fetus when given at appropriate levels and \"doses\" (durations). In fact, pregnancy lowers the threshold for HBO treatment of carbon monoxide-exposed patients. This is due to the high affinity of fetal hemoglobin for CO.\n\nTherapeutic principles\nThe therapeutic consequences of HBOT and recompression result from multiple effects.\n\nClinical pressure (2.0\u20133.0 Bar)\nThe increased overall pressure is of therapeutic value in the treatment of decompression sickness and air embolism as it provides a physical means of reducing the volume of inert gas bubbles within the body; Exposure to this increased pressure is maintained for a period long enough to ensure that most of the bubble gas is dissolved back into the tissues, removed by perfusion and eliminated in the lungs.\nThe improved concentration gradient for inert gas elimination (oxygen window) by using a high partial pressure of oxygen increases the rate of inert gas elimination in the treatment of decompression sickness.\nFor many other conditions, the therapeutic principle of HBOT lies in its ability to drastically increase partial pressure of oxygen in the tissues of the body. The oxygen partial pressures achievable using HBOT are much higher than those achievable while breathing pure oxygen under normobaric conditions (i.e. at normal atmospheric pressure). This effect is achieved by an increase in the oxygen transport capacity of the blood. At normal atmospheric pressure, oxygen transport is limited by the oxygen binding capacity of hemoglobin in red blood cells and very little oxygen is transported by blood plasma. Because the hemoglobin of the red blood cells is almost saturated with oxygen at atmospheric pressure, this route of transport cannot be exploited any further. Oxygen transport by plasma, however, is significantly increased using HBOT because of the higher solubility of oxygen as pressure increases.\n\nProangiogenic stem progenitor cell mobilization\nA study suggests that exposure to hyperbaric oxygen (HBOT) might also mobilize stem\/progenitor cells from the bone marrow by a nitric oxide-dependent mechanism.\n\nLow pressure hyperoxia, stem progenitor cell mobilization and inflammatory cytokine expression\nA more recent study suggests that stem cell mobilization, similar to that seen in the Thom study, is also invoked at relative normo-baric pressure with a significantly smaller increase in oxygen concentration. This study also found a significant decrease in the expression of the systemic inflammatory cytokine TNF-\u03b1 in venous blood. These results suggest that hyperbaria may not be required to invoke the transcriptional responses seen at higher partial pressures of oxygen and that the effect is due solely to oxygen.\n\nHyperbaric chambers\nConstruction\nThe traditional type of hyperbaric chamber used for therapeutic recompression and HBOT is a rigid shelled pressure vessel. Such chambers can be run at absolute pressures typically about 6 bars (87 psi), 600,000 Pa or more in special cases. Navies, professional diving organizations, hospitals, and dedicated recompression facilities typically operate these. They range in size from semi-portable, one-patient units to room-sized units that can treat eight or more patients. The larger units may be rated for lower pressures if they are not primarily intended for treatment of diving injuries.\nA rigid chamber may consist of:\n\na pressure vessel designed to a code such as ASME Boiler and Pressure Vessel Code\nviewports to allow the medical personnel to visually monitor the occupants, and can be used for hand signalling as an auxiliary emergency communications method. The major components are the window (transparent acrylic), the window seat (holds the acrylic window), and retaining ring. Interior lighting can be provided by mounting lights outside the viewports. Viewports are a feature specific to PVHOs due to the need to see the people inside and evaluate their health.  Other materials have been attempted, but they consistently fail to maintain their seal or have cracks which would progress rapidly to catastrphophic failure. Acrylic is more likely to have small cracks the operators can see and have time to take mitigation steps instead of failing catastrophically. Counterfeit chambers often do not use acrylic windows.\none or more human entry hatches \u2013 small and circular or wheel-in type hatches for patients on gurneys;\nthe entry lock that allows human entry \u2013 a separate chamber with two hatches, one to the outside and one to the main chamber, which can be independently pressurized to allow patients to enter or exit the main chamber while it is still pressurized;\na low volume medical or service airlock for medicines, instruments, and food;\ntransparent ports or closed-circuit television that allows technicians and medical staff outside the chamber to monitor the patient inside the chamber;\nan intercom system allowing two-way communication;\nan optional carbon dioxide scrubber \u2013 consisting of a fan that passes the gas inside the chamber through a soda lime canister;\na control panel outside the chamber to open and close valves that control air flow to and from the chamber, and regulate oxygen to hoods or masks;\nan over-pressure relief valve;\na built-in breathing system (BIBS) to supply and exhaust treatment gas;\na fire suppression system.\nFlexible monoplace chambers are available ranging from collapsible flexible aramid fiber-reinforced chambers which can be disassembled for transport via truck or SUV, with a maximum working pressure of 2 bar above ambient complete with BIBS allowing full oxygen treatment schedules. to portable, air inflated \"soft\" chambers that can operate at between 0.3 and 0.5 bars (4.4 and 7.3 psi) above atmospheric pressure with no supplemental oxygen, and longitudinal zipper closure.\n\nOxygen supply\nIn the larger multiplace chambers, patients inside the chamber breathe from either \"oxygen hoods\" \u2013 flexible, transparent soft plastic hoods with a seal around the neck similar to a space suit helmet \u2013 or tightly fitting oxygen masks, which supply pure oxygen and may be designed to directly exhaust the exhaled gas from the chamber. During treatment patients breathe 100% oxygen most of the time to maximise the effectiveness of their treatment, but have periodic \"air breaks\" during which they breathe chamber air (21% oxygen) to reduce the risk of oxygen toxicity. The exhaled treatment gas must be removed from the chamber to prevent the buildup of oxygen, which could present a fire risk. Attendants may also breathe oxygen some of the time to reduce their risk of decompression sickness when they leave the chamber. The pressure inside the chamber is increased by opening valves allowing high-pressure air to enter from storage cylinders, which are filled by an air compressor. Chamber air oxygen content is kept between 19% and 23% to control fire risk (US Navy maximum 25%). If the chamber does not have a scrubber system to remove carbon dioxide from the chamber gas, the chamber must be isobarically ventilated to keep the CO2 within acceptable limits.\nA soft chamber may be pressurized directly from a compressor. or from storage cylinders. \nSmaller \"monoplace\" chambers can only accommodate the patient, and no medical staff can enter. The chamber may be pressurised with pure oxygen or compressed air. If pure oxygen is used, no oxygen breathing mask or helmet is needed, but the cost of using pure oxygen is much higher than that of using compressed air. If compressed air is used, then an oxygen mask or hood is needed as in a multiplace chamber. Most monoplace chambers can be fitted with a demand breathing system for air breaks. In low pressure soft chambers, treatment schedules may not require air breaks, because the risk of oxygen toxicity is low due to the lower oxygen partial pressures used (usually 1.3 ATA), and short duration of treatment.\nFor alert, cooperative patients, air breaks provided by mask are more effective than changing the chamber gas because they provide a quicker gas change and a more reliable gas composition both during the break and treatment periods.\n\nTreatments\nInitially, HBOT was developed as a treatment for diving disorders involving bubbles of gas in the tissues, such as decompression sickness and gas embolism, It is still considered the definitive treatment for these conditions. The chamber treats decompression sickness and gas embolism by increasing pressure, reducing the size of the gas bubbles and improving the transport of blood to downstream tissues. After elimination of bubbles, the pressure is gradually reduced back to atmospheric levels. Hyperbaric chambers are also used for animals.\nAs of September 2023, a number of hyperbaric chambers in the US are turning divers with decompression sickness away, and only treating more profitable scheduled cases. The number of hyperbaric medical facilities in the US is estimated at about 1500, of which 67 are treating diving accidents, according to Divers Alert Network. Many facilities only provide hyperbaric treatment for wound care for economic reasons. Emergency hyperbaric services are more expensive to train and staff, and liability is increased.\n\nProtocol\nEmergency HBOT for decompression illness follows treatment schedules laid out in treatment tables. Most cases employ a recompression to 2.8 bars (41 psi) absolute, the equivalent of 18 metres (60 ft) of water, for 4.5 to 5.5 hours with the casualty breathing pure oxygen, but taking air breaks every 20 minutes to reduce oxygen toxicity. For extremely serious cases resulting from very deep dives, the treatment may require a chamber capable of a maximum pressure of 8 bars (120 psi), the equivalent of 70 metres (230 ft) of water, and the ability to supply heliox as a breathing gas.\nU.S. Navy treatment charts are used in Canada and the United States to determine the duration, pressure, and breathing gas of the therapy. The most frequently used tables are Table 5 and Table 6. In the UK the Royal Navy 62 and 67 tables are used.\nThe Undersea and Hyperbaric Medical Society (UHMS) publishes a report that compiles the latest research findings and contains information regarding the recommended duration and pressure of the longer-term conditions.\n\nHome and out-patient clinic treatment\nThere are several sizes of portable chambers, which are used for home treatment. These are usually referred to as \"mild personal hyperbaric chambers\", which is a reference to the lower pressure (compared to hard chambers) of soft-sided chambers. The American Medical Association is opposed to home use or any other use of hyperbaric chambers if it is not \"in facilities with appropriately trained staff including physician supervision and prescription and only when the intervention has scientific support or rationale\" due demonstrated hazard \nIn the US, these \"mild personal hyperbaric chambers\" are categorized by the FDA as CLASS II medical devices and requires a prescription in order to purchase one or take treatments. As with any hyperbaric chamber, the FDA require compliance with ASME and NFPA standards.  The most common option (but not approved by FDA) some patients choose is to acquire an oxygen concentrator which typically delivers 85\u201396% oxygen as the breathing gas.\nOxygen is never fed directly into soft chambers but is rather introduced via a line and mask directly to the patient. FDA approved oxygen concentrators for human consumption in confined areas used for HBOT are regularly monitored for purity (\u00b11%) and flow (10 to 15 liters per minute outflow pressure). An audible alarm will sound if the purity ever drops below 80%. Personal hyperbaric chambers use 120 volt or 220 volt outlets. The FDA warns against the use of oxygen concentrators or oxygen tanks with chambers that does not meet ASME and FDA standards, regardless of if the concentrators are FDA approved.\n\nPossible complications and concerns\nThere are risks associated with HBOT, similar to some diving disorders. Pressure changes can cause a \"squeeze\" or barotrauma in the tissues surrounding trapped air inside the body, such as the lungs, behind the eardrum, inside paranasal sinuses, or trapped underneath dental fillings. Breathing high-pressure oxygen may cause oxygen toxicity. Temporarily blurred vision can be caused by swelling of the lens, which usually resolves in two to four weeks.\nThere are reports that cataracts may progress following HBOT, and rarely, may develop de novo, but this may be unrecognized and under reported. The cause is not fully explained, but evidence suggests that lifetime exposure of the lens to high partial pressure oxygen may be a major factor. Oxidative damage to lens proteins is thought to be responsible. This may be an end-stage of the relatively well documented myopic shift detected in most hyperbaric patients after a course of multiple treatments.\n\nEffects of pressure\nPatients inside the chamber may notice discomfort inside their ears as a pressure difference develops between their middle ear and the chamber atmosphere. This can be relieved by ear clearing using the Valsalva maneuver or other techniques. Continued increase of pressure without equalizing may cause ear drums to rupture, resulting in severe pain. As the pressure in the chamber increases further, the air may become warm.\nTo reduce the pressure, a valve is opened to allow air out of the chamber. As the pressure falls, the patient's ears may \"squeak\" as the pressure inside the ear equalizes with the chamber. The temperature in the chamber will fall. The speed of pressurization and de-pressurization can be adjusted to each patient's needs.\n\nSide effects\nOxygen toxicity is a limitation on both maximum partial pressure of oxygen, and on length of each treatment.\nHBOT can accelerate the development of cataracts over multiple repetitive treatments, and can cause temporary relative myopia over the shorter term.\n\nRegulation and legality\nThe use of hyperbaric chambers for medical and therapeutic procedures is generally regulated. Authorities have warned of potential risks to patients receiving treatment in unlicensed facilities, notably in Israel, Canada, and the United States. In Italy, the use of hyperbaric chambers for therapy was severely restricted to limited medical settings after a serious fire which killed ten patients in 1997.\nIn some jurisdictions, the use and availability of HBOT is further restricted at the subnational level. In the U.S. state of North Carolina, several cities including Durham, Raleigh and Charlotte have ordered operators of mild hyperbaric oxygen therapy to close to protect public safety due to a risk of fire.\nUnlicensed and fraudulent operators have been subject to prosecution. In Australia, Oxymed Australia Pty Ltd and director Malcolm Hooper were ordered to pay AUS $3 million in fines after advertising hyperbaric therapy against the country's Therapeutic Goods Act. In Canada, certain soft-shelled hyperbaric chambers were removed from the market for a potential risk to patients.\n\nCosts\nHBOT is recognized by Medicare in the United States as a reimbursable treatment for 14 UHMS \"approved\" conditions. A 1-hour HBOT session may cost between $300 and higher in private clinics, and over $2,000 in hospitals. U.S. physicians (M.D. or D.O.) may lawfully prescribe HBOT for \"off-label\" conditions such as stroke, and migraine. Such patients are treated in outpatient clinics. In the United Kingdom most chambers are financed by the National Health Service, although some, such as those run by Multiple Sclerosis Therapy Centres, are non-profit. In Australia, HBOT is not covered by Medicare as a treatment for multiple sclerosis. China and Russia treat more than 80 maladies, conditions and trauma with HBOT.\n\nPersonnel\nHyperbaric medical practitioner - a specialist in hyperbaric medicine\nDiving medical practitioner \u2013 a specialist in diving medicine\nChamber operator \u2013 a person competent to operate  a hyperbaric chamber\nHyperbaric nurse \u2013 a nurse responsible for administering hyperbaric oxygen therapy to patients and supervising them throughout the treatment.\nDiving medical technician \u2013 member of a dive team who is trained in advanced first aid.\nChamber attendant \u2013 a person trained in basic first aid who is medically fit to dive in a chamber, usually a member of a diving team allocated to looking after the diver being treated.\n\nResearch\nAspects under research include radiation-induced hemorrhagic cystitis; and inflammatory bowel disease, rejuvenation.\nSome research found evidence that HBOT improves local tumor control, mortality, and local tumor recurrence for cancers of the head and neck.\nSome research also found evidence of an increase in stem progenitor cells and a decrease in inflammation.\n\nNeurological\nTentative evidence shows a possible benefit in cerebrovascular diseases. Rats subjected to HBOT after some time following the acute phase of experimentally-induced stroke showed reduced inflammation, increased brain-derived neurotrophic factor, and evidence of neurogenesis. Another rat study showed improved neurofunctional recovery as well as neurogenesis following the late-chronic phase of experimentally-induced stroke.\nThe clinical experience and results so far published has promoted the use of HBOT therapy in patients with cerebrovascular injury and focal cerebrovascular injuries. However, the power of clinical research is limited because of the shortage of randomized controlled trials.\n\nRadiation wounds\nA 2010 review of studies of HBOT applied to wounds from radiation therapy reported that, while most studies suggest a beneficial effect, more experimental and clinical research is needed to validate its clinical use.\n\nHistory\nHyperbaric air\nJunod built a chamber in France in 1834 to treat pulmonary conditions at pressures between 2 and 4 atmospheres absolute.\nDuring the following century \"pneumatic centres\" were established in Europe and the USA which used hyperbaric air to treat a variety of conditions.\nOrval J Cunningham, a professor of anesthesia at the University of Kansas in the early 1900s observed that people with circulatory disorders did better at sea level than at altitude and this formed the basis for his use of hyperbaric air. In 1918, he successfully treated patients with the Spanish flu with hyperbaric air. In 1930 the American Medical Association forced him to stop hyperbaric treatment, since he did not provide acceptable evidence that the treatments were effective.\n\nHyperbaric oxygen\nThe English scientist Joseph Priestley discovered oxygen in 1775. Shortly after its discovery, there were reports of toxic effects of hyperbaric oxygen on the central nervous system and lungs, which delayed therapeutic applications until 1937, when Behnke and Shaw first used it in the treatment of decompression sickness.\nIn 1955 and 1956 Churchill-Davidson, in the UK, used hyperbaric oxygen to enhance the radiosensitivity of tumours, while Ite Boerema, at the University of Amsterdam, successfully used it in cardiac surgery.\nIn 1961 Willem Hendrik Brummelkamp et al. published on the use of hyperbaric oxygen in the treatment of clostridial gas gangrene.\nIn 1962 Smith and Sharp reported successful treatment of carbon monoxide poisoning with hyperbaric oxygen.\nThe Undersea Medical Society (now Undersea and Hyperbaric Medical Society) formed a Committee on Hyperbaric Oxygenation which has become recognized as the authority on indications for hyperbaric oxygen treatment.\n\nIncidents\nFires inside a hyperbaric chamber are extremely dangerous. A review article published in 1997 found 77 human fatalities in 35 different hyperbaric chamber fires that occurred from 1923 to 1996. Further studies indicate while the treatment is often considered safe, the use of hyperbaric equipment comes with risks to the operating personnel when improperly used. Proper equipment maintenance and safety procedures for the use  of pressure equipmrnt is mandatory.\n\n1997: Ten patients and a nurse were killed in Milan, Italy after a fire broke out inside a hyperbaric oxygen chamber.\n2009: A grandmother and her four year old grandson died after a hyperbaric chamber caught fire and exploded in Florida. The boy was receiving treatment in the chamber for cerebral palsy and had traveled from Italy where the treatment is outlawed to undergo the procedure.\n2012: A hyperbaric oxygen chamber exploded in Florida, killing a woman and a thoroughbred horse who was receiving treatment. The explosion occurred after the horse kicked out at the chamber, creating sparks which ignited a fire.\n2015: A dog was killed in Georgia when the chamber it was receiving treatment in caught fire and exploded. The dog was being treated for arthritis.\n2016: A fire killed four people who were receiving treatment inside a hyperbaric chamber at Mintohardjo Navy Hospital in Jakarta, Indonesia. The fire was reportedly caused by an electrical short circuit. After the fire broke out, operators used a sprinkler system and an emergency shut off system to rescue the victims, but live-saving efforts were prevented as the machine became engulfed in flames.\n2016: A man in Victoria, Australia died in a hyperbaric chamber of undisclosed causes while receiving treatment. The practitioners overseeing his care were found responsible for failing to ensure the patient's safety leading to his death. They were later fined AU$716,750.\n\nSee also\nUndersea and Hyperbaric Medical Society \u2013 US based organisation for research and education in hyperbaric physiology and medicine.\nSouth Pacific Underwater Medicine Society \u2013 Publisher for diving and hyperbaric medicine and physiology\nDecompression chamber \u2013 Any pressure vessel for huma occupancy used to decompress a person\nHyperbaric treatment schedules \u2013 Planned hyperbaric exposure using a specified breathing gas as medical treatment\nTransdermal continuous oxygen therapy \u2013 Wound closure technique using external oxygen exposure\n\nReferences\nFurther reading\nExternal links\nHyperbaric Oxygen Therapy from eMedicine\nDuke University Medical Center Archives contains collections of multiple individuals who worked with hyperbaric medicine\nDunning, Brian (November 19, 2019). \"Skeptoid #702: Hyperbaric Oxygen Therapy\". Skeptoid.","178":"Stroke (also known as a cerebrovascular accident (CVA) or brain attack) is a medical condition in which poor blood flow to the brain causes cell death. There are two main types of stroke: \n\nischemic, due to lack of blood flow, and\nhemorrhagic, due to bleeding.\nBoth cause parts of the brain to stop functioning properly.\nSigns and symptoms of stroke may include an inability to move or feel on one side of the body, problems understanding or speaking, dizziness, or loss of vision to one side. Signs and symptoms often appear soon after the stroke has occurred. If symptoms last less than one or two hours, the stroke is a transient ischemic attack (TIA), also called a mini-stroke. Hemorrhagic stroke may also be associated with a severe headache. The symptoms of stroke can be permanent. Long-term complications may include pneumonia and loss of bladder control.\nThe biggest risk factor for stroke is high blood pressure. Other risk factors include high blood cholesterol, tobacco smoking, obesity, diabetes mellitus, a previous TIA, end-stage kidney disease, and atrial fibrillation. Ischemic stroke is typically caused by blockage of a blood vessel, though there are also less common causes. Hemorrhagic stroke is caused by either bleeding directly into the brain or into the space between the brain's membranes. Bleeding may occur due to a ruptured brain aneurysm. Diagnosis is typically based on a physical exam and supported by medical imaging such as a CT scan or MRI scan. A CT scan can rule out bleeding, but may not necessarily rule out ischemia, which early on typically does not show up on a CT scan. Other tests such as an electrocardiogram (ECG) and blood tests are done to determine risk factors and rule out other possible causes. Low blood sugar may cause similar symptoms.\nPrevention includes decreasing risk factors, surgery to open up the arteries to the brain in those with problematic carotid narrowing, and warfarin in people with atrial fibrillation. Aspirin or statins may be recommended by physicians for prevention. Stroke is a medical emergency. Ischemic strokes, if detected within three to four-and-a-half hours, may be treatable with medication that can break down the clot, while hemorrhagic strokes sometimes benefit from surgery. Treatment to attempt recovery of lost function is called stroke rehabilitation, and ideally takes place in a stroke unit; however, these are not available in much of the world.\nIn 2023, 15 million people worldwide had a stroke. In 2021, stroke was the third biggest cause of death, responsible for approximately 10% of total deaths. In 2015, there were about 42.4 million people who had previously had stroke and were still alive. Between 1990 and 2010 the annual incidence of stroke decreased by approximately 10% in the developed world, but increased by 10% in the developing world. In 2015, stroke was the second most frequent cause of death after coronary artery disease, accounting for 6.3 million deaths (11% of the total). About 3.0 million deaths resulted from ischemic stroke while 3.3 million deaths resulted from hemorrhagic stroke. About half of people who have had stroke live less than one year. Overall, two thirds of cases of stroke occurred in those over 65 years old.\n\nClassification\nStroke can be classified into two major categories: ischemic and hemorrhagic. Ischemic stroke is caused by interruption of the blood supply to the brain, while hemorrhagic stroke results from the rupture of a blood vessel or an abnormal vascular structure. \nAbout 87% of stroke is ischemic, with the rest being hemorrhagic. Bleeding can develop inside areas of ischemia, a condition known as \"hemorrhagic transformation.\" It is unknown how many cases of hemorrhagic stroke actually start as ischemic stroke.\n\nDefinition\nIn the 1970s the World Health Organization defined \"stroke\" as a \"neurological deficit of cerebrovascular cause that persists beyond 24 hours or is interrupted by death within 24 hours\", although the word \"stroke\" is centuries old. This definition was supposed to reflect the reversibility of tissue damage and was devised for the purpose, with the time frame of 24 hours being chosen arbitrarily. The 24-hour limit divides stroke from transient ischemic attack, which is a related syndrome of stroke symptoms that resolve completely within 24 hours. With the availability of treatments that can reduce stroke severity when given early, many now prefer alternative terminology, such as \"brain attack\" and \"acute ischemic cerebrovascular syndrome\" (modeled after heart attack and acute coronary syndrome, respectively), to reflect the urgency of stroke symptoms and the need to act swiftly.\n\nIschemic\nDuring ischemic stroke, blood supply to part of the brain is decreased, leading to dysfunction of the brain tissue in that area. There are four reasons why this might happen:\n\nThrombosis (obstruction of a blood vessel by a blood clot forming locally)\nEmbolism (obstruction due to an embolus from elsewhere in the body),\nSystemic hypoperfusion (general decrease in blood supply, e.g., in shock)\nCerebral venous sinus thrombosis.\nStroke without an obvious explanation is termed cryptogenic stroke (idiopathic); this constitutes 30\u201340% of all cases of ischemic stroke.\nThere are classification systems for acute ischemic stroke. The Oxford Community Stroke Project classification (OCSP, also known as the Bamford or Oxford classification) relies primarily on the initial symptoms; based on the extent of the symptoms, the stroke episode is classified as total anterior circulation infarct (TACI), partial anterior circulation infarct (PACI), lacunar infarct (LACI) or posterior circulation infarct (POCI). These four entities predict the extent of the stroke, the area of the brain that is affected, the underlying cause, and the prognosis. \nThe TOAST (Trial of Org 10172 in Acute Stroke Treatment) classification is based on clinical symptoms as well as results of further investigations; on this basis, stroke is classified as being due to \n(1) thrombosis or embolism due to atherosclerosis of a large artery, \n(2) an embolism originating in the heart, \n(3) complete blockage of a small blood vessel, \n(4) other determined cause, \n(5) undetermined cause (two possible causes, no cause identified, or incomplete investigation). \nUsers of stimulants such as cocaine and methamphetamine are at a high risk for ischemic stroke.\n\nHemorrhagic\nThere are two main types of hemorrhagic stroke:\n\nIntracerebral hemorrhage, which is bleeding within the brain itself (when an artery in the brain bursts, flooding the surrounding tissue with blood), due to either intraparenchymal hemorrhage (bleeding within the brain tissue) or intraventricular hemorrhage (bleeding within the brain's ventricular system).\nSubarachnoid hemorrhage, which is bleeding that occurs outside of the brain tissue but still within the skull, and precisely between the arachnoid mater and pia mater (the delicate innermost layer of the three layers of the meninges that surround the brain).\nThe above two main types of hemorrhagic stroke are also two different forms of intracranial hemorrhage, which is the accumulation of blood anywhere within the cranial vault; but the other forms of intracranial hemorrhage, such as epidural hematoma (bleeding between the skull and the dura mater, which is the thick outermost layer of the meninges that surround the brain) and subdural hematoma (bleeding in the subdural space), are not considered \"hemorrhagic stroke\".\nHemorrhagic stroke may occur on the background of alterations to the blood vessels in the brain, such as cerebral amyloid angiopathy, cerebral arteriovenous malformation and an intracranial aneurysm, which can cause intraparenchymal or subarachnoid hemorrhage.\nIn addition to neurological impairment, hemorrhagic stroke usually causes specific symptoms (for instance, subarachnoid hemorrhage classically causes a severe headache known as a thunderclap headache) or reveal evidence of a previous head injury.\n\nSigns and symptoms\nStroke symptoms typically start suddenly, over seconds to minutes, and in most cases do not progress further. The symptoms depend on the area of the brain affected. The more extensive the area of the brain affected, the more functions that are likely to be lost. Some forms of stroke can cause additional symptoms. For example, in intracranial hemorrhage, the affected area may compress other structures. Most forms of stroke are not associated with a headache, apart from subarachnoid hemorrhage and cerebral venous thrombosis and occasionally intracerebral hemorrhage.\n\nEarly recognition\nSystems have been proposed to increase recognition of stroke. Sudden-onset face weakness, arm drift (i.e., if a person, when asked to raise both arms, involuntarily lets one arm drift downward) and abnormal speech are the findings most likely to lead to the correct identification of a case of stroke, increasing the likelihood by 5.5 when at least one of these is present. Similarly, when all three of these are absent, the likelihood of stroke is decreased (\u2013 likelihood ratio of 0.39). While these findings are not perfect for diagnosing stroke, the fact that they can be evaluated relatively rapidly and easily make them very valuable in the acute setting.\nA mnemonic to remember the warning signs of stroke is FAST (facial droop, arm weakness, speech difficulty, and time to call emergency services), as advocated by the Department of Health (United Kingdom) and the Stroke Association, the American Stroke Association, and the National Stroke Association (US). FAST is less reliable in the recognition of posterior circulation stroke. The revised mnemonic BE FAST, which adds balance (sudden trouble keeping balance while walking or standing) and eyesight (new onset of blurry or double vision or sudden, painless loss of sight)  to the assessment, has been proposed to address this shortcoming and improve early detection of stroke even further. Other scales for prehospital detection of stroke include the Los Angeles Prehospital Stroke Screen (LAPSS) and the Cincinnati Prehospital Stroke Scale (CPSS), on which the FAST method was based. Use of these scales is recommended by professional guidelines.\nFor people referred to the emergency room, early recognition of stroke is deemed important as this can expedite diagnostic tests and treatments. A scoring system called ROSIER (recognition of stroke in the emergency room) is recommended for this purpose; it is based on features from the medical history and physical examination.\n\nAssociated symptoms\nLoss of consciousness, headache, and vomiting usually occur more often in hemorrhagic stroke than in thrombosis because of the increased intracranial pressure from the leaking blood compressing the brain.\nIf symptoms are maximal at onset, the cause is more likely to be a subarachnoid hemorrhage or an embolic stroke.\n\nSubtypes\nIf the area of the brain affected includes one of the three prominent central nervous system pathways\u2014the spinothalamic tract, corticospinal tract, and the dorsal column\u2013medial lemniscus pathway, symptoms may include:\n\nhemiplegia and muscle weakness of the face\nnumbness\nreduction in sensory or vibratory sensation\ninitial flaccidity (reduced muscle tone), replaced by spasticity (increased muscle tone), excessive reflexes, and obligatory synergies.\nIn most cases, the symptoms affect only one side of the body (unilateral). The defect in the brain is usually on the opposite side of the body. However, since these pathways also travel in the spinal cord and any lesion there can also produce these symptoms, the presence of any one of these symptoms does not necessarily indicate stroke. In addition to the above central nervous system pathways, the brainstem gives rise to most of the twelve cranial nerves. A brainstem stroke affecting the brainstem and brain, therefore, can produce symptoms relating to deficits in these cranial nerves:\n\naltered smell, taste, hearing, or vision (total or partial)\ndrooping of eyelid (ptosis) and weakness of ocular muscles\ndecreased reflexes: gag, swallow, pupil reactivity to light\ndecreased sensation and muscle weakness of the face\nbalance problems and nystagmus\naltered breathing and heart rate\nweakness in sternocleidomastoid muscle with inability to turn head to one side\nweakness in tongue (inability to stick out the tongue or move it from side to side)\nIf the cerebral cortex is involved, the central nervous system pathways can again be affected, but can also produce the following symptoms:\n\naphasia (difficulty with verbal expression, auditory comprehension, reading and writing; Broca's or Wernicke's area typically involved)\ndysarthria (motor speech disorder resulting from neurological injury)\napraxia (altered voluntary movements)\nvisual field defect\nmemory deficits (involvement of temporal lobe)\nhemineglect (involvement of parietal lobe)\ndisorganized thinking, confusion, hypersexual gestures (with involvement of frontal lobe)\nlack of insight of his or her, usually stroke-related, disability\nIf the cerebellum is involved, ataxia might be present and this includes:\n\naltered walking gait\naltered movement coordination\nvertigo and or disequilibrium\n\nPreceding signs and symptoms\nIn the days before a stroke (generally in the previous 7 days, even the previous one), a considerable proportion of patients have a \"sentinel headache\": a severe and unusual headache that indicates a problem. Its appearance makes it advisable to seek medical review and to consider prevention against stroke.\n\nCauses\nThrombotic stroke\nIn thrombotic stroke, a thrombus (blood clot) usually forms around atherosclerotic plaques. Since blockage of the artery is gradual, onset of symptomatic thrombotic stroke is slower than that of hemorrhagic stroke. A thrombus itself (even if it does not completely block the blood vessel) can lead to an embolic stroke (see below) if the thrombus breaks off and travels in the bloodstream, at which point it is called an embolus. Two types of thrombosis can cause stroke:\n\nLarge vessel disease involves the common and internal carotid arteries, the vertebral artery, and the Circle of Willis. Diseases that may form thrombi in the large vessels include (in descending incidence): atherosclerosis, vasoconstriction (tightening of the artery), aortic, carotid or vertebral artery dissection, inflammatory diseases of the blood vessel wall (Takayasu arteritis, giant cell arteritis, vasculitis), noninflammatory vasculopathy, Moyamoya disease and fibromuscular dysplasia. Strokes caused by artery dissections are in the strictest sense not always caused by a 'defined disease state', such events can occur in very young people and can be caused by physical injury such as hyperextension of the neck area or often by other forms of trauma.\nSmall vessel disease involves the smaller arteries inside the brain: branches of the circle of Willis, middle cerebral artery, stem, and arteries arising from the distal vertebral and basilar artery. Diseases that may form thrombi in the small vessels include (in descending incidence): lipohyalinosis (build-up of fatty hyaline matter in the blood vessel as a result of high blood pressure and aging) and fibrinoid degeneration (stroke involving these vessels is known as a lacunar stroke) and microatheroma (small atherosclerotic plaques).\nAnemia causes increase blood flow in the blood circulatory system. This causes the endothelial cells of the blood vessels to express adhesion factors which encourages the clotting of blood and formation of thrombus. Sickle-cell anemia, which can cause blood cells to clump up and block blood vessels, can also lead to stroke. Stroke is the second leading cause of death in people under 20 with sickle-cell anemia. Air pollution may also increase stroke risk.\n\nEmbolic stroke\nAn embolic stroke refers to an arterial embolism (a blockage of an artery) by an embolus, a traveling particle or debris in the arterial bloodstream originating from elsewhere. An embolus is most frequently a thrombus, but it can also be a number of other substances including fat (e.g., from bone marrow in a broken bone), air, cancer cells or clumps of bacteria (usually from infectious endocarditis).\nBecause an embolus arises from elsewhere, local therapy solves the problem only temporarily. Thus, the source of the embolus must be identified. Because the embolic blockage is sudden in onset, symptoms are usually maximal at the start. Also, symptoms may be transient as the embolus is partially resorbed and moves to a different location or dissipates altogether.\nEmboli most commonly arise from the heart (especially in atrial fibrillation) but may originate from elsewhere in the arterial tree. In paradoxical embolism, a deep vein thrombosis embolizes through an atrial or ventricular septal defect in the heart into the brain.\nCauses of stroke related to the heart can be distinguished between high- and low-risk:\n\nHigh risk: atrial fibrillation and paroxysmal atrial fibrillation, rheumatic disease of the mitral or aortic valve disease, artificial heart valves, known cardiac thrombus of the atrium or ventricle, sick sinus syndrome, sustained atrial flutter, recent myocardial infarction, chronic myocardial infarction together with ejection fraction <28 percent, symptomatic congestive heart failure with ejection fraction <30 percent, dilated cardiomyopathy, Libman-Sacks endocarditis, Marantic endocarditis, infective endocarditis, papillary fibroelastoma, left atrial myxoma, and coronary artery bypass graft (CABG) surgery.\nLow risk\/potential: calcification of the annulus (ring) of the mitral valve, patent foramen ovale (PFO), atrial septal aneurysm, atrial septal aneurysm with patent foramen ovale, left ventricular aneurysm without thrombus, isolated left atrial \"smoke\" on echocardiography (no mitral stenosis or atrial fibrillation), and complex atheroma in the ascending aorta or proximal arch\nAmong those who have a complete blockage of one of the carotid arteries, the risk of stroke on that side is about one percent per year.\nA special form of embolic stroke is the embolic stroke of undetermined source (ESUS). This subset of cryptogenic stroke is defined as a non-lacunar brain infarct without proximal arterial stenosis or cardioembolic sources. About one out of six cases of ischemic stroke could be classified as ESUS.\n\nCerebral hypoperfusion\nCerebral hypoperfusion is the reduction of blood flow to all parts of the brain. The reduction could be to a particular part of the brain depending on the cause. It is most commonly due to heart failure from cardiac arrest or arrhythmias, or from reduced cardiac output as a result of myocardial infarction, pulmonary embolism, pericardial effusion, or bleeding. Hypoxemia (low blood oxygen content) may precipitate the hypoperfusion. Because the reduction in blood flow is global, all parts of the brain may be affected, especially vulnerable \"watershed\" areas\u2014border zone regions supplied by the major cerebral arteries. A watershed stroke refers to the condition when the blood supply to these areas is compromised. Blood flow to these areas does not necessarily stop, but instead it may lessen to the point where brain damage can occur.\n\nVenous thrombosis\nCerebral venous sinus thrombosis leads to stroke due to locally increased venous pressure, which exceeds the pressure generated by the arteries. Infarcts are more likely to undergo hemorrhagic transformation (leaking of blood into the damaged area) than other types of ischemic stroke.\n\nIntracerebral hemorrhage\nIt generally occurs in small arteries or arterioles and is commonly due to hypertension, intracranial vascular malformations (including cavernous angiomas or arteriovenous malformations), cerebral amyloid angiopathy, or infarcts into which secondary hemorrhage has occurred. Other potential causes are trauma, bleeding disorders, amyloid angiopathy, illicit drug use (e.g., amphetamines or cocaine). The hematoma enlarges until pressure from surrounding tissue limits its growth, or until it decompresses by emptying into the ventricular system, CSF or the pial surface. A third of intracerebral bleed is into the brain's ventricles. ICH has a mortality rate of 44 percent after 30 days, higher than ischemic stroke or subarachnoid hemorrhage (which technically may also be classified as a type of stroke).\n\nOther\nOther causes may include spasm of an artery. This may occur due to cocaine. Cancer is also another well recognized potential cause of stroke. Although, malignancy in general can increase the risk of stroke, certain types of cancer such as pancreatic, lung and gastric are typically associated with a higher thromboembolism risk. The mechanism with which cancer increases stroke risk is thought to be secondary to an acquired hypercoagulability.\n\nSilent stroke\nSilent stroke is stroke that does not have any outward symptoms, and people are typically unaware they had experienced stroke. Despite not causing identifiable symptoms, silent stroke still damages the brain and places the person at increased risk for both transient ischemic attack and major stroke in the future. Conversely, those who have had major stroke are also at risk of having silent stroke. In a broad study in 1998, more than 11 million people were estimated to have experienced stroke in the United States. Approximately 770,000 of these were symptomatic and 11 million were first-ever silent MRI infarcts or hemorrhages. Silent stroke typically causes lesions which are detected via the use of neuroimaging such as MRI. Silent stroke is estimated to occur at five times the rate of symptomatic stroke. The risk of silent stroke increases with age, but they may also affect younger adults and children, especially those with acute anemia.\n\nPathophysiology\nIschemic\nIschemic stroke occurs because of a loss of blood supply to part of the brain, initiating the ischemic cascade. Atherosclerosis may disrupt the blood supply by narrowing the lumen of blood vessels leading to a reduction of blood flow by causing the formation of blood clots within the vessel or by releasing showers of small emboli through the disintegration of atherosclerotic plaques. Embolic infarction occurs when emboli formed elsewhere in the circulatory system, typically in the heart as a consequence of atrial fibrillation, or in the carotid arteries, break off, enter the cerebral circulation, then lodge in and block brain blood vessels. Since blood vessels in the brain are now blocked, the brain becomes low in energy, and thus it resorts to using anaerobic metabolism within the region of brain tissue affected by ischemia. Anaerobic metabolism produces less adenosine triphosphate (ATP) but releases a by-product called lactic acid. Lactic acid is an irritant which could potentially destroy cells since it is an acid and disrupts the normal acid-base balance in the brain. The ischemia area is referred to as the \"ischemic penumbra\". After the initial ischemic event the penumbra transitions from a tissue remodeling characterized by damage to a remodeling characterized by repair.\n\nAs oxygen or glucose becomes depleted in ischemic brain tissue, the production of high energy phosphate compounds such as adenosine triphosphate (ATP) fails, leading to failure of energy-dependent processes (such as ion pumping) necessary for tissue cell survival. This sets off a series of interrelated events that result in cellular injury and death. A major cause of neuronal injury is the release of the excitatory neurotransmitter glutamate. The concentration of glutamate outside the cells of the nervous system is normally kept low by so-called uptake carriers, which are powered by the concentration gradients of ions (mainly Na+) across the cell membrane. However, stroke cuts off the supply of oxygen and glucose which powers the ion pumps maintaining these gradients. As a result, the transmembrane ion gradients run down, and glutamate transporters reverse their direction, releasing glutamate into the extracellular space. Glutamate acts on receptors in nerve cells (especially NMDA receptors), producing an influx of calcium which activates enzymes that digest the cells' proteins, lipids, and nuclear material. Calcium influx can also lead to the failure of mitochondria, which can lead further toward energy depletion and may trigger cell death due to programmed cell death.\nIschemia also induces production of oxygen free radicals and other reactive oxygen species. These react with and damage a number of cellular and extracellular elements. Damage to the blood vessel lining or endothelium may occur. These processes are the same for any type of ischemic tissue and are referred to collectively as the ischemic cascade. However, brain tissue is especially vulnerable to ischemia since it has little respiratory reserve and is completely dependent on aerobic metabolism, unlike most other organs.\n\nCollateral flow\nThe brain can compensate inadequate blood flow in a single artery by the collateral system. This system relies on the efficient connection between the carotid and vertebral arteries through the circle of Willis and, to a lesser extent, the major arteries supplying the cerebral hemispheres. However, variations in the circle of Willis, caliber of collateral vessels, and acquired arterial lesions such as atherosclerosis can disrupt this compensatory mechanism, increasing the risk of brain ischemia resulting from artery blockage.\nThe extent of damage depends on the duration and severity of the ischemia. If ischemia persists for more than 5 minutes with perfusion below 5% of normal, some neurons will die. However, if ischemia is mild, the damage will occur slowly and may take up to 6 hours to completely destroy the brain tissue. In case of severe ischemia lasting more than 15 to 30 minutes, all of the affected tissue will die, leading to infarction. The rate of damage is affected by temperature, with hyperthermia accelerating damage and hypothermia slowing it down and  other factors. Prompt restoration of blood flow to ischemic tissues can reduce or reverse injury, especially if the tissues are not yet irreversibly damaged. This is particularly important for the moderately ischemic areas (penumbras) surrounding areas of severe ischemia, which may still be salvageable due to collateral flow.\n\nHemorrhagic\nHemorrhagic stroke is classified based on their underlying pathology. Some causes of hemorrhagic stroke are hypertensive hemorrhage, ruptured aneurysm, ruptured AV fistula, transformation of prior ischemic infarction, and drug-induced bleeding. They result in tissue injury by causing compression of tissue from an expanding hematoma or hematomas. In addition, the pressure may lead to a loss of blood supply to affected tissue with resulting infarction, and the blood released by brain hemorrhage appears to have direct toxic effects on brain tissue and vasculature. Inflammation contributes to the secondary brain injury after hemorrhage.\n\nDiagnosis\nStroke is diagnosed through several techniques: a neurological examination (such as the NIHSS), CT scans (most often without contrast enhancements) or MRI scans, Doppler ultrasound, and arteriography. The diagnosis of stroke itself is clinical, with assistance from the imaging techniques. Imaging techniques also assist in determining the subtypes and cause of stroke. There is yet no commonly used blood test for the stroke diagnosis itself, though blood tests may be of help in finding out the likely cause of stroke. In deceased people, an autopsy of stroke may help establishing the time between stroke onset and death.\n\nPhysical examination\nA physical examination, including taking a medical history of the symptoms and a neurological status, helps giving an evaluation of the location and severity of stroke. It can give a standard score on e.g., the NIH stroke scale.\n\nImaging\nFor diagnosing ischemic (blockage) stroke in the emergency setting:\n\nCT scans (without contrast enhancements)\nsensitivity= 16% (less than 10% within first 3 hours of symptom onset)\nspecificity= 96%\nMRI scan\nsensitivity= 83%\nspecificity= 98%\nFor diagnosing hemorrhagic stroke in the emergency setting:\n\nCT scans (without contrast enhancements)\nsensitivity= 89%\nspecificity= 100%\nMRI scan\nsensitivity= 81%\nspecificity= 100%\nFor detecting chronic hemorrhages, an MRI scan is more sensitive.\nFor the assessment of stable stroke, nuclear medicine scans such as single-photon emission computed tomography (SPECT) and positron emission tomography\u2013computed tomography (PET\/CT) may be helpful. SPECT documents cerebral blood flow, whereas PET with an FDG isotope shows cerebral glucose metabolism.\nCT scans may not detect ischemic stroke, especially if it is small, of recent onset, or in the brainstem or cerebellum areas (posterior circulation infarct). MRI is better at detecting a posterior circulation infarct with diffusion-weighted imaging. A CT scan is used more to rule out certain stroke mimics and detect bleeding. The presence of leptomeningeal collateral circulation in the brain is associated with better clinical outcomes after recanalization treatment. Cerebrovascular reserve capacity is another factor that affects stroke outcome \u2013 it is the amount of increase in cerebral blood flow after a purposeful stimulation of blood flow by the physician, such as by giving inhaled carbon dioxide or intravenous acetazolamide. The increase in blood flow can be measured by PET scan or transcranial doppler sonography. However, in people with obstruction of the internal carotid artery of one side, the presence of leptomeningeal collateral circulation is associated with reduced cerebral reserve capacity.\n\nUnderlying cause\nWhen stroke has been diagnosed, other studies may be performed to determine the underlying cause. With the treatment and diagnosis options available, it is of particular importance to determine whether there is a peripheral source of emboli. Test selection may vary since the cause of stroke varies with age, comorbidity and the clinical presentation. The following are commonly used techniques:\n\nan ultrasound\/doppler study of the carotid arteries (to detect carotid stenosis) or dissection of the precerebral arteries;\nan electrocardiogram (ECG) and echocardiogram (to identify arrhythmias and resultant clots in the heart which may spread to the brain vessels through the bloodstream);\na Holter monitor study to identify intermittent abnormal heart rhythms;\nan angiogram of the cerebral vasculature (if a bleed is thought to have originated from an aneurysm or arteriovenous malformation);\nblood tests to determine if blood cholesterol is high, if there is an abnormal tendency to bleed, and if some rarer processes such as homocystinuria might be involved.\nFor hemorrhagic stroke, a CT or MRI scan with intravascular contrast may be able to identify abnormalities in the brain arteries (such as aneurysms) or other sources of bleeding, and structural MRI if this shows no cause. If this too does not identify an underlying reason for the bleeding, invasive cerebral angiography could be performed but this requires access to the bloodstream with an intravascular catheter and can cause further stroke as well as complications at the insertion site and this investigation is therefore reserved for specific situations. If there are symptoms suggesting that the hemorrhage might have occurred as a result of venous thrombosis, CT or MRI venography can be used to examine the cerebral veins.\n\nMisdiagnosis\nAmong people with ischemic stroke, misdiagnosis occurs 2 to 26% of the time. A \"stroke chameleon\" (SC) is stroke which is diagnosed as something else.\nPeople not having stroke may also be misdiagnosed with the condition. Giving thrombolytics (clot-busting) in such cases causes intracerebral bleeding 1 to 2% of the time, which is less than that of people with stroke. This unnecessary treatment adds to health care costs. Even so, the AHA\/ASA guidelines state that starting intravenous tPA in possible mimics is preferred to delaying treatment for additional testing.\nWomen, African-Americans, Hispanic-Americans, Asian and Pacific Islanders are more often misdiagnosed for a condition other than stroke when in fact having stroke. In addition, adults under 44 years of age are seven times more likely to have stroke missed than are adults over 75 years of age. This is especially the case for younger people with posterior circulation infarcts. Some medical centers have used hyperacute MRI in experimental studies for people initially thought to have a low likelihood of stroke, and in some of these people, stroke has been found which were then treated with thrombolytic medication.\n\nPrevention\nGiven the disease burden of stroke, prevention is an important public health concern. Primary prevention is less effective than secondary prevention (as judged by the number needed to treat to prevent one stroke per year). Recent guidelines detail the evidence for primary prevention in stroke. About the use of aspirin as a preventive medication for stroke, in healthy people aspirin does not appear beneficial and thus is not recommended,  but in people with high cardiovascular risk, or those who have had a myocardial infarction, it provides some protection against a first stroke. In those who have previously had stroke, treatment with medications such as aspirin, clopidogrel, and dipyridamole may be beneficial. The U.S. Preventive Services Task Force (USPSTF) recommends against screening for carotid artery stenosis in those without symptoms.\n\nRisk factors\nThe most important modifiable risk factors for stroke are high blood pressure and atrial fibrillation, although the size of the effect is small; 833 people have to be treated for 1 year to prevent one stroke. Other modifiable risk factors include high blood cholesterol levels, diabetes mellitus, end-stage kidney disease, cigarette smoking (active and passive), heavy alcohol use, drug use, lack of physical activity, obesity, processed red meat consumption, and unhealthy diet. Smoking just one cigarette per day increases the risk more than 30%. Alcohol use could predispose to ischemic stroke, as well as intracerebral and subarachnoid hemorrhage via multiple mechanisms (for example, via hypertension, atrial fibrillation, rebound thrombocytosis and platelet aggregation and clotting disturbances). Drugs, most commonly amphetamines and cocaine, can induce stroke through damage to the blood vessels in the brain and acute hypertension. Migraine with aura doubles a person's risk for ischemic stroke. Untreated, celiac disease regardless of the presence of symptoms can be an underlying cause of stroke, both in children and adults. According to a 2021 WHO study, working 55+ hours a week raises the risk of stroke by 35% and the risk of dying from heart conditions by 17%, when compared to a 35-40-hour week.\nHigh levels of physical activity reduce the risk of stroke by about 26%. There is a lack of high quality studies looking at promotional efforts to improve lifestyle factors. Nonetheless, given the large body of circumstantial evidence, best medical management for stroke includes advice on diet, exercise, smoking and alcohol use. Medication is the most common method of stroke prevention; carotid endarterectomy can be a useful surgical method of preventing stroke.\n\nBlood pressure\nHigh blood pressure accounts for 35\u201350% of stroke risk. Blood pressure reduction of 10 mmHg systolic or 5 mmHg diastolic reduces the risk of stroke by ~40%. Lowering blood pressure has been conclusively shown to prevent both ischemic and hemorrhagic stroke. It is equally important in secondary prevention. Even people older than 80 years and those with isolated systolic hypertension benefit from antihypertensive therapy. The available evidence does not show large differences in stroke prevention between antihypertensive drugs\u2014therefore, other factors such as protection against other forms of cardiovascular disease and cost should be considered. The routine use of beta-blockers following stroke or TIA has not been shown to result in benefits.\n\nBlood lipids\nHigh cholesterol levels have been inconsistently associated with (ischemic) stroke. Statins have been shown to reduce the risk of stroke by about 15%. Since earlier meta-analyses of other lipid-lowering drugs did not show a decreased risk, statins might exert their effect through mechanisms other than their lipid-lowering effects.\n\nDiabetes mellitus\nDiabetes mellitus increases the risk of stroke by 2 to 3 times. While intensive blood sugar control has been shown to reduce small blood vessel complications such as kidney damage and damage to the retina of the eye it has not been shown to reduce large blood vessel complications such as stroke.\n\nAnticoagulant drugs\nOral anticoagulants such as warfarin have been the mainstay of stroke prevention for over 50 years. However, several studies have shown that aspirin and other antiplatelets are highly effective in secondary prevention after stroke or transient ischemic attack. Low doses of aspirin (for example 75\u2013150 mg) are as effective as high doses but have fewer side effects; the lowest effective dose remains unknown. Thienopyridines (clopidogrel, ticlopidine) might be slightly more effective than aspirin and have a decreased risk of gastrointestinal bleeding but are more expensive. Both aspirin and clopidogrel may be useful in the first few weeks after a minor stroke or high-risk TIA. Clopidogrel has less side effects than ticlopidine. Dipyridamole can be added to aspirin therapy to provide a small additional benefit, even though headache is a common side effect. Low-dose aspirin is also effective for stroke prevention after having a myocardial infarction.\nThose with atrial fibrillation have a 5% a year risk of stroke, and those with valvular atrial fibrillation have an even higher risk. Depending on the stroke risk, anticoagulation with medications such as warfarin or aspirin is useful for prevention with various levels of comparative effectiveness depending on the type of treatment used. \nOral anticoagulants, especially Xa (apixaban) and thrombin (dabigatran) inhibitors, have been shown to be superior to warfarin in stroke reduction and have a lower or similar bleeding risk in patients with atrial fibrillation. Except in people with atrial fibrillation, oral anticoagulants are not advised for stroke prevention\u2014any benefit is offset by bleeding risk.\nIn primary prevention, however, antiplatelet drugs did not reduce the risk of ischemic stroke but increased the risk of major bleeding. Further studies are needed to investigate a possible protective effect of aspirin against ischemic stroke in women.\n\nSurgery\nCarotid endarterectomy or carotid angioplasty can be used to remove atherosclerotic narrowing of the carotid artery. There is evidence supporting this procedure in selected cases. Endarterectomy for a significant stenosis has been shown to be useful in preventing further stroke in those who have already had the condition. Carotid artery stenting has not been shown to be equally useful. People are selected for surgery based on age, gender, degree of stenosis, time since symptoms and the person's preferences. Surgery is most efficient when not delayed too long\u2014the risk of recurrent stroke in a person who has a 50% or greater stenosis is up to 20% after 5 years, but endarterectomy reduces this risk to around 5%. The number of procedures needed to cure one person was 5 for early surgery (within two weeks after the initial stroke), but 125 if delayed longer than 12 weeks.\nScreening for carotid artery narrowing has not been shown to be a useful test in the general population. Studies of surgical intervention for carotid artery stenosis without symptoms have shown only a small decrease in the risk of stroke. To be beneficial, the complication rate of the surgery should be kept below 4%. Even then, for 100 surgeries, 5 people will benefit by avoiding stroke, 3 will develop stroke despite surgery, 3 will develop stroke or die due to the surgery itself, and 89 will remain stroke-free but would also have done so without intervention.\n\nDiet\nNutrition, specifically the Mediterranean-style diet, has the potential to decrease the risk of having a stroke by more than half. It does not appear that lowering levels of homocysteine with folic acid affects the risk of stroke.\n\nWomen\nA number of specific recommendations have been made for women including taking aspirin after the 11th week of pregnancy if there is a history of previous chronic high blood pressure and taking blood pressure medications during pregnancy if the blood pressure is greater than 150 mmHg systolic or greater than 100 mmHg diastolic. In those who have previously had preeclampsia, other risk factors should be treated more aggressively.\n\nPrevious stroke or TIA\nKeeping blood pressure below 140\/90 mmHg is recommended. Anticoagulation can prevent recurrent ischemic stroke. Among people with nonvalvular atrial fibrillation, anticoagulation can reduce stroke by 60% while antiplatelet agents can reduce stroke by 20%. However, a recent meta-analysis suggests harm from anticoagulation started early after an embolic stroke. Stroke prevention treatment for atrial fibrillation is determined according to the CHA2DS2\u2013VASc score. The most widely used anticoagulant to prevent thromboembolic stroke in people with nonvalvular atrial fibrillation is the oral agent warfarin while a number of newer agents including dabigatran are alternatives which do not require prothrombin time monitoring.\nAnticoagulants, when used following stroke, should not be stopped for dental procedures.\nIf studies show carotid artery stenosis, and the person has a degree of residual function on the affected side, carotid endarterectomy (surgical removal of the stenosis) may decrease the risk of recurrence if performed rapidly after stroke.\n\nManagement\nStroke, whether ischemic or hemorrhagic, is an emergency that warrants immediate medical attention. The specific treatment will depend on the type of stroke, the time elapsed since the onset of symptoms, and the underlying cause or presence of comorbidities.\n\nIschemic stroke\nAspirin reduces the overall risk of recurrence by 13% with greater benefit early on. Definitive therapy within the first few hours is aimed at removing the blockage by breaking the clot down (thrombolysis), or by removing it mechanically (thrombectomy). The philosophical premise underlying the importance of rapid stroke intervention was summed up as Time is Brain! in the early 1990s. Years later, that same idea, that rapid cerebral blood flow restoration results in fewer brain cells dying, has been proved and quantified.\nTight blood sugar control in the first few hours does not improve outcomes and may cause harm. High blood pressure is also not typically lowered as this has not been found to be helpful. Cerebrolysin, a mixture of pig brain-derived neurotrophic factors used widely to treat acute ischemic stroke in China, Eastern Europe, Russia, post-Soviet countries, and other Asian countries, does not improve outcomes or prevent death and may increase the risk of severe adverse events. There is also no evidence that cerebrolysin\u2010like peptide mixtures which are extracted from cattle brain is helpful in treating acute ischemic stroke.\n\nThrombolysis\nThrombolysis, such as with recombinant tissue plasminogen activator (rtPA), in acute ischemic stroke, when given within three hours of symptom onset, results in an overall benefit of 10% with respect to living without disability. It does not, however, improve chances of survival. Benefit is greater the earlier it is used. Between three and four and a half hours the effects are less clear. The AHA\/ASA recommend it for certain people in this time frame. A 2014 review found a 5% increase in the number of people living without disability at three to six months; however, there was a 2% increased risk of death in the short term. After four and a half hours thrombolysis worsens outcomes. These benefits or lack of benefits occurred regardless of the age of the person treated. There is no reliable way to determine who will have an intracranial bleed post-treatment versus who will not. In those with findings of savable tissue on medical imaging between 4.5 hours and 9 hours or who wake up with stroke, alteplase results in some benefit.\nIts use is endorsed by the American Heart Association, the American College of Emergency Physicians and the American Academy of Neurology as the recommended treatment for acute stroke within three hours of onset of symptoms as long as there are no other contraindications (such as abnormal lab values, high blood pressure, or recent surgery). This position for tPA is based upon the findings of two studies by one group of investigators which showed that tPA improves the chances for a good neurological outcome. When administered within the first three hours thrombolysis improves functional outcome without affecting mortality. 6.4% of people with large stroke developed substantial brain bleeding as a complication from being given tPA thus part of the reason for increased short term mortality. The American Academy of Emergency Medicine had previously stated that objective evidence regarding the applicability of tPA for acute ischemic stroke was insufficient. In 2013 the American College of Emergency Medicine refuted this position, acknowledging the body of evidence for the use of tPA in ischemic stroke; but debate continues. Intra-arterial fibrinolysis, where a catheter is passed up an artery into the brain and the medication is injected at the site of thrombosis, has been found to improve outcomes in people with acute ischemic stroke.\n\nEndovascular treatment\nMechanical removal of the blood clot causing the ischemic stroke, called mechanical thrombectomy, is a potential treatment for occlusion of a large artery, such as the middle cerebral artery. In 2015, one review demonstrated the safety and efficacy of this procedure if performed within 12 hours of the onset of symptoms. It did not change the risk of death but did reduce disability compared to the use of intravenous thrombolysis, which is generally used in people evaluated for mechanical thrombectomy. Certain cases may benefit from thrombectomy up to 24 hours after the onset of symptoms.\n\nCraniectomy\nStroke affecting large portions of the brain can cause significant brain swelling with secondary brain injury in surrounding tissue. This phenomenon is mainly encountered in stroke affecting brain tissue dependent upon the middle cerebral artery for blood supply and is also called \"malignant cerebral infarction\" because it carries a dismal prognosis. Relief of the pressure may be attempted with medication, but some require hemicraniectomy, the temporary surgical removal of the skull on one side of the head. This decreases the risk of death, although some people \u2013 who would otherwise have died \u2013 survive with disability.\n\nHemorrhagic stroke\nPeople with intracerebral hemorrhage require supportive care, including blood pressure control if required. People are monitored for changes in the level of consciousness, and their blood sugar and oxygenation are kept at optimum levels. Anticoagulants and antithrombotics can make bleeding worse and are generally discontinued (and reversed if possible). A proportion may benefit from neurosurgical intervention to remove the blood and treat the underlying cause, but this depends on the location and the size of the hemorrhage as well as patient-related factors, and ongoing research is being conducted into the question as to which people with intracerebral hemorrhage may benefit.\nIn subarachnoid hemorrhage, early treatment for underlying cerebral aneurysms may reduce the risk of further hemorrhages. Depending on the site of the aneurysm this may be by surgery that involves opening the skull or endovascularly (through the blood vessels).\n\nStroke unit\nIdeally, people who have had stroke are admitted to a \"stroke unit\", a ward or dedicated area in a hospital staffed by nurses and therapists with experience in stroke treatment. It has been shown that people admitted to stroke units have a higher chance of surviving than those admitted elsewhere in hospital, even if they are being cared for by doctors without experience in stroke. Nursing care is fundamental in maintaining skin care, feeding, hydration, positioning, and monitoring vital signs such as temperature, pulse, and blood pressure.\n\nRehabilitation\nStroke rehabilitation is the process by which those with disabling stroke undergo treatment to help them return to normal life as much as possible by regaining and relearning the skills of everyday living. It also aims to help the survivor understand and adapt to difficulties, prevent secondary complications, and educate family members to play a supporting role. Stroke rehabilitation should begin almost immediately with a multidisciplinary approach. The rehabilitation team may involve physicians trained in rehabilitation medicine, neurologists, clinical pharmacists, nursing staff, physiotherapists, occupational therapists, speech-language pathologists, and orthotists. Some teams may also include psychologists and social workers, since at least one-third of affected people manifests post stroke depression. Validated instruments such as the Barthel scale may be used to assess the likelihood of a person who has had stroke being able to manage at home with or without support subsequent to discharge from a hospital.\nStroke rehabilitation should be started as quickly as possible and can last anywhere from a few days to over a year. Most return of function is seen in the first few months, and then improvement falls off with the \"window\" considered officially by U.S. state rehabilitation units and others to be closed after six months, with little chance of further improvement. However, some people have reported that they continue to improve for years, regaining and strengthening abilities like writing, walking, running, and talking. Daily rehabilitation exercises should continue to be part of the daily routine for people who have had stroke. Complete recovery is unusual but not impossible and most people will improve to some extent: proper diet and exercise are known to help the brain to recover.\n\nSpatial neglect\nThe body of evidence is uncertain on the efficacy of cognitive rehabilitation for reducing the disabling effects of neglect and increasing independence remains unproven. However, there is limited evidence that cognitive rehabilitation may have an immediate beneficial effect on tests of neglect. Overall, no rehabilitation approach can be supported by evidence for spatial neglect.\n\nAutomobile driving\nThe body of evidence is uncertain whether the use of rehabilitation can improve on-road driving skills following stroke. There is limited evidence that training on a driving simulator will improve performance on recognizing road signs after training. The findings are based on low-quality evidence as further research is needed involving large numbers of participants.\n\nYoga\nBased on low quality evidence, it is uncertain whether yoga has a significant benefit for stroke rehabilitation on measures of quality of life, balance, strength, endurance, pain, and disability scores. Yoga may reduce anxiety and could be included as part of patient-centred stroke rehabilitation. Further research is needed assessing the benefits and safety of yoga in stroke rehabilitation.\n\nAction observation physical therapy for upper limbs\nLow-quality evidence suggests that action observation (a type of physiotherapy that is meant to improve neural plasticity through the mirror-neuronal system) may be of some benefit and has no significant adverse effects, however this benefit may not be clinically significant and further research is suggested.\n\nCognitive rehabilitation for attention deficits\nThe body of scientific evidence is uncertain on the effectiveness of cognitive rehabilitation for attention deficits in patients following stroke. While there may be an immediate effect after treatment on attention, the findings are based on low to moderate quality and small number of studies. Further research is needed to assess whether the effect can be sustained in day-to-day tasks requiring attention.\n\nMotor imagery for gait rehabilitation\nThe latest evidence supports the short-term benefits of motor imagery (MI) on walking speed in individuals who have had stroke, in comparison to other therapies. MI does not improve motor function after stroke and does not seem to cause significant adverse events. The findings are based on low-quality evidence as further research is needed to estimate the effect of MI on walking endurance and the dependence on personal assistance.\n\nPhysical and occupational therapy\nPhysical and occupational therapy have overlapping areas of expertise; however, physical therapy focuses on joint range of motion and strength by performing exercises and relearning functional tasks such as bed mobility, transferring, walking and other gross motor functions. Physiotherapists can also work with people who have had stroke to improve awareness and use of the hemiplegic side. Rehabilitation involves working on the ability to produce strong movements or the ability to perform tasks using normal patterns. Emphasis is often concentrated on functional tasks and people's goals. One example physiotherapists employ to promote motor learning involves constraint-induced movement therapy. Through continuous practice the person relearns to use and adapt the hemiplegic limb during functional activities to create lasting permanent changes. Physical therapy is effective for recovery of function and mobility after stroke. Occupational therapy is involved in training to help relearn everyday activities known as the activities of daily living (ADLs) such as eating, drinking, dressing, bathing, cooking, reading and writing, and toileting. Approaches to helping people with urinary incontinence include physical therapy, cognitive therapy, and specialized interventions with experienced medical professionals, however, it is not clear how effective these approaches are at improving urinary incontinence following stroke.\nTreatment of spasticity related to stroke often involves early mobilizations, commonly performed by a physiotherapist, combined with elongation of spastic muscles and sustained stretching through different positions. Gaining initial improvement in range of motion is often achieved through rhythmic rotational patterns associated with the affected limb. After full range has been achieved by the therapist, the limb should be positioned in the lengthened positions to prevent against further contractures, skin breakdown, and disuse of the limb with the use of splints or other tools to stabilize the joint. Cold ice wraps or ice packs may briefly relieve spasticity by temporarily reducing neural firing rates. Electrical stimulation to the antagonist muscles or vibrations has also been used with some success. Physical therapy is sometimes suggested for people who experience sexual dysfunction following stroke.\n\nInterventions for age-related visual problems in patients with stroke\nWith the prevalence of vision problems increasing with age in stroke patients, the overall effect of interventions for age-related visual problems is uncertain. It is also not sure whether people with stroke respond differently from the general population when treating eye problems. Further research in this area is needed as the body of evidence is very low quality.\n\nSpeech and language therapy\nSpeech and language therapy is appropriate for people with the speech production disorders: dysarthria and apraxia of speech, aphasia, cognitive-communication impairments, and problems with swallowing. \nSpeech and language therapy for aphasia following stroke improves functional communication, reading, writing and expressive language. Speech and language therapy that is higher intensity, higher dose or provided over a long duration of time leads to significantly better functional communication but people might be more likely to drop out of high intensity treatment (up to 15 hours per week). A total of 20-50 hours of speech and language therapy is necessary for the best recovery. The most improvement happens when 2-5 hours of therapy is provided each week over 4-5 days. Recovery is further improved when besides the therapy people practice tasks at home. Speech and language therapy is also effective if it is delivered online through video or by a family member who has been trained by a professional therapist.\nRecovery with therapy for aphasia is also dependent on the recency of stroke and the age of the person. Receiving therapy within a month after the stroke leads to the greatest improvements. 3 or 6 months after the stroke more therapy will be needed but symptoms can still be improved. People with aphasia who are younger than 55 years are the most likely to improve but people older than 75 years can still get better with therapy.\nPeople who have had stroke may have particular problems, such as dysphagia, which can cause swallowed material to pass into the lungs and cause aspiration pneumonia. The condition may improve with time, but in the interim, a nasogastric tube may be inserted, enabling liquid food to be given directly into the stomach. If swallowing is still deemed unsafe, then a percutaneous endoscopic gastrostomy (PEG) tube is passed and this can remain indefinitely. Swallowing therapy has mixed results as of 2018.\n\nDevices\nOften, assistive technology such as wheelchairs, walkers and canes may be beneficial. Many mobility problems can be improved by the use of ankle foot orthoses.\n\nPhysical fitness\nStroke can also reduce people's general fitness. Reduced fitness can reduce capacity for rehabilitation as well as general health. Physical exercises as part of a rehabilitation program following stroke appear safe. Cardiorespiratory fitness training that involves walking in rehabilitation can improve speed, tolerance and independence during walking, and may improve balance. There are inadequate long-term data about the effects of exercise and training on death, dependence and disability after stroke. The future areas of research may concentrate on the optimal exercise prescription and long-term health benefits of exercise. The effect of physical training on cognition also may be studied further.\nThe ability to walk independently in their community, indoors or outdoors, is important following stroke. Although no negative effects have been reported, it is unclear if outcomes can improve with these walking programs when compared to usual treatment.\n\nOther therapy methods\nSome current and future therapy methods include the use of virtual reality and video games for rehabilitation. These forms of rehabilitation offer potential for motivating people to perform specific therapy tasks that many other forms do not. While virtual reality and interactive video gaming are not more effective than conventional therapy for improving upper limb function, when used in conjunction with usual care these approaches may improve upper limb function and ADL function. There are inadequate data on the effect of virtual reality and interactive video gaming on gait speed, balance, participation and quality of life. Many clinics and hospitals are adopting the use of these off-the-shelf devices for exercise, social interaction, and rehabilitation because they are affordable, accessible and can be used within the clinic and home.\nMirror therapy is associated with improved motor function of the upper extremity in people who have had stroke.\nOther non-invasive rehabilitation methods used to augment physical therapy of motor function in people recovering from stroke include transcranial magnetic stimulation and transcranial direct-current stimulation. and robotic therapies. Constraint\u2010induced movement therapy (CIMT), mental practice, mirror therapy, interventions for sensory impairment, virtual reality and a relatively high dose of repetitive task practice may be effective in improving upper limb function. However, further primary research, specifically of CIMT, mental practice, mirror therapy and virtual reality is needed.\n\nOrthotics\nClinical studies confirm the importance of orthoses in stroke rehabilitation. The orthosis supports the therapeutic applications and also helps to mobilize the patient at an early stage. With the help of an orthosis, physiological standing and walking can be learned again, and late health consequences caused by a wrong gait pattern can be prevented. A treatment with an orthosis can therefore be used to support the therapy.\n\nSelf-management\nStroke can affect the ability to live independently and with quality. Self-management programs are a special training that educates stroke survivors about stroke and its consequences, helps them acquire skills to cope with their challenges, and helps them set and meet their own goals during their recovery process. These programs are tailored to the target audience, and led by someone trained and expert in stroke and its consequences (most commonly professionals, but also stroke survivors and peers). A 2016 review reported that these programs improve the quality of life after stroke, without negative effects. People with stroke felt more empowered, happy and satisfied with life after participating in this training.\n\nPrognosis\nDisability affects 75% of stroke survivors enough to decrease their ability to work.\nStroke can affect people physically, mentally, emotionally, or a combination of the three. The results of stroke vary widely depending on size and location of the lesion.\n\nPhysical effects\nSome of the physical disabilities that can result from stroke include muscle weakness, numbness, pressure sores, pneumonia, incontinence, apraxia (inability to perform learned movements), difficulties carrying out daily activities, appetite loss, speech loss, vision loss and pain. If the stroke is severe enough, or in a certain location such as parts of the brainstem, coma or death can result. Up to 10% of people following stroke develop seizures, most commonly in the week subsequent to the event; the severity of the stroke increases the likelihood of a seizure. An estimated 15% of people experience urinary incontinence for more than a year following stroke. 50% of people have a decline in sexual function (sexual dysfunction) following stroke.\n\nEmotional and mental effects\nEmotional and mental dysfunctions correspond to areas in the brain that have been damaged. Emotional problems following stroke can be due to direct damage to emotional centers in the brain or from frustration and difficulty adapting to new limitations. Post-stroke emotional difficulties include anxiety, panic attacks, flat affect (failure to express emotions), mania, apathy and psychosis. Other difficulties may include a decreased ability to communicate emotions through facial expression, body language and voice.\nDisruption in self-identity, relationships with others, and emotional well-being can lead to social consequences after stroke due to the lack of ability to communicate. Many people who experience communication impairments after stroke find it more difficult to cope with the social issues rather than physical impairments. Broader aspects of care must address the emotional impact speech impairment has on those who experience difficulties with speech after stroke. Those who experience a stroke are at risk of paralysis, which could result in a self-disturbed body image, which may also lead to other social issues.\n30 to 50% of stroke survivors develop post-stroke depression, which is characterized by lethargy, irritability, sleep disturbances, lowered self-esteem and withdrawal. Depression can reduce motivation and worsen outcome, but can be treated with social and family support, psychotherapy and, in severe cases, antidepressants. Psychotherapy sessions may have a small effect on improving mood and preventing depression after stroke. Antidepressant medications may be useful for treating depression after stroke but are associated with central nervous system and gastrointestinal adverse events.\nEmotional lability, another consequence of stroke, causes the person to switch quickly between emotional highs and lows and to express emotions inappropriately, for instance with an excess of laughing or crying with little or no provocation. While these expressions of emotion usually correspond to the person's actual emotions, a more severe form of emotional lability causes the affected person to laugh and cry pathologically, without regard to context or emotion. Some people show the opposite of what they feel, for example crying when they are happy. Emotional lability occurs in about 20% of those who have had stroke. Those with a right hemisphere stroke are more likely to have empathy problems which can make communication harder.\nCognitive deficits resulting from stroke include perceptual disorders, aphasia, dementia, and problems with attention and memory. Stroke survivors may be unaware of their own disabilities, a condition called anosognosia. In a condition called hemispatial neglect, the affected person is unable to attend to anything on the side of space opposite to the damaged hemisphere. Cognitive and psychological outcome after stroke can be affected by the age at which the stroke happened, pre-stroke baseline intellectual functioning, psychiatric history and whether there is pre-existing brain pathology.\n\nEpidemiology\nStroke was the second most frequent cause of death worldwide in 2011, accounting for 6.2 million deaths (~11% of the total). Approximately 17 million people had stroke in 2010 and 33 million people have previously had stroke and were still alive. Between 1990 and 2010 the incidence of stroke decreased by approximately 10% in the developed world and increased by 10% in the developing world. Overall, two-thirds of stroke occurred in those over 65 years old. South Asians are at particularly high risk of stroke, accounting for 40% of global stroke deaths. Incidence of ischemic stroke is ten times more frequent than haemorrhagic stroke.\nIt is ranked after heart disease and before cancer. In the United States stroke is a leading cause of disability, and recently declined from the third leading to the fourth leading cause of death. Geographic disparities in stroke incidence have been observed, including the existence of a \"stroke belt\" in the southeastern United States, but causes of these disparities have not been explained.\nThe risk of stroke increases exponentially from 30 years of age, and the cause varies by age. Advanced age is one of the most significant stroke risk factors. 95% of stroke occurs in people age 45 and older, and two-thirds of stroke occurs in those over the age of 65.\nA person's risk of dying if he or she does have stroke also increases with age. However, stroke can occur at any age, including in childhood.\nFamily members may have a genetic tendency for stroke or share a lifestyle that contributes to stroke. Higher levels of Von Willebrand factor are more common amongst people who have had ischemic stroke for the first time. The results of this study found that the only significant genetic factor was the person's blood type. Having stroke in the past greatly increases one's risk of future stroke.\nMen are 25% more likely to develop stroke than women, yet 60% of deaths from stroke occur in women. Since women live longer, they are older on average when they have stroke and thus more often killed. Some risk factors for stroke apply only to women. Primary among these are pregnancy, childbirth, menopause, and the treatment thereof (HRT).\n\nHistory\nEpisodes of stroke and familial stroke have been reported from the 2nd millennium BC onward in ancient Mesopotamia and Persia. Hippocrates (460 to 370 BC) was first to describe the phenomenon of sudden paralysis that is often associated with ischemia. Apoplexy, from the Greek word meaning \"struck down with violence\", first appeared in Hippocratic writings to describe this phenomenon.\nThe word stroke was used as a synonym for apoplectic seizure as early as 1599, and is a fairly literal translation of the Greek term. The term apoplectic stroke is an archaic, nonspecific term, for a cerebrovascular accident accompanied by haemorrhage or haemorrhagic stroke. Martin Luther was described as having an apoplectic stroke that deprived him of his speech shortly before his death in 1546.\nIn 1658, in his Apoplexia, Johann Jacob Wepfer (1620\u20131695) identified the cause of hemorrhagic stroke when he suggested that people who had died of apoplexy had bleeding in their brains.\nWepfer also identified the main arteries supplying the brain, the vertebral and carotid arteries, and identified the cause of a type of ischemic stroke known as a cerebral infarction when he suggested that apoplexy might be caused by a blockage to those vessels. Rudolf Virchow first described the mechanism of thromboembolism as a major factor.\nThe term cerebrovascular accident was introduced in 1927, reflecting a \"growing awareness and acceptance of vascular theories and (...) recognition of the consequences of a sudden disruption in the vascular supply of the brain\". Its use is now discouraged by a number of neurology textbooks, reasoning that the connotation of fortuitousness carried by the word accident insufficiently highlights the modifiability of the underlying risk factors. Cerebrovascular insult may be used interchangeably.\nThe term brain attack was introduced for use to underline the acute nature of stroke according to the American Stroke Association, which has used the term since 1990, and is used colloquially to refer to both ischemic as well as hemorrhagic stroke.\n\nResearch\nAs of 2017, angioplasty and stents were under preliminary clinical research to determine the possible therapeutic advantages of these procedures in comparison to therapy with statins, antithrombotics, or antihypertensive drugs.\n\nSee also\nReferences\nFurther reading\nExternal links\n\nStroke at Curlie\nDRAGON Score for Post-Thrombolysis Archived 2020-10-27 at the Wayback Machine\nTHRIVE score for stroke outcome Archived 2016-09-13 at the Wayback Machine\nNational Institute of Neurological Disorders and Stroke","179":"The tone decay test (also known as the threshold tone decay test or TTDT) is used in audiology to detect and measure auditory fatigue. It was developed by Raymond Carhart in 1957. In people with normal hearing, a tone whose intensity is only slightly above their absolute threshold of hearing can be heard continuously for 60 seconds. The tone decay test produces a measure of the \"decibels of decay\", i.e. the number of decibels above the patient's absolute threshold of hearing that are required for the tone to be heard for 60 seconds. A decay of between 15 and 20 decibels is indicative of cochlear hearing loss. A decay of more than 25 decibels is indicative of damage to the vestibulocochlear nerve.\n\nProcedure\nA tone at the frequency of 4000 Hz is presented for 60 seconds at an intensity of 5 decibels above the patient's absolute threshold of hearing. If the patient stops hearing the tone before 60 seconds, the intensity level is increased by another 5 decibels with the procedure repeated until the tone can be heard for the full 60 seconds or until no decibel level can be found where the tone can be heard for the full 60 seconds. The resultant measure is given as the decibels of decay.\n\nInterpretation of TDT\nTD is a procedure for diagnosing retro-cochlear pathology (RCP, damage to the auditory nerve). It is part of battery of tests that aim to differentiate between cochlear and retro-cochlear pathology.\nAccording to Rosenberg, 1958:\n\n0-5 dB Decay - Normal or Conductive\n10-15 dB Decay - Mild\n20-25 dB Decay - Moderate\n30->35 dB Decay - Marked Decay\n\nMarked tone decay indicates probability of RCP. Glaslow, 1968 stated that positive TD is one where there is at least 30 dB decay. Tillman, 1969 agreed that patients with RCP, typically have TD exceeding 30 dB.\nHowever, at the same time it would be dangerous to assume that anyone with 30 dB decay, has RCP. While everyone with less than this amount, does not have.\nA more predictive way of looking at TD is that each dB of decay above 15 dB, should raise the suspicion that RCP lesion may exist.\nThe greater the TD and the number of frequencies involved, particularly the low frequencies, and then there is greater possibility of serious pathology.\nThe index of suspicion should also be raised if the rate of decay does not diminish with increased stimulus intensity. Patients with acoustic tumor, frequently exhibit extreme an often complete TD. However, tumor size appears to be related to the severity of symptoms. Partial or complete TD was found in 60% of tumors classified as large, while, 40% of tumor is classified as small.\nFowler noted that equal loudness between the recruiting impaired ear with normal ear can be achieved only with larger sensation levels (SLs) to the normal ear. E.g. A tone at SL of 60 dB in normal ear and 30 dB in impaired ear may sound equally loud. This result suggests that the growth of loudness requiring an intensity increase of 60 dB in normal ear is achieved with an intensity increase of 30 dB in impaired ear. This indicates that recruitment for loudness growth must be occurring much more in impaired ear. This is due to abnormality in cochlea such as hypersensitivity of haircells due to damage. Recruitment is a landmark feature of SNHL of cochlear origin. Reverse Recruitment \/ Decruitment is a hallmark feature of SNHL of Retro Cochlear region. When recruitment is found to be associated with presence of cochlear pathology then the recruitment is known as complete recruitment. When the recruitment is associated with cochlea then the concept is known as Partial Recruitment.\n\nAdvantages\nLow cost and general accessibility\n\nDisadvantages\nThe Pathophysiology behind tone decay is not very well known. The actual value of any tone decay procedure in accurately identifying 8 cranial nerve pathology has not been extensively investigated\n\nReferences\nFurther reading\nCarhart, Raymond (1957). \"Clinical Determination of Abnormal Auditory Adaptation\". A.M.A. archives of otolaryngology, 65 (1), pp. 32\u201339. (subscription required)\nRieber, R. W. (2013). Communication Disorders, p. 66. Springer. ISBN 1475797605\nStach, Brad (2008). Clinical Audiology: An Introduction, p. 304. Cengage Learning. ISBN 0766862887","180":"Tinnitus is a variety of sound that is heard when no corresponding external sound is present. Nearly everyone experiences faint \"normal tinnitus\" in a completely quiet room; but it is of concern only if it is bothersome, interferes with normal hearing, or is associated with other problems. The word tinnitus comes from the Latin tinnire, \"to ring\". In some people, it interferes with concentration, and can be associated with anxiety and depression.\n\nTinnitus is usually associated with hearing loss and decreased comprehension of speech in noisy environments. It is common, affecting about 10\u201315% of people. Most tolerate it well, and it is a significant problem in only 1\u20132% of people. It can trigger a fight-or-flight response, as the brain may perceive it as dangerous and important.\nRather than a disease, tinnitus is a symptom that may result from a variety of underlying causes and may be generated at any level of the auditory system as well as outside that system. The most common causes are hearing damage, noise-induced hearing loss, or age-related hearing loss, known as presbycusis. Other causes include ear infections, disease of the heart or blood vessels, M\u00e9ni\u00e8re's disease, brain tumors, acoustic neuromas (tumors on the auditory nerves of the ear), migraines, temporomandibular joint disorders, exposure to certain medications, a previous head injury, and earwax. It can suddenly emerge during a period of emotional stress. It is more common in those with depression.\nThe diagnosis of tinnitus is usually based on a patient's description of the symptoms they are experiencing. Such a diagnosis is commonly supported by an audiogram, and an otolaryngological and neurological examination. How much tinnitus interferes with a person's life may be quantified with questionnaires. If certain problems are found, medical imaging, such as magnetic resonance imaging (MRI), may be performed. Other tests are suitable when tinnitus occurs with the same rhythm as the heartbeat. Rarely, the sound may be heard by someone other than the patient by using a stethoscope, in which case it is known as \"objective tinnitus.\" Occasionally, spontaneous otoacoustic emissions, sounds produced normally by the inner ear, may result in tinnitus.\nMeasures to prevent tinnitus include avoiding chronic or extended exposure to loud noise, and limiting exposure to ototoxic drugs and substances. If there is an underlying cause, treating that cause may lead to improvements. Otherwise, typically, tinnitus management involves psychoeducation or counseling, such as talk therapy. Sound generators or hearing aids may help. No medication directly targets tinnitus.\n\nSigns and symptoms\nTinnitus is often described as ringing, but it may also sound like clicking, buzzing, hissing, or roaring. It may be soft or loud, low- or high-pitched, and may seem to come from either one or both ears, or from the head itself. It may be intermittent or continuous. In some individuals, its intensity may be changed by shoulder, neck, head, tongue, jaw, or eye movements.\n\nCourse\nDue to variations in study designs, data on the course of tinnitus shows few consistent results. Generally, prevalence increases with age in adults, and the ratings of annoyance decreases with duration.\n\nPsychological effects\nAlthough it is an annoying condition to which most people adapt, persistent tinnitus may cause anxiety and depression in some people. Tinnitus annoyance is more strongly associated with the psychological condition of the person than the loudness or frequency range of the perceived sound. Psychological problems such as depression, anxiety, sleep disturbances, and concentration difficulties are common in those with strongly annoying tinnitus. 45% of people with tinnitus have an anxiety disorder at some time in their lives.\nPsychological research has focused on the tinnitus distress reaction to account for differences in tinnitus severity. The research indicates that conditioning at the initial perception of tinnitus linked it with negative emotions, such as fear and anxiety.\n\nTypes\nCommonly tinnitus is classified into \"subjective and objective tinnitus\". Tinnitus is usually subjective, meaning that the sounds the person hears are not detectable by means currently available to physicians and hearing technicians. Subjective tinnitus has also been called \"tinnitus aurium\", \"non-auditory\", or \"non-vibratory\" tinnitus. In rare cases, tinnitus can be heard by someone else using a stethoscope. Even more rarely, in some cases it can be measured as a spontaneous otoacoustic emission (SOAE) in the ear canal. This is classified as objective tinnitus, also called \"pseudo-tinnitus\" or \"vibratory\" tinnitus.\n\nSubjective tinnitus\nSubjective tinnitus is the most frequent type. It can have many causes, but most commonly it results from hearing loss. When it is caused by disorders of the inner ear or auditory nerve, it can be called \"otic\" (from the Greek word for ear). These otological or neurological disorders include those triggered by infections, drugs, or trauma. A frequent cause is traumatic noise exposure that damages hair cells in the inner ear.. Some evidence suggests that long-term exposure to noise pollution from heavy traffic may increase the risk of developing tinnitus.\nWhen there does not seem to be a connection with a disorder of the inner ear or auditory nerve, tinnitus can be called \"non-otic\". In 30% of cases, tinnitus is influenced by the somatosensory system; for instance, people can increase or decrease their tinnitus by moving their face, head, jaw, or neck. This type is called somatic or craniocervical tinnitus, since it is only head or neck movements that have an effect.\nSome tinnitus may be caused by neuroplastic changes in the central auditory pathway. In this theory, the disturbance of sensory input caused by hearing loss results in such changes as a homeostatic response of neurons in the central auditory system, causing tinnitus. When some frequencies of sound are lost to hearing loss, the auditory system compensates by amplifying those frequencies, eventually producing sound sensations at those frequencies constantly even when there is no corresponding external sound.\n\nHearing loss\nThe most common cause of tinnitus is hearing loss. Hearing loss may have many different causes, but among those with tinnitus, the major cause is cochlear injury.\nIn many cases no underlying cause is identified.\nOtotoxic drugs also may cause subjective tinnitus, as they may cause hearing loss, or increase the damage done by exposure to loud noise. This damage may occur even at doses not considered ototoxic. More than 260 medications have been reported to cause tinnitus as a side effect.\nTinnitus can also occur from the discontinuation of therapeutic doses of benzodiazepines. It can sometimes be a protracted symptom of benzodiazepine withdrawal and may persist for many months. Medications such as bupropion may also cause tinnitus.\n\nAssociated factors\nFactors associated with tinnitus include:\n\nEar problems and hearing loss:\nConductive hearing loss\nAcoustic shock\nLoud noise or music\nMiddle ear effusion\nOtitis\nOtosclerosis\nEustachian tube dysfunction\nSensorineural hearing loss\nExcessive or loud noise; e.g. acoustic trauma\nPresbycusis (age-associated hearing loss)\nM\u00e9ni\u00e8re's disease\nEndolymphatic hydrops\nSuperior canal dehiscence\nAcoustic neuroma\nMercury or lead poisoning\nOtotoxic medications\nNeurologic disorders:\nChiari malformation\nMultiple sclerosis\nHead injury\nGiant cell arteritis\nTemporomandibular joint dysfunction\nMetabolic disorders:\nVitamin B12 deficiency\nIron deficiency anemia\nPsychiatric disorders\nDepression\nAnxiety disorders\nOther factors:\nVasculitis\nSome psychedelic drugs can produce temporary tinnitus-like symptoms as a side effect:\n5-MeO-DET\nDiisopropyltryptamine (DiPT)\nBenzodiazepine withdrawal\nIntracranial hyper or hypotension caused by, for example, encephalitis or a cerebrospinal fluid leak\n\nObjective tinnitus\nA specific type of tinnitus, objective tinnitus, is characterized by hearing the sounds of one's own muscle contractions or pulse, typically a result of sounds that have been created by the movement of jaw muscles or sounds related to blood flow in the neck or face. It is sometimes caused by an involuntary twitching of a muscle or a group of muscles (myoclonus) or by a vascular condition. In some cases, tinnitus is generated by muscle spasms around the middle ear.\nSpontaneous otoacoustic emissions (SOAEs)\u2014faint high-frequency tones that are produced in the inner ear and can be measured in the ear canal with a sensitive microphone\u2014may also cause tinnitus. About 8% of those with SOAEs and tinnitus have SOAE-linked tinnitus, while the percentage of all cases of tinnitus caused by SOAEs is estimated at 4%.\n\nPediatric tinnitus\nChildren may be subject to pulsatile or continuous tinnitus, involving anomalies and variants of the vascular parts affecting the middle\/inner ear structures. CT scans may be used to check the integrity of the structures, and MR scans can evaluate nerves and potential masses or malformations. Early diagnosis can prevent long-term impairments to development.\n\nPulsatile tinnitus\nSome people experience a sound that beats in time with their pulse, known as pulsatile tinnitus or vascular tinnitus. Pulsatile tinnitus is usually objective in nature, resulting from altered blood flow or increased blood turbulence near the ear, such as from atherosclerosis or venous hum, but it can also arise as a subjective phenomenon from an increased awareness of blood flow in the ear.\nThe differential diagnosis for pulsatile tinnitus is wide and includes vascular etiologies, tumors, disorders of the middle ear or inner ear, and other intracranial pathologies. Vascular causes of pulsatile tinnitus include venous causes (e.g., high riding or dehiscent jugular bulb, sigmoid sinus diverticulum), arterial causes (e.g., cervical atherosclerosis, potentially life-threatening conditions such as carotid artery aneurysm or carotid artery dissection), or dural arteriovenous fistula or arteriovenous malformations. Pulsatile tinnitus may also indicate vasculitis, or more specifically, giant cell arteritis. Pulsatile tinnitus may also be caused by tumors such as paragangliomas (e.g., glomus tympanicum, glomus jugulare) or hemangiomas (e.g., facial nerve or cavernous). Middle ear causes of pulsatile tinnitus include patulous eustachian tube, otosclerosis, or middle ear myoclonus (e.g., stapedial or tensor tympani myoclonus). The most common inner ear cause of pulsatile tinnitus is superior semicircular canal dehiscence. Pulsatile tinnitus may also indicate idiopathic intracranial hypertension. Pulsatile tinnitus can be a symptom of intracranial vascular abnormalities and should be evaluated for irregular noises of blood flow (bruits).\n\nPathophysiology\nTinnitus may be caused by increased neural activity in the auditory brainstem, where the brain processes sounds, causing some auditory nerve cells to become overexcited. The basis of this theory is that many with tinnitus also have hearing loss.\nThree reviews in 2016 emphasized the large range and possible combinations of pathologies involved in tinnitus, which result in a great variety of symptoms and specifically adapted therapies.\n\nDiagnosis\nThe diagnostic approach is based on a history of the condition and an examination of the head, neck, and neurological system. Typically an audiogram is done, and occasionally medical imaging or electronystagmography. Treatable conditions may include middle ear infection, acoustic neuroma,\nconcussion, and otosclerosis.\nEvaluation of tinnitus can include a hearing test (audiogram), measurement of acoustic parameters of the tinnitus like pitch and loudness, and psychological assessment of comorbid conditions like depression, anxiety, and stress that are associated with severity of the tinnitus.\nOne definition of tinnitus, in contrast to normal ear noise experience, is that tinnitus lasts five minutes at least twice a week. However, people with tinnitus often experience the noise more frequently than this. Tinnitus can be present constantly or intermittently. Some people with constant tinnitus might not be aware of it all the time, but only, for example, during the night when there is less environmental noise to mask it. Chronic tinnitus can be defined as tinnitus with a duration of six months or more.\n\nAudiology\nSince most people with tinnitus also have hearing loss, a pure tone hearing test resulting in an audiogram may help diagnose a cause. An audiogram may also facilitate fitting of a hearing aid in those cases where hearing loss is significant. The pitch of tinnitus is often in the range of the hearing loss.\n\nPsychoacoustics\nAcoustic qualification of tinnitus includes measurement of several acoustic parameters like frequency in cases of monotone tinnitus or frequency range and bandwidth in cases of narrow band noise tinnitus, loudness in dB above hearing threshold at the indicated frequency, mixing-point, and minimum masking level. In most cases, tinnitus pitch or frequency range is between 5 kHz and 10 kHz, and loudness between 5 and 15 dB above the hearing threshold.\nAnother relevant parameter of tinnitus is residual inhibition: the temporary suppression or disappearance of tinnitus following a period of masking. The degree of residual inhibition may indicate how effective tinnitus maskers would be as treatment.\nAn assessment of hyperacusis, a frequent accompaniment of tinnitus, may also be made. Hyperacusis is related to negative reactions to sound and can take many forms. One parameter that can be measured is Loudness Discomfort Level (LDL) in dB, the subjective level of acute discomfort at specified frequencies over the frequency range of hearing. This defines a dynamic range between the hearing threshold at that frequency and the loudness discomfort level. A compressed dynamic range over a particular frequency range can be associated with hyperacusis. Normal hearing threshold is generally defined as 0\u201320 decibels (dB). Normal loudness discomfort levels are 85\u201390+ dB, with some authorities citing 100 dB. A dynamic range of 55 dB or less is indicative of hyperacusis.\n\nSeverity\nTinnitus is often rated on a scale from \"slight\" to \"severe\" according to the effects it has, such as interference with sleep, quiet activities, and normal daily activities.\nAssessment of psychological processes related to tinnitus involves measurement of tinnitus severity and distress, as measured subjectively by validated self-report tinnitus questionnaires. Such questionnaires measure the degree of psychological distress and handicap associated with tinnitus, including effects on hearing, lifestyle, health, and emotional functioning. A broader assessment of general functioning, such as levels of anxiety, depression, stress, life stressors, and sleep difficulties, is also important in the assessment of tinnitus due to higher risk of negative well-being across these areas, which may be affected by or exacerbate the tinnitus symptoms. Current assessment measures aim to identify levels of distress and interference, coping responses, and perceptions of tinnitus to inform treatment and monitor progress. However, wide variability, inconsistencies, and lack of consensus regarding assessment methodology are evidenced in the literature, limiting comparison of treatment effectiveness. Developed to guide diagnosis or classify severity, most tinnitus questionnaires have been shown to be treatment-sensitive outcome measures.\n\nPulsatile tinnitus\nIf examination reveals a bruit (sound due to turbulent blood flow), imaging studies such as transcranial doppler (TCD) or magnetic resonance angiography (MRA) should be performed.\n\nDifferential diagnosis\nOther potential sources of the sounds normally associated with tinnitus should be ruled out. For instance, two recognized sources of high-pitched sounds might be electromagnetic fields common in modern wiring and various sound signal transmissions. A common and often misdiagnosed condition that mimics tinnitus is radio frequency (RF) hearing, in which subjects hear objectively audible high-pitched transmission frequencies that sound similar to tinnitus.\n\nPrevention\nProlonged exposure to loud sound or noise levels can lead to tinnitus. Custom made ear plugs or other measures can help with prevention. Employers may use hearing loss prevention programs to help educate and prevent dangerous levels of exposure to noise. Government organizations set regulations to ensure employees, if following the protocol, should have minimal risk to permanent damage to their hearing.\nCertain groups are advised to wear ear plugs to avoid the risk of tinnitus, such as that caused by overexposure to loud noises like wind noise for motorcycle riders. This includes military personnel, musicians, DJs, agricultural workers, and construction workers as people in those occupations are at a greater risk compared to the general population.\nSeveral medicines have ototoxic effects, which can have a cumulative effect that increases the damage done by noise. If ototoxic medications must be administered, close attention by the physician to prescription details, such as dose and dosage interval, can reduce the damage done.\n\nManagement\nIf a specific underlying cause is determined, treating it may lead to improvements. Otherwise, the primary treatment for tinnitus is talk therapy, sound therapy, or hearing aids. There are no effective drugs that treat tinnitus.\n\nPsychological\nThe best-supported treatment for tinnitus is cognitive behavioral therapy (CBT). It decreases the stress those with tinnitus feel. This appears to be independent of any effect on depression or anxiety. Acceptance and commitment therapy (ACT) also shows promise in the treatment of tinnitus. Relaxation techniques may also help. A clinical protocol called Progressive Tinnitus Management has been developed by the United States Department of Veterans Affairs.\n\nSound-based interventions\nThe application of sound therapy by either hearing aids or tinnitus maskers may help the brain ignore the specific tinnitus frequency. Although these methods are poorly supported by evidence, there are no negative effects. There are several approaches for tinnitus sound therapy. The first is sound modification to compensate for the individual's hearing loss. The second is a signal spectrum notching to eliminate energy close to the tinnitus frequency. There is some tentative evidence supporting tinnitus retraining therapy, which aims to reduce tinnitus-related neuronal activity. An alternative tinnitus treatment uses mobile applications that include various methods including masking, sound therapy, and relaxation exercises. Such applications can work as a separate device or as a hearing aid control system.\nNeuromonics is another sound-based intervention. Its protocol follows the principle of systematic desensitization and involves a structured rehabilitation program lasting 12 months. Neuromonics therapy employs customized sound signals delivered through a device worn by the patient, which aims to target the specific frequency range associated with their tinnitus perception.\n\nMedications\nAs of 2018 there were no medications effective for idiopathic tinnitus. There is not enough evidence to determine if antidepressants or acamprosate are useful. There are conflicting studies regarding the effectiveness of benzodiazepines for tinnitus. Usefulness of melatonin, as of 2015, is unclear. It is unclear if anticonvulsants are useful for treating tinnitus. Steroid injections into the middle ear also do not seem to be effective. There is no evidence to suggest that the use of betahistine to treat tinnitus is effective.\nBotulinum toxin injection has succeeded in some of the rare cases of objective tinnitus from a palatal tremor.\nCaroverine is used in a few countries to treat tinnitus. The evidence for its usefulness is very weak.\n\nNeuromodulation\nIn 2020, information about clinical trials indicated that bimodal neuromodulation may reduce the symptoms of tinnitus. It is a noninvasive technique that involves applying an electrical stimulus to the tongue while also administering sounds. Equipment associated with the treatments is available through physicians. Studies with it and similar devices continue in several research centers.\nIn March 2023, the US Food and Drug Administration (FDA) approved Neuromod's Lenire device as a treatment option for tinnitus. In June 2024, the US Department of Veterans Affairs (VA) announced it would begin offering the treatment to veterans with tinnitus, making it the first bimodal neuromodulation device to be awarded a Federal Supply Schedule (FSS) contract from the US Government.\nSome evidence supports neuromodulation techniques such as transcranial magnetic stimulation, transcranial direct current stimulation, and neurofeedback.\n\nAlternative medicine\nGinkgo biloba does not appear to be effective. The American Academy of Otolaryngology recommends against taking melatonin or zinc supplements to relieve symptoms of tinnitus, and reported that evidence for the efficacy of many dietary supplements (such as lipoflavonoids, garlic, traditional Chinese\/Korean herbal medicine, honeybee larvae, and various other vitamins and minerals, as well as homeopathic preparations) did not exist. A 2016 Cochrane Review also concluded that evidence was not sufficient to support taking zinc supplements to reduce symptoms associated with tinnitus.\n\nPrognosis\nWhile there is no cure, most people with tinnitus get used to it over time; for a minority, it remains a significant problem.\n\nEpidemiology\nAdults\nTinnitus affects 10\u201315% of people. About a third of North Americans over 55 experience it. It affects one third of adults at some time in their lives, whereas 10\u201315% are disturbed enough to seek medical evaluation.\n70 million people in Europe are estimated to have tinnitus.\n\nChildren\nTinnitus is commonly thought of as a symptom of adulthood, and is often overlooked in children. Children with hearing loss have a high incidence of pediatric tinnitus, even though they do not express the condition or its effect on their lives. Children do not generally report tinnitus spontaneously and their complaints may not be taken seriously. Among those who do complain, there is an increased likelihood of associated otological or neurological pathology such as migraine, juvenile Meniere's disease, or chronic suppurative otitis media. Its reported prevalence varies from 12 to 36% in children with normal hearing thresholds, and up to 66% in children with a hearing loss. Approximately 3\u201310% of children have been reported to be troubled by tinnitus.\n\nSee also\nAudiology\nAuditory hallucination \u2013 Form of hallucination that involves perceiving sounds without auditory stimulus\nHealth effects from noise \u2013 Health consequences of exposure to elevated sound levels\nHearing protection\nNoise-induced hearing loss\nList of people with tinnitus\nList of unexplained sounds \u2013 List of unidentified, or formerly unidentified, sounds\nSafe listening \u2013 Avoiding hearing damage from intentionally heard sounds\nPhantom vibration syndrome \u2013 False belief of one's mobile phone vibrating or ringing\nZwicker tone \u2013 Short-term auditory illusion\n\nReferences\nFurther reading\nBaguley D, Andersson G, McFerran D, McKenna L (2013) [2004]. Tinnitus: A Multidisciplinary Approach (2nd ed.). Indianapolis, IN: Wiley-Blackwell. ISBN 978-1-4051-9989-6. LCCN 2012032714. OCLC 712915603.\nLangguth B, Hajak G, Kleinjung T, Cacace A, M\u00f8ller AR, eds. (2007). Tinnitus: pathophysiology and treatment. Progress in brain research no. 166 (1st ed.). Amsterdam; Boston: Elsevier. ISBN 978-0-444-53167-4. LCCN 2012471552. OCLC 648331153. Retrieved 5 November 2012. Alt URL\nM\u00f8ller AR, Langguth B, Ridder D, et al., eds. (2011). Textbook of Tinnitus. New York: Springer. doi:10.1007\/978-1-60761-145-5. ISBN 978-1-60761-144-8. LCCN 2010934377. OCLC 695388693, 771366370, 724696022.\n\nExternal links\nAmerican Tinnitus Association\nTinnitus.org UK","181":"Topiramate, sold under the brand name Topamax among others, is a medication used to treat epilepsy and prevent migraines. It has also been used in alcohol dependence and essential tremor. For epilepsy this includes treatment for generalized or focal seizures. It is taken orally (by mouth).\nCommon side effects include tingling, feeling tired, loss of appetite, abdominal pain, weight loss, and decreased cognitive function such as trouble concentrating. Serious side effects may include suicide, increased ammonia levels resulting in encephalopathy, and kidney stones. Topiramate can cause birth defects including cleft lip and palate. Risk\/benefit should be carefully discussed with the full treatment team. Topiramate is considered \"probably compatible\" with lactation and is not contraindicated in breastfeeding, though monitoring of the infant for diarrhea or poor weight gain may be considered. The mechanism of action is unclear.\nTopiramate was approved for medical use in the United States in 1996. It is available as a generic medication. In 2021, it was the 66th most commonly prescribed medication in the United States, with more than 10 million prescriptions.\n\nMedical uses\nTopiramate is used to treat epilepsy in children and adults, and it was originally used as an anticonvulsant. In children, it is indicated for the treatment of Lennox-Gastaut syndrome, a disorder that causes seizures and developmental delay. It is most frequently prescribed for the prevention of migraines as it decreases the frequency of attacks. Topiramate is used to treat medication overuse headache and is recommended by the European Federation of Neurological Societies as one of the few medications showing effectiveness for this indication.\n\nPain\nA 2018 review found topiramate of no use in chronic low back pain. Topiramate has not been shown to work as a pain medicine in diabetic neuropathy, the only neuropathic condition in which it has been adequately tested.\n\nOther\nOne common off-label use for topiramate is in the treatment of bipolar disorder. A review published in 2010 suggested a benefit of topiramate in the treatment of symptoms of borderline personality disorder, however the authors noted that this was based only on one randomized controlled trial and requires replication.\nTopiramate has been used as a treatment for alcoholism. The U.S. Veterans Affairs and Department of Defense 2015 guidelines on substance use disorders list topiramate as a \"strong for\" in its recommendations for alcohol use disorder.\nOther uses include treatment of obesity, binge eating disorder, and off-setting weight gain induced by taking antipsychotic medications. In 2012, the combination of phentermine\/topiramate was approved in the United States for weight loss.\n\nAdverse effects\nPeople taking topiramate should be aware of the following risks: \n\nAvoid activities requiring mental alertness and coordination until drug effects are realized.\nTopiramate may impair heat regulation, especially in children. Use caution with activities leading to an increased core temperature, such as strenuous exercise, exposure to extreme heat, or dehydration.\nTopiramate may cause visual field defects.\nTopiramate may decrease effectiveness of oestrogen-containing oral contraceptives.\nTaking topiramate in the first trimester of pregnancy may increase risk of cleft lip\/cleft palate in infant.\nAs is the case for all antiepileptic drugs, it is advisable not to suddenly discontinue topiramate as there is a theoretical risk of rebound seizures.\nSome studies have attributed loss of appetite and upper respiratory tract infection to topiramate, but studies have concluded their adverse events are not difficult to tolerate for most individuals.\n\nFrequency\nAdverse effects by incidence:\nVery common (>10% incidence) adverse effects include:\n\nRarely, the inhibition of carbonic anhydrase may be strong enough to cause metabolic acidosis of clinical importance.\nThe U.S. Food and Drug Administration (FDA) has notified prescribers that topiramate can cause acute myopia and secondary angle closure glaucoma in a small subset of people who take topiramate regularly. The symptoms, which typically begin in the first month of use, include blurred vision and eye pain. Discontinuation of topiramate may halt the progression of the ocular damage and may reverse the visual impairment.\nPreliminary data suggests that, as with several other anti-epileptic drugs, topiramate carries an increased risk of congenital malformations. This might be particularly important for women who take topiramate to prevent migraine attacks. In March 2011, the FDA notified healthcare professionals and patients of an increased risk of development of cleft lip and\/or cleft palate (oral clefts) in infants born to women treated with Topamax (topiramate) during pregnancy and placed it in Pregnancy Category D.\nCognitive and word-finding difficulties, as they may occur in some patients, may respond to piracetam.\nCarbonation dysgeusia (distortion of the sense of taste-sensation of carbonation) may respond to and\/or be prevented to with zinc.\nTopiramate has been associated with a statistically significant increase in suicidality, and \"suicidal thoughts or actions\" is now listed as one of the possible side effects of the drug \"in a very small number of people, about 1 in 500.\"\n\nOverdose\nSymptoms of acute and acute on chronic exposure to topiramate range from asymptomatic to status epilepticus, including in patients with no seizure history. In children, overdose may also result in hallucinations. Topiramate has been deemed the primary substance that led to fatal overdoses in cases that were complicated by polydrug exposure. The most common signs of overdose are dilated pupils, somnolence, dizziness, psychomotor agitation, and abnormal, uncoordinated body movements.\n\nInteractions\nTopiramate has many drug-drug interactions. Some of the most common are listed below:\n\nAs topiramate inhibits carbonic anhydrase, use with other inhibitors of carbonic anhydrase (e.g. acetazolamide) increases the risk of kidney stones.\nEnzyme inducers (e.g. carbamazepine) can increase the elimination of topiramate, possibly necessitating dose escalations of topiramate.\nTopiramate may increase the plasma-levels of phenytoin.\nTopiramate itself is a weak inhibitor of CYP2C19 and induces CYP3A4; a decrease in plasma levels of estrogens and digoxin has been noted during topiramate therapy. This can reduce the effectiveness of oral contraceptives (birth control  pills); use of alternative birth control methods is recommended. Neither intrauterine devices (IUDs) nor Depo-Provera are affected by topiramate.\nAlcohol may cause increased sedation or drowsiness, and increase the risk of having a seizure.\nAs topiramate may result in acidosis other treatments that also do so may worsen this effect.\nOligohidrosis and hyperthermia were reported in post-marketing reports about topiramate; antimuscarinic drugs (like trospium) can aggravate these disorders.\n\nPharmacology\nThe topiramate molecule is a sulfamate modified sugar, more specifically, fructose diacetonide, an unusual chemical structure for a pharmaceutical.\nTopiramate is quickly absorbed after oral use. It has a half life of 21 hours and a steady state of the drug is reached in 4 days in patients with normal renal function. Most of the drug (70%) is excreted in the urine unchanged. The remainder is extensively metabolized by hydroxylation, hydrolysis, and glucuronidation. Six metabolites have been identified in humans, none of which constitutes more than 5% of an administered dose.\nSeveral cellular targets have been proposed to be relevant to the therapeutic activity of topiramate. These include (1) voltage-gated sodium channels; (2) high-voltage-activated calcium channels; (3) GABA-A receptors; (4) AMPA\/kainate receptors; and (5) carbonic anhydrase isoenzymes. There is evidence that topiramate may alter the activity of its targets by modifying their phosphorylation state instead of by a direct action. The effect on sodium channels could be of particular relevance for seizure protection. Although topiramate does inhibit high-voltage-activated calcium channels, the relevance to clinical activity is uncertain. Effects on specific GABA-A receptor isoforms could also contribute to the antiseizure activity of the drug. Topiramate selectively inhibits cytosolic (type II) and membrane associated (type IV) forms of carbonic anhydrase. The action on carbonic anhydrase isoenzymes may contribute to the drug's side-effects, including its propensity to cause metabolic acidosis and calcium phosphate kidney stones.\nTopiramate inhibits maximal seizure activity in electroconvulsive therapy and in pentylenetetrazol-induced seizures as well as partial and secondarily generalized tonic-clonic seizures in the kindling model, findings predictive of a broad spectrum of activities clinically. Its action on mitochondrial permeability transition pores has been proposed as a mechanism.\nWhile many anticonvulsants have been associated with apoptosis in young animals, animal experiments have found that topiramate is one of the very few anticonvulsants [see: levetiracetam, carbamazepine, lamotrigine] that do not induce apoptosis in young animals at doses needed to produce an anticonvulsant effect.\n\nDetection in body fluids\nBlood, serum, or plasma topiramate concentrations may be measured using immunoassay or chromatographic methods to monitor therapy, confirm a diagnosis of poisoning in hospitalized patients, or to assist in a medicolegal death investigation. Plasma levels are usually less than 10 mg\/L during therapeutic administration, but can range from 10 to 150 mg\/L in overdose victims.\n\nHistory\nTopiramate was discovered in 1979 by Bruce E. Maryanoff and Joseph F. Gardocki during their research work at McNeil Pharmaceuticals. Topiramate was first sold\nin 1996. Mylan Pharmaceuticals was granted final approval by the FDA for the sale of generic topiramate in the United States and the generic version was made available in September 2006. The last patent for topiramate in the U.S. was for use in children and expired on 28 February 2009.\n\nResearch\nTopiramate is being studied as a potential treatment for post traumatic stress disorder.\nThere is some evidence for the use of topiramate in the management of cravings related to withdrawal from dextromethorphan.\nA 2023 systematic review of seizure treatment for infants aged 1 to 36 months identified three studies that evaluated the use of topiramate. Though its adverse effects including upper respiratory tract infection and loss of appetite were rarely severe enough for the medication to be discontinued in this age group, its effectiveness in reducing seizures was inconclusive. The available research suffers from small sample sizes, inconsistent findings, and inadequate comparison groups.\n\nReferences\nExternal links\n Media related to Topiramate at Wikimedia Commons","182":"A transient ischemic attack (TIA), commonly known as a mini-stroke, is a minor stroke whose noticeable symptoms usually end in less than an hour. A TIA causes the same symptoms associated with a stroke, such as weakness or numbness on one side of the body, sudden dimming or loss of vision, difficulty speaking or understanding language, slurred speech, or confusion.\nAll forms of stroke, including a TIA, result from a disruption in blood flow to the central nervous system.  A TIA is caused by a temporary disruption in blood flow to the brain, or cerebral blood flow (CBF).  The primary difference between a major stroke and the TIA's minor stroke is how much tissue death (infarction) can be detected afterwards through medical imaging. While a TIA must by definition be associated with symptoms, strokes can also be asymptomatic or silent. In a silent stroke, also known as a silent cerebral infarct (SCI), there is permanent infarction detectable on imaging, but there are no immediately observable symptoms. The same person can have major strokes, minor strokes, and silent strokes, in any order.\nThe occurrence of a TIA is a risk factor for having a major stroke, and many people with TIA have a major stroke within 48 hours of the TIA. All forms of stroke are associated with increased risk of death or disability. Recognition that a TIA has occurred is an opportunity to start treatment, including medications and lifestyle changes, to prevent future strokes.\n\nSigns and symptoms\nSigns and symptoms of TIA are widely variable and can mimic other neurologic conditions, making the clinical context and physical exam crucial in ruling in or out the diagnosis. The most common presenting symptoms of TIA are focal neurologic deficits, which can include, but are not limited to:\n\nAmaurosis fugax (painless, temporary loss of vision)\nOne-sided facial droop\nOne-sided motor weakness\nDiplopia (double vision)\nProblems with balance and spatial orientation or dizziness\nVisual field deficits, such as homonymous hemianopsia or monocular blindness\nSensory deficits in one or more limbs and of the face\nLoss of ability to understand or express speech (aphasia)\nDifficulty with articulation of speech (dysarthria)\nUnsteady gait\nDifficulties with swallowing (dysphagia)\nNumbness or weakness generally occur on the opposite side of the body from the affected hemisphere of the brain.\nA detailed neurologic exam, including a thorough cranial nerve exam, is important to identify these findings and to differentiate them from mimickers of TIA. Symptoms such as unilateral weakness, amaurosis fugax, and double vision have higher odds of representing TIA compared to memory loss, headache, and blurred vision. Below is a table of symptoms at presentation, and what percentage of the time they are seen in TIAs versus conditions that mimic TIA. In general, focal deficits make TIA more likely, but the absence of focal findings do not exclude the diagnosis and further evaluation may be warranted if clinical suspicion for TIA is high (see \"Diagnosis\" section below).\n\nTIA vis-\u00e0-vis mimics\nNon-focal symptoms such as amnesia, confusion, incoordination of limbs, unusual cortical visual symptoms (such as isolated bilateral blindness or bilateral positive visual phenomena), headaches and transient loss of consciousness are usually not associated with TIA, however patient assessment is still needed. Public awareness on the need to seek a medical assessment for these non-focal symptoms is also low, and can result in a delay by patients to seek treatment\nSymptoms of TIAs can last on the order of minutes to one\u2013two hours, but occasionally may last for a longer period of time. TIAs used to be defined as ischemic events in the brain that last less than 24 hours, but given the variation in duration of symptoms, this definition holds less significance. A pooled study of 808 patients with TIAs from 10 hospitals showed that 60% lasted less than one hour, 71% lasted less than two hours, and 14% lasted greater than six hours. Importantly, patients with symptoms that last more than one hour are more likely to have permanent neurologic damage, making prompt diagnosis and treatment important to maximize recovery.\n\nCause\nThe most common underlying pathology leading to TIA and stroke is a cardiac condition called atrial fibrillation, where poor coordination of contraction may lead to a formation of a clot in the atrial chamber that can become dislodged and travel to a cerebral artery. Unlike in stroke, the blood flow can become restored prior to infarction which leads to the resolution of neurologic symptoms. Another common culprit of TIA is an atherosclerotic plaque located in the common carotid artery, typically by the bifurcation between the internal and external carotids, that becomes an embolism to the brain vasculature similar to the clot in the prior example. A portion of the plaque can become dislodged and lead to embolic pathology in the cerebral vessels.\nIn-situ thrombosis, an obstruction that forms directly in the cerebral vasculature unlike the remote embolism previously mentioned, is another vascular occurrence with possible presentation as TIA. Also, carotid stenosis secondary to atherosclerosis narrowing the diameter of the lumen and thus limiting blood flow is another common cause of TIA. Individuals with carotid stenosis may present with TIA symptoms, thus labeled symptomatic, while others may not experience symptoms and be asymptomatic.\n\nRisk factors\nRisk factors associated with TIA are categorized as modifiable or non-modifiable. Non-modifiable risk factors include age greater than 55, sex, family history, genetics, and race\/ethnicity. Modifiable risk factors include cigarette smoking, hypertension (elevated blood pressure), diabetes, hyperlipidemia, level of carotid artery stenosis (asymptomatic or symptomatic) and activity level. The modifiable risk factors are commonly targeted in treatment options to attempt to minimize risk of TIA and stroke.\n\nPathogenesis\nThere are three major mechanisms of ischemia in the brain: embolism traveling to the brain, in situ thrombotic occlusion in the intracranial vessels supplying the parenchyma of the brain, and stenosis of vessels leading to poor perfusion secondary to flow-limiting diameter. Globally, the vessel most commonly affected is the middle cerebral artery. Embolisms can originate from multiple parts of the body.\nCommon mechanisms of stroke and TIA:\n\nDiagnosis\nThe initial clinical evaluation of a suspected TIA involves obtaining a history and physical exam (including a neurological exam). History taking includes defining the symptoms and looking for mimicking symptoms as described above. Bystanders can be very helpful in describing the symptoms and giving details about when they started and how long they lasted. The time course (onset, duration, and resolution), precipitating events, and risk factors are particularly important.\nThe definition, and therefore the diagnosis, has changed over time.  TIA was classically based on duration of neurological symptoms. The current widely accepted definition is called \"tissue-based\" because it is based on imaging, not time. The American Heart Association and the American Stroke Association (AHA\/ASA) now define TIA as a brief episode of neurological dysfunction with a vascular cause, with clinical symptoms typically lasting less than one hour, and without evidence of significant infarction on imaging.\n\nLaboratory workup\nLaboratory tests should focus on ruling out metabolic conditions that may mimic TIA (e.g. hypoglycemia causing altered mental status), in addition to further evaluating a patient's risk factors for ischemic events. All patients should receive a complete blood count with platelet count, blood glucose, basic metabolic panel, prothrombin time\/international normalized ratio, and activated partial thromboplastin time as part of their initial workup. These tests help with screening for bleeding or hypercoagulable conditions. Other lab tests, such as a full hypercoagulable state workup or serum drug screening, should be considered based on the clinical situation and factors, such as age of the patient and family history. A fasting lipid panel is also appropriate to thoroughly evaluate the patient's risk for atherosclerotic disease and ischemic events in the future. Other lab tests may be indicated based on the history and presentation; such as obtaining inflammatory markers (erythrocyte sedimentation rate and C-reactive protein) to evaluate for giant cell arteritis (which can mimic a TIA) in those presenting with headaches and monocular blindness.\n\nCardiac rhythm monitoring\nAn electrocardiogram is necessary to rule out abnormal heart rhythms, such as atrial fibrillation, that can predispose patients to clot formation and embolic events. Hospitalized patients should be placed on heart rhythm telemetry, which is a continuous form of monitoring that can detect abnormal heart rhythms. Prolonged heart rhythm monitoring (such as with a Holter monitor or implantable heart monitoring) can be considered to rule out arrhythmias like paroxysmal atrial fibrillation that may lead to clot formation and TIAs, however this should be considered if other causes of TIA have not been found.\n\nImaging\nAccording to guidelines from the American Heart Association and American Stroke Association Stroke Council, patients with TIA should have head imaging \"within 24 hours of symptom onset, preferably with magnetic resonance imaging, including diffusion sequences\". MRI is a better imaging modality for TIA than computed tomography (CT), as it is better able to pick up both new and old ischemic lesions than CT. CT, however, is more widely available and can be used particularly to rule out intracranial hemorrhage. Diffusion sequences can help further localize the area of ischemia and can serve as prognostic indicators. Presence of ischemic lesions on diffusion weighted imaging has been correlated with a higher risk of stroke after a TIA.\nVessels in the head and neck may also be evaluated to look for atherosclerotic lesions that may benefit from interventions, such as carotid endarterectomy. The vasculature can be evaluated through the following imaging modalities: magnetic resonance angiography (MRA), CT angiography (CTA), and carotid ultrasonography\/transcranial doppler ultrasonography. Carotid ultrasonography is often used to screen for carotid artery stenosis, as it is more readily available, is noninvasive, and does not expose the person being evaluated to radiation. However, all of the above imaging methods have variable sensitivities and specificities, making it important to supplement one of the imaging methods with another to help confirm the diagnosis (for example: screen for the disease with ultrasonography, and confirm with CTA). Confirming a diagnosis of carotid artery stenosis is important because the treatment for this condition, carotid endarterectomy, can pose significant risk to the patient, including heart attacks and strokes after the procedure. For this reason, the U.S. Preventive Services Task Force (USPSTF) \"recommends against screening for asymptomatic carotid artery stenosis in the general adult population\". This recommendation is for asymptomatic patients, so it does not necessarily apply to patients with TIAs as these may in fact be a symptom of underlying carotid artery disease (see \"Causes and Pathogenesis\" above). Therefore, patients who have had a TIA may opt to have a discussion with their clinician about the risks and benefits of screening for carotid artery stenosis, including the risks of surgical treatment of this condition.\nCardiac imaging can be performed if head and neck imaging do not reveal a vascular cause for the patient's TIA (such as atherosclerosis of the carotid artery or other major vessels of the head and neck). Echocardiography can be performed to identify patent foramen ovale (PFO), valvular stenosis, and atherosclerosis of the aortic arch that could be sources of clots causing TIAs, with transesophageal echocardiography being more sensitive than transthoracic echocardiography in identifying these lesions.\n\nDifferential diagnosis\nPrevention\nAlthough there is a lack of robust studies demonstrating the efficacy of lifestyle changes in preventing TIA, many medical professionals recommend them. These include:\n\nAvoiding smoking\nCutting down on fats to help reduce the amount of plaque buildup\nEating a healthy diet including plenty of fruits and vegetables\nLimiting sodium in the diet, thereby reducing blood pressure\nExercising regularly\nModerating intake of alcohol, stimulants, sympathomimetics, etc.\nMaintaining a healthy weight\nIn addition, it is important to control any underlying medical conditions that may increase the risk of stroke or TIA, including:\n\nHypertension\nHigh cholesterol\nDiabetes mellitus\nAtrial fibrillation\n\nTreatment\nBy definition, TIAs are transient, self-resolving, and do not cause permanent impairment. However, they are associated with an increased risk of subsequent ischemic strokes, which can be permanently disabling. Therefore, management centers on the prevention of future ischemic strokes and addressing any modifiable risk factors. The optimal regimen depends on the underlying cause of the TIA.\n\nLifestyle modification\nLifestyle changes have not been shown to reduce the risk of stroke after TIA. While no studies have looked at the optimal diet for secondary prevention of stroke, some observational studies have shown that a Mediterranean diet can reduce stroke risk in patients without cerebrovascular disease. A Mediterranean diet is rich in fruits, vegetables and whole grains, and limited in red meats and sweets. Vitamin supplementation has not been found to be useful in secondary stroke prevention.\n\nAntiplatelet medications\nThe antiplatelet medications, aspirin and clopidogrel, are both recommended for secondary prevention of stroke after high-risk TIAs. The clopidogrel can generally be stopped after 10 to 21 days. An exception is TIAs due to blood clots originating from the heart, in which case anticoagulants are generally recommended. After TIA or minor stroke, aspirin therapy has been shown to reduce the short-term risk of recurrent stroke by 60\u201370%, and the long-term risk of stroke by 13%.\nThe typical therapy may include aspirin alone, a combination of aspirin plus extended-release dipyridamole, or clopidogrel alone. Clopidogrel and aspirin have similar efficacies and side effect profiles. Clopidogrel is more expensive and has a slightly decreased risk of GI bleed. Another antiplatelet, ticlopidine, is rarely used due to increased side effects.\n\nAnticoagulant medications\nAnticoagulants may be started if the TIA is thought to be attributable to atrial fibrillation. Atrial fibrillation is an abnormal heart rhythm that may cause the formation of blood clots that can travel to the brain, resulting in TIAs or ischemic strokes. Atrial fibrillation increases stroke risk by five times, and is thought to cause 10-12% of all ischemic strokes in the US. Anticoagulant therapy can decrease the relative risk of ischemic stroke in those with atrial fibrillation by 67% Warfarin is a common anticoagulant used, but direct acting oral anticoagulants (DOACs), such as apixaban, have been shown to be equally effective while also conferring a lower risk of bleeding. Generally, anticoagulants and antiplatelets are not used in combination, as they result in increased bleeding risk without a decrease in stroke risk. However, combined antiplatelet and anticoagulant therapy may be warranted if the patient has symptomatic coronary artery disease in addition to atrial fibrillation.\nSometimes, myocardial infarction (\"heart attack\") may lead to the formation of a blood clot in one of the chambers of the heart. If this is thought to be the cause of the TIA, people may be temporarily treated with warfarin or other anticoagulant to decrease the risk of future stroke.\n\nBlood pressure control\nBlood pressure control may be indicated after TIA to reduce the risk of ischemic stroke. About 70% of patients with recent ischemic stroke are found to have hypertension, defined as systolic blood pressure (SBP) > 140 mmHg, or diastolic blood pressure (DBP) > 90 mmHg. Until the first half of the 2010s, blood pressure goals have generally been SBP < 140 mmHg and DBP < 90 mmHg. However, newer studies suggest that a goal of SBP <130 mmHg may confer even greater benefit. Blood pressure control is often achieved using diuretics or a combination of diuretics and angiotensin converter enzyme inhibitors, although the optimal treatment regimen depends on the individual.\nStudies that evaluated the application of Blood pressure\u2010lowering drugs in people who had a TIA or stroke, concluded that this type of medication helps to reduce the possibility of a recurrent stroke, of a major vascular event and dementia. The effects achieved in stroke recurrence were mainly obtained through the ingestion of angiotensin-converting enzyme (ACE) inhibitor or a diuretic.\n\nCholesterol control\nThere is inconsistent evidence regarding the effect of LDL-cholesterol levels on stroke risk after TIA. Elevated cholesterol may increase ischemic stroke risk while decreasing the risk of hemorrhagic stroke. While its role in stroke prevention is currently unclear, statin therapy has been shown to reduce all-cause mortality and may be recommended after TIA.\n\nDiabetes control\nDiabetes mellitus increases the risk of ischemic stroke by 1.5\u20133.7 times, and may account for at least 8% of first ischemic strokes. While intensive glucose control can prevent certain complications of diabetes such as kidney damage and retinal damage, there has previously been little evidence that it decreases the risk of stroke or death. However, data from 2017 suggests that metformin, pioglitazone and semaglutide may reduce stroke risk.\n\nSurgery\nIf the TIA affects an area that is supplied by the carotid arteries, a carotid ultrasound scan may demonstrate stenosis, or narrowing, of the carotid artery. For people with extra-cranial carotid stenosis, if 70-99% of the carotid artery is clogged, carotid endarterectomy can decrease the five-year risk of ischemic stroke by approximately half. For those with extra-cranial stenosis between 50 and 69%, carotid endarterectomy decreases the 5-year risk of ischemic stroke by about 16%. For those with extra-cranial stenosis less than 50%, carotid endarterectomy does not reduce stroke risk and may, in some cases, increase it. The effectiveness of carotid endarterectomy or carotid artery stenting in reducing stroke risk in people with intra-cranial carotid artery stenosis is currently unknown.\nIn carotid endarterectomy, a surgeon makes an incision in the neck, opens up the carotid artery, and removes the plaque occluding the blood vessel. The artery may then be repaired by adding a graft from another vessel in the body, or a woven patch. In patients who undergo carotid endarterectomy after a TIA or minor stroke, the 30-day risk of death or stroke is 7%.\nCarotid artery stenting is a less invasive alternative to carotid endarterectomy for people with extra-cranial carotid artery stenosis. In this procedure, the surgeon makes a small cut in the groin and threads a small flexible tube, called a catheter, into the patient's carotid artery. A balloon is inflated at the site of stenosis, opening up the clogged artery to allow for increased blood flow to the brain. To keep the vessel open, a small wire mesh coil, called a stent, may be inflated along with the balloon. The stent remains in place, and the balloon is removed.\nFor people with symptomatic carotid stenosis, carotid endarterectomy is associated with fewer perioperative deaths or strokes than carotid artery stenting. Following the procedure, there is no difference in effectiveness if you compare carotid endarterectomy and carotid stenting procedures, however, endarterectomy is often the procedure of choice as it is a safer procedure and is often effective in the longer term for preventing recurrent stroke. For people with asymptomatic carotid stenosis, the increased risk of stroke or death during the stenting procedure compared to an endarterectomy is less certain.\nPeople who undergo carotid endarterectomy or carotid artery stenting for stroke prevention are medically managed with antiplatelets, statins, and other interventions as well.\n\nPrognosis\nWithout treatment, the risk of an ischemic stroke in the three months after a TIA is about 20% with the greatest risk occurring within two days of the TIA. Other sources cite that 10% of TIAs will develop into a stroke within 90 days, half of which will occur in the first two days following the TIA. Treatment and preventative measures after a TIA (for example treating elevated blood pressure) can reduce the subsequent risk of an ischemic stroke by about 80%. The risk of a stroke occurring after a TIA can be predicted using the ABCD\u00b2 score. One limitation of the ABCD\u00b2 score is that it does not reliably predict the level of carotid artery stenosis, which is a major cause of stroke in TIA patients. The patient's age is the most reliable risk factor in predicting any level of carotid stenosis in transient ischemic attack. The ABCD2 score is no longer recommended for triage (to decide between outpatient management versus hospital admission) of those with a suspected TIA due to these limitations.\n\nEpidemiology\nWith the difficulty in diagnosing a TIA due to its nonspecific symptoms of neurologic dysfunction at presentation and a differential including many mimics, the exact incidence of the disease is unclear. It is currently estimated to have an incidence of approximately 200,000 to 500,000 cases per year in the US according to the American Heart Association. TIA incidence trends similarly to stroke, such that incidence varies with age, gender, and different race\/ethnicity populations. Associated risk factors include age greater than or equal to 60, blood pressure greater than or equal to 140 systolic or 90 diastolic, and comorbid diseases, such as diabetes, hypertension, atherosclerosis, and atrial fibrillation. It is thought that approximately 15 to 30 percent of strokes have a preceding TIA episode associated.\n\n\n== References ==","183":"In neuroanatomy, the trigeminal nerve (lit. triplet nerve), also known as the fifth cranial nerve, cranial nerve V, or simply CN V, is a cranial nerve responsible for sensation in the face and motor functions such as biting and chewing; it is the most complex of the cranial nerves. Its name (trigeminal, from Latin  tri- 'three' and  -geminus 'twin') derives from each of the two nerves (one on each side of the pons) having three major branches: the ophthalmic nerve (V1), the maxillary nerve (V2), and the mandibular nerve (V3). The ophthalmic and maxillary nerves are purely sensory, whereas the mandibular nerve supplies motor as well as sensory (or \"cutaneous\") functions. Adding to the complexity of this nerve is that autonomic nerve fibers as well as special sensory fibers (taste) are contained within it.\nThe motor division of the trigeminal nerve derives from the basal plate of the embryonic pons, and the sensory division originates in the cranial neural crest. Sensory information from the face and body is processed by parallel pathways in the central nervous system.\n\nStructure\nOrigin\nFrom the trigeminal ganglion, a single, large sensory root  enters the brainstem at the level of the pons. Immediately adjacent to the sensory root, a smaller motor root emerges from the pons slightly rostrally and medially to the sensory root.\nMotor fibers pass through the trigeminal ganglion without synapsing on their way to peripheral muscles, their cell bodies being located in the nucleus of the fifth nerve, deep within the pons.\n\nTrigeminal ganglion\nThe three major branches of the trigeminal nerve\u2014the ophthalmic nerve (V1), the maxillary nerve (V2) and the mandibular nerve (V3)\u2014converge on the trigeminal ganglion (also called the semilunar ganglion or gasserian ganglion), located within Meckel's cave and containing the cell bodies of incoming sensory-nerve fibers. The trigeminal ganglion is analogous to the dorsal root ganglia of the spinal cord, which contain the cell bodies of incoming sensory fibers from the rest of the body.\n\nSensory branches\nThe ophthalmic, maxillary and mandibular branches leave the skull through three separate foramina: the superior orbital fissure, the foramen rotundum and the foramen ovale, respectively. The ophthalmic nerve (V1) carries sensory information from the scalp and forehead, the upper eyelid, the conjunctiva and cornea of the eye, the nose (including the tip of the nose, except alae nasi), the nasal mucosa, the frontal sinuses and parts of the meninges (the dura and blood vessels). The maxillary nerve (V2) carries sensory information from the lower eyelid and cheek, the nares and upper lip, the upper teeth and gums, the nasal mucosa, the palate and roof of the pharynx, the maxillary, ethmoid and sphenoid sinuses and parts of the meninges. The mandibular nerve (V3) carries sensory information from the lower lip, the lower teeth and gums, the chin and jaw (except the angle of the jaw, which is supplied by C2-C3), parts of the external ear and parts of the meninges. The mandibular nerve carries touch-position and pain-temperature sensations from the mouth. Although it does not carry taste sensation (the chorda tympani is responsible for taste), one of its branches\u2014the lingual nerve\u2014carries sensation from the tongue.\nThe peripheral processes of mesencephalic nucleus of V neurons run in the motor root of the trigeminal nerve and terminate in the muscle spindles in the muscles of mastication. They are proprioceptive fibers, conveying information regarding the location of the masticatory muscles. The central processes of mesencephalic V neurons synapse in the motor nucleus V.\n\nDermatomes\nThe areas of cutaneous distribution (dermatomes) of the three sensory branches of the trigeminal nerve have sharp borders with relatively little overlap (unlike dermatomes in the rest of the body, which have considerable overlap). The injection of a local anesthetic, such as lidocaine, results in the complete loss of sensation from well-defined areas of the face and mouth. For example, teeth on one side of the jaw can be numbed by injecting the mandibular nerve. Occasionally, injury or disease processes may affect two (or all three) branches of the trigeminal nerve; in these cases, the involved branches may be termed:\n\nV1\/V2 distribution \u2013 Referring to the ophthalmic and maxillary branches\nV2\/V3 distribution \u2013 Referring to the maxillary and mandibular branches\nV1-V3 distribution \u2013 Referring to all three branches\nNerves on the left side of the jaw slightly outnumber the nerves on the right side of the jaw.\n\nFunction\nThe sensory function of the trigeminal nerve is to provide tactile, proprioceptive, and nociceptive afference to the face and mouth. Its motor function activates the muscles of mastication, the tensor tympani, tensor veli palatini, mylohyoid and the anterior belly of the digastric.\nThe trigeminal nerve carries general somatic afferent fibers (GSA), which innervate the skin of the face via ophthalmic (V1), maxillary (V2) and mandibular (V3) divisions. The trigeminal nerve also carries special visceral efferent (SVE) axons, which innervate the muscles of mastication via the mandibular (V3) division.\n\nMuscles\nThe motor component of the mandibular division (V3) of the trigeminal nerve controls the movement of eight muscles, including the four muscles of mastication: the masseter, the temporal muscle, and the medial and lateral pterygoids. The other four muscles are the tensor veli palatini, the mylohyoid, the anterior belly of the digastric and the tensor tympani. \nWith the exception of the tensor tympani, all these muscles are involved in biting, chewing and swallowing and all have bilateral cortical representation.  A unilateral central lesion (for example, a stroke), no matter how large, is unlikely to produce an observable deficit. Injury to a peripheral nerve can cause paralysis of muscles on one side of the jaw, with the jaw deviating towards the paralyzed side when it opens. This direction of the mandible is due to the action of the functioning pterygoids on the opposite side.\n\nSensation\nThe two basic types of sensation are touch-position and pain-temperature. Touch-position input comes to attention immediately, but pain-temperature input reaches the level of consciousness after a delay; when a person steps on a pin, the awareness of stepping on something is immediate but the pain associated with it is delayed.\nTouch-position information is generally carried by myelinated (fast-conducting) nerve fibers, and pain-temperature information by unmyelinated (slow-conducting) fibers. The primary sensory receptors for touch-position (Meissner's corpuscles, Merkel's receptors, Pacinian corpuscles, Ruffini's corpuscles, hair receptors, muscle spindle organs and Golgi tendon organs) are structurally more complex than those for pain-temperature, which are nerve endings.\nSensation in this context refers to the conscious perception of touch-position and pain-temperature information, rather than the special senses (smell, sight, taste, hearing and balance) processed by different cranial nerves and sent to the cerebral cortex through different pathways. The perception of magnetic fields, electrical fields, low-frequency vibrations and infrared radiation by some nonhuman vertebrates is processed by their equivalent of the fifth cranial nerve.\nTouch in this context refers to the perception of detailed, localized tactile information, such as two-point discrimination (the difference between touching one point and two closely spaced points) or the difference between coarse, medium or fine sandpaper. People without touch-position perception can feel the surface of their bodies and perceive touch in a broad sense, but they lack perceptual detail.\nPosition, in this context, refers to conscious proprioception. Proprioceptors (muscle spindle and Golgi tendon organs) provide information about joint position and muscle movement. Although much of this information is processed at an unconscious level (primarily by the cerebellum and the vestibular nuclei), some is available at a conscious level.\nTouch-position and pain-temperature sensations are processed by different pathways in the central nervous system. This hard-wired distinction is maintained up to the cerebral cortex. Within the cerebral cortex, sensations are linked with other cortical areas.\n\nSensory pathways\nSensory pathways from the periphery to the cortex are separate for touch-position and pain-temperature sensations. All sensory information is sent to specific nuclei in the thalamus. Thalamic nuclei, in turn, send information to specific areas in the cerebral cortex. Each pathway consists of three bundles of nerve fibers connected in series:\n\nThe secondary neurons in each pathway decussate (cross the spinal cord or brainstem), because the spinal cord develops in segments. Decussated fibers later reach and connect these segments with the higher centers. The optic chiasm is the primary cause of decussation; nasal fibers of the optic nerve cross (so each cerebral hemisphere receives contralateral\u2014opposite\u2014vision) to keep the interneuronal connections responsible for processing information short. All sensory and motor pathways converge and diverge to the contralateral hemisphere.\nAlthough sensory pathways are often depicted as chains of individual neurons connected in series, this is an oversimplification. Sensory information is processed and modified at each level in the chain by interneurons and input from other areas of the nervous system. For example, cells in the main trigeminal nucleus (Main V in the diagram below) receive input from the reticular formation and cerebellar cortex. This information contributes to the final output of the cells in Main V to the thalamus.\n\nTouch-position information from the body is carried to the thalamus by the medial lemniscus, and from the face by the trigeminal lemniscus (both the anterior and posterior trigeminothalamic tracts). Pain-temperature information from the body is carried to the thalamus by the spinothalamic tract, and from the face by the anterior division of the trigeminal lemniscus (also called the anterior trigeminothalamic tract).\nPathways for touch-position and pain-temperature sensations from the face and body merge in the brainstem, and touch-position and pain-temperature sensory maps of the entire body are projected onto the thalamus. From the thalamus, touch-position and pain-temperature information is projected onto the cerebral cortex.\n\nSummary\nThe complex processing of pain-temperature information in the thalamus and cerebral cortex (as opposed to the relatively simple, straightforward processing of touch-position information) reflects a phylogenetically older, more primitive sensory system. The detailed information received from peripheral touch-position receptors is superimposed on a background of awareness, memory and emotions partially set by peripheral pain-temperature receptors.\nAlthough thresholds for touch-position perception are relatively easy to measure, those for pain-temperature perception are difficult to define and measure. \"Touch\" is an objective sensation, but \"pain\" is an individualized sensation which varies among different people and is conditioned by memory and emotion. Anatomical differences between the pathways for touch-position perception and pain-temperature sensation help explain why pain, especially chronic pain, is difficult to manage.\n\nTrigeminal nuclei\nAll sensory information from the face, both touch-position and pain-temperature, is sent to the trigeminal nucleus. In classical anatomy most sensory information from the face is carried by the fifth nerve, but sensation from parts of the mouth, parts of the ear and parts of the meninges is carried by general somatic afferent fibers in cranial nerves VII (the facial nerve), IX (the glossopharyngeal nerve) and X (the vagus nerve).\nAll sensory fibers from these nerves terminate in the trigeminal nucleus. On entering the brainstem, sensory fibers from V, VII, IX and X are sorted and sent to the trigeminal nucleus (which contains a sensory map of the face and mouth). The spinal counterparts of the trigeminal nucleus (cells in the dorsal horn and dorsal column nuclei of the spinal cord) contain a sensory map of the rest of the body.\nThe trigeminal nucleus extends throughout the brainstem, from the midbrain to the medulla, continuing into the cervical cord (where it merges with the dorsal horn cells of the spinal cord). The nucleus is divided into three parts, visible in microscopic sections of the brainstem. From caudal to rostral (ascending from the medulla to the midbrain), they are the spinal trigeminal, the principal sensory and the mesencephalic nuclei. The parts of the trigeminal nucleus receive different types of sensory information; the spinal trigeminal nucleus receives pain-temperature fibers, the principal sensory nucleus receives touch-position fibers and the mesencephalic nucleus receives proprioceptor and mechanoreceptor fibers from the jaws and teeth.\n\nSpinal trigeminal nucleus\nThe spinal trigeminal nucleus represents pain-temperature sensation from the face. Pain-temperature fibers from peripheral nociceptors are carried in cranial nerves V, VII, IX and X. On entering the brainstem, sensory fibers are grouped and sent to the spinal trigeminal nucleus. This bundle of incoming fibers can be identified in cross-sections of the pons and medulla as the spinal tract of the trigeminal nucleus, which parallels the spinal trigeminal nucleus. The spinal tract of V is analogous to, and continuous with, Lissauer's tract in the spinal cord.\nThe spinal trigeminal nucleus contains a pain-temperature sensory map of the face and mouth. From the spinal trigeminal nucleus, secondary fibers cross the midline and ascend in the trigeminothalamic (quintothalamic) tract to the contralateral thalamus. Pain-temperature fibers are sent to multiple thalamic nuclei. The central processing of pain-temperature information differs from the processing of touch-position information.\n\nSomatotopic representation\nExactly how pain-temperature fibers from the face are distributed to the spinal trigeminal nucleus is disputed. The present general understanding is that pain-temperature information from all areas of the human body is represented in the spinal cord and brainstem in an ascending, caudal-to-rostral fashion. Information from the lower extremities is represented in the lumbar cord, and that from the upper extremities in the thoracic cord. Information from the neck and the back of the head is represented in the cervical cord, and that from the face and mouth in the spinal trigeminal nucleus.\nWithin the spinal trigeminal nucleus, information is represented in a layered, or \"onion-skin\" fashion. The lowest levels of the nucleus (in the upper cervical cord and lower medulla) represent peripheral areas of the face (the scalp, ears and chin). Higher levels (in the upper medulla) represent central areas (nose, cheeks and lips). The highest levels (in the pons) represent the mouth, teeth and pharyngeal cavity. \nThe onion skin distribution differs from the dermatome distribution of the peripheral branches of the fifth nerve. Lesions which destroy lower areas of the spinal trigeminal nucleus (but spare higher areas) preserve pain-temperature sensation in the nose (V1), upper lip (V2) and mouth (V3) and remove pain-temperature sensation from the forehead (V1), cheeks (V2) and chin (V3). Although analgesia in this distribution is \"nonphysiologic\" in the traditional sense (because it crosses several dermatomes), this analgesia is found in humans after surgical sectioning of the spinal tract of the trigeminal nucleus.\nThe spinal trigeminal nucleus sends pain-temperature information to the thalamus and sends information to the mesencephalon and the reticular formation of the brainstem. The latter pathways are analogous to the spinomesencephalic and spinoreticular tracts of the spinal cord, which send pain-temperature information from the rest of the body to the same areas. The mesencephalon modulates painful input before it reaches the level of consciousness. The reticular formation is responsible for the automatic (unconscious) orientation of the body to painful stimuli.  Incidentally, Sulfur-containing compounds found in plants in the onion family stimulate receptors found in trigeminal ganglia, bypassing the olfactory system.\n\nPrincipal nucleus\nThe principal nucleus represents touch-pressure sensation from the face. It is located in the pons, near the entrance for the fifth nerve. Fibers carrying touch-position information from the face and mouth via cranial nerves V, VII, IX, and X are sent to this nucleus when they enter the brainstem.\nThe principal nucleus contains a touch-position sensory map of the face and mouth, just as the spinal trigeminal nucleus contains a complete pain-temperature map. This nucleus is analogous to the dorsal column nuclei (the gracile and cuneate nuclei) of the spinal cord, which contain a touch-position map of the rest of the body.\nFrom the principal nucleus, secondary fibers cross the midline and ascend in the ventral trigeminothalamic tract to the contralateral thalamus. The ventral trigeminothalamic tract runs parallel to the medial lemniscus, which carries touch-position information from the rest of the body to the thalamus.\nSome sensory information from the teeth and jaws is sent from the principal nucleus to the ipsilateral thalamus via the small dorsal trigeminal tract. Touch-position information from the teeth and jaws of one side of the face is represented bilaterally in the thalamus and cortex.\n\nMesencephalic nucleus\nThe mesencephalic nucleus is not a true nucleus; it is a sensory ganglion (like the trigeminal ganglion) embedded in the brainstem and the sole exception to the rule that sensory information passes through peripheral sensory ganglia before entering the central nervous system. It has been found in all vertebrates except lampreys and hagfishes. They are the only vertebrates without jaws and have specific cells in their brainstems. These \"internal ganglion\" cells were discovered in the late 19th century by medical student Sigmund Freud.\nTwo types of sensory fibers have cell bodies in the mesencephalic nucleus: proprioceptor fibers from the jaw and mechanoreceptor fibers from the teeth. Some of these incoming fibers go to the motor nucleus of the trigeminal nerve (V), bypassing the pathways for conscious perception. The jaw jerk reflex is an example; tapping the jaw elicits a reflex closure of the jaw in the same way that tapping the knee elicits a reflex kick of the lower leg. Other incoming fibers from the teeth and jaws go to the main nucleus of V. This information is projected bilaterally to the thalamus and available for conscious perception.\nActivities such as biting, chewing and swallowing require symmetrical, simultaneous coordination of both sides of the body. They are automatic activities, requiring little conscious attention and involving a sensory component (feedback about touch-position) processed at the unconscious level in the mesencephalic nucleus.\n\nPathways to the thalamus and cortex\nSensation has been defined as the conscious perception of touch-position and pain-temperature information. With the exception of smell, all sensory input (touch-position, pain-temperature, sight, taste, hearing and balance) is sent to the thalamus and then the cortex. The thalamus is anatomically subdivided into nuclei.\n\nTouch-position sensation\nTouch-position information from the body is sent to the ventral posterolateral nucleus (VPL) of the thalamus. Touch-position information from the face is sent to the ventral posteromedial nucleus (VPM) of the thalamus. From the VPL and VPM, information is projected to the primary somatosensory cortex (SI) in the parietal lobe.\nThe representation of sensory information in the postcentral gyrus is organized somatotopically. Adjacent areas of the body are represented by adjacent areas in the cortex. When body parts are drawn in proportion to the density of their innervation, the result is a \"little man\": the cortical homunculus.\nMany textbooks have reproduced the outdated Penfield-Rasmussen diagram [ref?], with the toes and genitals on the mesial surface of the cortex when they are actually represented on the convexity. The classic diagram implies a single primary sensory map of the body, when there are multiple primary maps. At least four separate, anatomically distinct sensory homunculi have been identified in the postcentral gyrus. They represent combinations of input from surface and deep receptors and rapidly  and slowly adapting peripheral receptors; smooth objects will activate certain cells, and rough objects will activate other cells.\nInformation from all four maps in SI is sent to the secondary sensory cortex (SII) in the parietal lobe. SII contains two more sensory homunculi. Information from one side of the body is generally represented on the opposite side in SI, but on both sides in SII. Functional MRI imaging of a defined stimulus (for example, stroking the skin with a toothbrush) \"lights up\" a single focus in SI and two foci in SII.\n\nPain-temperature sensation\nPain-temperature information is sent to the VPL (body) and VPM (face) of the thalamus (the same nuclei which receive touch-position information). From the thalamus, pain-temperature and touch-position information is projected onto SI.\nUnlike touch-position information, however, pain-temperature information is also sent to other thalamic nuclei and projected onto additional areas of the cerebral cortex. Some pain-temperature fibers are sent to the medial dorsal thalamic nucleus (MD), which projects to the anterior cingulate cortex. Other fibers are sent to the ventromedial (VM) nucleus of the thalamus, which projects to the insular cortex. Finally, some fibers are sent to the intralaminar nucleus (IL) of the thalamus via the reticular formation. The IL projects diffusely to all parts of the cerebral cortex.\nThe insular and cingulate cortices are parts of the brain which represent touch-position and pain-temperature in the context of other simultaneous perceptions (sight, smell, taste, hearing and balance) in the context of memory and emotional state. Peripheral pain-temperature information is channeled directly to the brain at a deep level, without prior processing. Touch-position information is handled differently. Diffuse thalamic projections from the IL and other thalamic nuclei are responsible for a given level of consciousness, with the thalamus and reticular formation \"activating\" the brain; peripheral pain-temperature information also feeds directly into this system.\n\nClinical significance\nTrigeminal neuralgia\nCluster headache\nMigraine\n\nLateral medullary syndrome\nLateral medullary syndrome (Wallenberg syndrome) is a clinical demonstration of the anatomy of the trigeminal nerve, summarizing how it processes sensory information. A stroke usually affects only one side of the body; loss of sensation due to a stroke will be lateralized to the right or the left side of the body. The only exceptions to this rule are certain spinal-cord lesions and the medullary syndromes, of which Wallenberg syndrome is the best-known example. In this syndrome, a stroke causes a loss of pain-temperature sensation from one side of the face and the other side of the body.\nThis is explained by the anatomy of the brainstem. In the medulla, the ascending spinothalamic tract (which carries pain-temperature information from the opposite side of the body) is adjacent to the ascending spinal tract of the trigeminal nerve (which carries pain-temperature information from the same side of the face). A stroke which cuts off the blood supply to this area (for example, a clot in the posterior inferior cerebellar artery) destroys both tracts simultaneously. The result is a loss of pain-temperature (but not touch-position) sensation in a \"checkerboard\" pattern (ipsilateral face, contralateral body), facilitating diagnosis.\n\nSensory neuronopathy\nSensory neuronopathy (also known as sensory ganglionopathy) is a type of peripheral neuropathy in which sensory nerve cell bodies in the dorsal root ganglia, commonly including the trigeminal ganglion of the trigeminal nerve, are damaged due to a variety of mechanisms leading to sensory symptoms such as parasthesias, dysesthesias, or hyperalgesia in the affected nerve distribution including the distribution of the trigeminal nerve.\n\nAdditional images\nSee also\nList of mnemonics#Anatomy\nTrigeminovascular system\nAlveolar nerve (Dental nerve)\n\nReferences\nSources\nExternal links\n\nPigeons Detect Magnetic Fields An experiment indicating that the trigeminal nerve in Columba livia may be the mechanism through which \"homing pigeons\" detect magnetic fields\ncranialnerves at The Anatomy Lesson by Wesley Norman (Georgetown University) (V)\nTrigeminal nerve anatomy, part 1 and part 2 on YouTube\nTrigeminal neuralgia","184":"Tympanometry is an acoustic evaluation of the condition of the middle ear eardrum (tympanic membrane) and the conduction bones by creating variations of air pressure in the ear canal.\nTympanometry is an objective test of middle-ear function. It is not a hearing test, but rather a measure of energy transmission through the middle ear. It is not a measure of eardrum or middle ear mobility. It is an acoustic measure, measured by a microphone, as part of the ear canal probe, inserted into the ear canal. The test should not be used to assess the sensitivity of hearing and the results of this test should always be viewed in conjunction with pure tone audiometry.\nTympanometry is a valuable component of the audiometric evaluation. In evaluating hearing loss, tympanometry permits a distinction between sensorineural and conductive hearing loss, when evaluation is not apparent via Weber and Rinne testing. Furthermore, in a primary care setting, tympanometry can be helpful in making the diagnosis of otitis media by demonstrating the presence of fluid build up in the middle ear cavity.\n\nOperation\nA tone of 226 Hz is generated by a probe tip inserted into the external ear canal, where the sound strikes the tympanic membrane, causing vibration of the middle ear, which in turn results in the conscious perception of hearing. Some of this sound is reflected back and picked up by the instrument. Most middle ear problems result in stiffening of the middle ear, which causes more of the sound to be reflected back.\nWhile 226 Hz is the most common probe tone, others can be used. In infants under 4 months of age, research has shown a 1000 Hz tone yields more accurate results. Multi-frequency tympanometry is conducted at multiple frequencies between 250 and 2000 Hz and is used to help identify ossicular abnormalities.\nAdmittance is how energy is transmitted through the middle ear. The instrument measures the reflected sound and expresses it as an admittance or compliance, plotting the results on a chart known as a tympanogram.\nNormally, the air pressure in the ear canal is the same as ambient pressure. Also, under normal conditions, the air pressure in the middle ear is approximately the same as ambient pressure since the eustachian tube opens periodically to ventilate the middle ear and equalize pressure. In a healthy individual, the maximum sound is transmitted through the middle ear when the ambient air pressure in the ear canal is equal to the pressure in the middle ear.\n\nProcedure\nAfter an otoscopy (examination of the ear with an otoscope) to ensure that the path to the eardrum is clear and there is no perforation, the test is performed by inserting the tympanometer probe in the ear canal. The instrument changes the pressure in the ear, generates a pure tone, and measures the eardrum responses to the sound at different pressures.  This produces a series of data measuring how admittance varies with pressure, which is plotted as a tympanogram:\n\nTympanograms are categorized according to the shape of the plot. A normal tympanogram (left) is labelled Type A. There is a normal pressure in the middle ear with normal mobility of the eardrum and ossicles.  Type B  tympanogram may reveal (a)  fluid in the middle ear, (b) perforation of the tympanic membrane or patent pressure equalization tube, or (c) a tumor in the middle ear. Type C tympanograms are consistent with negative pressure in the middle ear space resulting from compromised eustachian tube function and a retracted tympanic membrane.\nThe categorising of tympanometric data should not be used as a diagnostic indicator. It is merely a description of shape. There is a distinction between the three types as well as the two subtypes of type A, namely AS and AD. For example, AS (a shallow tympanogram) will show a stiff middle ear system or AD (a deep tympanogram) consistent with ossicular discontinuity or a monomeric membrane. Only measures of static acoustic admittance, ear canal volume, and tympanometric width\/gradient compared to sex, age, and race specific normative data can be used to somewhat accurately diagnose middle ear pathology along with the use of other audiometric data (e.g. air and bone conduction thresholds, otoscopic examination, normal word recognition at elevated presentation levels, etc.).\n\nTympanometry by end users\nSmartphones\nThere have been a few efforts to lower the cost of tympanometers by using smartphones. In 2022, University of Washington researchers had demonstrated an end-to-end smartphone-based tympanometer system that consists of a portable phone attachment used to change the air pressure in the ear. The smartphone computes and displays a tympanogram and reports peak acoustic admittance in real-time. The smartphone-based system operated at 226 Hz and in proof-of-concept testing showed comparable results to commercial tympanometers. Given the ubiquity of smartphones across the world, these mobile systems may help make these audiology tools accessible across the world.\n\nConsumer devices\nConsumer devices have been made to allow checking for middle-ear fluid at home. A 2009 study found the Ear Check Middle Ear Monitor consumer model acoustic reflectometer to give few false negative results in detecting middle-ear fluid; specificity and positive predictive values were modest, with many false positives. The device was no longer available in 2023.\n\nReferences\nExternal links\nMedscape article by Kathleen C M Campbel\nBasic Multifrequency Tympanometry: The Physical Background","185":"Tympanosclerosis is a condition caused by hyalinization and subsequent calcification of subepithelial connective tissue of the tympanic membrane and middle ear, sometimes resulting in a detrimental effect to hearing.\n\nSigns and symptoms\nMyringosclerosis rarely causes any symptoms. Tympanosclerosis, on the other hand, can cause significant hearing loss or chalky, white patches on the middle ear or tympanic membrane.\n\nCauses\nThe aetiology for tympanosclerosis is not extensively understood. There are several probable factors which could result in the condition appearing, including:\n\nLong term otitis media (or 'glue ear')\nInsertion of a tympanostomy tube. If aspiration is performed as part of the insertion, the risk of tympanosclerosis occurring increases. Risk also increases if a larger tube is used, or if the procedure is repeated.\nAtherosclerosis\nThere is ongoing research as to whether or not cholesteatoma is associated with tympanosclerosis. If there is an association, it is likely that the two conditions co-exist.\n\nPathophysiology\nIncreased fibroblast activity results in deposition of collagen. Calcium phosphate plaques then form in the lamina propria of the tympanic membrane.\n\nDiagnosis\nIf lesions are typical, non-extensive and with no detriment to hearing, investigation into the condition is rarely required. Audiometry is used to determine the extent of hearing loss, if any. Tympanometry produces tympanograms which can be different when tympanosclerosis is present. Computerised tomography (CT) can be used to determine if disease is present in the middle ear. Whilst hearing loss is a common symptom in many diseases of the ear, for example in otosclerosis (abnormal bone growth in the ear), the white, chalky patches on the tympanic membrane are fairly characteristic of tympanosclerosis. Cholesteatoma is similar in appearance but the whiteness is behind the tympanic membrane, rather than inside.\n\nClassification\nMyringosclerosis refers to a calcification only within the tympanic membrane and is usually less extensive than intratympanic tympanosclerosis, which refers to any other location within the middle ear such as the ossicular chain, middle ear mucosa or, less frequently, the mastoid cavity.\n\nTreatment\nHearing aids are a common treatment for hearing loss disorders. A more specific treatment is surgical, involving excision of the sclerotic areas and then further repair of the ossicular chain. There are several techniques, sometimes involving two surgeries; success rates are, however, variable. Damage to the inner ear as a result of surgical procedures is a possible and serious concern, as it can result in forms of sensorineural deafness.\n\nPrognosis\nIn most cases, tympanosclerosis does not cause any recognisable hearing loss up to ten years after the initial disease onset. Sclerotic changes seem to stabilise, but not resolve or dissolve, after 3 years.\n\nEpidemiology\nMyringosclerosis seems to be more common than tympanosclerosis. Most research has not been conducted upon the general, healthy population, but rather those with otitis media or patients who have had tympanostomy tubes in prior procedures. Of the children studied who had 'glue ear', and who were treated with tympanostomy tubing, 23-40% of cases had tympanosclerosis. One study suggested that people with atherosclerosis were more likely to have tympanosclerosis than otherwise healthy individuals.\n\nReferences\nExternal links\n\nTympanosclerosis - Image of tympanosclerosis as revealed by otoscopy.","186":"Trimix is a breathing gas consisting of oxygen, helium and nitrogen and is used in deep commercial diving, during the deep phase of dives carried out using technical diving techniques, and in advanced recreational diving.\nThe helium is included as a substitute for some of the nitrogen, to reduce the narcotic effect of the breathing gas at depth. With a mixture of three gases it is possible to create mixes suitable for different depths or purposes by adjusting the proportions of each gas. Oxygen content can be optimised for the depth to limit the risk of toxicity, and the inert component balanced between nitrogen (which is cheap but narcotic) and helium (which is not narcotic and reduces work of breathing, but is more expensive and increases heat loss).\nThe mixture of helium and oxygen with a 0% nitrogen content is generally known as heliox. This is frequently used as a breathing gas in deep commercial diving operations, where it is often recycled to save the expensive helium component. Analysis of two-component gases is much simpler than three-component gases.\n\nFunction of the helium\nThe main reason for adding helium to the breathing mix is to reduce the proportions of nitrogen and oxygen below those of air, to allow the gas mix to be breathed safely on deep dives. A lower proportion of nitrogen is required to reduce nitrogen narcosis and other physiological effects of the gas at depth. Helium has very little narcotic effect. A lower proportion of oxygen reduces the risk of oxygen toxicity on deep dives.\nThe lower density of helium reduces breathing resistance at depth. Work of breathing can limit the use of breathing gas mixtures in underwater breathing apparatus, as with increasing depth a point may be reached where work of breathing exceeds the available effort from the diver. Beyond this point accumulation of carbon dioxide will eventually result in severe and debilitating hypercapnia, which, if not corrected quickly, will cause the diver to attempt to breathe faster, exacerbating the work of breathing, which will lead to loss of consciousness and a high risk of drowning.\nBecause of its low molecular weight, helium enters and leaves tissues by diffusion more rapidly than nitrogen as the pressure is increased or reduced (this is called on-gassing and off-gassing). Because of its lower solubility, helium does not load tissues as heavily as nitrogen, but at the same time the tissues can not support as high an amount of helium when super-saturated. In effect, helium is a faster gas to saturate and desaturate, which is a distinct advantage in saturation diving, but less so in bounce diving, where the increased rate of off-gassing is largely counterbalanced by the equivalently increased rate of on-gassing.\nSome divers suffer from compression arthralgia during deep descent, and trimix has been shown to help avoid or delay the symptoms of compression arthralgia.\n\nDisadvantages of the helium\nHelium conducts heat six times faster than air, so helium-breathing divers often carry a separate supply of a different gas to inflate drysuits. This is to avoid the risk of hypothermia caused by using helium as inflator gas. Argon, carried in a small, separate tank connected only to the inflator of the drysuit, is preferred to air, since air conducts heat 50% faster than argon. Dry suits (if used together with a buoyancy compensator) still require a minimum of inflation to avoid \"squeezing\", i.e. damage to skin caused by pinching by tight dry suit folds.\nHelium dissolves into tissues (this is called on-gassing) more rapidly than nitrogen as the ambient pressure is increased. A consequence of the higher loading in some tissues is that many decompression algorithms require deeper decompression stops than a similar pressure exposure dive using air, and helium is more likely to come out of solution and cause decompression sickness following a fast ascent.\nIn addition to physiological disadvantages, the use of trimix also has economic and logistic disadvantages. The price of helium increased by over 51% between the years 2000 and 2011. This price increase affects open-circuit divers more than closed-circuit divers due to the larger volume of helium consumed on a typical trimix dive. Additionally, as trimix fills require more expensive analysis equipment than air and nitrox fills, there are fewer trimix filling stations. The relative scarcity of trimix filling stations may necessitate going far out of one's way in order to procure the necessary mix for a deep dive that requires the gas.\n\nAdvantages of controlling the oxygen fraction\nLowering the oxygen content of a breathing gas mixture increases the maximum operating depth and duration of the dive before which oxygen toxicity becomes a limiting factor. Most trimix divers limit their working oxygen partial pressure [PO2] to 1.4 bar and may reduce the PO2 further to 1.3 bar or 1.2 bar depending on the depth, the duration and the kind of breathing system used. A maximum oxygen partial pressure of 1.4 bar for the active sectors of the dive, and 1.6 bar for decompression stops is recommended by several recreational and technical diving certification agencies for open circuit, and 1.2 bar or 1.3 bar as maximum for the active sectors of a dive on closed-circuit rebreather. Increasing the oxygen fraction in a trimix to be used as a decompression gas can accelerate decompression with a lowered risk of isobaric counter diffusion complications.\n\nAdvantages of keeping some nitrogen in the mix\nRetaining nitrogen in trimix can contribute to the prevention of High Pressure Nervous Syndrome, a problem that can occur when breathing heliox at depths beyond about 130 metres (430 ft). Nitrogen is also much less expensive than helium.\n\nNaming conventions\nThe term trimix implies that the gas has three functional components, which are helium, nitrogen and oxygen. Since the nitrogen and all or part of the oxygen is usually provided from air, the other components of ordinary atmospheric air are generally ignored. Conventionally, the composition of a mix is specified by its oxygen percentage, helium percentage and optionally the balance percentage, nitrogen, in that order. For example, a mix named \"trimix 10\/70\" or trimix 10\/70\/20, consisting of 10% oxygen, 70% helium, 20% nitrogen is suitable for a 100-metre (330 ft) dive. Hyperoxic trimix is sometimes referred to as Helitrox, TriOx, or HOTx (High Oxygen Trimix) with the \"x\" in HOTx representing the mixture's fraction of helium as a percentage.\nThe basic term Trimix is sufficient, modified as appropriate with the terms hypoxic, normoxic and hyperoxic, and the usual forms for indicating constituent gas fraction, to describe any possible ratio of gases, but the National Association of Underwater Instructors (NAUI) uses the term \"helitrox\" for hyperoxic 26\/17 Trimix, i.e. 26% oxygen, 17% helium, 57% nitrogen. Helitrox requires decompression stops similar to Nitrox-I (EAN32) and has a maximum operating depth of 44 metres (144 ft), where it has an equivalent narcotic depth of 35 metres (115 ft). This allows diving throughout the usual recreational range, while decreasing decompression obligation and narcotic effects compared to air. GUE and UTD also promote hyperoxic trimix for this depth range, but prefer the term \"TriOx\".\n\nApplications\nIn open-circuit scuba, two classes of trimix are commonly used: normoxic trimix\u2014with a minimum PO2 at the surface of 0.18 and hypoxic trimix\u2014with a PO2 less than 0.18 at the surface. A normoxic mix such as \"19\/30\" is used in the 30 to 60 m (100 to 200 ft) depth range; a hypoxic mix such as \"10\/50\" is used for deeper diving, as a bottom gas only, and cannot safely be breathed at shallow depths where the PO2 is less than 0.18 bar.\nIn fully closed-circuit rebreathers that use trimix diluents, the mix in the breathing loop can be hyperoxic (meaning more oxygen than in air, as in enriched air nitrox) in shallow water, because the rebreather automatically adds oxygen to maintain a specific partial pressure of oxygen. Hyperoxic trimix is also sometimes used on open circuit scuba, to reduce decompression obligations.\n\nBlending\nGas blending of trimix generally involves mixing helium and oxygen with air to the desired proportions and pressure. Two methods are in common use:\nPartial pressure blending is done by decanting oxygen and helium into the diving cylinder and then topping up the mix with air from a diving air compressor. To ensure an accurate mix, after each helium and oxygen transfer, the mix is allowed to cool, its pressure is measured and further gas is decanted until the correct pressure is achieved. This process often takes hours and is sometimes spread over days at busy blending stations. Corrections can be made for temperature effect, but this requires accurate monitoring of the temperature of the mixture inside the cylinder, which is generally not available.\nA second method called 'continuous blending' is done by mixing oxygen and helium into the intake air of a compressor. The oxygen and helium are fed into mixing tubes in the intake air stream using flow meters or analysis of the oxygen content after oxygen addition and before and after the helium addition, and the oxygen and helium flows adjusted accordingly. On the high pressure side of the compressor a regulator or bleed orifice is used to reduce pressure of a sample flow and the trimix is analyzed (preferably for both helium and oxygen) so that the fine adjustment to the intake gas flows can be made. The benefit of such a system is that the helium delivery tank pressure need not be as high as that used in the partial pressure method of blending and residual gas can be 'topped up' to best mix after the dive. This is important mainly because of the high cost of helium. Drawbacks may be that the high heat of compression of helium results in the compressor overheating, especially in hot weather. Temperature of the trimix entering the analyser should be kept constant for best reliability of the analysis, and the analyser should be calibrated at ambient temperature before use. The mixing tube is a very simple device, and DIY versions of the continuous blend units can be made for a relatively low cost compared to the cost of analysers and compressor.\n\nChoice of mixture composition\nThe ratio of gases in a particular mix is chosen to give a safe maximum operating depth and comfortable equivalent narcotic depth for the planned dive. Safe limits for mix of gases in trimix are generally accepted to be a maximum partial pressure of oxygen (PO2\u2014see Dalton's law) of 1.0 to 1.6 bar and maximum equivalent narcotic depth of 30 to 50 m (100 to 160 ft). At 100 m (330 ft), \"12\/52\" has a PO2 of 1.3 bar and an equivalent narcotic depth of 43 m (141 ft).\n\n\"Standard\" mixes\nAlthough theoretically trimix can be blended with almost any combination of helium and oxygen, a number of \"standard\" mixes have evolved (such as 21\/35, 18\/45 and 15\/55\u2014see Naming conventions). Most of these mixes originated from starting by decanting a given pressure of helium into an empty cylinder, and then topping up the mix with 32% nitrox. The \"standard\" mixes evolved because of three coinciding factors \u2014 the desire to keep the equivalent narcotic depth (END) of the mix at approximately 34 metres (112 ft), the requirement to keep the partial pressure of oxygen at 1.4 ATA or below at the deepest point of the dive, and the fact that many dive shops stored standard 32% nitrox in banks, which simplifies mixing. The use of standard mixes makes it relatively easy to top up diving cylinders after a dive using residual mix \u2014 only helium and banked nitrox are needed to top up the residual gas from the last fill.\nThe method of mixing a known nitrox mix with helium allows analysis of the fractions of each gas using only an oxygen analyser, since the ratio of the oxygen fraction in the final mix to the oxygen fraction in the initial nitrox gives the fraction of nitrox in the final mix, hence the fractions of the three components are easily calculated. It is demonstrably true that the END of a nitrox-helium mixture at its maximum operating depth (MOD) is equal to the MOD of the nitrox alone.\n\nHeliair\nHeliair is a breathing gas consisting of mixture of oxygen, nitrogen and helium and is often used during the deep phase of dives carried out using technical diving techniques. This term, first used by Sheck Exley, is mostly used by Technical Diving International (TDI).\nIt is easily blended from helium and air and so has a fixed 21:79 ratio of oxygen to nitrogen with the balance consisting of a variable amount of helium. It is sometimes referred to as \"poor man's trimix\", because it is much easier to blend than trimix blends with variable oxygen content, since all that is required is to insert the requisite partial pressure of helium, and then top up with air from a conventional compressor. The more complicated (and dangerous) step of adding pure oxygen at pressure required to blend trimix is absent when blending heliair.\nHeliair blends are similar to the standard Trimix blends made with helium and Nitrox 32, but with a deeper END at MOD.\nHeliair will always have less than 21% oxygen, and will be hypoxic (less than 17% oxygen) for mixes with more than 20% helium.\n\nHistory as a diving gas\n1919: Professor Elihu Thomson speculates that helium could be used instead of nitrogen to reduce the breathing resistance at great depth. Heliox was used with air tables resulting in a high incidence of decompression sickness, so the use of helium was discontinued.\n1924: The US Navy begins examining helium's potential usage and by the mid-1920s lab animals were exposed to experimental chamber dives using heliox. Soon, human subjects breathing heliox 20\/80 (20% oxygen, 80% helium) had been successfully decompressed from deep dives.\n1937: Several test dives are conducted with helium mixtures, including salvage diver Max \"Gene\" Nohl's dive to 127 meters.\n1939: US Navy uses heliox in USS Squalus salvage operation. Heliox usage, coupled with the absence of decrement in co-ordination and cognitive function in the salvage divers, confirms Behnke's theory of nitrogen narcosis.\n1965: Nic Flemming's work to study sand ribbons in the English Channel becomes the first to compare diver performance while breathing air and heliox in the open water.\n1963: First saturation dives using trimix as part of Project Genesis.\n1970: Hal Watts recovers two bodies at Mystery Sink (126 m).\n1979: A research team headed by Peter B. Bennett at the Duke University Medical Center Hyperbaric Laboratory begins the \"Atlantis Dive Series\" which proves the mechanisms behind the use of trimix to prevent High Pressure Nervous Syndrome symptoms.\n1983: Cave diver Jochen Hasenmayer uses heliox to a depth of 212 meters. Depth is later repeated by Sheck Exley in 1987.\n1987: First mass use of trimix and heliox: Wakulla Springs Project. Exley teaches non-commercial divers in relation to trimix usage in cave diving.\n1991: Billy Deans commences teaching of trimix diving for recreational diving. Tom Mount develops first trimix training standards (IANTD). Use of trimix spreads rapidly to North East American wreck diving community.\n1992: The National Oceanographic and Atmospheric Administration (NOAA) develops \"Monitor Mix\" for dives to the USS Monitor. This mix became NOAA Trimix I, with decompression tables designed by Bill Hamilton published in the NOAA Diving Manual.\n1992: NOAA obtains training from Key West Divers to conduct the first NOAA-sponsored trimix dives on the wreck of the USS Monitor off Cape Hatteras, NC.\n1994: Combined UK\/USA team, including wreck divers John Chatterton and Gary Gentile, successfully completes a series of wreck dives on the RMS Lusitania expedition to a depth of 100 meters using trimix.\n1994: Sheck Exley and Jim Bowden use \"heliair\" at Zacaton in the first attempt to make an open circuit scuba dive to 1000 ft. Exley, at the time holding the world record for an 881-foot dive, passes out and dies around 900 feet; Bowden aborts at 925 feet and survives despite several life-threatening obstacles.\n2001: John Bennett becomes the first scuba diver to dive to 300 metres (1,000 ft), using trimix.\n2005: David Shaw sets depth record for using a trimix rebreather, and dies while repeating the dive to attempt to recover the body of another diver.\n2015: The United States Navy Experimental Diving Unit shows that bounce dives using trimix are not more decompression efficient than dives on heliox.\n\nTraining and certification\nTechnical diver training and certification agencies may differentiate between levels of trimix diving qualifications, The usual distinction is between normoxic trimix and hypoxic trimix, sometimes also called full trimix. The basic distinction is that for hypoxic trimix diving the dive cannot be started on the bottom mix, and procedures for use of a travel mix for the first part of the descent, and gas switching during the descent to avoid oxygen toxicity are added to the required skills. Longer decompression using a larger variety of mixtures may also complicate procedures. In closed circuit rebreather diving, use of a hypoxic diluent prevents the diver from conducting a diluent flush at shallow depths while breathing from the loop, so that it remains possible at the maximum depth of the dive, where it may be more critical.\n\nSee also\nArgox \u2013 Gas mixture occasionally used by scuba divers for dry-suit inflation\nHeliox \u2013 A breathing gas mixed from helium and oxygen\nHydreliox \u2013 Breathing gas mixture of hydrogen, helium, and oxygen\nHydrox \u2013 Breathing gas mixture experimentally used for very deep diving\nNitrox \u2013 Breathing gas, mixture of nitrogen and oxygen\n\n\n== References ==","187":"The Unterberger test, also Unterberger's test and Unterberger's stepping test, is a test used in otolaryngology to help assess whether a patient has a vestibular pathology.  It is not useful for detecting central (brain) disorders of balance.\n\nTechnique\nThe patient is asked to walk in place with their eyes closed.\n\nInterpretation\nIf the patient rotates to one side they may have a labyrinthine lesion on that side, but this test should not be used to diagnose lesions without the support of other tests.\n\n\n== References ==","188":"The utricle and saccule are the two otolith organs in the vertebrate inner ear. The word utricle comes from Latin  uter 'leather bag'. The utricle and saccule are part of the balancing system (membranous labyrinth) in the vestibule of the bony labyrinth (small oval chamber). They use small stones and a viscous fluid to stimulate hair cells to detect motion and orientation. The utricle detects linear accelerations and head-tilts in the horizontal plane.\n\nStructure\nThe utricle is larger than the saccule and is of an oblong form, compressed transversely, and occupies the upper and back part of the vestibule, lying in contact with the recessus ellipticus and the part below it.\n\nMacula\nThe macula of utricle (macula acustica utriculi) is a small (2 by 3 mm) thickening lying horizontally on the floor of the utricle where the epithelium contains vestibular hair cells that allow a person to perceive changes in latitudinal acceleration as well as the effects of gravity; it receives the utricular filaments of the acoustic nerve.\nThe hair cells are mechanoreceptors which have 40 to 70 stereocilia and only one true cilium called a kinocilium. The kinocilium is the only sensory aspect of the hair cell and is what causes hair cell polarization. The tips of these stereocilia and kinocilium are embedded in a gelatinous layer, which together with the statoconia form the otolithic membrane. This membrane is weighted with calcium carbonate-protein granules called otoliths. The otolithic membrane adds weight to the tops of the hair cells and increases their inertia. The addition in weight and inertia is vital to the utricle's ability to detect linear acceleration, as described below, and to determine the orientation of the head. When the head is tilted such that gravity pulls on the statoconia, the gelatinous layer is pulled in the same direction also, causing the sensory hairs to bend. Labyrinthine activity responsible for the nystagmus induced by off-vertical axis rotation arises in the otolith organs and couples to the oculomotor system through the velocity storage mechanism.\n\nMicroanatomy\nThe cavity of the utricle communicates behind with the semicircular ducts by five orifices.\nThe ductus utriculosaccularis comes off of the anterior wall of the utricle and opens into the ductus endolymphaticus.\n\nFunction\nThe utricle contains mechanoreceptors called hair cells that distinguish between degrees of tilting of the head, thanks to their apical stereocilia set-up. These are covered by otoliths which, due to gravity, pull on the stereocilia and tilt them. Depending on whether the tilt is in the direction of the kinocilium or not, the resulting hair cell polarisation is excitatory (depolarising) or inhibitory (hyperpolarisation), respectively.  Any orientation of the head causes a combination of stimulation to the utricles and saccules of the two ears.  The brain interprets head orientation by comparing these inputs to each other and to other input from the eyes and stretch receptors in the neck, thereby detecting whether only the head is tilted or the entire body is tipping. The inertia of the otolithic membranes is especially important in detecting linear acceleration. Suppose you are sitting in a car at a stoplight and then begin to move. The otolithic membrane of the macula utriculi briefly lags behind the rest of the tissues, bends the stereocilia backward, and stimulates the cells. When you stop at the next light, the macula stops but the otolithic membrane keeps going for a moment, bending the stereocilia forward. The hair cells convert this pattern of stimulation to nerve signals, and the brain is thus advised of changes in your linear velocity.  This signal to the vestibular nerve (which takes it to the brainstem) does not adapt with time. The effect of this is that, for example, an individual lying down to sleep will continue to detect that they are lying down hours later when they awaken.\nUnbent and at rest hairs in the macula have a base rate of depolarization of 90-100 action potentials a second.  The brain suppresses this, and we ignore it and know that our body is stabilized.  If the head moves or the body accelerates or decelerates, then bending occurs. Depending on the direction of bending, the hair cells will either be excited or inhibited resulting in either an increase or decrease in firing frequency of the hair cells.\nThe macula is also sensitive to linear acceleration as the inertia possessed by the statoconia can also shift the gelatinous layer during increases and decreases in linear velocity.\n\nSee also\nReferences\nThis article incorporates text in the public domain from page 1051 of the 20th edition of Gray's Anatomy (1918)\n\nExternal links\nDiagram at ipfw.edu\nTHE INNER EAR: THE VESTIBULAR APPARATUS","189":"Underwater diving,  as a human activity, is the practice of descending below the water's surface to interact with the environment. It is also often referred to as diving, an ambiguous term with several possible meanings, depending on context.\nImmersion in water and exposure to high ambient pressure have physiological effects that limit the depths and duration possible in ambient pressure diving. Humans are not physiologically and anatomically well-adapted to the environmental conditions of diving, and various equipment has been developed to extend the depth and duration of human dives, and allow different types of work to be done.\nIn ambient pressure diving, the diver is directly exposed to the pressure of the surrounding water. The ambient pressure diver may dive on breath-hold (freediving) or use breathing apparatus for scuba diving or surface-supplied diving, and the saturation diving technique reduces the risk of decompression sickness (DCS) after long-duration deep dives. Atmospheric diving suits (ADS) may be used to isolate the diver from high ambient pressure. Crewed submersibles can extend depth range to full ocean depth, and remotely controlled or robotic machines can reduce risk to humans.\nThe environment exposes the diver to a wide range of hazards, and though the risks are largely controlled by appropriate diving skills, training, types of equipment and breathing gases used depending on the mode, depth and purpose of diving, it remains a relatively dangerous activity. Professional diving is usually regulated by occupational health and safety legislation, while recreational diving may be entirely unregulated.\nDiving activities are restricted to maximum depths of about 40 metres (130 ft) for recreational scuba diving, 530 metres (1,740 ft) for commercial saturation diving, and 610 metres (2,000 ft) wearing atmospheric suits. Diving is also restricted to conditions which are not excessively hazardous, though the level of risk acceptable can vary, and fatal incidents may occur.\nRecreational diving (sometimes called sport diving or subaquatics) is a popular leisure activity. Technical diving is a form of recreational diving under more challenging conditions. Professional diving (commercial diving, diving for research purposes, or for financial gain) involves working underwater. Public safety diving is the underwater work done by law enforcement, fire rescue, and underwater search and recovery dive teams. Military diving includes combat diving, clearance diving and ships husbandry.\nDeep sea diving is underwater diving, usually with surface-supplied equipment, and often refers to the use of standard diving dress with the traditional copper helmet. Hard hat diving is any form of diving with a helmet, including the standard copper helmet, and other forms of free-flow and lightweight demand helmets.\nThe history of breath-hold diving goes back at least to classical times, and there is evidence of prehistoric hunting and gathering of seafoods that may have involved underwater swimming. Technical advances allowing the provision of breathing gas to a diver underwater at ambient pressure are recent, and self-contained breathing systems developed at an accelerated rate following the Second World War.\n\nPhysiological constraints on diving\nImmersion in water and exposure to cold water and high pressure have physiological effects on the diver which limit the depths and duration possible in ambient pressure diving. Breath-hold endurance is a severe limitation, and breathing at high ambient pressure adds further complications, both directly and indirectly. Technological solutions have been developed which can greatly extend depth and duration of human ambient pressure dives, and allow useful work to be done underwater.\n\nImmersion\nImmersion of the human body in water affects the circulation, renal system, fluid balance, and breathing, because the external hydrostatic pressure of the water provides support against the internal hydrostatic pressure of the blood. This causes a blood shift from the extravascular tissues of the limbs into the chest cavity, and fluid losses known as immersion diuresis compensate for the blood shift in hydrated subjects soon after immersion. Hydrostatic pressure on the body from head-out immersion causes negative pressure breathing which contributes to the blood shift.\nThe blood shift causes an increased respiratory and cardiac workload. Stroke volume is not greatly affected by immersion or variation in ambient pressure, but slowed heartbeat reduces the overall cardiac output, particularly because of the diving reflex in breath-hold diving. Lung volume decreases in the upright position, owing to cranial displacement of the abdomen from hydrostatic pressure, and resistance to air flow in the airways increases because of the decrease in lung volume. There appears to be a connection between pulmonary edema and increased pulmonary blood flow and pressure, which results in capillary engorgement. This may occur during higher intensity exercise while immersed or submerged.\nThe diving reflex is a response to immersion that overrides the basic homeostatic reflexes. It optimises respiration by preferentially distributing oxygen stores to the heart and brain, which allows extended periods underwater. It is exhibited strongly in aquatic mammals (seals, otters, dolphins and muskrats), and also exists in other mammals, including humans. Diving birds, such as penguins, have a similar diving reflex. The diving reflex is triggered by chilling the face and holding the breath. The cardiovascular system constricts peripheral blood vessels, slows the pulse rate, redirects blood to the vital organs to conserve oxygen, releases red blood cells stored in the spleen, and, in humans, causes heart rhythm irregularities. Aquatic mammals have evolved physiological adaptations to conserve oxygen during submersion, but apnea, slowed pulse rate, and vasoconstriction are shared with terrestrial mammals.\n\nExposure\nCold shock response is the physiological response of organisms to sudden cold, especially cold water, and is a common cause of death from immersion in very cold water, such as by falling through thin ice. The immediate shock of the cold causes involuntary inhalation, which if underwater can result in drowning. The cold water can also cause heart attack due to vasoconstriction; the heart has to work harder to pump the same volume of blood throughout the body, and for people with heart disease, this additional workload can cause the heart to go into arrest. A person who survives the initial minute after falling into cold water can survive for at least thirty minutes provided they do not drown. The ability to stay afloat declines substantially after about ten minutes as the chilled muscles lose strength and co-ordination.\nHypothermia is reduced core body temperature that occurs when a body loses more heat than it generates. It is a major limitation to swimming or diving in cold water. The reduction in finger dexterity due to pain or numbness decreases general safety and work capacity, which in turn increases the risk of other injuries. Non-freezing cold injury can affect the extremities in cold water diving, and frostbite can occur when air temperatures are low enough to cause tissue freezing. Body heat is lost much more quickly in water than in air, so water temperatures that would be tolerable as outdoor air temperatures can lead to hypothermia, which may lead to death from other causes in inadequately protected divers.\nThermoregulation of divers is complicated by breathing gases at raised ambient pressure and by gas mixtures necessary for limiting inert gas narcosis, work of breathing, and for accelerating decompression.\n\nBreath-hold limitations\nBreath-hold diving by an air-breathing animal is limited to the physiological capacity to perform the dive on the oxygen available until it returns to a source of fresh breathing gas, usually the air at the surface. As this internal oxygen supply reduces, the animal experiences an increasing urge to breathe caused by buildup of carbon dioxide and lactate in the blood, followed by loss of consciousness due to cerebral hypoxia. If this occurs underwater, it will drown.\nBlackouts in freediving can occur when the breath is held long enough for metabolic activity to reduce the oxygen partial pressure sufficiently to cause loss of consciousness. This is accelerated by exertion, which uses oxygen faster, and can be exacerbated by hyperventilation directly before the dive, which reduces the carbon dioxide level in the blood. Lower carbon dioxide levels increase the oxygen-haemoglobin affinity, reducing availability of oxygen to brain tissue towards the end of the dive (Bohr effect); they also suppress the urge to breathe, making it easier to hold the breath to the point of blackout. This can happen at any depth.\nAscent-induced hypoxia is caused by a drop in oxygen partial pressure as ambient pressure is reduced. The partial pressure of oxygen at depth may be sufficient to maintain consciousness at that depth and not at the reduced pressures nearer the surface.\n\nAmbient pressure changes\nBarotrauma, a type of dysbarism, is physical damage to body tissues caused by a difference in pressure between a gas space inside, or in contact with the body, and the surrounding gas or fluid. It typically occurs when the organism is exposed to a large change in ambient pressure, such as when a diver ascends or descends. When diving, the pressure differences which cause the barotrauma are changes in hydrostatic pressure.\nThe initial damage is usually due to over-stretching the tissues in tension or shear, either directly by expansion of the gas in the closed space, or by pressure difference hydrostatically transmitted through the tissue.\nBarotrauma generally manifests as sinus or middle ear effects, decompression sickness, lung over-expansion injuries, and injuries resulting from external squeezes. Barotraumas of descent are caused by preventing the free change of volume of the gas in a closed space in contact with the diver, resulting in a pressure difference between the tissues and the gas space, and the unbalanced force due to this pressure difference causes deformation of the tissues resulting in cell rupture. Barotraumas of ascent are also caused when the free change of volume of the gas in a closed space in contact with the diver is prevented. In this case the pressure difference causes a resultant tension in the surrounding tissues which exceeds their tensile strength. Besides tissue rupture, the overpressure may cause ingress of gases into the adjoining tissues and further afield by bubble transport through the circulatory system.  This can cause blockage of circulation at distant sites, or interfere with the normal function of an organ by its presence.\n\nBreathing under pressure\nProvision of breathing gas at ambient pressure can greatly prolong the duration of a dive, but there are other problems that may result from this technological solution. Absorption of metabolically inert gases is increased as a function of time and pressure, and these may both produce undesirable effects immediately, as a consequence of their presence in the tissues in the dissolved state, such as nitrogen narcosis and high pressure nervous syndrome, or cause problems when coming out of solution within the tissues during decompression.\nOther problems arise when the concentration of metabolically active gases is increased. These range from the toxic effects of oxygen at high partial pressure, through buildup of carbon dioxide due to excessive work of breathing, increased dead space, or inefficient removal, to the exacerbation of the toxic effects of contaminants in the breathing gas due to the increased concentration at high pressures. Hydrostatic pressure differences between the interior of the lung and the breathing gas delivery, increased breathing gas density due to ambient pressure, and increased flow resistance due to higher breathing rates may all cause increased work of breathing, fatigue of the respiratory muscles, and a physiological limit to effective ventilation.\n\nSensory impairment\nUnderwater vision is affected by the clarity and the refractive index of the medium. Visibility underwater is reduced because light passing through water attenuates rapidly with distance, leading to lower levels of natural illumination. Underwater objects are also blurred by scattering of light between the object and the viewer, resulting in lower contrast. These effects vary with the wavelength of the light, and the colour and turbidity of the water. The human eye is optimised for air vision, and when it is immersed in direct contact with water, visual acuity is adversely affected by the difference in refractive index between water and air. Provision of an airspace between the cornea and the water can compensate, but causes scale and distance distortion. Artificial illumination can improve visibility at short range. Stereoscopic acuity, the ability to judge relative distances of different objects, is considerably reduced underwater, and this is affected by the field of vision. A narrow field of vision caused by a small viewport in a helmet results in greatly reduced stereoacuity, and an apparent movement of a stationary object when the head is moved. These effects lead to poorer hand-eye coordination.\nWater has different acoustic properties from those of air. Sound from an underwater source can propagate relatively freely through body tissues where there is contact with the water as the acoustic properties are similar. When the head is exposed to the water, some sound is transmitted by the eardrum and middle ear, but a significant part reaches the cochlea independently, by bone conduction. Some sound localisation is possible, though difficult. Human hearing underwater, in cases where the diver's ear is wet, is less sensitive than in air. Frequency sensitivity underwater also differs from that in air, with a consistently higher threshold of hearing underwater; sensitivity to higher frequency sounds is reduced the most. The type of headgear affects noise sensitivity and noise hazard depending on whether transmission is wet or dry. Human hearing underwater is less sensitive with wet ears than in air, and a neoprene hood causes substantial attenuation. When wearing a helmet, hearing sensitivity is similar to that in surface air, as it is not greatly affected by the breathing gas or chamber atmosphere composition or pressure. Because sound travels faster in heliox than in air, voice formants are raised, making divers' speech high-pitched and distorted, and hard to understand for people not used to it. The increased density of breathing gases under pressure has a similar and additive effect.\nTactile sensory perception in divers may be impaired by the environmental protection suit and low temperatures. The combination of instability, equipment, neutral buoyancy and resistance to movement by the inertial and viscous effects of the water encumbers the diver. Cold causes losses in sensory and motor function and distracts from and disrupts cognitive activity. The ability to exert large and precise force is reduced.\nBalance and equilibrium depend on vestibular function and secondary input from visual, organic, cutaneous, kinesthetic and sometimes auditory senses which are processed by the central nervous system to provide the sense of balance. Underwater, some of these inputs may be absent or diminished, making the remaining cues more important. Conflicting input may result in vertigo, disorientation and motion sickness. The vestibular sense is essential in these conditions for rapid, intricate and accurate movement. Proprioceptive perception makes the diver aware of personal position and movement, in association with the vestibular and visual input, and allows the diver to function effectively in maintaining physical equilibrium and balance in the water. In the water at neutral buoyancy, the proprioceptive cues of position are reduced or absent. This effect may be exacerbated by the diver's suit and other equipment.\nTaste and smell are not very important to the diver in the water but more important to the saturation diver while in accommodation chambers. There is evidence of a slight decrease in threshold for taste and smell after extended periods under pressure.\n\nDiving modes\nThere are several modes of diving distinguished largely by the breathing gas supply system used, and whether the diver is exposed to the ambient pressure. The diving equipment, support equipment and procedures are largely determined by the mode.\n\nFreediving\nThe ability to dive and swim underwater while holding one's breath is considered a useful emergency skill, an important part of water sport and Navy safety training, and an enjoyable leisure activity. Underwater diving without breathing apparatus can be categorised as underwater swimming, snorkelling and freediving. These categories overlap considerably. Several competitive underwater sports are practised without breathing apparatus.\nFreediving precludes the use of external breathing devices, and relies on the ability of divers to hold their breath until resurfacing. The technique ranges from simple breath-hold diving to competitive apnea dives. Fins and a diving mask are often used in free diving to improve vision and provide more efficient propulsion. A short breathing tube called a snorkel allows the diver to breathe at the surface while the face is immersed.  Snorkelling on the surface with no intention of diving is a popular water sport and recreational activity.\n\nScuba diving\nScuba diving is diving with a self-contained underwater breathing apparatus, which is completely independent of surface supply.  Scuba gives the diver mobility and horizontal range far beyond the reach of an umbilical hose attached to surface-supplied diving equipment (SSDE).\nScuba divers engaged in armed forces covert operations may be referred to as frogmen, combat divers or attack swimmers.\nOpen circuit scuba systems discharge the breathing gas into the environment as it is exhaled, and consist of one or more diving cylinders containing breathing gas at high pressure which is supplied to the diver through a diving regulator. They may include additional cylinders for decompression gas or emergency breathing gas.\nClosed-circuit or semi-closed circuit rebreather scuba systems allow recycling of exhaled gases. The volume of gas used is reduced compared to that of open circuit, so a smaller cylinder or cylinders may be used for an equivalent dive duration. They greatly extend the time spent underwater as compared to open circuit for the same gas consumption. Rebreathers produce fewer bubbles and less noise than scuba which makes them attractive to covert military divers to avoid detection, scientific divers to avoid disturbing marine animals, and media divers to avoid bubble interference.\nA scuba diver moves underwater primarily by using fins attached to the feet; external propulsion can be provided by a diver propulsion vehicle, or a towboard pulled from the surface. Other equipment includes a diving mask to improve underwater vision, a protective diving suit, equipment to control buoyancy, and equipment related to the specific circumstances and purpose of the dive. Scuba divers are trained in the procedures and skills appropriate to their level of certification by instructors affiliated to the diver certification organisations which issue these diver certifications. These include standard operating procedures for using the equipment and dealing with the general hazards of the underwater environment, and emergency procedures for self-help and assistance of a similarly equipped diver experiencing problems. A minimum level of fitness and health is required by most training organisations, and a higher level of fitness may be needed for some applications.\n\nSurface-supplied diving\nAn alternative to self-contained breathing systems is to supply breathing gases from the surface through a hose. When combined with a communication cable, a pneumofathometer hose and a safety line it is called the diver's umbilical, which may include a hot water hose for heating, video cable and breathing gas reclaim line. The diver wears a full-face mask or helmet, and gas may be supplied on demand or as a continuous free flow. More basic equipment that uses only an air hose is called an airline or hookah system. This allows the diver to breathe using an air supply hose from a high pressure cylinder or diving air compressor at the surface. Breathing gas is supplied through a mouth-held demand valve or light full-face mask. Airline diving is used for work such as hull cleaning and archaeological surveys, for shellfish harvesting, and as snuba, a shallow water activity typically practised by tourists and those who are not scuba-certified.\nSaturation diving lets professional divers live and work under pressure for days or weeks at a time. After working in the water, the divers rest and live in a dry pressurised underwater habitat on the bottom or a saturation life support system of pressure chambers on the deck of a diving support vessel, oil platform or other floating platform at a similar pressure to the work depth. They are transferred between the surface accommodation and the underwater workplace in a pressurised closed diving bell. Decompression at the end of the dive may take many days, but since it is done only once for a long period of exposure, rather than after each of many shorter exposures, the overall risk of decompression injury to the diver and the total time spent decompressing are reduced. This type of diving allows greater work efficiency and safety.\nCommercial divers refer to diving operations where the diver starts and finishes the diving operation at atmospheric pressure as surface oriented, or bounce diving. The diver may be deployed from the shore or a diving support vessel and may be transported on a diving stage or in a diving bell. Surface-supplied divers almost always wear diving helmets or full-face diving masks. The bottom gas can be air, nitrox, heliox or trimix; the decompression gases may be similar, or may include pure oxygen. Decompression procedures include in-water decompression or surface decompression in a deck chamber.\nA wet bell with a gas filled dome provides more comfort and control than a stage and allows for longer time in water. Wet bells are used for air and mixed gas, and divers can decompress on oxygen at 12 metres (40 ft). Small closed bell systems have been designed that can be easily mobilised, and include a two-man bell, a launch and recovery system and a chamber for decompression after transfer under pressure (TUP). Divers can breathe air or mixed gas at the bottom and are usually recovered with the chamber filled with air. They decompress on oxygen supplied through built in breathing systems (BIBS) towards the end of the decompression. Small bell systems support bounce diving down to 120 metres (390 ft) and for bottom times up to 2 hours.\nA relatively portable surface gas supply system using high pressure gas cylinders for both primary and reserve gas, but using the full diver's umbilical system with pneumofathometer and voice communication, is known in the industry as \"scuba replacement\".\nCompressor diving is a rudimentary method of surface-supplied diving used in some tropical regions such as the Philippines and the Caribbean. The divers swim with a half mask and fins and are supplied with air from an industrial low-pressure air compressor on the boat through plastic tubes. There is no reduction valve; the diver holds the hose end in his mouth with no demand valve or mouthpiece and allows excess air to spill out between the lips.\n\nAtmospheric pressure diving\nSubmersibles and rigid atmospheric diving suits (ADS) enable diving to be carried out in a dry environment at normal atmospheric pressure. An ADS is a small one-person articulated submersible which resembles a suit of armour, with elaborate joints to allow bending, while maintaining an internal pressure of one atmosphere. An ADS can be used for dives of up to about 700 metres (2,300 ft) for many hours.  It eliminates the majority of physiological dangers associated with deep diving \u2013 the occupant does not need to decompress, there is no need for special gas mixtures, and there is no danger of nitrogen narcosis \u2013 at the expense of higher cost, complex logistics and loss of dexterity.\nCrewed submeribles have been built rated to full ocean depth and have dived to the deepest known points of all the oceans.\n\nUnmanned diving\nAutonomous underwater vehicles (AUVs) and remotely operated underwater vehicles (ROVs) can carry out some functions of divers. They can be deployed at greater depths and in more dangerous environments. An AUV is a robot which travels underwater without requiring real-time input from an operator. AUVs constitute part of a larger group of unmanned undersea systems, a classification that includes non-autonomous ROVs, which are controlled and powered from the surface by an operator\/pilot via an umbilical or using remote control. In military applications AUVs are often referred to as unmanned undersea vehicles (UUVs).\n\nDiving activities\nPeople may dive for various reasons, both personal and professional. While a newly qualified recreational diver may dive purely for the experience of diving, most divers have some additional reason for being underwater. Recreational diving is purely for enjoyment and has several specialisations and technical disciplines to provide more scope for varied activities for which specialist training can be offered, such as cave diving, wreck diving, ice diving and deep diving. Several underwater sports are available for exercise and competition.\nThere are various aspects of professional diving that range from part-time work to lifelong careers. Professionals in the recreational diving industry include instructor trainers, diving instructors, assistant instructors, divemasters, dive guides, and scuba technicians. A scuba diving tourism industry has developed to service recreational diving in regions with popular dive sites. Commercial diving is industry related and includes engineering tasks such as in hydrocarbon exploration, offshore construction, dam maintenance and harbour works.  Commercial divers may also be employed to perform tasks related to marine activities, such as naval diving, ships husbandry, marine salvage or aquaculture.\nOther specialist areas of diving include military diving, with a long history of military frogmen in various roles.  They can perform roles including direct combat, reconnaissance, infiltration behind enemy lines, placing mines, bomb disposal or engineering operations.\nIn civilian operations, police diving units perform search and rescue operations, and recover evidence. In some cases diver rescue teams may also be part of a fire department, paramedical service, sea rescue or lifeguard unit, and this may be classed as public safety diving. There are also professional media divers such as underwater photographers and videographers, who record the underwater world, and scientific divers in fields of study which involve the underwater environment, including marine biologists, geologists, hydrologists, oceanographers, speleologists and underwater archaeologists.\nThe choice between scuba and surface-supplied diving equipment is based on both legal and logistical constraints. Where the diver requires mobility and a large range of movement, scuba is usually the choice if safety and legal constraints allow. Higher risk work, particularly commercial diving, may be restricted to surface-supplied equipment by legislation and codes of practice.\n\nHistory\nFreediving as a widespread means of hunting and gathering, both for food and other valuable resources such as pearls and coral, dates from before 4500 BCE. By classical Greek and Roman times commercial diving applications such as sponge diving and marine salvage were established. Military diving goes back at least as far as the Peloponnesian War, with recreational and sporting applications being a recent development. Technological development in ambient pressure diving started with stone weights (skandalopetra) for fast descent, with rope assist for ascent. The diving bell is one of the earliest types of equipment for underwater work and exploration. Its use was first described by Aristotle in the 4th century BCE.  In the 16th and 17th centuries CE, diving bells became more useful when a renewable supply of air could be provided to the diver at depth, and progressed to surface-supplied diving helmets \u2013 in effect miniature diving bells covering the diver's head and supplied with compressed air by manually operated pumps \u2013 which were improved by attaching a waterproof suit to the helmet. In the early 19th century these became the standard diving dress, which made a far wider range of marine civil engineering and salvage projects practicable.\nLimitations in mobility of the surface-supplied systems encouraged the development of both open circuit and closed circuit scuba in the 20th century, which allow the diver a much greater autonomy. These became popular during the Second World War for clandestine military operations, and post-war for scientific, search and rescue, media diving, recreational and technical diving. The heavy free-flow surface-supplied copper helmets evolved into lightweight demand helmets, which are more economical with breathing gas, important for deeper dives using expensive helium based breathing mixtures.  Saturation diving reduced the risks of decompression sickness for deep and long exposures.\nAn alternative approach was the development of the ADS or armoured suit, which isolates the diver from the pressure at depth, at the cost of mechanical complexity and limited dexterity. The technology first became practicable in the middle 20th century. Isolation of the diver from the environment was taken further by the development of remotely operated underwater vehicles (ROV or ROUV) in the late 20th century, where the operator controls the ROV from the surface, and autonomous underwater vehicles (AUV), which dispense with an operator altogether. All of these modes are still in use and each has a range of applications where it has advantages over the others, though diving bells have largely been relegated to a means of transport for surface-supplied divers. In some cases combinations are particularly effective, such as the simultaneous use of surface orientated or saturation surface-supplied diving equipment and work or observation class remotely operated vehicles.\n\nPhysiological discoveries\nBy the late 19th century, as salvage operations became deeper and longer, an unexplained malady began afflicting the divers; they would suffer breathing difficulties, dizziness, joint pain and paralysis, sometimes leading to death. The problem was already well known among workers building tunnels and bridge footings operating under pressure in caissons and was initially called caisson disease; it was later renamed the bends because the joint pain typically caused the sufferer to stoop. Early reports of the disease had been made at the time of Charles Pasley's salvage operation, but scientists were still ignorant of its causes.\nFrench physiologist Paul Bert was the first to understand it as decompression sickness (DCS). His work, La Pression barom\u00e9trique (1878), was a comprehensive investigation into the physiological effects of air pressure, both above and below the normal. He determined that inhaling pressurised air caused nitrogen to dissolve into the bloodstream; rapid depressurisation would then release the nitrogen into its gaseous state, forming bubbles that could block the blood circulation and potentially cause paralysis or death. Central nervous system oxygen toxicity was also first described in this publication and is sometimes referred to as the \"Paul Bert effect\".\nJohn Scott Haldane designed a decompression chamber in 1907, and he produced the first decompression tables for the Royal Navy in 1908 after extensive experiments with animals and human subjects. These tables established a method of decompression in stages \u2013 it remains the basis for decompression methods to this day. Following Haldane's recommendation, the maximum safe operating depth for divers was extended to 61 metres (200 ft).\nThe US Navy continued research into decompression, and in 1915 the first Bureau of Construction and Repair decompression tables were developed by French and Stilson. Experimental dives were conducted in the 1930s, forming the basis for the 1937 US Navy air decompression tables. Surface decompression and oxygen use were also researched in the 1930s. The US Navy 1957 tables were developed to correct problems found in the 1937 tables.\nIn 1965 Hugh LeMessurier and Brian Andrew Hills published their paper, A thermodynamic approach arising from a study on Torres Strait diving techniques, which suggested that decompression following schedules based on conventional models results in asymptomatic bubble formation which must then be re-dissolved at the decompression stops before it can be eliminated. This is slower than allowing the gas to be eliminated while it is still in solution, and indicates the importance of minimising bubble phase gas for efficient decompression.\nM.P. Spencer showed that Doppler ultrasonic methods can detect venous bubbles in asymptomatic divers, and Dr Andrew Pilmanis showed that safety stops reduced bubble formation. In 1981 D.E. Yount described the Varying Permeability Model, proposing a mechanism of bubble formation. Several other bubble models followed. The pathophysiology of decompression sickness is not yet fully understood, but decompression practice has reached a stage where the risk is fairly low, and most incidents are successfully treated by therapeutic recompression and hyperbaric oxygen therapy. Mixed breathing gases are used to reduce the effects of the hyperbaric environment on ambient pressure divers.\nEfficient decompression requires the diver to ascend fast enough to establish as high a decompression gradient, in as many tissues as safely possible, without provoking the development of symptomatic bubbles. This is facilitated by the highest acceptably safe oxygen partial pressure in the breathing gas, and avoiding gas changes that could cause counterdiffusion bubble formation or growth. The development of schedules that are both safe and efficient has been complicated by the large number of variables and uncertainties, including personal variation in response under varying environmental conditions and workload.\n\nDiving environment\nThe diving environment is limited by accessibility and risk, but includes water and occasionally other liquids. Most underwater diving is done in the shallower coastal parts of the oceans, and inland bodies of fresh water, including lakes, dams, quarries, rivers, springs, flooded caves, reservoirs, tanks, swimming pools, and canals, but may also be done in large bore ducting and sewers, power station cooling systems, cargo and ballast tanks of ships, and liquid-filled industrial equipment. The environment may affect gear configuration: for instance, freshwater is less dense than saltwater, so less added weight is needed to achieve diver neutral buoyancy in freshwater dives. Water temperature, visibility and movement also affect the diver and the dive plan. Diving in liquids other than water may present special problems due to density, viscosity and chemical compatibility of diving equipment, as well as possible environmental hazards to the diving team.\nBenign conditions, sometimes also referred to as confined water, are environments of low risk, where it is extremely unlikely or impossible for the diver to get lost or entrapped, or be exposed to hazards other than the basic underwater environment. These conditions are suitable for initial training in the critical survival skills, and include swimming pools, training tanks, aquarium tanks and some shallow and protected shoreline areas.\nOpen water is unrestricted water such as a sea, lake or flooded quarry, where the diver has unobstructed direct vertical access to the surface of the water in contact with the atmosphere. Open-water diving implies that if a problem arises, the diver can directly ascend vertically to the atmosphere to breathe air. Wall diving is done along a near vertical face. Blue-water diving is done in good visibility in mid-water where the bottom is out of sight of the diver and there may be no fixed visual reference. Black-water diving is mid-water diving at night, particularly on a moonless night.\nAn overhead or penetration diving environment is where the diver enters a space from which there is no direct, purely vertical ascent to the safety of breathable atmosphere at the surface. Cave diving, wreck diving, ice diving and diving inside or under other natural or artificial underwater structures or enclosures are examples. The restriction on direct ascent increases the risk of diving under an overhead, and this is usually addressed by adaptations of procedures and use of equipment such as redundant breathing gas sources and guide lines to indicate the route to the exit.\nNight diving can allow the diver to experience a different underwater environment, because many marine animals are nocturnal. Altitude diving, for example in mountain lakes, requires modifications to the decompression schedule because of the reduced atmospheric pressure.\n\nDepth range\nThe recreational diving depth limit set by the EN 14153-2 \/ ISO 24801-2 level 2 \"Autonomous Diver \" standard is 20 metres (66 ft). The recommended depth limit for more extensively trained recreational divers ranges from 30 metres (98 ft) for PADI divers, (this is the depth at which nitrogen narcosis symptoms generally begin to be noticeable in adults), to 40 metres (130 ft) specified by Recreational Scuba Training Council, 50 metres (160 ft) for divers of the British Sub-Aqua Club and Sub-Aqua Association breathing air, and 60 metres (200 ft) for teams of 2 to 3 French Level 3 recreational divers, breathing air.\nFor technical divers, the recommended maximum depths are greater on the understanding that they will use less narcotic gas mixtures. 100 metres (330 ft) is the maximum depth authorised for divers who have completed Trimix Diver certification with IANTD or Advanced Trimix Diver certification with TDI. 332 metres (1,089 ft) is the world record depth on scuba (2014). Commercial divers using saturation techniques and heliox breathing gases routinely exceed 100 metres (330 ft), but they are also limited by physiological constraints. Comex Hydra 8 experimental dives reached a record open water depth of 534 metres (1,752 ft) in 1988. Atmospheric pressure diving suits are mainly constrained by the technology of the articulation seals, and a US Navy diver has dived to 610 metres (2,000 ft) in one.\n\nDive sites\nThe common term for a place at which one may dive is a dive site. As a general rule, professional diving is done where the work needs to be done, and recreational diving is done where conditions are suitable. There are many recorded and publicised recreational dive sites which are known for their convenience, points of interest, and frequently favourable conditions. Diver training facilities for both professional and recreational divers generally use a small range of dive sites which are familiar and convenient, and where conditions are predictable and the environmental risk is relatively low.\n\nDiving procedures\nDue to the inherent risks of the environment and the necessity to operate the equipment correctly, both under normal conditions and during incidents where failure to respond appropriately and quickly can have fatal consequences, standard procedures are used in preparation of the equipment, preparation to dive, during the dive if all goes according to plan, after the dive, and in the event of a reasonably foreseeable contingency. The standard procedures are not necessarily the only course of action that will have a satisfactory outcome, but they are generally procedures which have been found by experiment and experience to work well and reliably when applied in response to the given circumstances. All formal diver training is based on the learning of standard skills and procedures, and in many cases the over-learning of critical skills until the procedures can be performed without hesitation even when distracting circumstances exist. Where reasonably practicable, checklists may be used to ensure that preparatory procedures are carried out in the correct sequence and that no steps are inadvertently omitted.\nSome procedures are common to all manned modes of diving, but most are specific to the mode of diving and many are specific to the equipment in use. Diving procedures are those which are directly relevant to diving safety and efficiency, but do not include task specific skills. Standard procedures are particularly helpful where communication is by hand or rope signal \u2013 the hand and line signals are examples of standard procedures themselves \u2013 as the communicating parties have a better idea of what the other is likely to do in response. Where voice communication is available, standardised communications protocol reduces the time needed to convey necessary information and the error rate in transmission.\nDiving procedures generally involve the correct application of the appropriate diving skills in response to the current circumstances, and range from selecting and testing equipment to suit the diver and the dive plan, to the rescue of oneself or another diver in a life-threatening emergency. In many cases, what might be a life-threatening emergency to an untrained or inadequately skilled diver, is a mere annoyance and minor distraction to a skilled diver who applies the correct procedure without hesitation. Professional diving operations tend to adhere more rigidly to standard operating procedures than recreational divers, who are not legally or contractually obliged to follow them, but the prevalence of diving accidents is known to be strongly correlated to human error, which is more common in divers with less training and experience. The Doing It Right philosophy of technical diving is strongly supportive of common standard procedures for all members of a dive team, and prescribes the procedures and equipment configuration which may affect procedures to the members of their organisations.\nThe terms diving skills and diving procedures are largely interchangeable, but a procedure may require the ordered application of several skills, and is a broader term. A procedure may also conditionally branch or require repeated applications of a skill, depending on circumstances. Diver training is structured around the learning and practice of standard procedures until the diver is assessed as competent to apply them reliably in reasonably foreseeable circumstances, and the certification issued limits the diver to environments and equipment that are compatible with their training and assessed skill levels. The teaching and assessment of diving skills and procedures is often restricted to registered instructors, who have been assessed as competent to teach and assess those skills by the certification or registration agency, who take the responsibility of declaring the diver competent against their assessment criteria. The teaching and assessment of other task oriented skills does not generally require a diving instructor.\nThere is considerable difference in the diving procedures of professional divers, where a diving team with formally appointed members in specific roles and with recognised competence is required by law, and recreational diving, where in most jurisdictions the diver is not constrained by specific laws, and in many cases is not required to provide any evidence of competence.\n\nDiver training\nUnderwater diver training is normally given by a qualified instructor who is a member of one of many diver training agencies or is registered with a government agency. Basic diver training entails the learning of skills required for the safe conduct of activities in an underwater environment, and includes procedures and skills for the use of diving equipment, safety, emergency self-help and rescue procedures, dive planning, and use of dive tables. Diving hand signals are used to communicate underwater. Professional divers will also learn other methods of communication.\nAn entry level diver must learn the techniques of breathing underwater through a demand regulator, including clearing it of water and recovering it if dislodged from the mouth, and clearing the mask if it is flooded. These are critical survival skills, and if not competent the diver is at a high risk of drowning. A related skill is sharing breathing gas with another diver, both as the donor and the recipient. This is usually done with a secondary demand valve carried for this purpose. Technical and professional divers will also learn how to use a backup gas supply carried in an independent scuba set, known as the emergency gas supply or bailout cylinder.\nTo avoid injury during descent, divers must be competent at equalising the ears, sinuses and mask; they must also learn not to hold their breath while ascending, to avoid barotrauma of the lungs. The speed of ascent must be controlled to avoid decompression sickness, which requires buoyancy control skills. Good buoyancy control and trim also allow the diver to manoeuvre and move about safely, comfortably and efficiently, using swimfins for propulsion.\nSome knowledge of physiology and the physics of diving is considered necessary by most diver certification agencies, as the diving environment is alien and relatively hostile to humans. The physics and physiology knowledge required is fairly basic, and helps the diver to understand the effects of the diving environment so that informed acceptance of the associated risks is possible. The physics mostly relates to gases under pressure, buoyancy, heat loss, and light underwater. The physiology relates the physics to the effects on the human body, to provide a basic understanding of the causes and risks of barotrauma, decompression sickness, gas toxicity, hypothermia, drowning and sensory variations. More advanced training often involves first aid and rescue skills, skills related to specialised diving equipment, and underwater work skills. Further training is required to develop the skills necessary for diving in a wider range of environments, with specialised equipment, and to become competent to perform a variety of underwater tasks.\n\nMedical aspects of diving\nThe medical aspects of diving and hyperbaric exposure include examination of divers to establish medical fitness to dive, diagnosis and treatment of diving disorders, treatment by recompression and hyperbaric oxygen therapy, toxic effects of gases in a hyperbaric environment, and treatment of injuries incurred while diving which are not directly associated with immersion, depth, or pressure.\n\nFitness to dive\nMedical fitness to dive is the medical and physical suitability of a diver to function safely in the underwater environment using underwater diving equipment and procedures. As a general principle, fitness to dive is dependent on the absence of conditions which would constitute an unacceptable risk for the diver, and for professional divers, to any member of the diving team. General physical fitness requirements are also often specified by a certifying agency, and are usually related to ability to swim and perform the activities that are associated with the relevant type of diving. The general hazards of diving are much the same for recreational divers and professional divers, but the risks vary with the diving procedures used. These risks are reduced by appropriate skills and equipment. Medical fitness to dive generally implies that the diver has no known medical conditions that limit the ability to do the job or jeopardise the safety of the diver or the team, that might get worse as an consequence of diving, or unacceptably predispose the diver to diving or occupational illness.\nDepending on the circumstances, fitness to dive may be established by a signed statement by the diver that he or she does not suffer from any of the disqualifying conditions and is able to manage the ordinary physical requirements of diving, by a detailed medical examination by a physician registered as a medical examiner of divers following a prescribed procedural checklist, attested by a legal document of fitness to dive issued by the medical examiner and recorded on a national database, or by alternatives between these extremes.\nPsychological fitness to dive is not normally evaluated before recreational or commercial diver training, but can influence the safety and success of a diving career.\n\nDiving medicine\nDiving medicine is the diagnosis, treatment and prevention of conditions caused by exposing divers to the underwater environment. It includes the effects of pressure on gas filled spaces in and in contact with the body, and of partial pressures of breathing gas components, the diagnosis and treatment of conditions caused by marine hazards and how fitness to dive and the side effects of drugs used to treat other conditions affects a diver's safety. Hyperbaric medicine is another field associated with diving, since recompression in a hyperbaric chamber with hyperbaric oxygen therapy is the definitive treatment for two of the most important diving-related illnesses, decompression sickness and arterial gas embolism.\nDiving medicine deals with medical research on issues of diving, the prevention of diving disorders, treatment of diving accident injuries and diving fitness. The field includes the effect on the human body of breathing gases and their contaminants under high pressure, and the relationship between the state of physical and psychological health of the diver and safety. In diving accidents it is common for multiple disorders to occur together and interact with each other, both causatively and as complications. Diving medicine is a branch of occupational medicine and sports medicine, and first aid and recognition of symptoms of diving disorders are important parts of diver education.\n\nRisks and safety\nRisk is a combination of hazard, vulnerability and likelihood of occurrence, which can be the probability of a specific undesirable consequence of a hazard, or the combined probability of undesirable consequences of all the hazards of an activity.\nThe presence of a combination of several hazards simultaneously is common in diving, and the effect is generally increased risk to the diver, particularly where the occurrence of an incident due to one hazard triggers other hazards with a resulting cascade of incidents. Many diving fatalities are the result of a cascade of incidents overwhelming the diver, who should be able to manage any single reasonably foreseeable incident and its probable direct consequences.\nCommercial diving operations may expose the diver to more and sometimes greater hazards than recreational diving, but the associated occupational health and safety legislation is less tolerant of risk than recreational, particularly technical divers, may be prepared to accept. Commercial diving operations are also constrained by the physical realities of the operating environment, and expensive engineering solutions are often necessary to control risk. A formal hazard identification and risk assessment is a standard and required part of the planning for a commercial diving operation, and this is also the case for offshore diving operations. The occupation is inherently hazardous, and great effort and expense are routinely incurred to keep the risk within an acceptable range. The standard methods of reducing risk are followed where possible.\nStatistics on injuries related to commercial diving are normally collected by national regulators. In the UK the Health and Safety Executive (HSE) is responsible for the overview of about 5,000 commercial divers; in Norway the corresponding authority is the Petroleum Safety Authority Norway (PSA), which has maintained the DSYS database since 1985, gathering statistics on over 50,000 diver-hours of commercial activity per year. The risks of dying during recreational, scientific or commercial diving are small, and for scuba diving, deaths are usually associated with poor gas management, poor buoyancy control, equipment misuse, entrapment, rough water conditions and pre-existing health problems. Some fatalities are inevitable and caused by unforeseeable situations escalating out of control, but the majority of diving fatalities can be attributed to human error on the part of the victim. During 2006 to 2015 there were an estimated 306 million recreational dives made by US residents and 563 recreational diving deaths from this population. The fatality rate was 1.8 per million recreational dives, and 47 deaths for every 1000 emergency department presentations for scuba injuries.\nScuba diving fatalities have a major financial impact by way of lost income, lost business, insurance premium increases and high litigation costs. Equipment failure is rare in open circuit scuba, and when the cause of death is recorded as drowning, it is usually the consequence of an uncontrollable series of events in which drowning is the endpoint because it occurred in water, while the initial cause remains unknown. Where the triggering event is known, it is most commonly a shortage of breathing gas, followed by buoyancy problems. Air embolism is also frequently cited as a cause of death, often as a consequence of other factors leading to an uncontrolled and badly managed ascent, occasionally aggravated by medical conditions. About a quarter of diving fatalities are associated with cardiac events, mostly in older divers. There is a fairly large body of data on diving fatalities, but in many cases the data are poor due to the standard of investigation and reporting. This hinders research which could improve diver safety.\nArtisanal fishermen and gatherers of marine organisms in less developed countries may expose themselves to relatively high risk using diving equipment if they do not understand the physiological hazards, particularly if they use inadequate equipment.\n\nDiving hazards\nDivers operate in an environment for which the human body is not well suited. They face special physical and health risks when they go underwater or use high pressure breathing gas. The consequences of diving incidents range from merely annoying to rapidly fatal, and the result often depends on the equipment, skill, response and fitness of the diver and diving team. The hazards include the aquatic environment, the use of breathing equipment in an underwater environment, exposure to a pressurised environment and pressure changes, particularly pressure changes during descent and ascent, and breathing gases at high ambient pressure. Diving equipment other than breathing apparatus is usually reliable, but has been known to fail, and loss of buoyancy control or thermal protection can be a major burden which may lead to more serious problems. There are also hazards of the specific diving environment, which include strong water movement and local pressure differentials, and hazards related to access to and egress from the water, which vary from place to place, and may also vary with time. Hazards inherent in the diver include pre-existing physiological and psychological conditions and the personal behaviour and competence of the individual. For those pursuing other activities while diving, there are additional hazards of task loading, of the dive task and of special equipment associated with the task.\n\nHuman factors\nThe major factors influencing diving safety are the environment, the diving equipment and the performance of the diver and the dive team. The underwater environment is alien, both physically and psychologically stressful, and usually not amenable to control, though divers can be selective of the conditions in which they are willing to dive. The other factors must be controlled to mitigate the overall stress on the diver and allow the dive to be completed in acceptable safety. The equipment is critical to diver safety for life support, but is generally reliable, controllable and predictable in its performance.\nHuman factors are the physical or cognitive properties of individuals, or social behaviour specific to humans, which influence functioning of technological systems as well as human-environment equilibrium. Human error is inevitable and everyone makes mistakes at some time, and the consequences of these errors are varied and depend on many factors. Most errors are minor and do not cause harm, but in a high risk environment, such as in diving, errors are more likely to have catastrophic consequences. Examples of human error leading to accidents are available in vast numbers, as it is the direct cause of 60% to 80% of all accidents. Human error and panic are considered to be the leading causes of diving accidents and fatalities. A study by William P. Morgan indicates that over half of all divers in the survey had experienced panic underwater at some time during their diving career, and these findings were independently corroborated by a survey that suggested 65% of recreational divers have panicked under water. Panic frequently leads to errors in a diver's judgement or performance, and may result in an accident. The safety of underwater diving operations can be improved by reducing the frequency of human error and the consequences when it does occur.\nOnly 4.46% of the recreational diving fatalities in a 1997 study were attributable to a single contributory cause. The remaining fatalities probably arose as a result of a progressive sequence of events involving two or more procedural errors or equipment failures, and since procedural errors are generally avoidable by a well-trained, intelligent and alert diver, working in an organised structure, and not under excessive stress, it was concluded that the low accident rate in professional scuba diving is due to this factor. The study also concluded that it would be impossible to eliminate all minor contraindications of scuba diving, as this would result in overwhelming bureaucracy and bring all diving to a halt.\nHuman factors in diving equipment design is the influence of the interaction between the diver and the equipment on the design of the equipment on which the diver relies to stay alive and in reasonable comfort, and to perform the planned tasks during a dive. The design of the equipment can strongly influence its effectiveness in performing the desired functions. Divers vary considerably in anthropometric dimensions, physical strength, joint flexibility, and other physiological characteristics within the range of acceptable fitness to dive. Diving equipment should allow as full a range of function as reasonably practicable, and should be matched to the diver, the environment, and the task. Diving support equipment is usually shared by a wide range of divers, and must work for them all.\nThe most difficult stages of a dive for recreational divers are out of water activities and transitions between water and the surface site such as carrying equipment on shore, exiting from water to boat and shore, surface swimming, and dressing into the equipment. Safety and reliability, adjustability to fit the individual, performance, and simplicity were rated the most important features for diving equipment by recreational divers. The professional diver is supported by a surface team, who are available to assist with the out-of-water activities to the extent necessary to reduce the risk associated with them to a level acceptable in terms of the governing regulations and codes of practice.\n\nRisk management\nRisk management of diving operations involves the usual measures of engineering controls, administrative controls and procedures, and personal protective equipment, including hazard identification and risk assessment (HIRA), protective equipment, medical screening, training and standardised procedures. Professional divers are generally legally obliged to carry out and formally record these measures, and though recreational divers are not legally required to do many of them, competent recreational divers, and particularly technical divers, generally perform them informally but routinely, and they are an important part of technical diver training. For example, a medical statement or examination for fitness, pre-dive site assessment and briefing, safety drills, thermal protection, equipment redundancy, alternative air source, buddy checks, buddy or team diving procedures, dive planning, use of dive computers to monitor and record the dive profile and decompression status, underwater hand signals, and carrying first aid and oxygen administration equipment are all routinely part of technical diving.\n\nLegal aspects\nInshore and inland commercial and military diving is regulated by legislation in many countries. Responsibility of the employer, client and diving personnel is specified in these cases;  offshore commercial diving may take place in international waters, and is often done following the guidelines of a voluntary membership organisation such as the International Marine Contractors Association (IMCA), which publishes codes of accepted best practice which their member organisations are expected to follow.\nRecreational diver training and dive leading are industry regulated in some countries, and only directly regulated by government in a subset of them. In the UK, HSE legislation includes recreational diver training and dive leading for reward; in the US and South Africa industry regulation is accepted, though non-specific health and safety legislation still applies. In Israel recreational diving activities are regulated by the Recreational Diving Act, 1979.\nThe legal responsibility for recreational diving service providers is usually limited as far as possible by waivers which they require the customer to sign before engaging in any diving activity. The extent of duty of care of recreational buddy divers is unclear and has been the subject of considerable litigation. It is probable that it varies between jurisdictions. In spite of this lack of clarity, buddy diving is recommended by recreational diver training agencies as safer than solo diving, and some service providers insist that customers dive in buddy pairs.\n\nEconomic aspects\nScuba diving tourism is the industry based on servicing the requirements of recreational divers at destinations other than where they live. It includes aspects of training, equipment sales, rental and service, guided experiences and environmental tourism. Provision of transport to and from dive sites without convenient shore entry may be provided by basic, day excursion, and live-aboard dive boats.\nMotivations to travel for scuba diving are complex and may vary considerably during the diver's development and experience. Participation can vary from once off to multiple dedicated trips per year over several decades. The popular destinations fall into several groups, including tropical reefs, shipwrecks and cave systems, each frequented by its own group of enthusiasts, with some overlap. Customer satisfaction is largely dependent on the quality of services provided, and personal communication has a strong influence on the popularity of specific service providers in a region.\nProfessional diving includes a wide range of applications, of varying economic impact. All of them are in support of specific sectors of industry, commerce, defence, or public service, and their economic impacts are closely related to their importance to the relevant sector, and their effects on the diving equipment manufacturing and support industries. The importance of diving to the scientific community is not well recorded, but analysis of publications shows that diving supports scientific research largely through efficient and targeted sampling.\nMost modes of diving are equipment intensive, and much of the equipment is either life-support or specialised equipment for the application. This has led to a manufacturing industry in support of both recreational and professional diving, where developments in one mode often find applications in another. In terms of total numbers of divers, the recreational diving industry has a far larger market, but the costs of equipment and relatively large manning requirements of professional diving make that market substantial in its own right. The international Diving Equipment and Marketing Association, (DEMA), exists to promote the scuba diving and snorkeling industry.\n\nDemographics\nThe number of active scuba divers is not recorded systematically, but has been estimated on occasions with varying levels of confidence. One of the problems is the lack of a generally accepted definition of what constitutes an active scuba diver. The situation for freedivers and snorkelers is even less clear, as most freedivers have no qualification registered anywhere.\nThe Diving Equipment and Marketing Association (DEMA) estimate from 2.5 to 3.5 million active scuba divers in the US and up to 6 million worldwide, about 11 million snorkelers in the US, and about 20 million snorkelers worldwide. The Sports and Fitness Industry Association (SFIA) reported 2,351,000 casual participants, and 823,000 core participants in 2019, also in the US. Divers Alert Network (DAN), reported 2019 membership numbers worldwide: DAN US\/Canada, 274,708; DAN Europe, 123,680; DAN Japan, 18,137; DAN World Asia Pacific, 12,163; DAN World Latin America\/Brazil, 8,008; DAN South Africa, 5,894.\nThe active US scuba diving population could be fewer than 1,000,000, possibly as low as 500,000, depending on the definition of active. Numbers outside the US are less clear. This may be compared with PADI worldwide statistics for 2021, in which they claim to have issued more than 28 million diver certifications since 1967.\nEntry of non-divers through certification courses also provides an indicator of numbers, though there is no record of whether a diver remains active after certification unless further training is registered. Three training and certification agencies \u2013 Professional Association of Diving Instructors (PADI), Scuba Diving International (SDI), and Scuba Schools International (SSI) reported a combined average of 22,325 entry-level certifications per quarter. Estimating the number of active scuba instructors in the US and internationally is also difficult. Over 300 individual certifying agencies train and certify divers, dive leaders, and instructors, but there are also an unknown number of instructors who are registered with more than one agency. PADI reported 137,000 professional members (instructors and divemasters) worldwide in 2019. On the assumption that PADI represents 70% of the market share, the number of instructors globally may be about 195,000.\nThe American Academy of Underwater Sciences (AAUS) reports 4,500 divers at 150 organisational member scientific diving programmes (2020), and the Centers for Disease Control and Prevention (CDC) and Bureau of Labor Statistics reported 3,380 commercial divers in the US (2018). The number of active public safety divers in the US is also uncertain, but estimated to be between 3,000 and 5,000 in 2019.\n\nEnvironmental impact\nThe environmental impact of recreational diving is the effects of diving tourism on the marine environment. Usually these are considered to be adverse effects, and include damage to reef organisms by incompetent and ignorant divers, but there may also be positive effects as the environment is recognised by the local communities to be worth more in good condition than degraded by inappropriate use, which encourages conservation efforts. During the 20th century recreational scuba diving was considered to have generally low environmental impact, and was consequently one of the activities permitted in most marine protected areas. Since the 1970s diving has changed from an elite activity to a more accessible recreation, marketed to a very wide demographic. To some extent better equipment has been substituted for more rigorous training, and the reduction in perceived risk has shortened minimum training requirements by several training agencies. Training has concentrated on an acceptable risk to the diver, and paid less attention to the environment. The increase in the popularity of diving and in tourist access to sensitive ecological systems has led to the recognition that the activity can have significant environmental consequences.\nRecreational scuba diving has grown in popularity during the 21st century, as is shown by the number of certifications issued worldwide, which has increased to about 23 million by 2016 at about one million per year. Scuba diving tourism is a growth industry, and it is necessary to consider environmental sustainability, as the expanding impact of divers can adversely affect the marine environment in several ways, and the impact also depends on the specific environment. Tropical coral reefs are more easily damaged by poor diving skills than some temperate reefs, where the environment is more robust due to rougher sea conditions and fewer fragile, slow-growing organisms. The same pleasant sea conditions that allow development of relatively delicate and highly diverse ecologies also attract the greatest number of tourists, including divers who dive infrequently, exclusively on vacation and never fully develop the skills to dive in an environmentally friendly way. Low impact diving training has been shown to be effective in reducing diver contact.\nThe ecological impact of commercial diving is a small part of the impact of the specific industry supported by the diving operations, as commercial diving is not done in isolation. In most cases the impact of diving operations is insignificant in comparison with the overall project, and environmental impact assessments may be required before the project is authorised for some classes of project. Underwater ships husbandry may be an exception to this general tendency, and specific precautions to limit ecological impact may be required. Several of these operations will release some quantity of harmful material into the water, particularly hull cleaning operations which will release antifouling toxins. Alien biofouling organisms may also be released during this process.:\u200a15\u200a Other forms of professional diving, such as scientific and archaeological dives, are planned to minimise impact, which may be a condition for the application for a permit.\n\nNotes\nReferences\nSources\nBennett, Peter B; Rostain, Jean Claude (2003). \"The High Pressure Nervous Syndrome\". In Brubakk, Alf O.; Neuman, Tom S. (eds.). Bennett and Elliott's physiology and medicine of diving, 5th Rev ed. United States: Saunders. pp. 323\u201357. ISBN 978-0-7020-2571-6.\nUS Navy Diving Manual, 6th revision. Washington, DC.: US Naval Sea Systems Command. 2006.\nJoiner, James T, ed. (28 February 2001). NOAA Diving Manual, Diving for Science and Technology (4th ed.). Silver Spring, Maryland: National Oceanic and Atmospheric Administration, Office of Oceanic and Atmospheric Research, National Undersea Research Program. ISBN 978-0-941332-70-5. CD-ROM prepared and distributed by the National Technical Information Service (NTIS) in partnership with NOAA and Best Publishing Company\n\nFurther reading\nCousteau J.Y. (1953) Le Monde du Silence, translated as The Silent World, Hamish Hamilton Ltd., London; ASIN B000QRK890\nLang M.A. & Brubakk A.O. (eds., 2009) The Future of Diving: 100 Years of Haldane and Beyond, Smithsonian Institution Scholarly Press, Washington DC\n\nExternal links\n Media related to Underwater diving at Wikimedia Commons","190":"Usher syndrome, also known as Hallgren syndrome, Usher\u2013Hallgren syndrome, retinitis pigmentosa\u2013dysacusis syndrome or dystrophia retinae dysacusis syndrome, is a rare genetic disorder caused by a mutation in any one of at least 11 genes resulting in a combination of hearing loss and visual impairment. It is the most common cause of deafblindness and is at present incurable.\nUsher syndrome is classed into three subtypes (I, II and III) according to the genes responsible and the onset of deafness. All three subtypes are caused by mutations in genes involved in the function of the inner ear and retina. These mutations are inherited in an autosomal recessive pattern.\nThe occurrence of Usher syndrome varies across the world and across the different syndrome types, with rates as high as 1 in 12,500 in Germany to as low as 1 in 28,000 in Norway. Type I is most common in Ashkenazi Jewish and Acadian populations, and type III is rarely found outside Ashkenazi Jewish and Finnish populations. Usher syndrome is named after Scottish ophthalmologist Charles Usher, who examined the pathology and transmission of the syndrome in 1914.\n\nTypes\nUsher syndrome I\nPeople with Usher I are born profoundly deaf and begin to lose their vision in the first decade of life. They also exhibit balance difficulties and learn to walk slowly as children, due to problems in their vestibular system.\nUsher syndrome type I can be caused by mutations in any one of several different genes: CDH23, MYO7A, PCDH15, USH1C and USH1G. These genes function in the development and maintenance of inner ear structures such as hair cells (stereocilia), which transmit sound and motion signals to the brain. Alterations in these genes can cause an inability to maintain balance (vestibular dysfunction) and hearing loss. The genes also play a role in the development and stability of the retina by influencing the structure and function of both the rod photoreceptor cells and supporting cells called the retinal pigmented epithelium. Mutations that affect the normal function of these genes can result in retinitis pigmentosa and resultant vision loss.\nWorldwide, the estimated prevalence of Usher syndrome type I is 3 to 6 per 100,000 people in the general population. Type I has been found to be more common in people of Ashkenazi Jewish ancestry (central and eastern European) and in the French-Acadian populations (Louisiana). Among Acadians, research into haplotype data is consistent with one single mutation being responsible for all cases of Usher syndrome type I.\n\nUsher syndrome II\nPeople with Usher II are not born deaf and are generally hard-of-hearing rather than deaf, and their hearing does not degrade over time; moreover, they do not seem to have noticeable problems with balance. They also begin to lose their vision later (in the second decade of life) and may preserve some vision even into middle age.\nUsher syndrome type II may be caused by mutations in any of three different genes: USH2A, GPR98 and DFNB31. The protein encoded by the USH2A gene, usherin, is located in the supportive tissue in the inner ear and retina. Usherin is critical for the proper development and maintenance of these structures, which may help explain its role in hearing and vision loss. The location and function of the other two proteins are not yet known.\nUsher syndrome type II occurs at least as frequently as type I, but because type II may be underdiagnosed or more difficult to detect, it could be up to three times as common as type I.\n\nUsher syndrome III\nPeople with Usher syndrome III are not born deaf but experience a progressive loss of hearing, and roughly half have balance difficulties.\nMutations in only one gene, CLRN1, have been linked to Usher syndrome type III. CLRN1 encodes clarin-1, a protein important for the development and maintenance of the inner ear and retina. However, the protein's function in these structures, and how its mutation causes hearing and vision loss, is still poorly understood.\nThe frequency of Usher syndrome type III is significant only in the Finnish population as well as the population of Birmingham, UK, and individuals of Ashkenazi Jewish heritage. It has been noted rarely in a few other ethnic groups.\n\nSymptoms and signs\nUsher syndrome is characterized by hearing loss and a gradual visual impairment. The hearing loss is caused by a defective inner ear, whereas the vision loss results from retinitis pigmentosa (RP), a degeneration of the retinal cells. Usually, the rod cells of the retina are affected first, leading to early night blindness (nyctalopia) and the gradual loss of peripheral vision. In other cases, early degeneration of the cone cells in the macula occurs, leading to a loss of central acuity. In some cases, the foveal vision is spared, leading to \"doughnut vision\"; central and peripheral vision are intact, but an annulus exists around the central region in which vision is impaired.\n\nCause\nUsher syndrome is inherited in an autosomal recessive pattern. Several genes have been associated with Usher syndrome using linkage analysis of patient families (Table 1) and DNA sequencing of the identified loci. A mutation in any one of these genes is likely to result in Usher syndrome.\nThe clinical subtypes Usher I and II are associated with mutations in any one of six (USH1B-G) and three (USH2A, C-D) genes, respectively, whereas only one gene, USH3A, has been linked to Usher III so far. Two other genes, USH1A and USH2B, were initially associated with Usher syndrome, but USH2B has not been verified and USH1A was incorrectly determined and does not exist. Research in this area is ongoing.\nUsing interaction analysis techniques, the identified gene products could be shown to interact with one another in one or more larger protein complexes. If one of the components is missing, this protein complex cannot fulfil its function in the living cell, and it probably comes to the degeneration the same. The function of this protein complex has been suggested to participate in the signal transduction or in the cell adhesion of sensory cells.\nA study shows that three proteins related to Usher syndrome genes (PCDH15, CDH23, GPR98) are also involved in auditory cortex development, in mouse and macaque. Their lack of expression induces a decrease in the number of parvalbumin interneurons. Patients with mutations for these genes could have consequently auditory cortex defects.\n\nPathophysiology\nThe progressive blindness of Usher syndrome results from retinitis pigmentosa. The photoreceptor cells usually start to degenerate from the outer periphery to the center of the retina, including the macula. The degeneration is usually first noticed as night blindness (nyctalopia); peripheral vision is gradually lost, restricting the visual field (tunnel vision), which generally progresses to complete blindness. The qualifier pigmentosa reflects the fact that clumps of pigment may be visible by an ophthalmoscope in advanced stages of degeneration.\nThe hearing impairment associated with Usher syndrome is caused by damaged hair cells in the cochlea of the inner ear inhibiting electrical impulses from reaching the brain. It is a form of dysacusis.\n\nDiagnosis\nSince Usher syndrome is incurable at present, it is helpful to diagnose children well before they develop the characteristic night blindness. Some preliminary studies have suggested as many as 10% of children with congenital severe to profound deafness may have Usher syndrome. However, a misdiagnosis can have bad consequences.\nThe simplest approach to diagnosing Usher syndrome is to test for the characteristic chromosomal mutations. An alternative approach is electroretinography, although this is often disfavored for children, since its discomfort can also make the results unreliable. Parental consanguinity is a significant factor in diagnosis. Usher syndrome I may be indicated if the child is profoundly deaf from birth and especially slow in walking.\nThirteen other syndromes may exhibit signs similar to Usher syndrome, including Alport syndrome, Alstr\u00f6m syndrome, Bardet\u2013Biedl syndrome, Cockayne syndrome, spondyloepiphyseal dysplasia congenita, Flynn\u2013Aird syndrome, Friedreich ataxia, Hurler syndrome (MPS-1), Kearns\u2013Sayre syndrome (CPEO), Norrie syndrome, osteopetrosis (Albers\u2013Schonberg disease), Refsum disease (phytanic acid storage disease) and Zellweger syndrome (cerebrohepatorenal syndrome).\n\nClassification\nAlthough Usher syndrome has been classified clinically in several ways, the prevailing approach is to classify it into three clinical sub-types called Usher I, II and III in order of decreasing severity of deafness. Although it was previously believed that there was an Usher syndrome type IV, researchers at the University of Iowa recently confirmed that there is no USH type IV. As described below, these clinical subtypes may be further subdivided by the particular gene mutated; people with Usher I and II may have any one of six and three genes mutated, respectively, whereas only one gene has been associated with Usher III. The function of these genes is still poorly understood.\nUsher syndrome is a variable condition; the degree of severity is not tightly linked to whether it is Usher I, II or III. For example, someone with type III may be unaffected in childhood but go on to develop a profound hearing loss and a very significant loss of sight by early-to-mid adulthood. Similarly, someone with type I, who is therefore profoundly deaf from birth, may keep good central vision until the sixth decade of life or even beyond. People with type II, who have useful hearing with a hearing aid, can experience a wide range of severity of the RP. Some may maintain good reading vision into their 60s, while others cannot see to read while still in their 40s.\nSince Usher syndrome is inherited in an autosomal recessive pattern, both males and females are equally likely to inherit it. Consanguinity of the parents is a risk factor.\n\nTreatment\nSince Usher syndrome results from the loss of a gene, gene therapy that adds the proper protein back (\"gene replacement\") may alleviate it, provided the added protein becomes functional. Recent studies of mouse models have shown one form of the disease\u2014that associated with a mutation in myosin VIIa\u2014can be alleviated by replacing the mutant gene using a lentivirus. However, some of the mutated genes associated with Usher syndrome encode very large proteins\u2014most notably, the USH2A and GPR98 proteins, which have roughly 6000 amino-acid residues. Scientists have successfully treated mice with Usher syndrome type 1C, which has a relatively small affected gene.\n\nEpidemiology\nUsher syndrome is responsible for the majority of deafblindness. It occurs in roughly 1 in 23,000 people in the United States, 1 in 28,000 in Norway, and 1 in 12,500 in Germany. People with Usher syndrome represent roughly one-sixth of people with retinitis pigmentosa.\n\nHistory\nUsher syndrome is named after the Scottish ophthalmologist Charles Usher, who examined the pathology and transmission of this illness in 1914 on the basis of 69 cases. However, it was first described in 1858 by Albrecht von Gr\u00e4fe, a pioneer of modern ophthalmology. He reported the case of a deaf patient with retinitis pigmentosa, who had two brothers with the same symptoms. Three years later, one of his students, Richard Liebreich, examined the population of Berlin for disease pattern of deafness with retinitis pigmentosa. Liebreich noted Usher syndrome to be recessive, since the cases of blind-deafness combinations occurred particularly in the siblings of blood-related marriages or in families with patients in different generations. His observations supplied the first proofs for the coupled transmission of blindness and deafness, since no isolated cases of either could be found in the family trees.\nAnimal models of this human disease (such as knockout mice and zebrafish) have been developed recently to study the effects of these gene mutations and to test potential cures for Usher syndrome.\n\nNotable cases\nRebecca Alexander, a psychotherapist, author, and recipient of the Helen Keller Achievement Award.\nChristine \"Coco\" Roschaert, director of the Nepal Deafblind Project, kick-off speaker for Deaf Awareness Week at the University of Vermont, and participant in the Gallaudet United Now Movement.\nCatherine Fischer wrote her autobiography of growing up with Usher syndrome in Louisiana, entitled Orchid of the Bayou.\nVendon Wright has written two books describing his life with Usher syndrome, I Was Blind but Now I Can See and Through My Eyes.\nChristian Markovic, and blind-deaf illustrator and designer; Fuzzy Wuzzy Designs.\nJohn Tracy, the son of actor Spencer Tracy and namesake of the oralist John Tracy Clinic.\nThe Israeli Nalaga'at (do touch) Deaf-blind Acting Ensemble consists of 11 deaf-blind actors, most of whom are diagnosed with Usher syndrome. The theater group has put on several productions and appeared both locally in Israel and abroad in London and Broadway.\nKatie Kelly, a gold medal-winning paralympian.\nTeigan Van Roosmalen, paraolympian.\nCyril Axelrod, Catholic priest.\nRobert Tarango, first deafblind person to star in a movie, in the role of Artie in the Oscar-nominated short film Feeling Through.\n\nReferences\nFurther reading\nStiefel SH, Lewis RA (1991). The Madness of Usher's: Coping With Vision and Hearing Loss\/Usher Syndrome Type II. Business of Living Publications. ISBN 978-1-879518-06-3.\nDuncan E, Prickett HT (1988). Usher's Syndrome: What It Is, How to Cope, and How to Help. Charles C. Thomas. ISBN 978-0-398-05481-6.\nVernon M (1986). Answers to your questions about Usher's syndrome (retinitis pigmentosa with hearing loss). Foundation Fighting Blindness. ASIN B00071QLJ6.\nVernon M (1969). Usher's syndrome: Deafness and progressive blindness : clinical cases, prevention, theory and literature survey. Pergamon Press. ASIN B0007JHOJ4.\n\nExternal links\n\nGeneReviews\/NCBI\/NIH\/UW entry on Usher Syndrome Type I\nGeneReviews\/NCBI\/NIH\/UW entry on Usher Syndrome Type II\nNCBI Genetic Testing Registry\nGeneral overview from the NIH\nUsher Syndrome Information from the National Institute on Deafness and Other Communication Disorders (NIDCD).","191":"Valproate (valproic acid, VPA, sodium valproate, and valproate semisodium forms) are medications primarily used to treat epilepsy and bipolar disorder and prevent migraine headaches. They are useful for the prevention of seizures in those with absence seizures, partial seizures, and generalized seizures. They can be given intravenously or by mouth, and the tablet forms exist in both long- and short-acting formulations.\nCommon side effects of valproate include nausea, vomiting, somnolence, and dry mouth. Serious side effects can include liver failure, and regular monitoring of liver function tests is therefore recommended. Other serious risks include pancreatitis and an increased suicide risk. Valproate is known to cause serious abnormalities in fetuses if taken during pregnancy, and is contra-indicated for women of childbearing age unless the drug is essential to their medical condition and the recipient is also prescribed a contraceptive. Reproductive warnings have also been issued for men using the drug. The United States Food and Drug Administration has indicated a black box warning given the frequency and severity of the side effects and teratogenicity. Additionally, there is also a black box warning due to risk of  hepatotoxicity and pancreatitis. As of 2022 the drug was still prescribed in the UK to potentially pregnant women, but use declined by 51% from 2018\u201319 to 2020\u201321.\nValproate's precise mechanism of action is unclear. Proposed mechanisms include affecting GABA levels, blocking voltage-gated sodium channels, inhibiting histone deacetylases, and increasing LEF1. Valproic acid is a branched short-chain fatty acid (SCFA), a derivative of valeric acid.\nValproate was originally synthesized in 1881 and came into medical use in 1962. It is on the World Health Organization's List of Essential Medicines and is available as a generic medication. In 2021, it was the 155th most commonly prescribed medication in the United States, with more than 3 million prescriptions.\n\nMedical uses\nIt is used primarily to treat epilepsy and bipolar disorder and to prevent migraine headaches.\n\nEpilepsy\nValproate has a broad spectrum of anticonvulsant activity, although it is primarily used as a first-line treatment for tonic\u2013clonic seizures, absence seizures and myoclonic seizures and as a second-line treatment for partial seizures and infantile spasms. It has also been successfully given intravenously to treat status epilepticus.\nIn the US, valproic acid is an anti-epileptic drug indicated for the treatment of manic episodes associated with bipolar disorder; monotherapy and adjunctive therapy of complex partial seizures and simple and complex absence seizures; adjunctive therapy in people with multiple seizure types that include absence seizures.\n\nMental illness\nValproate products are used to treat manic or mixed episodes of bipolar disorder.\nA 2016 systematic review compared the efficacy of valproate as an add-on for people with schizophrenia:\n\nOther neurological indications\nBased upon five case reports, valproic acid may have efficacy in controlling the symptoms of the dopamine dysregulation syndrome that arise from the treatment of Parkinson's disease with levodopa.\nValproate is also used to prevent migraine headaches.\n\nOther\nThe medication has been tested in the treatment of AIDS and cancer, owing to its histone-deacetylase-inhibiting effects. It has cardioprotective, kidney protective, antiinflammatory, and antimicrobial effects.\n\nContraindications\nContraindications include:\n\nPre-existing acute or chronic liver dysfunction or family history of severe liver inflammation (hepatitis), particularly medicine related.\nKnown hypersensitivity to valproate or any of the ingredients used in the preparation\nUrea cycle disorders\nHepatic porphyria\nHepatotoxicity\nMitochondrial disease\nPancreatitis\nPorphyria\nPregnancy (except when no other treatments are available for the treatment of epilepsy)\n\nAdverse effects\nValproic acid has a black box warning for hepatotoxicity, pancreatitis, and fetal abnormalities.\nThere is evidence that valproic acid may cause premature growth plate ossification in children and adolescents, resulting in decreased height. Valproic acid can also cause mydriasis, a dilation of the pupils. There is evidence that shows valproic acid may increase the chance of polycystic ovary syndrome (PCOS) in women with epilepsy or bipolar disorder. Studies have shown this risk of PCOS is higher in women with epilepsy compared to those with bipolar disorder. Weight gain is also possible.\n\nPregnancy\nValproate causes birth defects; exposure during pregnancy is associated with about three times as many major abnormalities as usual, mainly spina bifida with the risks being related to the strength of medication used and use of more than one drug. \"Fetal Valproate Syndrome\" (FVS) has been used to refer to the effects of valproate exposure in utero. However, similar to the discussion about the adverse effect of exposure to alcohol in utero (\"fetal alcohol spectrum disorder\"), a 2019 study proposed the term \"Fetal valproate spectrum disorder\" (FVSD) because valproate exposure can lead to a wide range of possible presentations, which can be influenced by various factors (including dosage and timing of exposure). The dysmorphic features associated with VPA exposure can be subtle and age-dependent, making it challenging to designate individuals as having the characteristic dysmorphism or not, especially for those with limited expertise in the area. While the presence of typical facial dysmorphism is suggestive of the condition, it is not required for diagnosis. This change in terminology to FVSD would benefit individuals affected by the neurodevelopmental effects of VPA exposure without significant malformations, since they can experience impairments in their everyday functioning similar to those with classical FVS. Characteristics of valproate syndrome may include facial features that tend to evolve with age, including a triangle-shaped forehead, tall forehead with bifrontal narrowing, epicanthic folds, medial deficiency of eyebrows, flat nasal bridge, broad nasal root, anteverted nares, shallow philtrum, long upper lip and thin vermillion borders, thick lower lip and small downturned mouth. While developmental delay is usually associated with altered physical characteristics (dysmorphic features), this is not always the case.\nChildren of mothers taking valproate during pregnancy are at risk for lower IQs. Maternal valproate use during pregnancy increased the probability of autism in the offspring compared to mothers not taking valproate from 1.5% to 4.4%. A 2005 study found rates of autism among children exposed to sodium valproate before birth in the cohort studied were 8.9%. The normal incidence for autism in the general population in 2018 was estimated at 1 in 44 (2.3%). An updated March 2023 report estimates the number increased to 1 in 36 in 2020 (approximately 4% of boys and 1% of girls). A 2009 study found that the 3-year-old children of pregnant women taking valproate had an IQ nine points lower than that of a well-matched control group. However, further research in older children and adults is needed.\nSodium valproate has been associated with paroxysmal tonic upgaze of childhood, also known as Ouvrier\u2013Billson syndrome, from childhood or fetal exposure. This condition resolved after discontinuing valproate therapy.\nWomen who intend to become pregnant should switch to a different medication if possible or decrease their dose of valproate. Women who become pregnant while taking valproate should be warned that it causes birth defects and cognitive impairment in the newborn, especially at high doses (although valproate is sometimes the only drug that can control seizures, and seizures in pregnancy could have worse outcomes for the fetus than exposure to valproate). Studies have shown that taking folic acid supplements can reduce the risk of congenital neural tube defects. The use of valproate for migraine or bipolar disorder during pregnancy is contraindicated in the European Union and the United States, and the medicines are not recommended for epilepsy during pregnancy unless there is no other effective treatment available.\n\nPaternal exposure\nA 2023 retrospective study of Norway Denmark and Sweden found a significantly increased risk of neurodevelopmental disabilities in the children of fathers exposed to valproate up to 3 months prior to conception, compared to offspring paternally exposed to lamotrigine\/levetiracetam.:\u200a9\u200a\nThis led the EMA to recommend \"the need to consider effective contraception, while using valproate and for at least 3 months after treatment  discontinuation. Male patients should not donate sperm during treatment and for at least 3 months after treatment discontinuation.\":\u200a26\n\nElderly\nValproate may cause increased somnolence in the elderly. In a trial of valproate in elderly patients with dementia, a significantly higher portion of valproate patients had somnolence compared to placebo. In approximately one-half of such patients, there was associated reduced nutritional intake and weight loss.\n\nOverdose and toxicity\nExcessive amounts of valproic acid can result in somnolence, tremor, stupor, respiratory depression, coma, metabolic acidosis, and death. In general, serum or plasma valproic acid concentrations are in a range of 20\u2013100 mg\/L during controlled therapy, but may reach 150\u20131500 mg\/L following acute poisoning. Monitoring of the serum level is often accomplished using commercial immunoassay techniques, although some laboratories employ gas or liquid chromatography.\nIn contrast to other antiepileptic drugs, at present there is little favorable evidence for salivary therapeutic drug monitoring. Salivary levels of valproic acid correlate poorly with serum levels, partly due to valproate's weak acid property (pKa of 4.9).\nIn severe intoxication, hemoperfusion or hemofiltration can be an effective means of hastening elimination of the drug from the body. Supportive therapy should be given to all patients experiencing an overdose and urine output should be monitored. Supplemental L-carnitine is indicated in patients having an acute overdose and also prophylactically in high risk patients. Acetyl-L-carnitine lowers hyperammonemia less markedly than L-carnitine.\n\nInteractions\nValproate inhibits CYP2C9, glucuronyl transferase, and epoxide hydrolase and is highly protein bound and hence may interact with drugs that are substrates for any of these enzymes or are highly protein bound themselves. It may also potentiate the CNS depressant effects of alcohol. It should not be given in conjunction with other antiepileptics due to the potential for reduced clearance of other antiepileptics (including carbamazepine, lamotrigine, phenytoin and phenobarbitone) and itself. It may also interact with:\n\nAspirin: may increase valproate concentrations. May also interfere with valproate's metabolism.\nBenzodiazepines: may cause CNS depression and there are possible pharmacokinetic interactions.\nCarbapenem antibiotics: reduce valproate levels, potentially leading to seizures.\nCimetidine: inhibits valproate's metabolism in the liver, leading to increased valproate concentrations.\nErythromycin: inhibits valproate's metabolism in the liver, leading to increased valproate concentrations.\nEthosuximide: valproate may increase ethosuximide concentrations and lead to toxicity.\nFelbamate: may increase plasma concentrations of valproate.\nMefloquine: may increase valproate metabolism combined with the direct epileptogenic effects of mefloquine.\nOral contraceptives: may reduce plasma concentrations of valproate.\nPrimidone: may accelerate metabolism of valproate, leading to a decline of serum levels and potential breakthrough seizure.\nRifampicin: increases the clearance of valproate, leading to decreased valproate concentrations\nWarfarin: valproate may increase free warfarin concentration and prolong bleeding time.\nZidovudine: valproate may increase zidovudine serum concentration and lead to toxicity.\n\nPharmacology\nPharmacodynamics\nAlthough the mechanism of action of valproate is not fully understood, traditionally, its anticonvulsant effect has been attributed to the blockade of voltage-gated sodium channels and increased brain levels of the inhibitory synaptic neurotransmitter gamma-aminobutyric acid (GABA). The GABAergic effect is also believed to contribute towards the anti-manic properties of valproate. In animals, sodium valproate raises cerebral and cerebellar levels of GABA, possibly by inhibiting GABA degradative enzymes, such as GABA transaminase, succinate-semialdehyde dehydrogenase and by inhibiting the re-uptake of GABA by neuronal cells.\nPrevention of neurotransmitter-induced hyperexcitability of nerve cells via Kv7.2 channel and AKAP5 may also contribute to its mechanism. Valproate has been shown to protect against a seizure-induced reduction in phosphatidylinositol (3,4,5)-trisphosphate (PIP3) as a potential therapeutic mechanism.\nValproate is a histone deacetylase inhibitor. By inhibition of histone deacetylase, it promotes more transcriptionally active chromatin structures, that is it exerts an epigenetic effect. This has been proven in mice: Valproic acid induced histone hyperacetylation had brain function effects on the next generation of mice through changes in sperm DNA methylation. Intermediate molecules include VEGF, BDNF, and GDNF.\n\nEndocrine actions\nValproic acid has been found to be an antagonist of the androgen and progesterone receptors, and hence as a nonsteroidal antiandrogen and antiprogestogen, at concentrations much lower than therapeutic serum levels. In addition, the drug has been identified as a potent aromatase inhibitor, and suppresses estrogen concentrations. These actions are likely to be involved in the reproductive endocrine disturbances seen with valproic acid treatment.\nValproic acid has been found to directly stimulate androgen biosynthesis in the gonads via inhibition of histone deacetylases and has been associated with hyperandrogenism in women and increased 4-androstenedione levels in men. High rates of polycystic ovary syndrome and menstrual disorders have also been observed in women treated with valproic acid.\n\nPharmacokinetics\nTaken by mouth, valproate is rapidly and virtually completely absorbed from the gut. When in the bloodstream, 80\u201390% of the substance are bound to plasma proteins, mainly albumin. Protein binding is saturable: it decreases with increasing valproate concentration, low albumin concentrations, the patient's age, additional use of other drugs such as aspirin, as well as liver and kidney impairment. Concentrations in the cerebrospinal fluid and in breast milk are 1 to 10% of blood plasma concentrations.\nThe vast majority of valproate metabolism occurs in the liver. Valproate is known to be metabolized by the cytochrome P450 enzymes CYP2A6, CYP2B6, CYP2C9, and CYP3A5. It is also known to be metabolized by the UDP-glucuronosyltransferase enzymes UGT1A3, UGT1A4, UGT1A6, UGT1A8, UGT1A9, UGT1A10, UGT2B7, and UGT2B15.  Some of the known metabolites of valproate by these enzymes and uncharacterized enzymes include (see image):\n\nvia glucuronidation (30\u201350%): valproic acid \u03b2-O-glucuronide\nvia beta oxidation (>40%): 2E-ene-valproic acid, 2Z-ene-valproic acid, 3-hydroxyvalproic acid, 3-oxovalproic acid\nvia omega oxidation: 5-hydroxyvalproic acid, 2-propyl-glutaric acid\nsome others: 3E-ene-valproic acid, 3Z-ene-valproic acid, 4-ene-valproic acid, 4-hydroxyvalproic acid\nAll in all, over 20 metabolites are known.\nIn adult patients taking valproate alone, 30\u201350% of an administered dose is excreted in urine as the glucuronide conjugate.  The other major pathway in the metabolism of valproate is mitochondrial beta oxidation, which typically accounts for over 40% of an administered dose. Typically, less than 20% of an administered dose is eliminated by other oxidative mechanisms. Less than 3% of an administered dose of valproate is excreted unchanged (i.e., as valproate) in urine. Only a small amount is excreted via the faeces. Elimination half-life is 16\u00b13 hours and can decrease to 4\u20139 hours when combined with enzyme inducers.\n\nChemistry\nValproic acid is a branched short-chain fatty acid and the 2-n-propyl derivative of valeric acid.\n\nHistory\nValproic acid was first synthesized in 1882 by Beverly S. Burton as an analogue of valeric acid, found naturally in valerian. Valproic acid is a carboxylic acid, a clear liquid at room temperature. For many decades, its only use was in laboratories as a \"metabolically inert\" solvent for organic compounds. In 1962, the French researcher Pierre Eymard serendipitously discovered the anticonvulsant properties of valproic acid while using it as a vehicle for a number of other compounds that were being screened for antiseizure activity. He found it prevented pentylenetetrazol-induced convulsions in laboratory rats. It was approved as an antiepileptic drug in 1967 in France and has become the most widely prescribed antiepileptic drug worldwide. Valproic acid has also been used for migraine prophylaxis and bipolar disorder.\n\nSociety and culture\nValproate is available as a generic medication.\n\nApproval status\nOff-label uses\nIn 2012, pharmaceutical company Abbott paid $1.6 billion in fines to US federal and state governments for illegal promotion of off-label uses for Depakote, including the sedation of elderly nursing home residents.\nSome studies have suggested that valproate may reopen the critical period for learning absolute pitch and possibly other skills such as language.\n\nFormulations\nValproate exists in two main molecular variants: sodium valproate and valproic acid without sodium (often implied by simply valproate). A mixture between these two is termed semisodium valproate. It is unclear whether there is any difference in efficacy between these variants, except from the fact that about 10% more mass of sodium valproate is needed than valproic acid without sodium to compensate for the sodium itself.\n\nTerminology\nValproate is a negative ion. The conjugate acid of valproate is valproic acid (VPA). Valproic acid is fully ionized into valproate at the physiologic pH of the human body, and valproate is the active form of the drug. Sodium valproate is the sodium salt of valproic acid. Divalproex sodium is a coordination complex composed of equal parts of valproic acid and sodium valproate.\n\nBrand names of valproic acid\nBranded products include:\n\nBrand names of sodium valproate\nPortugal\nTablets \u2013  Diplexil-R by Bial.\n\nUnited States\nIntravenous injection \u2013  Depacon by Abbott Laboratories.\nSyrup \u2013  Depakene by Abbott Laboratories. (Note: Depakene capsules are valproic acid).\nDepakote tablets are a mixture of sodium valproate and valproic acid.\nTablets \u2013  Eliaxim by Bial.\n\nAustralia\nEpilim Crushable Tablets Sanofi\nEpilim Sugar Free Liquid Sanofi\nEpilim Syrup Sanofi\nEpilim Tablets Sanofi\nSodium Valproate Sandoz Tablets Sanofi\nValpro Tablets Alphapharm\nValproate Winthrop Tablets Sanofi\nValprease tablets Sigma\n\nNew Zealand\nEpilim by Sanofi-Aventis\nAll the above formulations are Pharmac-subsidised.\n\nUK\nDepakote Tablets (as in USA)\nTablets \u2013  Orlept by Wockhardt and Epilim by Sanofi\nOral solution \u2013  Orlept Sugar Free by Wockhardt and Epilim by Sanofi\nSyrup \u2013  Epilim by Sanofi-Aventis\nIntravenous injection \u2013  Epilim Intravenous by Sanofi\nExtended release tablets \u2013  Epilim Chrono by Sanofi is a combination of sodium valproate and valproic acid in a 2.3:1 ratio.\nEnteric-coated tablets \u2013  Epilim EC200 by Sanofi is a 200 mg sodium valproate enteric-coated tablet.\n\nUK only\nCapsules \u2013  Episenta prolonged release by Beacon\nSachets \u2013  Episenta prolonged release by Beacon\nIntravenous solution for injection \u2013  Episenta solution for injection by Beacon\n\nGermany, Switzerland, Norway, Finland, Sweden\nTablets \u2013  Orfiril by Desitin Pharmaceuticals\nIntravenous injection \u2013  Orfiril IV by Desitin Pharmaceuticals\n\nSouth Africa\nSyrup \u2013  Convulex by Byk Madaus\nTablets \u2013  Epilim by Sanofi-synthelabo\n\nMalaysia\nTablets \u2013  Epilim (200 ENTERIC COATED) by Sanofi-Aventis\nControlled release tablets \u2013  Epilim Chrono (500 CONTROLLED RELEASE) by Sanofi-Aventis\n\nRomania\nCompanies are SANOFI-AVENTIS FRANCE, GEROT PHARMAZEUTIKA GMBH and DESITIN ARZNEIMITTEL GMBH\nTypes are Syrup, Extended release mini tablets, Gastric resistant coated tablets, Gastric resistant soft capsules, Extended release capsules, Extended release tablets and Extended release coated tablets\n\nCanada\nIntravenous injection \u2013  Epival or Epiject by Abbott Laboratories.\nSyrup \u2013  Depakene by Abbott Laboratories its generic formulations include Apo-Valproic and ratio-Valproic.\n\nJapan\nTablets \u2013  Depakene by Kyowa Hakko Kirin\nExtended release tablets \u2013  Depakene-R by Kyowa Hakko Kogyo and Selenica-R by Kowa\nSyrup \u2013  Depakene by Kyowa Hakko Kogyo\n\nEurope\nIn much of Europe, D\u00e9pakine and Depakine Chrono (tablets) are equivalent to Epilim and Epilim Chrono above.\n\nTaiwan\nTablets (white round tablet) \u2013  Depakine (Chinese: \u5e1d\u62d4\u7672; pinyin: di-ba-dian) by Sanofi Winthrop Industrie (France)\n\nIran\nTablets \u2013  Epival 200 (enteric coated tablet) and Epival 500 (extended release tablet) by Iran Najo\nSlow release tablets \u2013  Depakine Chrono by Sanofi Winthrop Industrie (France)\n\nIsrael\nDepalept and Depalept Chrono (extended release tablets) are equivalent to Epilim and Epilim Chrono above. Manufactured and distributed by Sanofi-Aventis.\n\nIndia, Russia and CIS countries\nValparin Chrono by Sanofi India\nValprol CR by Intas Pharmaceutical (India)\nEncorate Chrono by Sun Pharmaceutical (India)\nServen Chrono by Leeven APL Biotech (India)\n\nUruguay\nTablets \u2013  DI DPA by Megalabs\n\nBrand names of valproate semisodium\nBrazil \u2013  Depakote by Abbott Laboratories and Torval CR by Torrent do Brasil\nCanada \u2013  Epival by Abbott Laboratories\nMexico \u2013  Epival and Epival ER (extended release) by Abbott Laboratories\nUnited Kingdom \u2013  Depakote (for psychiatric conditions) and Epilim (for epilepsy) by Sanofi-Aventis and generics\nUnited States \u2013  Depakote and Depakote ER (extended release) by Abbott Laboratories and generics\nIndia \u2013  Valance and Valance OD by Abbott Healthcare Pvt Ltd, Divalid ER by Linux laboratories Pvt Ltd, Valex ER by Sigmund Promedica, Dicorate by Sun Pharma\nGermany \u2013  Ergenyl Chrono by Sanofi-Aventis and generics\nChile \u2013  Valcote and Valcote ER by Abbott Laboratories\nFrance and other European countries \u2013  Depakote\nPeru \u2013  Divalprax by AC Farma Laboratories\nChina \u2013  Diprate OD\n\nResearch\nA 2023 systematic review of the literature identified only one study in which valproate was evaluated in the treatment of seizures in infants aged 1 to 36 months. In a randomized control trial, valproate alone was found to show poorer outcomes for infants than valproate plus levetiracetam in terms of reduction of seizures, freedom from seizures, daily living ability, quality of life, and cognitive abilities.\n\n\n== References ==","192":"Vasospasm refers to a condition in which an arterial spasm leads to vasoconstriction. This can lead to tissue ischemia (insufficient blood flow) and tissue death (necrosis). Cerebral vasospasm may arise in the context of subarachnoid hemorrhage.  Symptomatic vasospasm or delayed cerebral ischemia is a major contributor to post-operative stroke and death especially after aneurysmal subarachnoid hemorrhage.  Vasospasm typically appears 4 to 10 days after subarachnoid hemorrhage.\nAlong with physical resistance, vasospasm is a main cause of ischemia. Like physical resistance, vasospasms can occur due to atherosclerosis. Vasospasm is the major cause of Prinzmetal's angina.\n\nPathophysiology\nNormally endothelial cells release prostacyclin and nitric oxide (NO) which induce relaxation of the smooth muscle cells, and reduce aggregation of platelets. Aggregating platelets stimulate ADP to act on endothelial cells and help them induce relaxation of the smooth muscle cells. However, aggregating platelets also stimulate thromboxane A2 and serotonin which can induce contraction of the smooth muscle cells.  In general, the relaxations outweighs the contractions.\nIn atherosclerosis, a dysfunctional endothelium is observed on examination. It does not stimulate as much prostacyclin and NO to induce relaxation on smooth muscle cells. Also there is not as much inhibition of aggregation of platelets. In this case, the greater aggregation of platelets produce ADP, serotonin, and thromboxane A2. However the serotonin and the thromboxane A2 cause more contraction of the smooth muscle cells and as a result contractions outweigh the relaxations.\n\nComplications\nVasospasm can occur in a wide variety of peripheral vascular beds under poorly understood mechanisms. Prinzmetal angina, Buerger's disease, contrast mediated selective renal vasospasm, hypercoagulability and cryoglobulinemia likely represent just a few of the known pieces of this puzzling phenomena. Ischemia in the heart due to prolonged coronary vasospasm can lead to angina, myocardial infarction and even death. Vasospasm in the hands and fingers due to prolonged exposure to vibration (30 \u2013 300 Hz) and triggered by cold can lead to Hand-arm vibration syndrome in which feeling and manual dexterity are lost.\n\nAngiography\nIn angiography, vascular access through femoral and axillary arteries are preferred because they are less prone to vasospasm. Meanwhile, brachial artery is more prone to vasospasm during instrumental access.\n\nHypothermia Rewarming\nIn a case study in 2000, following surgery for head trauma, a patient developed mild hypothermia, a typical defense mechanism the brain uses to protect itself after injury. After the hypothermia rewarming period, the patient died from increased intracranial pressure and anisocoria. A sample of the cerebrospinal fluid and autopsy results indicated cerebral vasospasm.\n\nTreatment\nThe occurrence of vasospasm can be reduced by preventing the occurrence of atherosclerosis. This can be done in several ways, the most important being lifestyle modifications\u2014decreasing low-density lipoprotein (LDL), quitting smoking, physical activity, and control for other risk factors including diabetes, obesity, and hypertension. Pharmacological therapies include hypolipidemic agents, thrombolytics and anticoagulants. Pharmacological options for reducing the severity and occurrence of ischemic episodes include the organic nitrates, which are rapidly metabolized to release nitric oxide in many tissues, and are classified as having either long-acting (i.e. isosorbide dinitrate) or short-acting (i.e. nitroglycerin) durations of action.\nThese drugs work by increasing nitric oxide levels in the blood and inducing coronary vasodilation which will allow for more coronary blood flow due to a decreased coronary resistance, allowing for increased oxygen supply to the vital organs (myocardium). The nitric oxide increase in the blood resulting from these drugs also causes dilation of systemic veins which in turn causes a reduction in venous return, ventricular work load and ventricular radius. All of these reductions contribute to the decrease in ventricular wall stress which is significant because this causes the demand of oxygen to decrease. In general organic nitrates decrease oxygen demand and increase oxygen supply. It is this favourable change to the body that can decrease the severity of ischemic symptoms, particularly angina.\nOther medications used to reduce the occurrence and severity of vasospasm and ultimately ischemia include L-type calcium channel blockers (notably nimodipine, as well as verapamil, diltiazem, nifedipine) and beta-receptor antagonists (more commonly known as beta blockers or \u03b2-blockers) such as propranolol.\nL-type calcium channel blockers can induce dilation of the coronary arteries while also decreasing the heart's demand for oxygen by reducing contractility, heart rate, and wall stress. The reduction of these latter three factors decreases the contractile force that the myocardium must exert in order to achieve the same level of cardiac output.\nBeta-receptor antagonists do not cause vasodilation, but like L-type calcium channel blockers, they do reduce the heart's demand for oxygen. This reduction similarly results from a decrease in heart rate, afterload, and wall stress.\n\nAdverse effects\nLike most pharmacological therapeutic options, there are risks that should be considered. For these drugs in particular, vasodilation can be associated with some adverse effects which might include orthostatic hypotension, reflex tachycardia, headaches and palpitations. Tolerance may also develop over time due compensatory response of the body, as well as depletion of -SH groups of glutathione which are essential for the metabolism of the drugs to their active forms.\nPotential side effects:\n\nVerapamil: hypotension, bradycardia, constipation\nDiltiazem: hypotension, bradycardia, risk of heart block\nNifedipine: hypotension\nPropranolol: asystole, asthma attacks\n\nContraindications\nOrganic nitrates should not be taken with PDE5 inhibitors (i.e. sildenafil) since both NO and PDE5 inhibitors increase cyclic GMP levels and the sum of their pharmacodynamic effects will greatly exceed the optimal therapeutic levels. What you could see upon taking both medications at the same time, as caused by the much higher induction of relaxation of smooth muscle cells, include a severe drop in blood pressure.\nBeta-receptor antagonists should be avoided in patients with reactive pulmonary disease to avoid asthma attacks. Also Beta-receptor antagonists should be avoided in patients with AV node dysfunction and\/or patients on other medications which might cause bradycardia (i.e. calcium channel blockers). The potential for these contraindications and drug-drug interaction could lead to asystole and cardiac arrest.\nCertain calcium channel blocker should be avoided with some beta-receptor blockers since they may cause severe bradycardia and other potential side effects.\n\nCorrective therapy\nSince vasospasms can be caused by atherosclerosis and contribute to the severity of ischemia there are some surgical options which can restore circulation to these ischemic areas. Regarding coronary vasospasm, one surgical intervention, referred to as percutaneous coronary intervention or angioplasty, involves placing a stent at the site of stenosis in an artery and inflating the stent using a balloon catheter. Another surgical intervention is coronary artery bypass.\n\nSee also\nCoronary artery vasospasm\nRaynaud's phenomenon, a vasospastic disorder\nReversible cerebral vasoconstriction syndrome\n\nReferences\n\n\n== External links ==","193":"Vertebrobasilar insufficiency (VBI) describes a temporary set of symptoms due to decreased blood flow (ischemia) in the posterior circulation of the brain.  The posterior circulation supplies the medulla, pons, midbrain, cerebellum and (in 70-80% of people) supplies the posterior cerebellar artery to the thalamus and occipital cortex.  As a result, symptoms vary widely depending which brain region is predominantly affected. \nThe term 'vertebrobasilar insufficiency' may be used to describe disease in the vertebral and basilar arteries which predisposes to acute embolic events such as transient ischemic attacks (TIAs) and stroke.  Alternatively it may be used to describe recurrent symptoms which result from narrowing (stenosis) of these arteries in combination with changes of blood pressure or head position. \nVBI should not be confused with other conditions which may relate to the posterior circulation.  25% of strokes and TIAs affect parts of the brain supplied by the posterior circulation, but many of these are embolic from cardiac or other sources.  VBI should also not be confused with beauty parlour syndrome which refers to strokes caused by acute arterial dissection brought on by extreme head positions, such as those maintained during hair washing.\n\nSigns and symptoms\nSymptoms relate to impaired brain function in areas supplied by the posterior circulation, as seen in posterior circulation strokes.  However, symptoms may be far briefer than those seen in stroke.\nVertigo is a relatively common symptom that can result from ischemia to the cerebellum, medulla or (rarely) the internal auditory artery which supplies the vestibular system of the inner ear.  While vertigo is a common feature of VBI or posterior circulation stroke, VBI only rarely presents with vertigo alone (without other neurological signs).\n\nPathology\nVBI results from narrowing of posterior circulation arteries, known as stenosis.  The most common cause of arterial stenosis is atherosclerosis, however other pathologies such as fibromuscular dysplasia, dissection, trauma or external compression may occur.  Atherosclerotic plaque can rupture, resulting in a source of emboli.  These emboli can cause TIAs or strokes in the areas of the brain supplied by the affected artery. \nWhere stenosis is severe, abrupt changes in blood pressure can temporarily result in inadequate flow through the stenosis, causing symptoms that are usually very brief.  A common cause is orthostatic hypotension which results in a fall in blood pressure when the patient changes posture and may be exacerbated by medicines (particularly antihypertensives), dehydration and heat.  In reality, orthostatic hypotension can cause vague symptoms (e.g. dizziness) which are similar to those caused by VBI; VBI should only be considered where the stenosis is severe and\/or there are focal neurological symptoms specific to the posterior circulation.\nVBI may also result from altered blood flow as seen in subclavian steal syndrome.\nVBI is described as a cause of symptoms that occur with changes to head position.  Rotational vertebral artery syndrome (sometimes referred to as Bow Hunter's Syndrome) results from vertebral artery compression on rotating the neck.  The commonest cause is a bone spur from a cervical vertebra, in combination with disease in the opposite vertebral artery.  Rotational vertebral artery syndrome is rare.\n\nDiagnosis\nThe diagnosis of posterior circulation stroke or TIA can be made on the basis of history and physical examination, which should include exclusion of alternative causes for the patient's symptoms and consideration of risk factors for atherosclerosis. To confirm VBI, imaging studies of the posterior circulation can be performed. CT is often the first study performed in acute stroke or TIA, as it is effective at excluding intracranial haemorrhage, however MRI is better at detecting ischemic strokes in the posterior distribution. CT angiography and Magnetic Resonance Angiography (MRA) can be used to detect atherosclerosis and other diseases in the posterior circulation arteries. Both can over-estimate stenosis.\nAtherosclerosis is a common finding, and its presence does not confirm that this was the cause of the patient's symptoms.\n\nAlternative diagnoses\nVBI is often considered when a patient complains of dizziness. It is important to distinguish dizziness caused by vertigo from the sensation of being light-headed, as the latter is more commonly a result of other conditions.\nBrief episodes of vertigo brought on by head movements are more likely to be Benign paroxysmal positional vertigo.  Alternatively, carotid sinus hypersensitivity can cause episodes of dizziness and collapse on head turning if the neck brushes against clothing.\n\nTreatment\nThe main treatment for VBI is to address risk factors for atherosclerosis such as smoking, hypertension and diabetes.  Patients are often started on an antiplatelet (e.g. aspirin, clopidogrel) or occasionally an anticoagulant (e.g. warfarin) to reduce the risk of future strokes.  Where VBI is causing reproducible symptoms due to stenosis, lifestyle modification to avoid provoking factors (e.g. dehydration, standing rapidly from sitting or lying) may reduce symptoms.\nOpen surgical repair or stenting can be performed to re-open stenosed vertebral arteries, and intracranial stents have also been successfully used.  Further research is required to determine which patients with VBI are likely to benefit.\n\n\n== References ==","194":"Vertigo is a form of dizziness.\nVertigo may also refer to:\n\nAcrophobia, the fear of heights, often incorrectly called \"vertigo\"\n\nArts and entertainment\nAmusement parks and rides\nVertiGo (ride), a defunct amusement ride at Cedar Point and Knott's Berry Farm, US\nVertigo, a ride at Oakwood Theme Park, Wales\nVertigo, a looping plane ride at Tivoli, Denmark\n\nFictional entities\nVertigo (Marvel Comics), two Marvel Comics villains with similar powers\nCount Vertigo, a DC Comics supervillain\nVertigo, a character in Primal Rage\n\nFilm\nVertigo (1917 film), French title Vertige, a French silent film\nVertigo (1935 film), a French drama film directed by Paul Schiller\nDizziness (film), Spanish title V\u00e9rtigo, a 1946 Mexican film\nVertigo (1947 film), a French drama film directed by Richard Pottier\nVertigo (1951 film), Spanish title V\u00e9rtigo, a Spanish drama film\nVertigo (film), a 1958 film by Alfred Hitchcock\nVertigo (film score), its soundtrack\nVertigo effect, or Dolly zoom, a special effect in film, named after the movie\n\nPublications\nVertigo Comics, a DC Comics imprint\nVertigo (Sebald novel), a 1990 novel by W. G. Sebald\nVertigo (UTS), a student newspaper at the University of Technology, Sydney\nVertigo (wordless novel), a 1937 wordless novel by Lynd Ward\n\nPlays\nVertigo (play), a 1997 play by Sean O'Connor (producer) based on the same source as the Hitchcock film\nV\u00e9rtigo, a play by Gast\u00f3n Su\u00e1rez\n\nTelevision\nV\u00e9rtigo (TV series), a Mexican telenovela\n\"Vertigo\" (The Best Years episode), an episode of The Best Years\n\"Vertigo\" (Arrow), an episode of Arrow\n\"Vertigo\", an episode of Code Lyoko\n\nMusic\nInvertigo (2000\u201303), an Australian hard rock band named Vertigo (1996\u201399)\nLe Vertigo, a rondeau for harpsichord by Joseph-Nicolas-Pancrace Royer (1703-55)\n\nAlbums\nV\u00e9rtigo (Pablo Albor\u00e1n album), 2020\nVertigo (Boxcar album), 1990\nVertigo (Jesse Cook album)\nVertigo (Eden album), 2018\nV\u00e9rtigo (Fey album), 2002\nVertigo (Griff album), 2024\nVertigo (Groove Armada album), 1999\nVertigo (John 5 album)\nVertigo (Jump, Little Children album)\nV\u00e9rtigo (La Ley album), La Ley's fifth album\nVertigo (Jackie McLean album), 1980\nVertigo (Billie Myers album), 2000\nVertigo (The Necks album), 2015\nVertigo (Chris Potter album), 1998\nVertigo (Zakk Sabbath album), 2020\nVertigo (Wand album), 2024\n\nSongs\n\"Vertigo\" (Olivia Lewis song), the Maltese entry to the Eurovision Song Contest 2007\n\"Vertigo\" (U2 song), a 2004 song by U2\nVertigo Tour, U2's international tour from 2005 to 2006\nVertigo 2005: Live from Chicago, a DVD of a U2 concert\n\"Vertigo\/Relight My Fire\", 1979 disco song by Dan Hartman and Loleatta Holloway\n\"Vertigo\", by American Hi-Fi from the American Pie 2 soundtrack\n\"Vertigo\", by Jason Derulo featuring Jordin Sparks from Tattoos\n\"Vertigo\", by The Libertines from Up the Bracket\n\"Vertigo\", by Monster Magnet from Dopes to Infinity\n\"Vertigo\", by Planet Us\n\"Vertigo\", by Aztec Camera from the album Dreamland\n\"Vertigo\", by Sarah Slean from the album Day One\n\"Vertigo\", by Matthew Sweet from the album Earth\n\"Vertigo (Do the Demolition)\", by Duran Duran from the album Notorious\n\"Vertigo\", by FM Belfast\n\"Vertigo\", by Swedish rock band Eclipse from their 2017 album Monumentum\n\"Vertigo\", by Dutch electro-pop singer Thomas Azier from his 2018 album Stray\n\"Vertigo\", by American post-metal band Deafheaven from their 2013 album Sunbather\n\"Vertigo\", by Canadian pop-punk band Marianas Trench from their 2006 album Fix Me\n\"Vertigo\", by Korean boyband Drippin released in 2021\n\nOther arts and entertainment\nVertigo (dance company), an Israeli dance company\nVertigo, a stunt performed by David Blaine\nVertigo, a defunct nightclub on the University of Victoria campus\nVertigo 42, a champagne bar in Tower 42 in London\nde_vertigo, a gaming map in the Valve game Counter-Strike: Global Offensive and Counter-Strike 1.6, situated on a 51-story skyscraper\n\nOrganisations\nVertigo Entertainment, an American film production company\nVertigo Films, a British film production and film distribution company\nVertigo Records, a UK-based record label\n\nOther uses\nVertigo (gastropod), a genus of minute land snails\nVertigo, the 5th century ascetic practice of standing on towers; See Stylite\nVertigo, an automobile by Gillet\nSeedwings Europe Vertigo, an Austrian hang glider design of the mid-2000s\nVertigo, a discontinued lollipop produced by Topps that consisted of one half chocolate, one half hard candy\n\nSee also\nCranfield Vertigo, 1980s British human-powered helicopter\nO Vertigo!, a 2014 album by Kate Miller-Heidke\nAll pages with titles beginning with Vertigo\nAll pages with titles containing Vertigo","195":"Vestibular migraine (VM) is vertigo with migraine, either as a symptom of migraine or as a related  neurological disorder.\nA 2010 report from the University of British Columbia published in the journal Headache said that \"'Migraine associated vertigo' is emerging as a popular diagnosis for patients with recurrent vertigo\" but, \"in contrast to basilar artery migraine, is neither clinically nor biologically plausible as a migraine variant.\" Epidemiological studies indicate a strong link between vertigo and migraine.\n\nSigns and symptoms\nVertigo is a medically recognized term for the symptom of a vestibular system disturbance.  It may include a feeling of rotation or illusory sensations of motion or both.  The general term dizziness is used by nonmedical people for those symptoms but often refers to a feeling of light-headedness, giddiness, drowsiness, or faintness, all of which must be differentiated from true vertigo, since the latter symptoms might have other causes.\nMotion sickness occurs more frequently in migraine patients (30\u201350% more than in controls). Benign paroxysmal vertigo of childhood is an example of migraine-associated vertigo in which headache does not often occur. Basilar artery migraine  (BAM) consists of two or more symptoms (vertigo, tinnitus, decreased hearing, ataxia, dysarthria, visual symptoms in both hemifields or both eyes, diplopia, bilateral paresthesias, paresis, decreased consciousness and\/or loss of consciousness) followed by throbbing headache. Auditory symptoms are rare.  However, a study showed a fluctuating low-tone sensorineural hearing loss in more than 50% of patients with BAM with a noticeable change in hearing just before the onset of a migraine headache. The attacks of vertigo are usually concurrent with a headache and the family history is usually positive. The diagnostician must rule out: transient ischemic attack (TIA), and paroxysmal vestibular disorder accompanied by headache.\nThere is also a familial vestibulopathy, familial benign recurrent vertigo (fBRV), where episodes of vertigo occur with or without a migraine headache. Testing may show profound vestibular loss. The syndrome responds to acetazolamide. Familial hemiplegic migraine (FHM) has been linked to mutations in the calcium channel gene. (Ophoff et al. 1966 cf. Lempert et al.)\n\nPathophysiology\nThe pathophysiology of MAV is not completely understood; both central and peripheral defects have been observed.\n\nDiagnosis\nBy the Consensus document of the Bar\u00e1ny Society and the International Headache Society on the diagnostic criteria of vestibular migraine, the diagnostic criteria of vestibular migraine are:\n\nThe diagnostic criteria of probable vestibular migraine are:\n\nClassification\nBenign paroxysmal positional vertigo\nMigraine is commonly associated with BPPV, the most common vestibular disorder in patients presenting with dizziness. The two may be linked by genetic factors or by vascular damage to the labyrinth.\nM\u00e9ni\u00e8re's disease\nThere is an increased prevalence of migraine in patients with M\u00e9ni\u00e8re's disease and migraine leads to a greater susceptibility of developing M\u00e9ni\u00e8re\u2019s disease. But they can be distinguished. M\u00e9ni\u00e8re's disease may go on for days or even years, while migraines typically do not last longer than 24 hours.\nMotion sickness\nMore prevalent in patients with migraine.\nPsychiatric syndromes\nDizziness and spinning vertigo are the second most common symptom of panic attacks, and they can also present as a symptom of major depression. Migraine is a risk factor for developing major depression and panic disorder and vice versa.\n\nTreatment\nTreatment of migraine-associated vertigo is the same as the treatment for migraine in general. There is not enough evidence to indicate which medications are most effective for preventing vestibular migraine.\n\nEpidemiology\nThe prevalence of migraine and vertigo is 1.6 times higher in 200 dizziness clinic patients than in 200 age- and sex-matched controls from an orthopaedic clinic.  Among the patients with unclassified or idiopathic vertigo, the prevalence of migraine was shown to be elevated. In another study, migraine patients reported 2.5 times more vertigo and also 2.5 more dizzy spells during headache-free periods than the controls.\nMAV may occur at any age with a female:male ratio of between 1.5 and 5:1.  Familial occurrence is not uncommon.  In most patients, migraine headaches begin earlier in life than MAV with years of headache-free periods before MAV manifests.\nIn a diary study, the 1-month prevalence of MAV was 16%, frequency of MAV was higher and duration longer on days with headache, and MAV was a risk factor for co-morbid anxiety.\n\nReferences\nExternal links\n\neMedicine article","196":"Vertigo is a 1958 American psychological thriller film directed and produced by Alfred Hitchcock. The story was based on the 1954 novel D'entre les morts (From Among the Dead) by Boileau-Narcejac, with a screenplay by Alec Coppel and Samuel A. Taylor. The film stars James Stewart as a former San Francisco police detective who has retired after an incident in the line of duty caused him to develop an extreme fear of heights accompanied by vertigo. He is hired as a private investigator to report on the strange behavior of an acquaintance's wife (Kim Novak).\nThe film was shot on location in San Francisco, as well as in Mission San Juan Bautista, Big Basin Redwoods State Park, Cypress Point on 17-Mile Drive, and at Paramount Studios in Hollywood. The film stock of the camera negative was Eastman 25 ASA tungsten-balanced 5248 with processing and prints by Technicolor. It was the first film to use the dolly zoom, an in-camera effect that distorts perspective to create disorientation, to convey Scottie's acrophobia; the technique is often referred to as \"the Vertigo effect\" in reference to its use in the film. In 1996, the film underwent a major restoration to create a new 70 mm print and DTS soundtrack.\nVertigo received mixed reviews on release, but it has since come to be considered Hitchcock's magnum opus and one of the greatest films of all time. In 1989, it was one of the first 25 films selected by the Library of Congress for preservation in the United States National Film Registry for being \"culturally, historically, or aesthetically significant\". The film appears repeatedly in polls of the best films by the American Film Institute, including a 2007 ranking as the ninth-greatest American film ever. Attracting significant scholarly attention, it replaced Citizen Kane as the greatest film ever made in the 2012 Sight & Sound Greatest Films of All Time poll, and came in second place in the 2022 edition of the poll.\n\nPlot\nAfter a rooftop chase in which a fellow policeman falls to his death, San Francisco detective John \"Scottie\" Ferguson retires due to acrophobia and accompanying vertigo caused by the incident. Midge, his ex-fianc\u00e9e, says that another severe emotional shock may be the only cure. Midge retains feelings for Scottie but he is not receptive to her intimations.\nGavin Elster, an acquaintance from college, asks Scottie to follow his wife, Madeleine, claiming that she has been behaving strangely. Scottie follows Madeleine to the grave of Carlotta Valdes (1831\u20131857) at the Mission San Francisco de As\u00eds and to the Legion of Honor art museum, where she gazes at the Portrait of Carlotta.\nA local historian explains that Carlotta Valdes committed suicide: she had been the mistress of a wealthy married man and borne his child, and the otherwise childless man kept the child and cast Carlotta aside. Carlotta, who Gavin fears is possessing Madeleine, was Madeleine's great-grandmother. However, Madeleine does not know this or remember the places she has visited while ostensibly possessed. Scottie trails her to Fort Point and rescues her after she jumps into San Francisco Bay.\nThe next day, Madeleine stops to deliver a letter of gratitude to Scottie, and they spend the day together. They travel to Muir Woods and Cypress Point on 17-Mile Drive, where they embrace. The next day, Madeleine recounts a nightmare, and Scottie identifies its setting as Mission San Juan Bautista, Carlotta's childhood home. He drives her there, and they express their love for each other. Madeleine suddenly runs into the church and up the bell tower, asking Scottie not to follow her. Scottie runs after her, but is halted on the steps by his fear of heights and sees her plunge to her death.\nAn inquest into Madeleine's death declares it a suicide, though the coroner rebukes Scottie for not doing more to save her. Gavin also does not fault Scottie, but Scottie becomes clinically depressed and is sent to a sanatorium in an almost catatonic state. Following his release, he frequents the places that Madeleine visited, often imagining that he sees her. One day, he notices a woman on the street who, although superficially very different, reminds him of Madeleine. He follows her into her apartment, where she identifies as Judy Barton, from Salina, Kansas.\nA flashback reveals that Judy was the person Scottie knew as \"Madeleine Elster\"; she had been impersonating Gavin's wife in an elaborate murder scheme. Gavin took advantage of Scottie's fear of heights to substitute his wife's freshly killed body in the apparent \"suicide jump\". Judy considers confessing her involvement in the plot to Scottie but continues the charade because she loves him.\nThe two begin seeing each other, but Scottie remains obsessed with \"Madeleine\" and asks Judy to change her clothes and dye her hair to resemble her. After she complies, he notices her wearing the necklace portrayed in Carlotta's painting. Realizing the truth, he drives Judy back to the mission.\nThere, he tells her that he must re-enact the event that led to his madness, and that he now knows that \"Madeleine\" and Judy are the same person, with Judy having been Gavin's mistress before being cast aside just as Carlotta had been. He forces her up the bell tower and makes her admit her deceit. He reaches the top, conquering his fear of heights. Judy confesses that Gavin paid her to impersonate a \"possessed\" Madeleine and begs Scottie to forgive her. He embraces Judy, but a shadowy figure\u2014a nun investigating the noise\u2014rises from the tower's trapdoor, startling her. Judy lunges backward off the tower to her death. Scottie, bereaved once again but cured of his fear of heights, stands on the ledge in shock while the nun rings the mission bell.\n\nCast\nJames Stewart as John \"Scottie\" Ferguson\nKim Novak as Judy Barton \/ Madeleine Elster\nTom Helmore as Gavin Elster\nBarbara Bel Geddes as Marjorie \"Midge\" Wood\nHenry Jones as the coroner\nRaymond Bailey as Scottie's doctor\nEllen Corby as the manager of the McKittrick Hotel\nKonstantin Shayne as bookstore owner Pop Leibel\nLee Patrick as the car owner mistaken for Madeleine\nAlfred Hitchcock makes his customary cameo appearance walking in front of Gavin Elster's shipyard, carrying a trumpet case.\n\nThemes and interpretations\nIn his monograph dedicated to the study of Vertigo, Charles Barr has stated that the central theme of the film is psychological obsession, concentrating in particular on Scottie as obsessed with the women in his life. Barr notes, \"This story of a man who develops a romantic obsession with the image of an enigmatic woman has commonly been seen, by his colleagues as well as by critics and biographers, as one that engaged Hitchcock in an especially profound way; and it has exerted a comparable fascination on many of its viewers. After first seeing it as a teenager in 1958, Donald Spoto had gone back for 26 more viewings by the time he wrote The Art of Alfred Hitchcock in 1976. In a 1996 magazine article, Geoffrey O'Brien cites other cases of 'permanent fascination' with Vertigo, and then casually reveals that he himself, starting at age 15, has seen it 'at least thirty times'.\"\nCritics have interpreted Vertigo variously as \"a tale of male aggression and visual control; as a map of female Oedipal trajectory; as a deconstruction of the male construction of femininity and of masculinity itself; as a stripping bare of the mechanisms of directorial, Hollywood studio and colonial oppression; and as a place where textual meanings play out in an infinite regress of self-reflexivity.\" Critic James F. Maxfield has suggested that Vertigo can be interpreted as a variation on Ambrose Bierce's 1890 short story \"An Occurrence at Owl Creek Bridge\", in which the main narrative of the film is actually imagined by Scottie as he dangles from a building at the end of the opening rooftop chase.\n\nProduction\nDevelopment\nThe screenplay of Vertigo is an adaptation of the 1954 French novel D'entre les morts (From Among the Dead) by Pierre Boileau and Thomas Narcejac. Hitchcock had attempted to buy the rights to the previous novel by the same authors, Celle qui n'\u00e9tait plus (She Who Was No More), but failed, and it was instead adapted by Henri-Georges Clouzot as Les Diaboliques. Although Fran\u00e7ois Truffaut once suggested that D'entre les morts was specifically written for Hitchcock by Boileau and Narcejac, Narcejac subsequently denied that this was their intention. However, Hitchcock's interest in their work meant that Paramount Pictures commissioned a synopsis of D'entre les morts in 1954, before it had even been translated into English (it appeared in translation as The Living and the Dead in 1956).\n\nIn the book, Judy's involvement in Madeleine's death was not revealed until the denouement; at the scriptwriting stage, Hitchcock suggested revealing the secret two-thirds of the way through the film so that the audience would understand Judy's dilemma. After the first preview, Hitchcock was unsure whether or not to keep the \"letter writing scene\", though he subsequently decided to remove it. Herbert Coleman, Vertigo's associate producer and a frequent collaborator with Hitchcock, felt the removal was a mistake; however, Hitchcock said to \"Release it just like that.\" James Stewart, acting as mediator, said to Coleman: \"Herbie, you shouldn't get so upset with Hitch. The picture's not that important.\" Hitchcock's decision was supported by screenwriter Joan Harrison, another member of his circle, who felt that the film had been improved. Coleman reluctantly made the necessary edits. When Paramount head Barney Balaban received news of this, he ordered Hitchcock to \"Put the picture back the way it was,\" ensuring that the scene remained in the final cut.\n\nWriting\nThree screenwriters were involved in the writing of Vertigo. Hitchcock originally hired playwright Maxwell Anderson to write a screenplay, but rejected his work, which was titled Darkling, I Listen (a quotation from John Keats's 1819 poem \"Ode to a Nightingale\"). According to Charles Barr in his monograph dedicated to Vertigo, \"Anderson was the oldest (at 68) [of the three writers involved], the most celebrated for his stage work, and the least committed to cinema, though he had a joint script credit for Hitchcock's preceding film The Wrong Man. He worked on adapting the novel during Hitchcock's absence abroad, and submitted a treatment in September 1956.\"\nA second version, written by Alec Coppel, again left the director dissatisfied. The final script was written by Samuel A. Taylor, who had been recommended to Hitchcock due to his knowledge of San Francisco, from notes by the director. Among Taylor's creations was the character of Midge. Taylor attempted to take sole credit for the screenplay, but Coppel protested to the Screen Writers Guild, which determined that both writers (but not Anderson) were entitled to a credit.\n\nCasting\nVera Miles, who was under personal contract to Hitchcock and had appeared both on Alfred Hitchcock Presents and in The Wrong Man, was originally scheduled to play Madeleine, and modeled for an early version of the portrait of Carlotta. Following delays, including Hitchcock becoming ill with gallbladder problems, Miles became pregnant and had to withdraw from the role. The director declined to postpone shooting, and cast Kim Novak as Miles' replacement. By that time, Novak had delayed prior film commitments and a vacation promised by Columbia Pictures, the studio that held her contract, and Miles had given birth and was available for the film. Nevertheless, Hitchcock proceeded with Novak. Columbia head Harry Cohn agreed to lend Novak to Vertigo if Stewart would agree to co-star with Novak in Bell, Book and Candle, a Columbia production released in December 1958.\n\nFilming\nInitial on-site principal photography\nVertigo was filmed from September to December 1957. Principal photography began on location in San Francisco in September 1957 under the working title From Among the Dead. The film uses extensive location footage of the San Francisco Bay Area. In the driving scenes shot in the city, the main characters' cars are almost always pictured heading down the city's steeply inclined streets. In October 1996, the restored print of Vertigo debuted at the Castro Theatre in San Francisco with a live on-stage introduction by Kim Novak. Visiting the San Francisco film locations has accrued modest tourist appeal; such a tour is featured in a subsection of Chris Marker's 1983 documentary montage Sans Soleil.\nThe scene in which Madeleine falls from the tower was filmed at Mission San Juan Bautista, a Spanish mission in San Juan Bautista, California. Associate producer Herbert Coleman's daughter Judy Lanini suggested the mission to Hitchcock as a filming location. A steeple, added sometime after the mission's original construction and secularization, had been demolished following a fire, so Hitchcock added a bell tower much larger than the one previously at the mission using scale models, matte paintings, and trick photography at Paramount Studios.\n\nList of shooting locations\nScottie's apartment (900 Lombard Street) is one block downhill from the \"crookedest street in the world\". The facade of the building remained mostly intact until 2012, when the owner of the property erected a wall enclosing the entrance area on the Lombard side of the building.\nThe rooftop chase took place on Taylor Street between 1302 and 1360. 1308 Taylor Street went up for sale in 2016 for $2.2 million.\nThe Mission San Juan Bautista, where Madeleine falls from the tower, is a real place, but the tower had to be matted in with a painting using studio effects; Hitchcock had first visited the mission before the tower was torn down due to dry rot, and was reportedly displeased to find it missing when he returned to film. The original tower was much smaller than the one depicted in the film.\nThe Carlotta Valdes headstone featured in the film, created by the props department, was left at Mission Dolores. The headstone was later removed, as the mission considered it disrespectful to the dead. All other cemeteries in San Francisco had been evicted from the city limits in 1912, so the screenwriters had no other option but to locate the grave at Mission Dolores.\nMadeleine jumps into the bay at Fort Point, underneath the Golden Gate Bridge.\nThe gallery where Carlotta's painting appears is the California Palace of the Legion of Honor in San Francisco. The Carlotta Valdes portrait was lost after being removed from the gallery, but many of the other paintings in the background of the portrait scenes are still on view.\nWhat purports to be Muir Woods National Monument in the film is in fact Big Basin Redwoods State Park; however, the cutaway of the redwood tree showing its age was copied from one that can still be found at Muir Woods.\nThe coastal region where Scottie and Madeleine first kiss is Cypress Point, along the 17 Mile Drive near Pebble Beach. However, the lone tree they kiss next to was a prop brought specially to the location.\nThe domed building Scottie and Judy walk past is the Palace of Fine Arts.\nCoit Tower appears in many background shots; Hitchcock once said that he included it as a phallic symbol. Also prominent in the background is the tower of the San Francisco Ferry Building.\nThe exterior of the sanatorium where Scottie is treated was St. Joseph's Hospital, located at 355 Buena Vista East, across from Buena Vista Park. The hospital, which was not a sanatorium, was closed in 1979 and then converted into condominiums. The building, built in 1928, is on the National Register of Historic Places.\nGavin and Madeleine's apartment building \"The Brocklebank\" at 1000 Mason Street on Nob Hill still looks essentially the same. It is across the street from the Fairmont Hotel, where Hitchcock usually stayed when he visited and where many of the cast and crew stayed during filming. Shots of the surrounding neighborhood feature the Flood Mansion and Grace Cathedral. Barely visible is the Mark Hopkins Hotel, mentioned in an early scene in the movie.\nThe \"McKittrick Hotel\" was a privately owned Victorian mansion from the 1880s at Gough and Eddy Streets. It was torn down in 1959 and is now an athletic practice field for Sacred Heart Cathedral Preparatory School. The St. Paulus Lutheran Church, seen across from the mansion, was destroyed in a fire in 1995.\nPodesta Baldocchi is the flower shop Madeleine visits as she is being followed by Scottie. The shop's location at the time of filming was 224 Grant Avenue; it has since moved to 410 Harriet Street.\nThe Empire Hotel is a real place, called the York Hotel, and now the Hotel Vertigo at 940 Sutter Street. Judy's room was created on set, but the green neon of the \"Hotel Empire\" sign outside is based on the actual hotel's sign.\nErnie's (847 Montgomery St.) was a real restaurant in Jackson Square, 1 mile (1.6 km) from Scottie's apartment. It is no longer operating.\nOne short scene shows Union Square at dawn.  Pop Leibel's bookstore, the Argosy, was not a real location, but one recreated on the Paramount lot in imitation of the real-life Argonaut Book Store, which still exists near Sutter and Jones.\nElster's fictitious Dogpatch shipyard office was filmed at the real (or simulated with mattes) Union Iron Works shipyard, by then the Bethlehem Steel shipyard. Elster's office has a Mission telephone exchange (MI or 64) prefix, regarding which Midge says \"Why, that's Skid Row\".\n\nSubsequent studio shooting\nFollowing 16 days of location shooting, the production moved to Paramount's studios in Hollywood for two months of filming. Hitchcock preferred to film in studios as he was able to control the environment. Once sufficient location footage had been obtained, interior sets were designed and constructed in the studio.\nHitchcock popularized the dolly zoom in this film, leading to the technique's sobriquet, amongst several others, \"the Vertigo effect\". This \"dolly-out\/zoom-in\" method involves the camera physically moving away from a subject whilst simultaneously zooming in (a similar effect can be achieved in reverse), so that the subject retains its size in the frame, but the background's perspective changes. Hitchcock used the effect to look down the tower shaft to emphasise its height and Scottie's disorientation. Following difficulties filming the shot on a full-sized set, a model of the tower shaft was constructed, and the dolly zoom was filmed horizontally. The \"special sequence\" (Scottie's nightmare sequence) was designed by artist John Ferren, who also created the painting of Carlotta.\nThe rotating patterns in the title sequence were created by animator John Whitney using a Kerrison Predictor, a mechanical computer which was used during World War II to aim anti-aircraft cannons at moving targets. This made it possible to produce an animated version of Lissajous curves.\n\nCostume design\nHitchcock and costume designer Edith Head used color to heighten emotion. Grey was chosen for Madeleine's suit in an attempt to be psychologically jarring, as it is not usually a blonde's color. In contrast, Novak's character wore a white coat when she visited Scottie's apartment, which Head and Hitchcock considered more natural for a blonde to wear.\n\nAlternative ending\nA coda to the film was shot that showed Midge at her apartment, listening to a radio report (voiced by San Francisco TV reporter Dave McElhatton) describing the pursuit of Gavin Elster across Europe. Midge switches the radio off when Scottie enters the room, and they then share a drink and look out of the window in silence. Contrary to reports that this scene was filmed to meet foreign censorship needs, this tag ending had originally been demanded by Geoffrey Shurlock of the U.S. Production Code Administration, who had noted: \"It will, of course, be most important that the indication that Elster will be brought back for trial is sufficiently emphasized.\"\nHitchcock ultimately succeeded in fending off most of Shurlock's demands, including toning down erotic allusions, and had the alternative ending dropped. The footage was discovered in Los Angeles in May 1993, and was added as an alternative ending on LaserDisc, DVD, and Blu-ray releases of the film.\n\nMusic\nThe film's score was written by regular Hitchcock collaborator Bernard Herrmann. It was conducted by Muir Mathieson and recorded in Europe because of a musicians' strike in the United States.\nIn a 2004 special issue of the British Film Institute's magazine Sight and Sound, director Martin Scorsese described the qualities of Herrmann's score:\n\nHitchcock's film is about obsession, which means that it's about circling back to the same moment, again and again... And the music is also built around spirals and circles, fulfillment and despair. Herrmann really understood what Hitchcock was going for \u2014 he wanted to penetrate to the heart of obsession.\n\nGraphic design\nGraphic designer Saul Bass used spiral motifs in both the title sequence and the movie poster, emphasizing what the documentary Obsessed with Vertigo calls the film's \"psychological vortex\". Bass's unconventional framing of actress Audrey Lowell's facial features in the first images of the titles was indebted to Bauhaus photography. According to her 1997 Guardian interview, Kim Novak wanted to do the opening title sequence but Harry Cohn insisted Hitchcock pay full rate for the single day's shooting and so another face was chosen.\n\nRelease\nVertigo premiered in San Francisco on May 9, 1958, at the Stage Door Theater (now the August Hall nightclub). While Vertigo did break even upon its original release, earning $3.2 million in North American distributor rentals against its $2,479,000 cost, it earned significantly less than other Hitchcock productions.\n\nRestoration and re-release\nIn October 1983, Rear Window and Vertigo were the first two Hitchcock films reissued by Universal Pictures after the studio acquired the rights from the director's estate. These two films \u2013 along with The Man Who Knew Too Much (1956), Rope (1948), and The Trouble with Harry (1955) \u2013 had been kept out of distribution by the director since 1968. Cleaning and restoration were performed on each film when new 35 mm prints were struck.\nIn 1996, the film was given a lengthy and controversial restoration by Robert A. Harris and James C. Katz and re-released to theaters. The new print featured restored color and newly created audio, using modern sound effects mixed in DTS digital surround sound. In October 1996, the restored Vertigo premiered at the Castro Theatre in San Francisco, with Kim Novak and Patricia Hitchcock attending. At this screening, the film was exhibited for the first time in DTS and 70mm, a format with a similar frame size to the VistaVision system in which it was originally shot.\nSignificant color correction was necessary because of the fading of original negatives. In some cases a new negative was created from the silver separation masters, but in many instances this was impossible because of differential separation shrinkage, and because the 1958 separations were poorly made. Separations used three individual films: one for each of the primary colors. In the case of Vertigo, these had shrunk in different and erratic proportions, making re-alignment impossible.\nAs such, significant amounts of computer assisted coloration were necessary. Although the results are not noticeable on viewing the film, some elements were as many as eight generations away from the original negative, in particular the entire \"Judy's Apartment\" sequence, perhaps the most pivotal sequence in the entire film. When such large portions of re-creation become necessary, then the danger of artistic license by the restorers becomes an issue, and the restorers received some criticism for their re-creation of colors that allegedly did not honor the director's and cinematographer's intentions. The restoration team argued that they did research on the colors used in the original locations, cars, wardrobe, and skin tones. One breakthrough came when the Ford Motor Company supplied a well-preserved green paint sample for a car used in the film. As the color green has extensive symbolic use in the film, matching a shade of green was important for restoration and provided a reference shade.\nWhen restoring the sound, Harris and Katz wanted to stay as close as possible to the original, and had access to the original music recordings that had been stored in the vaults at Paramount. However, as the project demanded a new 6-channel DTS stereo soundtrack, it was necessary to re-record some sound effects. The soundtrack was remixed at the Alfred Hitchcock Theatre at Universal Studios. Aware that the film had a considerable following, the restoration team knew that they were under particular pressure to restore the film as accurately as possible. To achieve this, they used Hitchcock's original dubbing notes for guidance of how the director wanted the film to sound in 1958. Harris and Katz sometimes added extra sound effects to camouflage \"hisses, pops, and bangs\" in the old soundtrack. In particular, they added extra seagull cries and a foghorn to the scene at Cypress Point. The new mix has also been accused of putting too much emphasis on the score at the expense of the sound effects.\n\nHome media\nIn 1996, director Harrison Engle produced the documentary Obsessed with Vertigo. Narrated by Roddy McDowall, the film played on American Movie Classics, and has since been included with DVD versions of Vertigo. Surviving members of the cast and crew participated, along with Martin Scorsese and Patricia Hitchcock. Engle first visited the Vertigo shooting locations in the summer of 1958, just months after completion of the film.\nVertigo was first released on DVD in March 1998. On October 4, 2011, the film was re-issued on DVD by Universal Pictures Home Entertainment as part of the Alfred Hitchcock: The Essentials Collection. Subsequently, the film was released on Blu-ray on September 25, 2012, as part of the Alfred Hitchcock: The Masterpiece Collection. In May 2014, the film was re-released as a stand-alone Blu-ray edition. Some home video releases, such as the 2005 Hitchcock Masterpiece Collection DVD set, contain the original mono track as an option.\nIn October 2014, a new 4K restoration was presented at the Castro Theatre in San Francisco. This version gives credit to Harris and Katz at the end of the film, and thanks them for providing some previously unknown stereo soundtracks. This version, however, removes some of the \"excessive\" Foley sound that was added in the 1996 restoration.\nIn September 2020, a Ultra HD Blu-ray was released by Universal Pictures Home Entertainment as a part of the first volume of The Alfred Hitchcock Classics Collection. In September 2021, a stand-alone version was released alongside a Steelbook.\n\nReception\nContemporaneous reception\nInitial critical reception for Vertigo was mixed. Variety wrote that the film showed Hitchcock's \"mastery\", but felt the film was \"too long and slow\" for \"what is basically only a psychological murder mystery\". Similarly, Philip K. Scheuer of the Los Angeles Times admired the scenery, but found the plot took \"too long to unfold\" and felt it \"bogs down in a maze of detail\". Scholar Dan Auiler says that this review \"sounded the tone that most popular critics would take with the film\". However, the Los Angeles Examiner loved it, admiring the \"excitement, action, romance, glamor and [the] crazy, off-beat love story\". The New York Times film critic Bosley Crowther also gave Vertigo qualified praise by stating that \"[the] secret [of the film] is so clever, even though it is devilishly far-fetched.\" Richard L. Coe of The Washington Post praised the film as a \"wonderful weirdie,\" writing that \"Hitchcock has even more fun than usual with trick angles, floor shots and striking use of color. More than once he gives us critical scenes in long shots establishing how he's going to get away with a couple of story tricks.\" John McCarten of The New Yorker wrote derisively that Hitchcock had \"never before indulged in such farfetched nonsense.\"\nThe New York Post review echoed many critics': \"Let's admit it right now. Hitchcock's surfaces are so smooth he thinks he can get away with murder in the logic and realism departments. If you want to tear 'Vertigo' apart, it rips easily. On the other hand, there's no denying that James Stewart's unactorish acting carries a heavy air of reality into the picture, and Kim Novak's somnambulistic behavior, called for by the script, is something she can do to perfection....It's doubtful that 'Vertigo' can take equal rank with the best of the Hitchcock studies\u2014it has too many holes\u2014but it assays high in visual confectionary of place, person, and celluloid wiles.\"\nContemporaneous response in England was summarized by Charles Barr in his monograph on Vertigo: \"In England, the reception was if anything rather less friendly. Of the 28 newspaper and magazine reviews that I have looked at, six are, with reservations, favourable, nine are very mixed, and 13 almost wholly negative. Common to all of these reviews is a lack of sympathy with the basic structure and drive of the picture. Even the friendlier ones single out for praise elements that seem, from today's perspective, to be marginal virtues and incidental pleasures \u2013 the 'vitality' of the supporting performances (Dilys Powell in The Sunday Times), the slickness with which the car sequences are put together (Isobel Quibley in The Spectator)\".\nIn France, \u00c9ric Rohmer noted in Cahiers du Cin\u00e9ma that \"Vertigo, so they say, repelled Americans. French critics, on the contrary, seem to be giving it a warm welcome.\" Praising the film's formal technique, he wrote that \"ideas and forms follow the same road, and it is because the form is pure, beautiful, rigorous, astonishingly rich, and free that we can say that Hitchcock's films, with Vertigo at their head, are about ideas, in the noble, platonic sense of the word.\"\nHitchcock fans were not pleased with Vertigo's departure from the romantic-thriller territory of earlier films, or with the mystery being solved well before the film's ending. Orson Welles disliked the film, telling Henry Jaglom that it was \"worse\" than Rear Window, which he had also disliked. In an interview with Fran\u00e7ois Truffaut, Hitchcock stated that Vertigo was one of his favourite films, with some reservations. He blamed the film's limited success on the 49-year-old Stewart looking too old to play a convincing love interest for the 24-year-old Novak.\nA young Martin Scorsese viewed the film with his friends during its original theatrical run, and later recalled that \"even though the film was not well received at the time... we responded to the film very strongly. [We] didn't know why... but we really went with the picture.\"\nThe film received awards at the San Sebasti\u00e1n International Film Festival, including a Silver Seashell for Best Director for Hitchcock (tied with Mario Monicelli for Big Deal on Madonna Street) and Best Actor for Stewart (tied with Kirk Douglas in The Vikings). The film was nominated for two technical Academy Awards for Best Art Direction \u2013 Black-and-White or Color (Hal Pereira, Henry Bumstead, Samuel M. Comer, Frank McKelvy) and Best Sound (George Dutton).\n\nRe-evaluation\nOver time the film has been re-evaluated by film critics and has moved higher in esteem in most critics' opinions. Every ten years since 1952, the British Film Institute magazine Sight and Sound has asked the world's leading film critics to compile a list of the ten greatest films of all time. In the 1962 and 1972 polls, Vertigo was not among the top 10 films in voting; only in 1982, after Hitchcock's death, did Vertigo enter the list, in seventh place. By 1992 it had advanced to fourth place, by 2002 to second, and in 2012 to first place in both the crime genre and overall, ahead of previous first-place entry Citizen Kane; in the 2022 poll, it took second place behind Jeanne Dielman, 23 quai du Commerce, 1080 Bruxelles. In the 2012 Sight & Sound director's poll of the greatest films ever made Vertigo was ranked seventh; In the 2002 and 2022 editions of the directors' list the film ranked sixth.\nCommenting upon the 2012 results, the magazine's editor Nick James said that Vertigo was \"the ultimate critics' film. It is a dream-like film about people who are not sure who they are but who are busy reconstructing themselves and each other to fit a kind of cinema ideal of the ideal soul-mate.\" In recent years, critics have noted that the casting of Stewart as a character who becomes disturbed and obsessive ultimately enhances the film's unconventionality and suspense, since Stewart had previously been known for warmhearted roles.\nIn 1998, Time Out conducted a poll in which Vertigo was voted the fifth greatest film of all time. The Village Voice ranked Vertigo at No. 3 in its Top 250 \"Best Films of the Century\" list in 1999, based on a poll of critics. Entertainment Weekly voted it the 19th Greatest film of all time in 1999. In January 2002, the film was voted at No. 96 on the list of the \"Top 100 Essential Films of All Time\" by the National Society of Film Critics. In 2009, the film was ranked at No. 10 on Japanese film magazine Kinema Junpo's Top 10 Non-Japanese Films of All Time list. In 2022, Time Out magazine ranked the film at No.15 on their list of \"The 100 best thriller films of all time\".\nAlready in the 1960s, Cahiers du Cin\u00e9ma critics had begun re-evaluating Hitchcock as a serious artist. The film ranked eighth on Cahiers du Cin\u00e9ma's Top 10 Films of the Year List in 1959. However, even Fran\u00e7ois Truffaut's 1962 book of interviews with Hitchcock devotes only a few pages to Vertigo. Dan Auiler has suggested that the real beginning of Vertigo's re-evaluation was the 1968 publication of British-Canadian scholar Robin Wood's book Hitchcock's Films, which called it \"Hitchcock's masterpiece to date and one of the four or five most profound and beautiful films the cinema has yet given us\".\nAdding to its mystique was the fact that Vertigo was one of five Hitchcock-owned films removed from circulation in 1973. When Vertigo was re-released in theaters in October 1983, and then on home video in October 1984, it achieved commercial success and laudatory reviews. The October 1996 showing of a restored print on 70 mm film with DTS sound at the Castro Theatre in San Francisco was met with a similarly strong reception. In his 1996 review of the film, critic Roger Ebert gave it four stars out of four and included it in his list of The Great Movies.\nA minority of critics have expressed dissenting opinions. In his 2004 book Blockbuster, British film critic Tom Shone suggested that Vertigo's critical re-evaluation has led to excessive praise: \"Hitchcock is a director who delights in getting his plot mechanisms buffed up to a nice humming shine, and so the Sight and Sound team praise the one film of his in which this is not the case \u2013 it's all loose ends and lopsided angles, its plumbing out on display for the critic to pick over at his leisure.\"\nIn 1989, Vertigo was recognized as a \"culturally, historically and aesthetically significant\" film by the United States Library of Congress and selected for preservation in the National Film Registry in the first year of the registry's voting.\nIn 2005, Vertigo was ranked at number two in Total Film magazine's 100 Greatest Movies of All Time, behind only Goodfellas. In 2008, an Empire poll of readers, actors, and critics named it the 40th greatest movie ever made. The film was voted at No. 8 on the 2008 list of \"100 Greatest Films\" by Cahiers du Cin\u00e9ma. In 2010, The Guardian ranked it as the third-best crime film of all time. Vertigo ranked third on the BBC's 2015 list of the 100 greatest American films.\nOn review aggregator Rotten Tomatoes, the film holds an approval rating of 93% based on 87 reviews, with an average rating of 8.90\/10. The website's critics consensus reads deems it \"an unpredictable scary thriller that doubles as a mournful meditation on love, loss, and human comfort\". As of February 2024, Vertigo is one of only fourteen films with a perfect score on Metacritic (two other Hitchcock films, Notorious and Rear Window, are also on the list).\nThe most recent edition of the American Film Institute's top 100 films of all time, released in 2007, placed Vertigo at number nine, up 52 positions from its placement at number 61 in the original 1998 list.\nAmerican Film Institute recognition\n\nAFI's 100 Years...100 Movies (1998) #61\nAFI's 100 Years...100 Thrills (2001) #18\nAFI's 100 Years...100 Passions (2002) #18\nAFI's 100 Years of Film Scores (2005) #12\nAFI's 100 Years...100 Movies (10th Anniversary Edition) (2007) #9\nAFI's 10 Top 10 (2008) #1 Mystery\nThe San Francisco locations have become celebrated amongst the film's fans, with organized tours across the area. In March 1997, the French magazine Les Inrockuptibles published a special issue about Vertigo's locations in San Francisco, Dans le d\u00e9cor.\nDirector Martin Scorsese has listed Vertigo as one of his favorite films of all time.\nThe renewed public appreciation for Vertigo is accompanied by a growing body of academic scholarship. Conferences like the Annual International Vertigo conference, for instance, showcase this trend, as evidenced by its 2018 event at Trinity College Dublin.\n\nCritical works on Vertigo\nRobin Wood's chapter on Vertigo in Hitchcock's Films\nMolly Haskell's essay, \"With Paintbrush and Mirror: 'Vertigo' & 'As You Desire Me'\" in The Village Voice\nLaura Mulvey's Visual Pleasure and Narrative Cinema, popularizing the concept of the male gaze\n\nClassification as film noir\nCritical opinion is divided on whether or not Vertigo should be considered an example of film noir. Some consider it a film noir on the basis of plot and tone and various motifs, despite it having mid-century modern visuals typical of the 1950s. Others say the use of Technicolor, color symbolism, and the specificity of Hitchcock's vision exclude it from the category. Nicholas Christopher, Robert Ottoson, and Silver and Ward, for instance, do not include Vertigo in their filmographies of film noir. By contrast, Foster Hirsch describes Vertigo as among the Hitchcock films that are \"richly, demonstrably noir\".\n\nDerivative works\nKalangarai Vilakkam, a 1965 Tamil adaptation of Vertigo.\nOne on Top of the Other, a 1969 giallo film directed by Lucio Fulci, is heavily influenced by Vertigo.\nObsession, a 1976 film by Brian De Palma, is heavily influenced by Vertigo, while his 1984 thriller Body Double combines the plot elements of both Vertigo and Rear Window.\nHigh Anxiety, a 1977 film by Mel Brooks, is a parody of suspense films directed by Alfred Hitchcock, but leans on Vertigo in particular.\nChris Marker's 1983 video-essay Sans Soleil makes reference to the movie, declaring it the only film \"capable of portraying impossible memory\" over footage of Vertigo's shooting locations and stills from the film.\nHarvey Danger's song \"Carlotta Valdez,\" from their 1997 album Where Have All the Merrymakers Gone?, largely summarizes the plot of the film.\nDavid Lynch's films Lost Highway (1997) and Mulholland Drive (2001) are reportedly influenced by Vertigo. Twin Peaks, a television series co-created by Lynch and Mark Frost, is also heavily inspired by Vertigo, particularly in the Doppelg\u00e4nger character of Maddy Ferguson and various surrealist elements.\nJoseph Kahn's 1997 music video for Faith No More's \"Last Cup of Sorrow\" is a parody of Vertigo, with frontman Mike Patton as Scottie and Jennifer Jason Leigh as Madeleine\/Judy.\nSuzhou River, a 2000 Chinese film by Lou Ye which critics saw as an homage to Vertigo.\nThe Testament of Judith Barton, a 2012 novel by Wendy Powers and Robin McLeod, tells the backstory of Judy Barton.\nThe lyric video for \"Look What You Made Me Do\" by Taylor Swift, co-produced by Kahn, pays homage to Saul Bass' designs for the film's poster and opening credits, with the use of a similar font and spiral motifs.\nAlfred Hitchcock \u2013 Vertigo, a 2021 adventure video game by developer Pendulo Studios and publisher Microids, contains a plot inspired by but not directly adapted from the film.\nFrancesco Ferrari Mines the Mission: A Homage to Vertigo, an original 2018 novel by Joseph Covino Jr., third in the San Francisco-set Francesco Ferrari private detective series, takes its template from both the Vertigo novel and film.\n\nPotential remake\nIn March 2023, it was reported that Paramount Pictures had acquired the remake rights to the film, with Steven Knight set to write the script and Robert Downey Jr. set to star.\n\nSee also\nAlfred Hitchcock filmography\nCinema of the United States\nList of American films of 1958\n\nNotes\nReferences\nBibliography\nAuiler, Dan (1999). Vertigo: The Making of a Hitchcock Classic. London: Titan Books.\nAuiler, Dan (2000). Vertigo: The Making of a Hitchcock Classic. Macmillan. ISBN 978-0-312-26409-3.\nBarr, Charles (2002). Vertigo. London: BFI. ISBN 978-0-85170-918-5.\nCanning, Bob (2010). Block, Alex Ben; Wilson, Lucy Autrey (eds.). George Lucas's Blockbusting: A Decade-By-Decade Survey of Timeless Movies Including Untold Secrets of Their Financial and Cultural Success. HarperCollins. ISBN 978-0-06-177889-6.\nEliot, Marc (2006). Jimmy Stewart: a biography. Harmony Books. ISBN 978-1-4000-5221-9.\nHyder, Paul (2018). HITCHCOCK'S VERTIGO: A Personal Journey Through the Greatest Film Ever Made. Independently published. ISBN 979-8670236461.\nKraft, Jeff; Leventhal, Aaron (2002). Footsteps in the Fog: Alfred Hitchcock's San Francisco. Santa Monica Press. ISBN 978-1-891661-27-3.\nJones, Dan (2002). The Dime Novel and the Master of Suspense: The Adaptation of D'Entre Les Morts Into Vertigo. Saint Paul, Minn.: University of St. Thomas.\nKlein, Richard B. (2005). Coles, Felice Anne (ed.). In memory of Richard B. Klein: essays in contemporary philology. Romance Monographs, University of Mississippi.\nLev, Peter (2006). Transforming the Screen, 1950\u20131959. Vol. 7 of History of the American Cinema. University of California Press. ISBN 978-0-520-24966-0.\nMamer, Bruce (2008). Film Production Technique: Creating the Accomplished Image (5th ed.). Cengage Learning. ISBN 978-0-495-41116-1.\nMcGilligan, Patrick (2003). Alfred Hitchcock: A Life in Darkness and Light. ReganBooks.\nMonaco, Paul (2010). A History of American Movies: A Film-By-Film Look at the Art, Craft, and Business of Cinema. Scarecrow Press. ISBN 978-0-8108-7434-3.\nParish, James Robert (2008). It's Good to Be the King: The Seriously Funny Life of Mel Brooks. John Wiley & Sons. ISBN 978-0-470-22526-4.\nShipka, Danny (2011). Perverse Titillation: The Exploitation Cinema of Italy, Spain and France, 1960\u20131980 (illustrated ed.). McFarland & Company. ISBN 978-0-7864-4888-3.\nShone, Tom (2004). Blockbuster: How Hollywood Learned to Stop Worrying and Love the Summer. Simon & Schuster. ISBN 978-0-7432-6838-7.\nSipos, Tomas M. (2010). Horror Film Aesthetics: Creating The Visual Language of Fear. McFarland. ISBN 978-0-7864-4972-9.\nSpinks, Randall (2017). \"The Hallucinatory (Cultural) Logic of Hitchcock's Vertigo\". Quarterly Review of Film and Video. 34 (3): 212\u2013242. doi:10.1080\/10509208.2016.1222571. S2CID 193454124.\nTruffaut, Fran\u00e7ois; Hitchcock, Alfred (1985). Hitchcock. New York: Simon and Schuster. OCLC 273102.\nMoldes, Diego (2004). La huella de V\u00e9rtigo. Madrid: Ediciones JC. ISBN 978-8489564398.\n\nExternal links\n\nVertigo at IMDb\nVertigo at AllMovie\nVertigo at the TCM Movie Database\nVertigo at the AFI Catalog of Feature Films\nVertigo at Metacritic \nVertigo at Rotten Tomatoes\nVertigo at Letterboxd\nVertigo at Universal Pictures Home Entertainment\nEssay by Thomas Leitch at National Film Registry\nA Very Different \"Slice of Cake:\" Restoring Alfred Hitchcock's Vertigo\nEssay by Daniel Eagan in America's Film Legacy: The Authoritative Guide to the Landmark Movies in the National Film Registry, A&C Black, 2010 ISBN 0826429777, pages 546-547","197":"The vestibular nerve is one of the two branches of the vestibulocochlear nerve (the cochlear nerve being the other). In humans the vestibular nerve transmits sensory information from vestibular hair cells located in the two otolith organs (the utricle and the saccule) and the three semicircular canals via the vestibular ganglion of Scarpa. Information from the otolith organs reflects gravity and linear accelerations of the head. Information from the semicircular canals reflects rotational movement of the head. Both are necessary for the sensation of body position and gaze stability in relation to a moving environment.\nAxons of the vestibular nerve synapse in the vestibular nucleus are found on the lateral floor and wall of the fourth ventricle in the pons and medulla.\nIt arises from bipolar cells in the vestibular ganglion which is situated in the upper part of the outer end of the internal auditory meatus.\n\nStructure\nThe peripheral fibers divide into three branches (some sources list two): \n\nthe superior branch passes through the foramina in the area vestibularis superior and ends in the utricle and in the osseous ampullae of the superior and lateral semicircular ducts;\nthe fibers of the inferior branch traverse the foramina in the area vestibularis inferior and end in the saccule;\nthe posterior branch runs through the foramen singulare and supplies the ampulla of the posterior semicircular duct.\n\nFunction\nThe primary role of the vestibular nerve is to transmit information about balance of the head in relation to the body. The vestibular nerve dynamically updates the frame of reference of motor movement based on the orientation of the head in relation to the body. As an example, when standing upright and facing forward, if you wished to tilt your head to the right you would need to perform a slight leftward motor movement (shifting more of your weight to your left side) to maintain balance. While the head is still in motion, the response magnitude of alteration to motor coordinates is significantly reduced when compared to when the head is fixated in one position.\n\nClinical significance\nDamage\nDue to its role in transforming motor coordinates, the vestibular nerve implicitly plays a role in maintaining stable blood pressure during movement, maintaining balance control, spatial memory and spatial navigation. The most common causes of damage to the vestibular nerve are exposure to ototoxic antibiotics, M\u00e9ni\u00e8re's disease, encephalitis and some rare autoimmune disorders. Typically, patients with a damaged nerve suffer from acute attacks of vertigo accompanied by nausea\/vomiting, inability to maintain posture and horizontal nystagmus.\n\nRehabilitation\nRapid compensation to damage of the vestibular nerve occurs within seven to ten days of receiving the damage. A small percentage of patients with damage to the vestibular nerve experience recurrent symptoms.  These patients have not been able to undergo vestibular compensation and are left with long-term attacks of vertigo.  By administering betahistine to the damaged nerve over a long period of time, the process of vestibular compensation can be accelerated to alleviate symptoms. Patients can also learn strategies to recover their balance through physical therapy.\n\nSee also\nVestibular system\n\nAdditional images\nReferences\nThis article incorporates text in the public domain from page 906 of the 20th edition of Gray's Anatomy (1918)\n\nExternal links\nIllustration at dizziness-and-balance.com","198":"Labyrinthitis is inflammation of the labyrinth, a maze of fluid-filled channels in the inner ear. Vestibular neuritis is inflammation of the vestibular nerve (the nerve in the ear that sends messages related to motion and position to the brain). Both conditions involve inflammation of the inner ear. Labyrinths that house the vestibular system sense changes in the head's position or the head's motion. Inflammation of these inner ear parts results in a vertigo (sensation of the world spinning) and also possible hearing loss or tinnitus (ringing in the ears). It can occur as a single attack, a series of attacks, or a persistent condition that diminishes over three to six weeks. It may be associated with nausea, vomiting, and eye nystagmus.\nThe cause is often not clear. It may be due to a virus, but it can also arise from bacterial infection, head injury, extreme stress, an allergy, or as a reaction to medication. 30% of affected people had a common cold prior to developing the disease. Either bacterial or viral labyrinthitis can cause a permanent hearing loss in rare cases. This appears to result from an imbalance of neuronal input between the left and right inner ears.\n\nSigns and symptoms\nThe main symptoms are severe vertigo and nystagmus. The most common symptom for vestibular neuritis is the onset of vertigo that has formed from an ongoing infection or trauma. The dizziness sensation that is associated with vertigo is thought to be from the inner ear labyrinth. Rapid and undesired eye motion (nystagmus) often results from the improper indication of rotational motion.  Nausea, anxiety, and a general ill feeling are common due to the distorted balance signals that the brain receives from the inner ear system. Other common symptoms include tinnitus, ear ache, and a feeling of fullness in the ear.\n\nCauses\nSome people will report having an upper respiratory infection (common cold) or flu prior to the onset of the symptoms of vestibular neuritis; others will have no viral symptoms prior to the vertigo attack.  \nSome cases of vestibular neuritis are thought to be caused by an infection of the vestibular ganglion by the herpes simplex type 1 virus. However, the cause of this condition is not fully understood, and in fact, many different viruses may be capable of infecting the vestibular nerve.\nAcute localized ischemia of these structures also may be an important cause. Especially in children, vestibular neuritis may be preceded by symptoms of a common cold.  However, the causative mechanism remains uncertain.\nThis can also be brought on by pressure changes such as those experienced while flying or scuba diving.\n\nMechanism\nIn the vestibular system, there are three canals that are semicircular in shape that input sensory clues. These canals allow the brain to sense rotational motion and linear motion changes. The brain then uses the sensory input clues and the visual input clues from the vestibular system to retain balance. The vestibulo\u2013ocular reflex retains continuous visual focus during motion which is also the vestibular systems job during activity.\n\nTreatment\nThe treatment for vestibular neuritis depends on the cause. However, symptoms of vertigo can be treated in the same way as other vestibular dysfunctions with vestibular rehabilitation.\n\nPhysical therapy\nTypical treatments include combinations of head and eye movements, postural changes, and walking exercises. Specifically, exercises that may be prescribed include keeping eyes fixated on a specific target while moving the head, moving the head right to left at two targets at a significant distance apart, walking while keeping eyes fixated on a specific target, and walking while keeping eyes fixated on a specific target while also turning the head in different directions.\nThe main function behind repeating a combination of head and eye movements, postural changes and walking is that through this repetition, compensatory changes for the dysfunctions arising from peripheral vestibular structures may be promoted in the central vestibular system (brainstem and cerebellum).\nVestibular rehabilitation therapy is a highly effective way to substantially reduce or eliminate residual dizziness from labyrinthitis. VRT works by causing the brain to use already existing neural mechanisms for adaptation, neuroplasticity, and compensation. Vestibular neuritis rehabilitation is an effective and safe management to improve symptoms. The vestibular neuritis rehabilitation can improve symptoms or resolve the symptoms which is dependent on each individual. \nRehabilitation strategies most commonly used are:\n\nGaze stability exercises \u2013 moving the head from side to side while fixated on a stationary object (aimed at assisting the eye to fixate during head rotation without the input from the lost canal vestibulo\u2013ocular reflex).  An advanced progression of this exercise would be walking in a straight line while looking side to side by turning the head.\nHabituation exercises \u2013 movements designed to provoke symptoms and subsequently reduce the negative vestibular response upon repetition. Examples of these include Brandt\u2013Daroff exercises.\nFunctional retraining \u2013 including postural control, relaxation, and balance training.\nThese exercises function by challenging the vestibular system.  Progression occurs by increasing the amplitude of the head or focal point movements, increasing the speed of movement, and combining movements such as walking and head turning.  \nOne study found that patients who believed their illness was out of their control showed the slowest progression to full recovery, long after the initial vestibular injury had healed. The study revealed that the patient who compensated well was one who, at the psychological level, was not afraid of the symptoms and had some positive control over them. Notably, a reduction in negative beliefs over time was greater in those patients treated with rehabilitation than in those untreated. \"Of utmost importance, baseline beliefs were the only significant predictor of change in a handicap at 6 months follow-up.\"\n\nMedication\nVestibular neuritis is generally a self-limiting disease. Treatment with drugs is neither necessary nor possible. The effect of glucocorticoids has been studied, but they have not been found to significantly affect long-term outcome.\nSymptomatic treatment with antihistaminics such as cinnarizine, however, can be used to suppress the symptoms of vestibular neuritis while it spontaneously regresses. Prochlorperazine is another commonly prescribed medication to help alleviate the symptoms of vertigo and nausea.\n\nMental disorders\nBecause mood disorders can hamper recovery from labyrinthitis, treatment may also include any co-occurring anxiety disorder or depression. Severe anxiety episodes are usually addressed by short-term benzodiazepine therapy.\n\nPrognosis\nRecovery from acute labyrinthine inflammation generally takes from one to six weeks, but it is not uncommon for residual symptoms such as dysequilibrium and dizziness to last for a couple of months.\nRecovery from a temporarily damaged inner ear typically follows two phases:\n\nAn acute period, which may include severe vertigo and vomiting\napproximately two weeks of sub-acute symptoms and rapid recovery\n\nEpidemiology\nLabyrinthitis affects approximately 35 million people per year (approximately 3.5 cases per 100,000 people). It typically occurs in those between 30 and 60 years of age, and there are no significant differences between male and female incidence rates. In 95% of cases, sufferers experience a single attack and fully recover. Vestibular rehabilitation showed a statistically significant increase in controlling symptoms over no intervention in people who have vestibular neuritis.\n\nReferences\nExternal links\n\nLabyrinthitis at Curlie","199":"A vestibular schwannoma (VS), also called acoustic neuroma, is a benign tumor that develops on the vestibulocochlear nerve that passes from the inner ear to the brain. The tumor originates when Schwann cells that form the insulating myelin sheath on the nerve malfunction. Normally, Schwann cells function beneficially to protect the nerves which transmit balance and sound information to the brain. However, sometimes a mutation in the tumor suppressor gene, NF2, located on chromosome 22, results in abnormal production of the cell protein named Merlin, and Schwann cells multiply to form a tumor. The tumor originates mostly on the vestibular division of the nerve rather than the cochlear division, but hearing as well as balance will be affected  as the tumor enlarges.\nThe great majority of these VSs (95%) are unilateral, in one ear only. They are called \"sporadic\" (i.e., by-chance, non-hereditary). Although non-cancerous, they can do harm or even become life-threatening if they grow to press on other cranial nerves and vital structures such as the brainstem. Variations in the mutation determine the nature of the tumor's development. The only environmental exposure that has been definitely associated with the growth of a VS is therapeutic radiation exposure to the head.\n\nSymptoms of Sporadic VS\nSporadic VSs originate within the confining bony walls of the small (ca. 2 cm long) internal auditory canal. The most common early symptoms of these intracanalicular (IAC) VSs are gradual hearing loss and a feeling of fullness in the affected ear, some imbalance or dizziness, and tinnitus (ringing or other noise in the ear). Gradual single-sided hearing loss in the high frequencies is the first most obvious symptom for the great majority of patients. Headache as a presenting symptom of VS specifically is rare; facial symptoms (facial numbness, weakness) usually occur only as the tumor grows out of the canal and\/or after therapeutic treatment. Delayed diagnosis and misdiagnosis are not unusual. Initial hearing loss is usually subtle and may be attributed mistakenly to aging, earwax buildup, or perhaps exposure to some loud environmental noise. A sudden hearing loss, which is uncommon, might be misdiagnosed as M\u00e9ni\u00e8re's disease, an abnormality of the inner ear that also has tinnitus as a symptom. The brain's vestibular system usually compensates for early balance problems.\nThere have been cases of tumors that were actually asymptomatic until very large and at a critical stage. Tumor growth rates are highly variable: some small VSs (perhaps 50%) do not grow at all; some few grow for a time and then shrink; some appear dormant but suddenly grow rapidly. In general, although studies differ, VSs that grow are slow-growing at an average rate of 1.2 to 1.9 mm per year. IAC tumors that grow beyond 1.5 cm in diameter expand into the relatively empty space of the cerebellopontine angle, taking on the characteristic 'ice-cream-cone' appearance seen on MRIs. As 'space-occupying-lesions,' the tumors can reach 3 to 4 cm or more in size and infringe on the facial nerve (facial expression) and trigeminal nerve (facial sensation). Advanced hearing loss and spells of true vertigo may occur. Very large tumors are life-threatening when they press on the cerebellum or cause brainstem compression. Late symptoms of very large VS include headache, nausea, vomiting, sleepiness, mental confusion and eventually coma.\n\nNeurofibromatosis Type 2 (NF2)\nFor the most part, unilateral sporadic vestibular schwannomas are readily treated successfully by modern medical techniques. Having bilateral VSs is a more troublesome condition. Bilateralism is considered to be the hallmark and main diagnostic criterion of Neurofibromatosis Type II (NF2), a genetic disorder that is heritable, progressive, difficult to manage, and has a 1 in 2 chance of being passed on to each offspring. NF2 patients tend to have a more severe mutation, although there are mild mosaic cases in which only some cells carry the mutation. Genetic testing confirming mutation of the NF2 gene is recommended. About 50% of people with NF2 have a de novo mutation, and about 50% of these new mutations will be mild mosaic cases which are less likely to be passed on. NF2 patients may develop other cranial and spine tumors. NF2 develops during the teens or early adulthood, whereas sporadic VSs are diagnosed mostly in patients between the ages of 40\u201360 years. Both varieties of VS (sporadic and NF2) are very rare, accounting for only about 8% of all primary brain tumors. The incidence of NF2 is approximately 1 per 60,000 people.\n\nTumor sizes\nPatient surveys in the U.S. by the national Acoustic Neuroma Association (1998, 2007\u201308, 2012, 2014) showed that the percentage of diagnosed tumors 1.5 cm or less increased significantly from 23% to 47%. Researchers in Denmark reported (2004): \"The size of diagnosed tumors has decreased from a median of 35 mm in 1979 to 10 mm in 2001.\" In general, tumor size (diameter) is described as small (less than 1.5 cm), medium (1.5 to 2.5 cm), large (2.5 to 4.0 cm) and giant (more than 4.0 cm). (Note: 1 inch = 2.54 cm) Radiologists reporting on MRI scans use the Koos Grading Scale which relates tumor size to its proximity to the brainstem and nearby cranial nerves. Thus Koos grade 1 is a purely intrameatal (IAC) tumor, 1\u201310 mm in size; Koos grade 2, 10\u201320 mm, has extended into the cerebellopontine angle (CPA), but with no brainstem contact; Koos grade 3, 20\u201330 mm, fills the CPA space and touches on the brainstem, but without compression; and Koos grade 4, more than 30 mm in size, compresses the brainstem and nearby nerves, and displaces critical arteries.\n\nDiagnosis\nPreliminary diagnostic procedures include ear examination, hearing and vestibular testing. Typical symptoms include unilateral tinnitus, progressive hearing loss and vertigo. Usually diagnostic sensitivity is increased with one or more otological symptom. The rate of VS pick up with unilateral tinnitus alone using MRI has been shown to be <0.1%. The auditory brainstem response test (ABR) is a cost-effective test to see if a VS has perhaps compromised the cochlear nerve.\nComputed tomography (CT scan) of the head will detect moderate to large sized VS but can miss small sized VS. VS appears as isodense to surrounding brain parenchyma on CT. VS does not have calcifications in it. A large VS may expand the size of internal acoustic meatus (IAC) and may compromise hearing function because the nerves within the IAC are compressed, particularly the cochlear nerve. However, the facial nerve is less commonly affected. The main advantage of a CT scan is to assess the extent of bony involvement by VS. VS enhances when iodinated contrast is given. A contrasted CT scan of the temporal bone can be done if the patient is unable to undergo MRI scan.\nMRI scan is the imaging of choice because it can more accurately differentiate the mass from other tumors such as meningioma, facial nerve schwannoma, epidermoid cyst, arachnoid cyst, aneurysm, and brain metastasis. MRI scan also helps in surgical planning and follow-up of the tumor after surgery. VS is usually isointense on T1 weighted images, hyperintense on T2 weighted images, and enhances after given gadolinium contrast.\n\nManagement\nMicrosurgery for Sporadic VS\nThe Guidelines on the Treatment of Adults with Vestibular Schwannoma issued in 2018 by the Congress of Neurological Surgeons in the U.S. looked at the long-term evolution of treatments for VS. The Introduction to the Guidelines stated: \"The evolution in treatment over the last century has ultimately led to an environment where functional outcome has taken precedence over disease eradication. With multiple noninvasive management options available, the tolerance of cranial neuropathy in patients with small and medium-sized tumors is low. Today, hearing preservation, facial nerve function, and tumor control remain the primary benchmarks used to evaluate treatment effectiveness and compare outcomes.\" In other words, tumor management was able to give greater attention to preserving quality of life.\nThe three main surgical approaches to the tumor are the translabyrinthine (incision behind the ear to reach the bony labyrinth), the retrosigmoid (incision behind the ear to reach cerebellopontine angle) and the middle cranial fossa (incision in front of the ear to access the IAC from above). Tumor size is a major factor in determining approach selection. Adjunctive use of the endoscope for enhanced visualization during surgery for IAC tumors has gained attention as an emerging technique with advancing technology. For large tumors, a 'facial nerve sparing surgery' offers partial removals, to be followed (as needed) by stereotactic radiosurgery or radiotherapy for 'residuals'. The rate of 'tumor control' appeared to be similar to that for gross total removal surgeries. For small to medium size tumors, the appropriateness of so-called 'hearing preservation surgery' via either the Middle Fossa or Retrosigmoid approach remained controversial. Data from Denmark indicated that primary observation offered the best chance to preserve good hearing the longest. But preserving good hearing in the affected ear remained an elusive goal. Even during observation, although tumors showed no significant growth, hearing deterioration occurred. Stangerup et al. reported (2010) that most patients with 100% speech discrimination at diagnosis had the best chance of maintaining good hearing after ten years of observation.\nThe overall mortality rate for VS surgery is around 0.2% - 0.5%. The most common complications include facial nerve disorder (25.0%), cerebrospinal fluid leakage (8.5%) and postoperative neurological complications (8.4%).\n\nRadiosurgery and radiotherapy\nThe 'Patient Survey' in the U.S. in 2014 by the national Acoustic Neuroma Association showed that 29% of VS patients reported radiosurgery (17%) or radiotherapy (12%) as their treatment of choice. Radiosurgery is the delivery to the VS of a concentrated high radiation dose in a one-day session, whereas radiotherapy involves multiple treatment sessions where the total radiation dose is spread out in fractions over a few days or 3\u20134 weeks. The main objective in either case is 'tumor control' by damaging tumor cell DNA and stopping blood vessel proliferation (angiogenesis) needed for tumor growth.\nTumors may swell following radiation, but this increase in size is transient and does not signal a failed procedure. The average success rate for stereotactic radiosurgery is reported to be 95.5%. Radiation doses are calculated in terms of Gray\/Gy\u2014the measure of energy deposited by ionizing radiation per kilogram of matter. Since VSs are noninvasive and well-demarcated from surrounding tissues, radiosurgeons are able to target the tumor volume closely and minimize normal tissue damage. Multisession radiotherapy recommends the advantage of giving time between sessions for biological repair of any damage to normal tissues that may occur, and allows for radiation of the tumor at different times in the cell growth cycle.\nThe CyberKnife radiation system introduced in 1994 recommends a protocol of three sessions known as hypofractionation. Radiation dosages overall were reduced over the years as experience showed that excellent tumor control rates could be maintained even as dosages were lowered to benefit hearing preservation and facial nerve function. Generally, single-session Gamma Knife radiosurgery is limited in use to VSs less than 3 cm in diameter to avoid possible complications with facial nerves, brainstem and the cochlea apparatus. The risk of radiation-induced secondary tumors is very small, in the range of 0.01-0.02%. The risk for NF2 patients appears to be slightly higher.\n\nMedical and gene therapies\nTo date, there is no fully efficacious medical therapy for VS. The complexity of the molecular biology research involved is truly challenging. Clinical trials are in progress for other drugs such as everolimus, lapatinib and mifepristone. Common aspirin has been studied as a low-risk therapeutic option, but emerging evidence suggests that aspirin and other NSAID use may not prevent VS tumor growth.\n\nObservation of Small VS\nThe 1991 NIH Consensus Statement observed: \"There is evidence that some patients with unilateral vestibular schwannoma and a subgroup of patients with NF2 may have tumors that fail to progress rapidly, resulting in stable neurologic function for a long time. The use of MRI with contrast enhancement has resulted in the identification of patients with very small, relatively asymptomatic vestibular schwannomas for whom the natural history is unknown. Conservative management may be appropriate for these patients.\" At the time, conservative management (i.e., observation, 'wait-and-watch'\/'wait-and-scan') was reserved mainly for elderly or infirm patients.\nData on tumor sizes at diagnosis and tumor growth behavior was sparse or contradictory and mainly short-term. The Central Brain Tumor Registry of the U.S., established in 1992, only began to keep records for benign tumors like VS in 2004. In 2006, a landmark study from Denmark, entitled \"The Natural History of Vestibular Schwannoma,\" initiated a significant trend toward observation for managing small VS. Researchers in Copenhagen had the advantage that data for all Danish patients diagnosed with VS since 1976 was entered into a national database. The 2006 study by Stangerup et al. looked at the data for 1,818 patients (1976\u20132004) comparing intrameatal VS (in the auditory canal) and extrameatal VS (into the cerebellopontine angle). Remarkably, for the 729 patients having observation management via interval MRI scans, tumor growth was observed in only 17% of intrameatal tumors and 29% of extrameatal tumors. The mean observation time was 3.6 years (range 1\u201315 years). The researchers concluded: \"VS growth occurs within the first 5 years after diagnosis in a limited number of tumors, primarily in tumors with an extrameatal extension. These findings justify primary observation of small tumors.\" The medical community and VS patients responded positively to these findings.\n'Patient Surveys' in the U.S. by the national Acoustic Neuroma Association showed an increase in \"Wait-and-Watch\" from 4% of respondents in 1998 to 20% in 2012. An important study in 2015 entitled \"The Changing Landscape of Vestibular Schwannoma Management in the United States \u2013 A Shift Toward Conservatism,\" predicted that half of all cases of VS would be managed initially with observation by 2026. Stangerup et al. have urged caution (2019): \"Most studies show that if tumor growth occurs, it is usually detected within the first few years of diagnosis. However, long-term observational studies are desperately needed to guide the development of evidence-based surveillance algorithms designed to detect late tumor progression.\" Also (see Medical and Gene Therapies, above): \"Basic science and identification of genes, molecular pathways, and networks related to tumor growth are likely to change our approach to treatment including conservative management.\"\n\nIncidence of Sporadic VS\nIn 2015, researchers at the Cleveland Clinic in Ohio used population-based data of the Central Brain Tumor Registry of the U.S. to calculate an incidence of 10.9 per million of population, or about 3,300 cases of VS per year. A higher incidence up to 29.3 per million of population was found for the 65-74 year-old age group. There was no significant difference in incidence by gender. Incidence was higher in Asian Pacific Islanders, and lower in African Americans and Hispanics. The annual number of diagnosed VS increased significantly worldwide by the early 1990s with the introduction of magnetic resonance imaging (MRI). Notably, epidemiologists in Denmark (population of 5.7 million in 2015) reported 193 cases of VS for 2015\u2014an incidence of 34 per million of population per year. The first MRI scanner in Denmark was functional in 1989, and by 2015 the number increased to approximately 100.\n\nHistory\nEarly descriptions\nIn 1777, Eduard Sandifort of Leiden, the Netherlands, wrote a postmortem first description of a vestibular schwannoma. He observed \"a certain hard body adherent to the auditory nerve,\" and concluded this cause of deafness was beyond the reach of medication or surgery and was therefore incurable. The Schwann cells that multiply to form a VS on the vestibulocochlear nerve were identified 60 years later in 1838 by the German physiologist Theodor Schwann. In 1895, Thomas Annandale, a general surgeon at the Royal Infirmary in Edinburgh, Scotland, was the first to successfully localize and surgically remove a VS. Finger dissection of VS to 'shell out' the tumor was typical. The main goal when dealing with large tumors was preservation of life.\n\nEarly operations\nIn the early 1900s the mortality rate for VS surgery was in the range of 75 to 85%. Surgeons typically delayed invasive intervention as long as possible as a last resort. Harvey Cushing (1869\u20131939) is known as 'the father of neurosurgery for VS.' His basic study published in 1917 was entitled Tumors of the Nervus Acusticus and the Syndrome of the Cerebellopontine Angle. Cushing perfected the retrosigmoid surgical approach, and by doing suboccipital craniotomy and subtotal removals he was able to reduce mortality to 4% by 1931. Cushing worked at Brigham Hospital in Boston. An equally famous specialist for VS at Johns Hopkins in Baltimore was Walter E. Dandy (1886\u20131946), a former pupil of Cushing who advocated total tumor removals. In 1931, he reported a complete removal with preservation of the facial nerve.\n\nImaging\nA major problem for the pioneers in VS neurosurgery was the lack of adequate imaging for spotting small tumors. Treating tumors that grew overly large in the cerebellopontine angle resulted in poor outcomes for the goals of facial nerve and hearing preservation. For imaging, conventional X-rays began to be used in the 1920s and CT scanners were introduced in the 1960s, but all were superseded by 'gold standard' MRIs in the 1980s. Facial nerve monitoring was added in 1979. William F. House (1923\u20132012) pioneered the use of the operating microscope, and (with William Hitselberger) popularized the translabyrinthine and middle fossa surgery approaches for VS. The operating microscope would be used in virtually all operations by 1998. House's son, John W. House, proposed (1983) and, with Derald E. Brackmann, developed the House-Brackmann grading system for reporting facial nerve outcomes following VS surgery.\n\nTreatment\nSurgical treatment\nIn 1986, at a meeting for neurosurgeons in San Francisco, the House group endorsed a guideline for the surgical treatment of VS: \"The best opportunity for successful removal of an acoustic neuroma is when it is small: when first diagnosed.\"  At the time of the NIH Consensus Development Conference for Acoustic Neuroma in 1991, microsurgery was definitely the predominant management strategy for VS. The consensus conference's panel of experts reported: \"Currently, the ideal treatment for symptomatic patients with vestibular schwannoma is the total excision of the tumor in a single stage with minimal morbidity and mortality and with preservation of neurological function.\" Total surgical removal was not, however, the only treatment indicated at the time. Partial removal was used to debulk very large VSs threatening to compress vital structures. And long-term observation management was deemed appropriate as MRI scans began to reveal more and more small tumors with stable neurological symptoms.\n\nRadiosurgery\nBy the 1980s, radiation therapy was also becoming an attractive option for VS patients. At the Karolinska Institute in Stockholm, Sweden, Lars Leksell (1907\u20131986) pioneered Gamma Knife radiosurgery. In 1951, he published his landmark scientific paper, \"The Stereotaxic Method and Radiosurgery of the Brain,\" defining radiosurgery as \"the destruction of intracranial targets without opening the skull using very high doses of ionizing radiation in stereotactically directed beams.\" The first Gamma Knife machine was operable in Sweden in 1969, and the first Gamma Knife in the U.S. was installed in 1987 at the Presbyterian University Hospital in Pittsburgh, PA. Departments of radiation oncology at major medical centers began to modify X-ray linear accelerators (linacs) to do single-session radiosurgery and multiple-session radiotherapy. In 1991, the U.S. National Institutes of Health convened a Consensus Development Conference (December 11\u201313, 1991) for Acoustic Neuroma (Vestibular Schwannoma) to evaluate management of the disorder and recommend areas for future activity and research.\"\n\nMedical and gene therapies\nThe 1991 NIH Consensus Statement recommended attention to \"the development of pharmaceutical and other alternative medical treatments, such as tumor suppressing agents.\" Ideally, a drug could be found to permanently shrink or eradicate VSs, with minimal side effects. A key step forward in 1993 was the identification of the NF2 gene and its protein product Merlin, which modulates the complex molecular signaling pathways that control cell proliferation. These pathways that drive VS formation (tumorigenesis) and growth are currently under investigation.\nA second important field of study in molecular biology investigates ways to stop the formation (angiogenesis) of the new blood vessels that are needed to support tumor growth by supplying nutrients and oxygen. In 1998, the glycoprotein named VEGF (vascular endothelial growth factor) that initiates proliferation was discovered. An anti-VEGF drug named bevacizumab (Avastin) was developed and showed promise in stopping this vascular proliferation. Unfortunately, when tested for NF2 tumors, the therapy required prolonged treatment resulting in hypertension and impaired wound healing.\nThe development of a new generation of drugs may become a secondary therapy in view of advances in genome editing during the 1990s that led to the invention of CRISPR in 2009. CRISPR has become the preferred genome editing tool whereby diseases may be treated by correcting causative mutations directly in a patient's genome.\n\nNotable people\nAmerican actor, director, humanitarian, social activist and film producer Mark Ruffalo was diagnosed with vestibular schwannoma in 2001 which resulted in a period of partial facial paralysis. He recovered from the paralysis; however, he became deaf in his left ear as a result of the tumor.\nGuitarist\/composer\/producer David Torn was diagnosed with an acoustic neuroma in 1992. It required intricate surgery that left him deaf in the right ear and burdened by many other health obstacles.\nAmerican actress and designer Tara Subkoff was diagnosed with schwannoma in 2009.  She successfully underwent surgery, but was left with permanent nerve damage and deafness in her right ear.\nTionne Watkins, better known by her stage name T-Boz, R&B singer from the R&B\/hip-hop group TLC, was diagnosed with a strawberry-sized acoustic neuroma on her vestibular nerve in 2006. Many physicians refused to remove the tumor due to her sickle-cell-related complications, leaving her alternatives grim. Ultimately, she underwent surgery at Cedars-Sinai Hospital in Los Angeles.\nEnglish comedian, artist, surrealist, musician, actor, and television presenter Vic Reeves revealed that he was diagnosed with a grape-sized vestibular schwannoma at age 62. He has lost all hearing in the left ear as a result.\n\nSee also\nCerebellopontine angle syndrome\n\nReferences\nFurther reading\nExternal links\n\nVestibular schwannoma at Curlie","200":"The vestibular system, in vertebrates, is a sensory system that creates the sense of balance and spatial orientation for the purpose of coordinating movement with balance. Together with the cochlea, a part of the auditory system, it constitutes the labyrinth of the inner ear in most mammals.\nAs movements consist of rotations and translations, the vestibular system comprises two components: the semicircular canals, which indicate rotational movements; and the otoliths, which indicate linear accelerations. The vestibular system sends signals primarily to the neural structures that control eye movement; these provide the anatomical basis of the vestibulo-ocular reflex, which is required for clear vision. Signals are also sent to the muscles that keep an animal upright and in general control posture; these provide the anatomical means required to enable an animal to maintain its desired position in space.\nThe brain uses information from the vestibular system in the head and from proprioception throughout the body to enable the animal to understand its body's dynamics and kinematics (including its position and acceleration) from moment to moment. How these two perceptive sources are integrated to provide the underlying structure of the sensorium is unknown.\n\nSemicircular canal system\nThe semicircular canal system detects rotational movements. Semicircular canals are its main tools to achieve this detection.\n\nStructure\nSince the world is three-dimensional, the vestibular system contains three semicircular canals in each labyrinth. They are approximately orthogonal (at right angles) to each other, and are the horizontal (or lateral), the anterior semicircular canal (or superior), and the posterior (or inferior)  semicircular canal. Anterior and posterior canals may collectively be called vertical semicircular canals.\n\nMovement of fluid within the horizontal semicircular canal corresponds to rotation of the head around a vertical axis (i.e. the neck), as when doing a pirouette.\nThe anterior and posterior semicircular canals detect rotations of the head in the sagittal plane (as when nodding), and in the frontal plane, as when cartwheeling. Both anterior and posterior canals are oriented at approximately 45\u00b0 between frontal and sagittal planes.\nThe movement of fluid pushes on a structure called the cupula which contains hair cells that transduce the mechanical movement to electrical signals.\n\nPush-pull systems\nThe canals are arranged in such a way that each canal on the left side has an almost parallel counterpart on the right side. Each of these three pairs works in a push-pull fashion: when one canal is stimulated, its corresponding partner on the other side is inhibited, and vice versa.\nThis push-pull system makes it possible to sense all directions of rotation: while the right horizontal canal gets stimulated during head rotations to the right (Fig 2), the left horizontal canal gets stimulated (and thus predominantly signals) by head rotations to the left.\nVertical canals are coupled in a crossed fashion, i.e. stimulations that are excitatory for an anterior canal are also inhibitory for the contralateral posterior, and vice versa.\n\nVestibulo-ocular reflex (VOR)\nThe vestibular-ocular reflex (VOR) is a reflex eye movement that stabilizes images on the retina during head movement by producing an eye movement in the direction opposite to head movement, thus preserving the image on the center of the visual field. For example, when the head moves to the right, the eyes move to the left, and vice versa. Since slight head movements are present all the time, the VOR is very important for stabilizing vision: patients whose VOR is impaired find it difficult to read because they cannot stabilize the eyes during small head tremors. The VOR reflex does not depend on visual input and works even in total darkness or when the eyes are closed.\nThis reflex, combined with the push-pull principle described above, forms the physiological basis of the Rapid head impulse test or Halmagyi-Curthoys-test, in which the head is rapidly and forcefully moved to the side while observing whether the eyes keep looking in the same direction.\n\nMechanics\nThe mechanics of the semicircular canals can be described by a damped oscillator. If we designate the deflection of the cupula with \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  \n, and the head velocity with \n  \n    \n      \n        \n          \n            \n              q\n              \u02d9\n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {q}}}\n  \n, the cupula deflection is approximately\n\n  \n    \n      \n        \u03b8\n        (\n        s\n        )\n        =\n        \n          \n            \n              \u03b1\n              s\n            \n            \n              (\n              \n                T\n                \n                  1\n                \n              \n              s\n              +\n              1\n              )\n              (\n              \n                T\n                \n                  2\n                \n              \n              s\n              +\n              1\n              )\n            \n          \n        \n        \n          \n            \n              q\n              \u02d9\n            \n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle \\theta (s)={\\frac {\\alpha s}{(T_{1}s+1)(T_{2}s+1)}}{\\dot {q}}(s)}\n  \n\n\u03b1 is a proportionality factor, and s corresponds to the frequency. For fluid simulations, the endolymph has roughly the same density and viscosity as water. The cupula has the same density as endolymph, and it is a jelly mostly made of polysaccharides with Young's modulus \n  \n    \n      \n        5.4\n        \n        \n          P\n          a\n        \n      \n    \n    {\\displaystyle 5.4\\;\\mathrm {Pa} }\n  \n. \nT1 is the characteristic time required for the cupula to accelerate until it reaches terminal velocity, and T2 is the characteristic time required for the cupula to relax back to neutral position. The cupula has a small inertia compared to the elastic force (due to the jelly) and the viscous force (due to the endolymph), so T1 is very small compared to T2. For humans, the time constants T1 and T2 are approximately 5 ms and 20 s, respectively. As a result, for typical head movements, which cover the frequency range of 0.1 Hz and 10 Hz, the deflection of the cupula is approximately proportional to the head velocity. This is very useful since the velocity of the eyes must be opposite to the velocity of the head to maintain clear vision.\n\nCentral processing\nSignals from the vestibular system also project to the cerebellum (where they are used to keep the VOR effective, a task usually referred to as learning or adaptation) and to different areas in the cortex. The projections to the cortex are spread out over different areas, and their implications are currently not clearly understood.\n\nProjection pathways\nThe vestibular nuclei on either side of the brainstem exchange signals regarding movement and body position. These signals are sent down the following projection pathways.\n\nTo the cerebellum. Signals sent to the cerebellum are relayed back as muscle movements of the head, eyes, and posture.\nTo nuclei of cranial nerves III, IV, and VI. Signals sent to these nerves cause the vestibular-ocular reflex. They allow the eyes to fix on a moving object while staying in focus.\nTo the reticular formation. Signals sent to the reticular formation signal the new posture the body has taken on, and how to adjust circulation and breathing due to body position.\nTo the spinal cord. Signals sent to the spinal cord allow quick reflex reactions to both the limbs and trunk to regain balance.\nTo the thalamus. Signals sent to the thalamus allow for head and body motor control as well as being conscious of body position.\n\nOtolithic organs\nWhile the semicircular canals respond to rotations, the otolithic organs sense linear accelerations. Humans have two otolithic organs on each side, one called the utricle, the other called the saccule. The utricle contains a patch of hair cells and supporting cells called a macula. Similarly, the saccule contains a patch of hair cells and a macula. Each hair cell of a macula has forty to seventy stereocilia and one true cilium called a kinocilium. The tips of these cilia are embedded in an otolithic membrane. This membrane is weighted down with protein-calcium carbonate granules called otoconia. These otoconia add to the weight and inertia of the membrane and enhance the sense of gravity and motion. With the head erect, the otolithic membrane bears directly down on the hair cells and stimulation is minimal. However, when the head is tilted, the otolithic membrane sags and bends the stereocilia, stimulating the hair cells. Any orientation of the head causes a combination of stimulation to the utricles and saccules of the two ears. The brain interprets head orientation by comparing these inputs to each other and other input from the eyes and stretch receptors in the neck, thereby detecting whether the head is tilted or the entire body is tipping. Essentially, these otolithic organs sense how quickly you are accelerating forward or backward, left or right, or up or down.  Most of the utricular signals elicit eye movements, while the majority of the saccular signals projects to muscles that control our posture.\nWhile the interpretation of the rotation signals from the semicircular canals is straightforward, the interpretation of otolith signals is more difficult: since gravity is equivalent to constant linear acceleration, one somehow has to distinguish otolith signals that are caused by linear movements from those caused by gravity. Humans can do that quite well, but the neural mechanisms underlying this separation are not yet fully understood.\nHumans can sense head tilting and linear acceleration even in dark environments because of the orientation of two groups of hair cell bundles on either side of the striola.  Hair cells on opposite sides move with mirror symmetry, so when one side is moved, the other is inhibited.  The opposing effects caused by a tilt of the head cause differential sensory inputs from the hair cell bundles allowing humans to tell which way the head is tilting. Sensory information is then sent to the brain, which can respond with appropriate corrective actions to the nervous and muscular systems to ensure that balance and awareness are maintained.\n\nExperience from the vestibular system\nExperience from the vestibular system is called equilibrioception. It is mainly used for the sense of balance and for spatial orientation. When the vestibular system is stimulated without any other inputs, one experiences a sense of self-motion. For example, a person in complete darkness and sitting in a chair will sense that he or she has turned to the left if the chair is turned to the left. A person in an elevator, with essentially constant visual input, will sense she is descending as the elevator starts to descend. There are a variety of direct and indirect vestibular stimuli which can make people sense they are moving when they are not, not moving when they are, tilted when they are not, or not tilted when they are. Although the vestibular system is a very fast sense used to generate reflexes, including the righting reflex, to maintain perceptual and postural stability, compared to the other senses of vision, touch and audition, vestibular input is perceived with delay.\n\nPathologies\nDiseases of the vestibular system can take different forms and usually induce vertigo and instability or loss of balance, often accompanied by nausea. The most common vestibular diseases in humans are vestibular neuritis, a related condition called labyrinthitis, M\u00e9ni\u00e8re's disease, and BPPV. In addition, the vestibular system's function can be affected by tumours on the vestibulocochlear nerve, an infarct in the brain stem or in cortical regions related to the processing of vestibular signals, and cerebellar atrophy.\nSince the function of the vestibular hair cells is to detect levels of carbon dioxide in the blood and to transmit such information to the brain, a loss of vestibular hair cells can cause death.\nWhen the vestibular system and the visual system deliver incongruous results, nausea often occurs. When the vestibular system reports movement but the visual system reports no movement, the motion disorientation is often called motion sickness (or seasickness, car sickness, simulation sickness, or airsickness). In the opposite case, such as when a person is in a zero-gravity environment or during a virtual reality session, the disoriented sensation is often called space sickness or space adaptation syndrome. Either of these \"sicknesses\" usually cease once the congruity between the two systems is restored.\nAlcohol can also cause alterations in the vestibular system for short periods and will result in vertigo and possibly nystagmus due to the variable viscosity of the blood and the endolymph during the consumption of alcohol. The term for this is positional alcohol nystagmus (PAN):\n\nPAN I - The alcohol concentration is higher in the blood than in the vestibular system, hence the endolymph is relatively dense.\nPAN II - The alcohol concentration is lower in the blood than in the vestibular system, hence the endolymph is relatively dilute.\nPAN I will result in subjective vertigo in one direction and typically occurs shortly after ingestion of alcohol when blood alcohol levels are highest. PAN II will eventually cause subjective vertigo in the opposite direction. This occurs several hours after ingestion and after a relative reduction in blood alcohol levels.\nBenign paroxysmal positional vertigo (BPPV) is a condition resulting in acute symptoms of vertigo. It is probably caused when pieces that have broken off otoliths have slipped into one of the semicircular canals. In most cases, it is the posterior canal that is affected. In certain head positions, these particles shift and create a fluid wave which displaces the cupula of the canal affected, which leads to dizziness, vertigo and nystagmus.\nA similar condition to BPPV may occur in dogs and other mammals, but the term vertigo cannot be applied because it refers to subjective perception. Terminology is not standardized for this condition.\nA common vestibular pathology of dogs and cats is colloquially known as \"old dog vestibular disease\", or more formally idiopathic peripheral vestibular disease, which causes a sudden episode of loss of balance, circling head tilt, and other signs. This condition is very rare in young dogs but fairly common in geriatric animals, and may affect cats of any age.\nVestibular dysfunction has also been found to correlate with cognitive and emotional disorders, including depersonalization and derealization.\n\nOther vertebrates\nThough humans as well as most other vertebrates exhibit three semicircular canals in their vestibular systems, lampreys and hagfish are vertebrates that deviate from this trend. The vestibular systems of lampreys contain two semicircular canals while those of hagfish contain a single canal. The lamprey's two canals are developmentally similar to the anterior and posterior canals found in humans. The single canal found in hagfish appears to be secondarily derived.\nAdditionally, the vestibular systems of lampreys and hagfish differ from those found in other vertebrates in that the otolithic organs of lampreys and hagfish are not segmented like the utricle and saccule found in humans, but rather form one continuous structure referred to as the macula communis.\nBirds possess a second vestibular organ in the back, the lumbosacral canals. Behavioral evidence suggests that this system is responsible for stabilizing the body during walking and standing.\n\nInvertebrates\nA large variety of vestibular organs are present in invertebrates. A well-known example is the halteres of flies (Diptera) which are modified hind wings.\n\nSee also\nDark cell\nMigraine-associated vertigo\nStatocyst\nList of distinct cell types in the adult human body\n\nReferences\nFurther reading\nBrandt, Thomas (2003). Vertigo: It's Multisensory Syndromes. Berlin: Springer. ISBN 978-0-387-40500-1. OCLC 52472049. (Comment: For clinicians, and other professionals working with dizzy patients.)\nBrill, Christopher; Hancock, Peter A.; Gilson, Richard D. (2003). \"Driver Fatigue: Is Something Missing?\" (PDF). University of Central Florida. Archived from the original (PDF) on 2016-03-04. Retrieved 2013-08-09. (Comment: Research on driver or motion-induced sleepiness aka 'sopite syndrome' links it to the vestibular labyrinths.)\nCullen, Soroush; Cullen, Kathleen (2008). \"Vestibular system\". Scholarpedia. 3 (1): 3013. Bibcode:2008SchpJ...3.3013C. doi:10.4249\/scholarpedia.3013.\nHighstein, S. M.; Fay, R. R.; Popper, A. N., eds. (2004). The vestibular system. Berlin: Springer. ISBN 978-0-387-98314-1. OCLC 56068617. (Comment: A book for experts, summarizing the state of the art in our understanding of the balance system)\nLawson, Ben D; Rupert, Angus H; Kelley, Amanda M. \"Mental Disorders Comorbid with Vestibular Pathology\". A preview of an article on how vestibular disorders can cause symptoms that look like mental disorders.\n\nExternal links\nVestibular Disorders Association For more information about vestibular (inner ear balance) disorders.\n(Video) Head Impulse Testing site (vHIT) Site with thorough information about vHIT\nSensesWeb Archived 2007-05-20 at the Wayback Machine, which contains animations of all sensory systems, and additional links.\nDizzytimes.com Archived 2010-08-05 at the Wayback Machine Online Community for people with vertigo and dizziness.\nVestibular System, Neuroscience Online (electronic neuroscience textbook)","201":"The vestibule is the central part of the bony labyrinth in the inner ear, and is situated medial to the eardrum, behind the cochlea, and in front of the three semicircular canals.\nThe name comes from the Latin vestibulum, literally an entrance hall.\n\nStructure\nThe vestibule is somewhat oval in shape, but flattened transversely; it measures about 5 mm from front to back, the same from top to bottom, and about 3 mm across.\nIn its lateral or tympanic wall is the oval window, closed, in the fresh state, by the base of the stapes and annular ligament.\nOn its medial wall, at the forepart, is a small circular depression, the recessus sph\u00e6ricus, which is perforated, at its anterior and inferior part, by several minute holes (macula cribrosa media) for the passage of filaments of the acoustic nerve to the saccule; and behind this depression is an oblique ridge, the crista vestibuli, the anterior end of which is named the pyramid of the vestibule.\nThis ridge bifurcates below to enclose a small depression, the fossa cochlearis, which is perforated by a number of holes for the passage of filaments of the acoustic nerve which supply the vestibular end of the cochlear duct.\nThe orifice of the vestibular aqueduct is the hind part of the medial wall; it extends to the posterior surface of the petrous portion of the temporal bone.\nIt transmits a small vein and contains a tubular prolongation of the membranous labyrinth, the endolymphatic duct, which ends in a cul-de-sac between the layers of the dura mater within the cranial cavity.\nOn the upper wall or roof, there is a transversely oval depression, the recessus ellipticus, separated from the recessus sph\u00e6ricus by the crista vestibuli already mentioned.\nThe pyramid and adjoining part of the recessus ellipticus are perforated by a number of holes (macula cribrosa superior).\nThe apertures in the pyramid transmit the nerves to the utricle; those in the recessus ellipticus are the nerves to the ampull\u00e6 of the superior and lateral semicircular ducts.\nBehind, the five orifices of the semicircular canals can be found.\nIn the frontal view, there is an elliptical opening which communicates with the vestibular duct of the cochlea.\n\nAdditional images\nReferences\nThis article incorporates text in the public domain from page 1047 of the 20th edition of Gray's Anatomy (1918)\n\nSee also\nVestibular system","202":"The vestibulo-ocular reflex (VOR) is a reflex that acts to stabilize gaze during head movement, with eye movement due to activation of the vestibular system, it is also known as the Cervico-ocular reflex. The reflex acts to stabilize images on the retinas of the eye during head movement. Gaze is held steadily on a location by producing eye movements in the direction opposite that of head movement. For example, when the head moves to the right, the eyes move to the left, meaning the image a person sees stays the same even though the head has turned. Since slight head movement is present all the time, VOR is necessary for stabilizing vision: people with an impaired reflex find it difficult to read using print, because the eyes do not stabilise during small head tremors, and also because damage to reflex can cause nystagmus.\nThe VOR does not depend on what is seen. It can also be activated by hot or cold stimulation of the inner ear, where the vestibular system sits, and works even in total darkness or when the eyes are closed. However, in the presence of light, the fixation reflex is also added to the movement. Most features of VOR are present in kittens raised in complete darkness. \nIn lower animals, the organs that coordinate balance and movement are not independent from eye movement. A fish, for instance, moves its eyes by reflex when its tail is moved.  Humans have semicircular canals, neck muscle  \"stretch\" receptors, and the utricle (gravity organ).  Though the semicircular canals cause most of the reflexes which are responsive to acceleration, the maintaining of balance is mediated by the stretch of neck muscles and the pull of gravity on the utricle (otolith organ) of the inner ear.\nThe VOR has both rotational and translational aspects. When the head rotates about any axis (horizontal, vertical, or torsional) distant visual images are stabilized by rotating the eyes about the same axis, but in the opposite direction. When the head translates, for example during walking, the visual fixation point is maintained by rotating gaze direction in the opposite direction, by an amount that depends on distance.\n\nFunction\nThe vestibulo-ocular reflex is driven by signals arising from the vestibular system of the inner ear. The semicircular canals detect head rotation and provide the rotational component, whereas the otoliths detect head translation and drive the translational component. The signal for the horizontal rotational component travels via the vestibular nerve through the vestibular ganglion and end in the vestibular nuclei in the brainstem. From these nuclei, fibers cross to the abducens nucleus of the opposite side of the brain. Here, fibres synapse with 2 additional pathways. One pathway projects directly to the lateral rectus muscle of the eye via the abducens nerve. Another nerve tract projects from the abducens nucleus by the medial longitudinal fasciculus to the oculomotor nucleus of the opposite side, which contains motor neurons that drive eye muscle activity, specifically activating the medial rectus muscle of the eye through the oculomotor nerve.\nAnother pathway (not in picture) directly projects from the vestibular nucleus through the ascending tract of Deiter's to the medial rectus muscle motor neuron of the same side. In addition there are inhibitory vestibular pathways to the ipsilateral abducens nucleus. However no direct vestibular neuron to medial rectus motoneuron pathway exists.\nSimilar pathways exist for the vertical and torsional components of the VOR.\n\nOculomotor integrator\nIn addition to these direct pathways, which drive the velocity of eye rotation, there is an indirect pathway that builds up the position signal needed to prevent the eye from rolling back to center when the head stops moving. This pathway is particularly important when the head is moving slowly because here position signals dominate over velocity signals. David A. Robinson discovered that the eye muscles require this dual velocity-position drive, and also proposed that it must arise in the brain by mathematically integrating the velocity signal and then sending the resulting position signal to the motoneurons. Robinson was correct: the 'neural integrator' for horizontal eye position was found in the nucleus prepositus hypoglossi in the medulla, and the neural integrator for vertical and torsional eye positions was found in the interstitial nucleus of Cajal in the midbrain. The same neural integrators also generate eye position for other conjugate eye movements such as saccades and smooth pursuit.\nThe integrator is leaky, with a characteristic leaking time of 20 s. For example, when the subject is sitting still and focusing on an object, and suddenly the light is turned off, the eyes would return to their neutral position in around 40 seconds even as the subject is attempting to keep the focus.\n\nExample\nFor instance, if the head is turned clockwise as seen from above, then excitatory impulses are sent from the semicircular canal on the right side via the vestibular nerve through Scarpa's ganglion and end in the right vestibular nuclei in the brainstem. From this nuclei excitatory fibres cross to the left abducens nucleus. There they project and stimulate the lateral rectus of the left eye via the abducens nerve. In addition, by the medial longitudinal fasciculus and oculomotor nuclei, they activate the medial rectus muscles on the right eye. As a result, both eyes will turn counter-clockwise.\nFurthermore, some neurons from the right vestibular nucleus directly stimulate the right medial rectus motor neurons, and inhibits the right abducens nucleus.\n\nIntegrated neural control\nThe VOR is controlled by a neural integrator. The neuron from each horizontal semicircular canal fires at a rate of \n  \n    \n      \n        (\n        90\n        +\n        0.4\n        \n          \n            \n              H\n              \u02d9\n            \n          \n        \n        )\n        \n        \n          H\n          z\n        \n      \n    \n    {\\displaystyle (90+0.4{\\dot {H}})\\;\\mathrm {Hz} }\n  \n, where \n  \n    \n      \n        \n          \n            \n              H\n              \u02d9\n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {H}}}\n  \n is the sensed horizontal angular velocity of the semicircular canal. The motoneuron commanding the horizontal eye muscles fires at a rate of \n  \n    \n      \n        (\n        4\n        \n        \n          H\n          z\n          \n            \/\n          \n          d\n          e\n          g\n        \n        )\n        \u03b8\n        +\n        (\n        1.0\n        \n        \n          H\n          z\n          \n            \/\n          \n          (\n          d\n          e\n          g\n          \n            \/\n          \n          s\n          e\n          c\n          )\n        \n        )\n        \n          \n            \n              \u03b8\n              \u02d9\n            \n          \n        \n      \n    \n    {\\displaystyle (4\\;\\mathrm {Hz\/deg} )\\theta +(1.0\\;\\mathrm {Hz\/(deg\/sec)} ){\\dot {\\theta }}}\n  \n, where \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  \n is the horizontal turning angle, and \n  \n    \n      \n        \n          \n            \n              \u03b8\n              \u02d9\n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {\\theta }}}\n  \n is its horizontal angular speed. The two terms account for the elasticity and viscosity of ocular tissue.\nThe rotational moment of inertia of the eye is negligible, as individuals wearing weighted contact lens that increases the rotational moment of inertia almost 100-fold still has the same VOR (p. 94 ).\n\nSpeed\nThe vestibulo-ocular reflex needs to be fast: for clear vision, head movement must be compensated almost immediately; otherwise, vision corresponds to a photograph taken with a shaky hand. Signals are sent from the semicircular canals using only three neurons, called the three neuron arc. This results in eye movements that lag head movement by less than 10 ms. The vestibulo-ocular reflex is one of the fastest reflexes in the human body.\n\nVOR suppression\nWhen a person tracks the movement of something with both their eyes and head together, the VOR is counterproductive to the goal of keeping the gaze and head angle aligned. Research indicates that there exists mechanisms in the brain to suppress the VOR using the active visual (retinal) feedback obtained by watching the object in motion. In the absence of visual feedback, such as when the object passes behind an opaque barrier, humans can continue to visually track the apparent position of the object using anticipatory (extra-retinal) systems within the brain, and the VOR is also suppressed during this activity. The VOR can even be cognitively suppressed, such as when following an imagined target with the eyes and head together, although the effect tends to be less dramatic than with visual feedback.\n\nGain\nThe \"gain\" of the VOR is defined as the change in the eye angle divided by the change in the head angle during the head turn. Ideally the gain of the rotational VOR is 1.0. The gain of the horizontal and vertical VOR is usually close to 1.0, but the gain of the torsional VOR (rotation around the line of sight) is generally low. The gain of the translational VOR has to be adjusted for distance, because of the geometry of motion parallax. When the head translates, the angular direction of near targets changes faster than the angular direction of far targets.\nIf the gain of the VOR is wrong (different from 1)\u2014for example, if eye muscles are weak, or if a person puts on a new pair of eyeglasses\u2014then head movement results in image motion on the retina, resulting in blurred vision. Under such conditions, motor learning adjusts the gain of the VOR to produce more accurate eye motion. This is what is referred to as VOR adaptation.\nNearsighted people who habitually wear negative spectacles have lower VOR gain. Farsighted people or aphakes who habitually wear positive spectacle have higher VOR gain. People who habitually wear contact lens show no change in VOR gain. Monocular, disconjugate adaptation of the VOR is possible, for example, after extraocular muscle palsy. (p. 27 )\nThe phase of the VOR can also adapt.\n\nLeak\nThe oculomotor integrator is a leaky integrator, with a characteristic leaking time of ~20 s. If the leaking time is too low, some form of adaptation occurs to \"patch the leak\" to raise the leaking time. It is hypothesized that the leaking integrator is constructed by a feedback circuit with a gain of slightly below 1, and adaptation occurs by adjusting the gain of the feedback circuit. The hypothesis is tested by using an specially patterned optokinetic drum that simulates the visual effect of having a very leaky oculomotor integrator. After 1 hour of viewing, the integrator becomes \"anti-leaky\", meaning that its value grows exponentially even in the absence of input. The eye motion becomes positive-feedback, meaning that if it is slightly to the left of a fixation target, it would drift even further to the left, and similarly for the right. It is also accompanied by nausea. (p. 84 )\n\nDisruption by ethanol\nEthanol consumption can disrupt the VOR, reducing dynamic visual acuity. In normal conditions, the cupula and the endolymph are equal in density (both are ). After ingesting ethanol, the ethanol diffuses into the cupula before it diffuses into the endolymph, because it is closer to blood capillaries. This makes the cupula temporarily lighter. In this state, if a person lies down with right cheek touching the ground, then the cupula in the left ear would float towards the left, creating an illusory sense of slow left-to-right head rotation. To compensate for this, the VOR moves the eyes towards the left slowly until it reaches the limit, and the eyes pull to the right rapidly (nystagmus). This is the positional alcohol nystagmus, phase I (PAN I). The unusual vestibular stimulation also caused motion sickness symptoms: illusions of bodily rotations, dizziness, and nausea. These symptoms subside in a few seconds after assuming an upright posture.\nAfter some time, the density of cupula and endolymph equalizes, removing the nystagmus effect. After ethanol is fully metabolized, the cupula returns to normal density first, creating nystagmus in the opposite direction (PAN II) during the hangover.\nAs predicted, heavy water (1.1 density of water) consumption has the exact opposite nystagmus effect compared to ethanol consumption. Consuming a mixture of heavy water (\n  \n    \n      \n        4\n        \n        \n          m\n          l\n          \n            \/\n          \n          k\n          g\n        \n      \n    \n    {\\displaystyle 4\\;\\mathrm {ml\/kg} }\n  \n) and ethanol (\n  \n    \n      \n        2\n        \n        \n          m\n          l\n          \n            \/\n          \n          k\n          g\n        \n      \n    \n    {\\displaystyle 2\\;\\mathrm {ml\/kg} }\n  \n) largely cancels out the effect. Macroglobulinaemia, or consuming glycerol (1.26 density of water), have similar effects as heavy water.\n\nClinical significance\nTesting\nThis reflex can be tested by the rapid head impulse test or Halmagyi\u2013Curthoys test, in which the head is rapidly moved to the side with force, and is controlled if the eyes succeed to remain to look in the same direction. When the function of the right balance system is reduced, by a disease or by an accident, a quick head movement to the right cannot be sensed properly anymore. As a consequence, no compensatory eye movement is generated, and the patient cannot fixate a point in space during this rapid head movement.\nThe head impulse test can be done at the bed side and used as a screening tool for problems with a person's vestibular system. It can also be diagnostically tested by doing a video-head impulse test (VHIT). In this diagnostic test, a person wears highly sensitive goggles that detect rapid changes in eye movement. This test can provide site-specific information on vestibular system and its function.\nAnother way of testing the VOR response is a caloric reflex test, which is an attempt to induce nystagmus (compensatory eye movement in the absence of head motion) by pouring cold or warm water into the ear. Also available is bi-thermal air caloric irrigations, in which warm and cool air is administered into the ear.\nThe vestibulo-ocular reflex can be tested by the aforementioned caloric reflex test; this plays an important part in confirming diagnosis of brainstem death.  A code of practice must be followed in this process, namely that of the Academy of Medical Royal Colleges.\n\nRelated terms\nCervico-ocular reflex\nSummary: Cervico-ocular reflex, also known by its acronym COR, involves the achievement of stabilization of a visual target, and image on the retina, through adjustments of gaze impacted by neck and, or head movements or rotations.  The process works in conjunction with the vestibulo-ocular reflex (VOR). It is conspicuous in certain animals that cannot move their eyes much, such as owls.\n\nSee also\nReferences\nExternal links\n(Video) Head Impulse Testing site (vHIT) Site with thorough information about vHIT\nMotor Learning in the VOR in Mice at edboyden.org\nent\/482 at eMedicine \u2013 \"Vestibuloocular Reflex Testing\"\nDepiction of Oculocephalic and Caloric reflexes\nVideos of animals demonstrating VOR","203":"Visual reinforcement audiometry (VRA) is a key behavioural test for evaluating hearing in young children. First introduced by Liden and Kankkunen in 1969, VRA is a good indicator of how responsive a child is to sound and speech and whether the child is developing awareness to sound as expected. Performed by an audiologist, VRA is the preferred behavioral technique for children that are 6 \u2013 24 months of age. Using classic operant conditioning, a stimulus is presented, which is followed by a 90 degree head turn from midline by the child, resulting in the child being reinforced with an animation. The child is typically seated in a high chair or on a parent's lap while facing forward. A loud speaker or two are situated at 45 or 90 degrees from the child. As the auditory stimulus is presented, the child will naturally search for the sound source, resulting in a head turn and reinforcement is followed shortly after through an animated toy or video next to the speaker where the auditory stimulus was presented. Using VRA, an audiologist can obtain minimal hearing thresholds ranging in frequencies from 250 Hz - 8000 Hz using speakers, headphones, inserts earphones or through a bone conduction transducer and plot them on an audiogram. The results from the audiogram, paired with other objective measures such as a Tympanogram, Otoacoustic emissions testing and\/or Auditory Brainstem Response testing can provide further insight into the child's auditory hearing status as well as future treatment plans if deemed necessary. VRA works well until about 18\u201324 months of age. Above 18\u201324 months of age, children need more interesting tasks to hold their attention, which is when audiologists introduce Conditioned Play Audiometry.\nConditioned orientation reflex (COR) is a variant of VRA where more than one sound is used. The key difference between COR and VRA is that COR is dependent on the child to have the ability to detect and localize the sound, whereas VRA only requires the child to have a head turn response after the auditory stimulus is presented, they do not need to accurately localize the sound as well.\n\n\n== References ==","204":"The vestibulo-ocular reflex (VOR) is a reflex that acts to stabilize gaze during head movement, with eye movement due to activation of the vestibular system, it is also known as the Cervico-ocular reflex. The reflex acts to stabilize images on the retinas of the eye during head movement. Gaze is held steadily on a location by producing eye movements in the direction opposite that of head movement. For example, when the head moves to the right, the eyes move to the left, meaning the image a person sees stays the same even though the head has turned. Since slight head movement is present all the time, VOR is necessary for stabilizing vision: people with an impaired reflex find it difficult to read using print, because the eyes do not stabilise during small head tremors, and also because damage to reflex can cause nystagmus.\nThe VOR does not depend on what is seen. It can also be activated by hot or cold stimulation of the inner ear, where the vestibular system sits, and works even in total darkness or when the eyes are closed. However, in the presence of light, the fixation reflex is also added to the movement. Most features of VOR are present in kittens raised in complete darkness. \nIn lower animals, the organs that coordinate balance and movement are not independent from eye movement. A fish, for instance, moves its eyes by reflex when its tail is moved.  Humans have semicircular canals, neck muscle  \"stretch\" receptors, and the utricle (gravity organ).  Though the semicircular canals cause most of the reflexes which are responsive to acceleration, the maintaining of balance is mediated by the stretch of neck muscles and the pull of gravity on the utricle (otolith organ) of the inner ear.\nThe VOR has both rotational and translational aspects. When the head rotates about any axis (horizontal, vertical, or torsional) distant visual images are stabilized by rotating the eyes about the same axis, but in the opposite direction. When the head translates, for example during walking, the visual fixation point is maintained by rotating gaze direction in the opposite direction, by an amount that depends on distance.\n\nFunction\nThe vestibulo-ocular reflex is driven by signals arising from the vestibular system of the inner ear. The semicircular canals detect head rotation and provide the rotational component, whereas the otoliths detect head translation and drive the translational component. The signal for the horizontal rotational component travels via the vestibular nerve through the vestibular ganglion and end in the vestibular nuclei in the brainstem. From these nuclei, fibers cross to the abducens nucleus of the opposite side of the brain. Here, fibres synapse with 2 additional pathways. One pathway projects directly to the lateral rectus muscle of the eye via the abducens nerve. Another nerve tract projects from the abducens nucleus by the medial longitudinal fasciculus to the oculomotor nucleus of the opposite side, which contains motor neurons that drive eye muscle activity, specifically activating the medial rectus muscle of the eye through the oculomotor nerve.\nAnother pathway (not in picture) directly projects from the vestibular nucleus through the ascending tract of Deiter's to the medial rectus muscle motor neuron of the same side. In addition there are inhibitory vestibular pathways to the ipsilateral abducens nucleus. However no direct vestibular neuron to medial rectus motoneuron pathway exists.\nSimilar pathways exist for the vertical and torsional components of the VOR.\n\nOculomotor integrator\nIn addition to these direct pathways, which drive the velocity of eye rotation, there is an indirect pathway that builds up the position signal needed to prevent the eye from rolling back to center when the head stops moving. This pathway is particularly important when the head is moving slowly because here position signals dominate over velocity signals. David A. Robinson discovered that the eye muscles require this dual velocity-position drive, and also proposed that it must arise in the brain by mathematically integrating the velocity signal and then sending the resulting position signal to the motoneurons. Robinson was correct: the 'neural integrator' for horizontal eye position was found in the nucleus prepositus hypoglossi in the medulla, and the neural integrator for vertical and torsional eye positions was found in the interstitial nucleus of Cajal in the midbrain. The same neural integrators also generate eye position for other conjugate eye movements such as saccades and smooth pursuit.\nThe integrator is leaky, with a characteristic leaking time of 20 s. For example, when the subject is sitting still and focusing on an object, and suddenly the light is turned off, the eyes would return to their neutral position in around 40 seconds even as the subject is attempting to keep the focus.\n\nExample\nFor instance, if the head is turned clockwise as seen from above, then excitatory impulses are sent from the semicircular canal on the right side via the vestibular nerve through Scarpa's ganglion and end in the right vestibular nuclei in the brainstem. From this nuclei excitatory fibres cross to the left abducens nucleus. There they project and stimulate the lateral rectus of the left eye via the abducens nerve. In addition, by the medial longitudinal fasciculus and oculomotor nuclei, they activate the medial rectus muscles on the right eye. As a result, both eyes will turn counter-clockwise.\nFurthermore, some neurons from the right vestibular nucleus directly stimulate the right medial rectus motor neurons, and inhibits the right abducens nucleus.\n\nIntegrated neural control\nThe VOR is controlled by a neural integrator. The neuron from each horizontal semicircular canal fires at a rate of \n  \n    \n      \n        (\n        90\n        +\n        0.4\n        \n          \n            \n              H\n              \u02d9\n            \n          \n        \n        )\n        \n        \n          H\n          z\n        \n      \n    \n    {\\displaystyle (90+0.4{\\dot {H}})\\;\\mathrm {Hz} }\n  \n, where \n  \n    \n      \n        \n          \n            \n              H\n              \u02d9\n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {H}}}\n  \n is the sensed horizontal angular velocity of the semicircular canal. The motoneuron commanding the horizontal eye muscles fires at a rate of \n  \n    \n      \n        (\n        4\n        \n        \n          H\n          z\n          \n            \/\n          \n          d\n          e\n          g\n        \n        )\n        \u03b8\n        +\n        (\n        1.0\n        \n        \n          H\n          z\n          \n            \/\n          \n          (\n          d\n          e\n          g\n          \n            \/\n          \n          s\n          e\n          c\n          )\n        \n        )\n        \n          \n            \n              \u03b8\n              \u02d9\n            \n          \n        \n      \n    \n    {\\displaystyle (4\\;\\mathrm {Hz\/deg} )\\theta +(1.0\\;\\mathrm {Hz\/(deg\/sec)} ){\\dot {\\theta }}}\n  \n, where \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  \n is the horizontal turning angle, and \n  \n    \n      \n        \n          \n            \n              \u03b8\n              \u02d9\n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {\\theta }}}\n  \n is its horizontal angular speed. The two terms account for the elasticity and viscosity of ocular tissue.\nThe rotational moment of inertia of the eye is negligible, as individuals wearing weighted contact lens that increases the rotational moment of inertia almost 100-fold still has the same VOR (p. 94 ).\n\nSpeed\nThe vestibulo-ocular reflex needs to be fast: for clear vision, head movement must be compensated almost immediately; otherwise, vision corresponds to a photograph taken with a shaky hand. Signals are sent from the semicircular canals using only three neurons, called the three neuron arc. This results in eye movements that lag head movement by less than 10 ms. The vestibulo-ocular reflex is one of the fastest reflexes in the human body.\n\nVOR suppression\nWhen a person tracks the movement of something with both their eyes and head together, the VOR is counterproductive to the goal of keeping the gaze and head angle aligned. Research indicates that there exists mechanisms in the brain to suppress the VOR using the active visual (retinal) feedback obtained by watching the object in motion. In the absence of visual feedback, such as when the object passes behind an opaque barrier, humans can continue to visually track the apparent position of the object using anticipatory (extra-retinal) systems within the brain, and the VOR is also suppressed during this activity. The VOR can even be cognitively suppressed, such as when following an imagined target with the eyes and head together, although the effect tends to be less dramatic than with visual feedback.\n\nGain\nThe \"gain\" of the VOR is defined as the change in the eye angle divided by the change in the head angle during the head turn. Ideally the gain of the rotational VOR is 1.0. The gain of the horizontal and vertical VOR is usually close to 1.0, but the gain of the torsional VOR (rotation around the line of sight) is generally low. The gain of the translational VOR has to be adjusted for distance, because of the geometry of motion parallax. When the head translates, the angular direction of near targets changes faster than the angular direction of far targets.\nIf the gain of the VOR is wrong (different from 1)\u2014for example, if eye muscles are weak, or if a person puts on a new pair of eyeglasses\u2014then head movement results in image motion on the retina, resulting in blurred vision. Under such conditions, motor learning adjusts the gain of the VOR to produce more accurate eye motion. This is what is referred to as VOR adaptation.\nNearsighted people who habitually wear negative spectacles have lower VOR gain. Farsighted people or aphakes who habitually wear positive spectacle have higher VOR gain. People who habitually wear contact lens show no change in VOR gain. Monocular, disconjugate adaptation of the VOR is possible, for example, after extraocular muscle palsy. (p. 27 )\nThe phase of the VOR can also adapt.\n\nLeak\nThe oculomotor integrator is a leaky integrator, with a characteristic leaking time of ~20 s. If the leaking time is too low, some form of adaptation occurs to \"patch the leak\" to raise the leaking time. It is hypothesized that the leaking integrator is constructed by a feedback circuit with a gain of slightly below 1, and adaptation occurs by adjusting the gain of the feedback circuit. The hypothesis is tested by using an specially patterned optokinetic drum that simulates the visual effect of having a very leaky oculomotor integrator. After 1 hour of viewing, the integrator becomes \"anti-leaky\", meaning that its value grows exponentially even in the absence of input. The eye motion becomes positive-feedback, meaning that if it is slightly to the left of a fixation target, it would drift even further to the left, and similarly for the right. It is also accompanied by nausea. (p. 84 )\n\nDisruption by ethanol\nEthanol consumption can disrupt the VOR, reducing dynamic visual acuity. In normal conditions, the cupula and the endolymph are equal in density (both are ). After ingesting ethanol, the ethanol diffuses into the cupula before it diffuses into the endolymph, because it is closer to blood capillaries. This makes the cupula temporarily lighter. In this state, if a person lies down with right cheek touching the ground, then the cupula in the left ear would float towards the left, creating an illusory sense of slow left-to-right head rotation. To compensate for this, the VOR moves the eyes towards the left slowly until it reaches the limit, and the eyes pull to the right rapidly (nystagmus). This is the positional alcohol nystagmus, phase I (PAN I). The unusual vestibular stimulation also caused motion sickness symptoms: illusions of bodily rotations, dizziness, and nausea. These symptoms subside in a few seconds after assuming an upright posture.\nAfter some time, the density of cupula and endolymph equalizes, removing the nystagmus effect. After ethanol is fully metabolized, the cupula returns to normal density first, creating nystagmus in the opposite direction (PAN II) during the hangover.\nAs predicted, heavy water (1.1 density of water) consumption has the exact opposite nystagmus effect compared to ethanol consumption. Consuming a mixture of heavy water (\n  \n    \n      \n        4\n        \n        \n          m\n          l\n          \n            \/\n          \n          k\n          g\n        \n      \n    \n    {\\displaystyle 4\\;\\mathrm {ml\/kg} }\n  \n) and ethanol (\n  \n    \n      \n        2\n        \n        \n          m\n          l\n          \n            \/\n          \n          k\n          g\n        \n      \n    \n    {\\displaystyle 2\\;\\mathrm {ml\/kg} }\n  \n) largely cancels out the effect. Macroglobulinaemia, or consuming glycerol (1.26 density of water), have similar effects as heavy water.\n\nClinical significance\nTesting\nThis reflex can be tested by the rapid head impulse test or Halmagyi\u2013Curthoys test, in which the head is rapidly moved to the side with force, and is controlled if the eyes succeed to remain to look in the same direction. When the function of the right balance system is reduced, by a disease or by an accident, a quick head movement to the right cannot be sensed properly anymore. As a consequence, no compensatory eye movement is generated, and the patient cannot fixate a point in space during this rapid head movement.\nThe head impulse test can be done at the bed side and used as a screening tool for problems with a person's vestibular system. It can also be diagnostically tested by doing a video-head impulse test (VHIT). In this diagnostic test, a person wears highly sensitive goggles that detect rapid changes in eye movement. This test can provide site-specific information on vestibular system and its function.\nAnother way of testing the VOR response is a caloric reflex test, which is an attempt to induce nystagmus (compensatory eye movement in the absence of head motion) by pouring cold or warm water into the ear. Also available is bi-thermal air caloric irrigations, in which warm and cool air is administered into the ear.\nThe vestibulo-ocular reflex can be tested by the aforementioned caloric reflex test; this plays an important part in confirming diagnosis of brainstem death.  A code of practice must be followed in this process, namely that of the Academy of Medical Royal Colleges.\n\nRelated terms\nCervico-ocular reflex\nSummary: Cervico-ocular reflex, also known by its acronym COR, involves the achievement of stabilization of a visual target, and image on the retina, through adjustments of gaze impacted by neck and, or head movements or rotations.  The process works in conjunction with the vestibulo-ocular reflex (VOR). It is conspicuous in certain animals that cannot move their eyes much, such as owls.\n\nSee also\nReferences\nExternal links\n(Video) Head Impulse Testing site (vHIT) Site with thorough information about vHIT\nMotor Learning in the VOR in Mice at edboyden.org\nent\/482 at eMedicine \u2013 \"Vestibuloocular Reflex Testing\"\nDepiction of Oculocephalic and Caloric reflexes\nVideos of animals demonstrating VOR","205":"Vitiligo () is a chronic autoimmune disorder that causes patches of skin to lose pigment or color. The cause of vitiligo is unknown, but it may be related to immune system changes, genetic factors, stress, or sun exposure. Treatment options include topical medications, light therapy, surgery and cosmetics. The condition can show up on any skin type as a light peachy color and can appear on any place on the body in all sizes. The spots on the skin known as vitiligo are also able to \u201cchange\u201d as spots lose and regain pigment; they will stay in relatively the same areas but can move over time and some big patches can move through the years but never disappear overnight.\n\nSigns and symptoms\nThe only sign of vitiligo is the presence of pale patchy areas of depigmented skin which tend to occur on the extremities. Some people may experience itching before a new patch appears. The patches are initially small, but often grow and change shape. When skin lesions occur, they are most prominent on the face, hands and wrists. The loss of skin pigmentation is particularly noticeable around body orifices, such as the mouth, eyes, nostrils, genitalia and umbilicus. Some lesions have increased skin pigment around the edges. Those affected by vitiligo who are stigmatized for their condition may experience depression and similar mood disorders.\n\nCauses\nAlthough multiple hypotheses have been suggested as potential triggers that cause vitiligo, studies strongly imply that changes in the immune system are responsible for the condition. Vitiligo has been proposed to be a multifactorial disease with genetic susceptibility and environmental factors both thought to play a role. It is hypothesized that damaging environmental factors can disrupt redox reactions necessary for protein folding, so skin cells may initiate the unfolded protein response which releases cytokines, thus mounting an immune response \nThe National Institutes of Health states that sometimes an event, like a sunburn, emotional distress, or exposure to a chemical, can trigger or exacerbate the condition, Skin depigmentation in particular areas in vitiligo can also be triggered by mechanical trauma: this is an example of the Koebner phenomenom. Unlike in other skin diseases, this can be caused by daily activities, especially chronic friction on particular areas of the body.\n\nImmune\nMelanin is the pigment that gives skin its color; it is produced by skin cells called melanocytes.\nVariations in genes that are part of the immune system or part of melanocytes have both been associated with vitiligo. It is also thought to be caused by the immune system attacking and destroying the melanocytes of the skin. A genome wide association study found approximately 36 independent susceptibility loci for generalized vitiligo.\nThe TYR gene encodes the protein tyrosinase, which is not a component of the immune system, but is an enzyme of the melanocyte that catalyzes melanin biosynthesis, and a major autoantigen in generalized vitiligo.\n\nAutoimmune associations\nVitiligo is sometimes associated with autoimmune and inflammatory diseases such as Hashimoto's thyroiditis, scleroderma, rheumatoid arthritis, type 1 diabetes mellitus, psoriasis, Addison's disease, pernicious anemia, alopecia areata, systemic lupus erythematosus, and celiac disease.\nAmong the inflammatory products of NLRP1 are caspase 1 and caspase 7, which activate the inflammatory cytokine interleukin-1\u03b2. Interleukin-1\u03b2 and interleukin-18 are expressed at high levels in people with vitiligo. In one of the mutations, the amino acid leucine in the NALP1 protein was replaced by histidine (Leu155 \u2192 His). The original protein and sequence is highly conserved in evolution, and is found in humans, chimpanzee, rhesus monkey, and the bush baby. Addison's disease (typically an autoimmune destruction of the adrenal glands) may also be seen in individuals with vitiligo.\n\nOxidative stress\nNumerous whole-exome sequencing studies have demonstrated that vitiligo is associated with polymorphisms in genes involved in the response to oxidative stress such as CAT, SOD1, SOD2, SOD3, NFE2L2, HMOX1, GST-M1 or GST-T1 supporting the association of elevated levels of reactive oxygen species in melanocytes with the induction of an auto-immune response.\nThus, diseases presenting with altered mitochondrial function such as MELAS, Vogt-Koyanagi-Harada syndrome, Kabuki syndrome are associated with increased risk of vitiligo. \nIn line with these observations, genetic alterations in mitochondrial DNA (mtDNA) of melanocytes associated with altered mitochondrial function lead to a release of mtDNA that can be detected in the skin of vitiligo patients.  This mtDNA can be sensed by the cGAS-STING pathway resulting in pro-inflammatory cytokine and chemokines production promoting the recruitment of cytotoxic CD8+ T cells. The use of mitochondrial antioxidants, NRF2 inhibitors and TBK1 inhibitors is emerging as potential therapeutic options to block this cascade of event.\n\nDiagnosis\nAn ultraviolet light can be used in the early phase of this disease for identification and to determine the effectiveness of treatment. Using a Wood's light, skin will change colour (fluoresce) when it is affected by certain bacteria, fungi, and changes to pigmentation of the skin.\n\nClassification\nClassification attempts to quantify vitiligo have been analyzed as being somewhat inconsistent, while recent consensus have agreed to a system of segmental vitiligo (SV) and non-segmental vitiligo (NSV). NSV is the most common type of vitiligo.\n\nNon-segmental\nIn non-segmental vitiligo (NSV), there is usually some form of symmetry in the location of the patches of depigmentation. New patches also appear over time and can be generalized over large portions of the body or localized to a particular area. Extreme cases of vitiligo, to the extent that little pigmented skin remains, are referred to as vitiligo universalis. NSV can come about at any age (unlike segmental vitiligo, which is far more prevalent in teenage years).\nClasses of non-segmental vitiligo include the following:\n\nGeneralized vitiligo: the most common pattern, wide and randomly distributed areas of depigmentation\nUniversal vitiligo: depigmentation encompasses most of the body\nFocal vitiligo: one or a few scattered macules in one area, most common in children\nAcrofacial vitiligo: fingers and periorificial areas\nMucosal vitiligo: depigmentation of only the mucous membranes\n\nSegmental\nSegmental vitiligo (SV) differs in appearance, cause, and frequency of associated illnesses. Its treatment is different from that of NSV. It tends to affect areas of skin that are associated with dorsal roots from the spinal cord and is most often unilateral. It is much more stable\/static in course and its association with autoimmune diseases appears to be weaker than that of generalized vitiligo. SV does not improve with topical therapies or UV light; however, surgical treatments such as cellular grafting can be effective.\n\nDifferential diagnosis\nChemical leukoderma is a similar condition due to multiple exposures to chemicals. Vitiligo however is a risk factor. Triggers may include inflammatory skin conditions, burns, intralesional steroid injections and abrasions.\nOther conditions with similar symptoms include the following:\n\nalbinism\nhalo nevus\nidiopathic guttate hypomelanosis (white sunspots)\npiebaldism\npityriasis alba\npostinflammatory hypopigmentation\nprimary adrenal insufficiency\nprogressive macular hypomelanosis\ntinea versicolor\ntuberculoid leprosy.\n\nTreatment\nThere is no cure for vitiligo but several treatment options are available. The best evidence is for applied steroids and ultraviolet light in combination with creams. Due to the higher risks of skin cancer, the United Kingdom's National Health Service suggests phototherapy be used only if primary treatments are ineffective. Lesions located on the hands, feet, and joints are the most difficult to repigment; those on the face are easiest to return to the natural skin color as the skin is thinner.\n\nImmune mediators\nTopical preparations of immune suppressing medications including glucocorticoids (such as 0.05% clobetasol or 0.10% betamethasone) and calcineurin inhibitors (such as tacrolimus or pimecrolimus) are considered to be first-line vitiligo  treatments.\nIn July 2022, ruxolitinib cream (sold under the brand name Opzelura) was approved for medical use in the United States for the treatment of vitiligo.\n\nPhototherapy\nPhototherapy is considered a second-line treatment for vitiligo. Exposing the skin to light from UVB lamps is the most common treatment for vitiligo. The treatments can be done at home with an UVB lamp or in a clinic. The exposure time is managed so that the skin does not suffer overexposure. Treatment can take a few weeks if the spots are on the neck and face and if they existed not more than 3 years. If the spots are on the hands and legs and have been there for more than 3 years, it can take a few months. Phototherapy sessions are done 2\u20133 times a week. Spots on a large area of the body may require full body treatment in a clinic or hospital. UVB broadband and narrowband lamps can be used, but narrowband ultraviolet peaked around 311 nm is the choice. It has been constitutively reported that a combination of UVB phototherapy with other topical treatments improves re-pigmentation. However, some people with vitiligo may not see any changes to skin or re-pigmentation occurring. A serious potential side effect involves the risk of developing skin cancer, the same risk as an overexposure to natural sunlight.\nUltraviolet light (UVA) treatments are normally carried out in a hospital clinic. Psoralen and ultraviolet A light (PUVA) treatment involves taking a drug that increases the skin's sensitivity to ultraviolet light, then exposing the skin to high doses of UVA light. Treatment is required twice a week for 6\u201312 months or longer. Because of the high doses of UVA and psoralen, PUVA may cause side effects such as sunburn-type reactions or skin freckling.\nNarrowband ultraviolet B (NBUVB) phototherapy lacks the side-effects caused by psoralens and is as effective as PUVA. As with PUVA, treatment is carried out twice weekly in a clinic or every day at home, and there is no need to use psoralen. Longer treatment is often recommended, and at least 6 months may be required for effects to phototherapy. NBUVB phototherapy appears better than PUVA therapy with the most effective response on the face and neck.\nWith respect to improved repigmentation: topical calcineurin inhibitors plus phototherapy are better than phototherapy alone, hydrocortisone plus laser light is better than laser light alone, ginkgo biloba is better than placebo, and oral mini-pulse of prednisolone (OMP) plus NB-UVB is better than OMP alone.\n\nSkin camouflage\nIn mild cases, vitiligo patches can be hidden with makeup or other cosmetic camouflage solutions. If the affected person is pale-skinned, the patches can be made less visible by avoiding tanning of unaffected skin.\n\nDepigmenting\nIn cases of extensive vitiligo the option to depigment the unaffected skin with topical drugs like monobenzone, mequinol, or hydroquinone may be considered to render the skin an even color. The removal of all the skin pigment with monobenzone is permanent and vigorous. Sun-safety must be adhered to for life to avoid severe sunburn and melanomas. Depigmentation takes about a year to complete.\n\nHistory\nDescriptions of a disease believed to be vitiligo date back to a passage in the medical text Ebers Papyrus c.\u20091500 BC in ancient Egypt. Also, the Hebrew word \"Tzaraath\" from the Old Testament book of Leviticus dating to 1280 BC (or 1312 BC) described a group of skin diseases associated with white spots, and a subsequent translation to Greek led to continued conflation of those with vitiligo with leprosy and spiritual uncleanliness.\nMedical sources in the ancient world such as Hippocrates often did not differentiate between vitiligo and leprosy, often grouping these diseases together. The name \"vitiligo\" was first used by the Roman physician Aulus Cornelius Celsus in his classic medical text De Medicina.\nThe term vitiligo is believed to be derived from \"vitium\", meaning \"defect\" or \"blemish\".\n\nSociety and culture\nThe change in appearance caused by vitiligo can affect a person's emotional and psychological well-being and may create difficulty in becoming or remaining employed, particularly if vitiligo develops on visible areas of the body, such as the face, hands or arms. Participating in a vitiligo support group may improve social coping skills and emotional resilience.\nNotable cases include American pop singer Michael Jackson, Canadian fashion model Winnie Harlow, New Zealand singer-songwriter Kimbra, American actor David Dastmalchian and Argentine musician Charly Garc\u00eda. Professional wrestler Bryan Danielson and French actor Micha\u00ebl Youn are also affected, as is former French Prime Minister \u00c9douard Philippe, and TV host, model and former Miss Colombia 2007, Taliana Vargas.\nThe Adult Swim animated sitcom The Boondocks satirizes the idea of vitiligo in Uncle Ruckus, one of the show's characters. Ruckus, who is black, frequently claims to be white, often stating that he has \"Re-vitiligo, the opposite of what Michael Jackson had.\" He frequently uses this argument to maintain that he is actually white, leading him to commit delusional and racist antics in nearly every episode.\n\nResearch\nAs of July 2013, afamelanotide is in phase II and III clinical trials for vitiligo and other skin diseases.\nA medication for rheumatoid arthritis, tofacitinib, has been tested for the treatment of vitiligo.\nIn October 1992, a scientific report was published of successfully transplanting melanocytes to vitiligo-affected areas, effectively repigmenting the region. The procedure involved taking a thin layer of pigmented skin from the person's gluteal region. Melanocytes were then separated out to a cellular suspension that was expanded in culture. The area to be treated was then denuded with a dermabrader and the melanocytes graft applied. Between 70 and 85 percent of people with vitiligo experienced nearly complete repigmentation of their skin. The longevity of the repigmentation differed from person to person.\n\nCurrent research suggests that the Janus kinase\/signal transducer and activator of transcription pathway (JAK\/STAT pathway) plays a crucial role in the loss of epidermal melanocytes. This pathway is activated by CXCR3+ CD8+ T cells, creating a positive-feedback loop with interferon-gamma (IFN-\u03b3) chemokines from keratinocytes, potentially contributing to vitiligo. JAK inhibitors like ruxolitinib show promise in targeting the IFN-\u03b3-chemokine signaling axis implicated in vitiligo pathogenesis, and improving nonsegmental vitiligo.\n\nReferences\nExternal links\n\nVitiligo at Curlie\nQuestions and Answers about Vitiligo \u2013 US National Institute of Arthritis and Musculoskeletal and Skin Diseases","206":"Vomiting (also known as emesis, puking and throwing up) is the involuntary, forceful expulsion of the contents of one's stomach through the mouth and sometimes the nose.\nVomiting can be the result of ailments like food poisoning, gastroenteritis, pregnancy, motion sickness, or hangover; or it can be an after effect of diseases such as brain tumors, elevated intracranial pressure, or overexposure to ionizing radiation. The feeling that one is about to vomit is called nausea; it often precedes, but does not always lead to vomiting. Impairment due to alcohol or anesthesia can cause inhalation of vomit. In severe cases, where dehydration develops, intravenous fluid may be required. Antiemetics are sometimes necessary to suppress nausea and vomiting. Self-induced vomiting can be a component of an eating disorder such as bulimia, and is itself now classified as an eating disorder on its own, purging disorder.\n\nComplications\nAspiration\nVomiting is dangerous if gastric content enters the respiratory tract. Under normal circumstances the gag reflex and coughing prevent this from occurring; however, these protective reflexes are compromised in persons who are under the influence of certain substances (including alcohol) or even mildly anesthetized. The individual may choke and asphyxiate or develop aspiration pneumonia.\n\nDehydration and electrolyte imbalance\nProlonged and excessive vomiting depletes the body of water (dehydration), and may alter the electrolyte status. Gastric vomiting leads to the loss of acid (protons) and chloride directly. Combined with the resulting alkaline tide, this leads to hypochloremic metabolic alkalosis (low chloride levels together with high HCO\u22123 and CO2 and increased blood pH) and often hypokalemia (potassium depletion). The hypokalemia is an indirect result of the kidney compensating for the loss of acid. With the loss of intake of food the individual may eventually become cachectic. A less frequent occurrence results from a vomiting of intestinal contents, including bile acids and HCO\u22123.\n\nMallory\u2013Weiss tear\nRepeated or profuse vomiting may cause erosions to the esophagus or small tears in the esophageal mucosa (Mallory\u2013Weiss tear). This may become apparent if fresh red blood is mixed with vomit after several episodes.\n\nDentistry\nRecurrent vomiting, such as observed in bulimia nervosa, may lead to the destruction of the tooth enamel due to the acidity of the vomit. Digestive enzymes can also have a negative effect on oral health, by degrading the tissue of the gums.\n\nPathophysiology\nReceptors on the floor of the fourth ventricle of the brain represent a chemoreceptor trigger zone, known as the area postrema, stimulation of which can lead to vomiting. The area postrema is a circumventricular organ and as such lies outside the blood\u2013brain barrier; it can therefore be stimulated by blood-borne drugs that can stimulate vomiting or inhibit it.\nThere are various sources of input to the vomiting center:\n\nThe chemoreceptor trigger zone at the base of the fourth ventricle has numerous dopamine D2 receptors, serotonin 5-HT3 receptors, opioid receptors, acetylcholine receptors, and receptors for substance P. Stimulation of different receptors are involved in different pathways leading to emesis, in the final common pathway substance P appears involved.\nThe vestibular system, which sends information to the brain via cranial nerve VIII (vestibulocochlear nerve), plays a major role in motion sickness, and is rich in muscarinic receptors and histamine H1 receptors.\nThe cranial nerve X (vagus nerve) is activated when the pharynx is irritated, leading to a gag reflex.\nThe vagal and enteric nervous system inputs transmit information regarding the state of the gastrointestinal system. Irritation of the GI mucosa by chemotherapy, radiation, distention, or acute infectious gastroenteritis activates the 5-HT3 receptors of these inputs.\nThe CNS mediates vomiting that arises from psychiatric disorders and stress from higher brain centers.\nThe medulla plays an important role for triggering the vomiting act.\nThe vomiting act encompasses three types of outputs initiated by the chemoreceptor trigger zone: Motor, parasympathetic nervous system (PNS), and sympathetic nervous system (SNS). They are as follows:\n\nIncreased salivation to protect tooth enamel from stomach acids. (Excessive vomiting leads to dental erosion.) This is part of the PNS output.\nThe body takes a deep breath to avoid aspirating vomit.\nRetroperistalsis starts from the middle of the small intestine and sweeps up digestive tract contents into the stomach, through the relaxed pyloric sphincter.\nIntrathoracic pressure lowers (by inspiration against a closed glottis), coupled with an increase in abdominal pressure as the abdominal muscles contract, propels stomach contents into the esophagus as the lower esophageal sphincter relaxes. The stomach itself does not contract in the process of vomiting except for at the angular notch, nor is there any retroperistalsis in the esophagus.\nVomiting is ordinarily preceded by retching.\nVomiting also initiates an SNS response causing both sweating and increased heart rate.\n\nPhases\nThe vomiting act has two phases. In the retching phase, the abdominal muscles undergo a few rounds of coordinated contractions together with the diaphragm and the muscles used in respiratory inspiration. For this reason, an individual may confuse this phase with an episode of violent hiccups. In this retching phase, nothing has yet been expelled. In the next phase, also termed the expulsive phase, intense pressure is formed in the stomach brought about by enormous shifts in both the diaphragm and the abdomen. These shifts are, in essence, vigorous contractions of these muscles that last for extended periods of time\u2014much longer than a normal period of muscular contraction. The pressure is then suddenly released when the upper esophageal sphincter relaxes resulting in the expulsion of gastric contents. As the mouth and nasal cavity are connected via the back of the throat, particularly forceful vomiting, or producing large quantities of vomit may result in material being ejected through the nostrils in addition to the mouth. Individuals who do not regularly exercise their abdominal muscles may experience pain in those muscles for a few days. The decrease in pressure and the release of endorphins into the bloodstream after the expulsion causes the vomiter to feel relief almost immediately after vomiting.\n\nContents\nGastric secretions and likewise vomit are highly acidic. Recent food intake appears in the gastric vomit. Irrespective of the content, vomit tends to be malodorous.\nThe content of the vomitus (vomit) may be of medical interest. Fresh blood in the vomit is termed hematemesis (\"blood vomiting\"). Altered blood bears resemblance to coffee grounds (as the iron in the blood is oxidized) and, when this matter is identified, the term coffee-ground vomiting is used. Bile can enter the vomit during subsequent heaves due to duodenal contraction if the vomiting is severe. Fecal vomiting is often a consequence of intestinal obstruction or a gastrocolic fistula and is treated as a warning sign of this potentially serious problem (signum mali ominis).\nIf the vomiting reflex continues for an extended period with no appreciable vomitus, the condition is known as non-productive emesis or \"dry heaves\", which can be painful and debilitating.\n\nColor of vomit\nBright red in the vomit suggests bleeding from the esophagus\nDark red vomit with liver-like clots suggests profuse bleeding in the stomach, such as from a perforated ulcer\nCoffee-ground-like vomit suggests less severe bleeding in the stomach because the gastric acid has had time to change the composition of the blood\nYellow or green vomit suggests bile, indicating that the pyloric valve is open and bile is flowing into the stomach from the duodenum. This may occur during successive episodes of vomiting after the stomach contents have been completely expelled.\n\nCauses\nVomiting may be due to a large number of causes, and protracted vomiting has a long differential diagnosis.\n\nDigestive tract\nCauses in the digestive tract\n\nGastritis (inflammation of the gastric wall)\nGastroenteritis\nGastroesophageal reflux disease\nCeliac disease\nNon-celiac gluten sensitivity\nPyloric stenosis (in babies, this typically causes a very forceful \"projectile vomiting\" and is an indication for urgent surgery)\nBowel obstruction\nOvereating (stomach too full)\nAcute abdomen and\/or peritonitis\nIleus\nFood allergies (often in conjunction with hives or swelling)\nCholecystitis, pancreatitis, appendicitis, hepatitis\nFood poisoning\nIn children, it can be caused by an allergic reaction to cow's milk proteins (milk allergy or lactose intolerance)\n\nSensory system and brain\nCauses in the sensory system:\n\nMovement leading to motion sickness (which is caused by overstimulation of the labyrinthine canals of the ear)\nM\u00e9ni\u00e8re's disease\nVertigo\nCauses in the brain:\n\nConcussion\nCerebral hemorrhage\nCerebral aneurysm\nMigraine\nBrain tumors, which can cause the chemoreceptors to malfunction\nBenign intracranial hypertension and hydrocephalus\nMetabolic disturbances (these may irritate both the stomach and the parts of the brain that coordinate vomiting):\n\nHypercalcemia (high calcium levels)\nUremia (urea accumulation, usually due to kidney failure)\nAdrenal insufficiency\nHypoglycemia\nHyperglycemia\nPregnancy:\n\nHyperemesis, morning sickness\nDrug reaction (vomiting may occur as an acute somatic response to):\n\nAlcohol, which can be partially oxidized into acetaldehyde that causes the symptoms of hangover, including nausea, vomiting, shortness of breath, and fast heart rate.\nOpioids\nSelective serotonin reuptake inhibitors\nMany chemotherapy drugs\nSome entheogens (such as peyote or ayahuasca)\nHigh altitude:\n\nAltitude sickness\nIllness (sometimes colloquially known as \"stomach flu\"\u2014a broad name that refers to gastric inflammation caused by a range of viruses and bacteria):\n\nNorovirus (formerly Norwalk virus or Norwalk agent)\nSwine influenza\nPsychiatric\/behavioral:\n\nBulimia nervosa\nFood neophobia\nPurging disorder\n\nEmetics\nAn emetic, such as syrup of ipecac, is a substance that induces vomiting when administered orally or by injection. An emetic is used medically when a substance has been ingested and must be expelled from the body immediately. (For this reason, many toxic and easily digestible products such as rat poison contain an emetic. This presents no problem for the effectiveness of the rodenticide as rodents are unable to vomit.) Inducing vomiting can remove the substance before it is absorbed into the body.\nEmetics can be divided into two categories, those which produce their effect by acting on the vomiting center in the medulla, and those which act directly on the stomach itself. Some emetics, such as ipecac, fall into both categories; they initially act directly on the stomach, while their further and more vigorous effect occurs by stimulation of the medullary center.\nSalt water and mustard water, which act directly on the stomach, have been used since ancient times as emetics. Care must be taken with salt, as excessive intake can potentially be harmful.\nCopper sulfate was also used in the past as an emetic. It is now considered too toxic for this use.\nHydrogen peroxide is used as an emetic in veterinary practice.\n\nSelf-induced\nEating disorders (anorexia nervosa or bulimia nervosa)\nTo eliminate an ingested poison (some poisons should not be vomited as they may be more toxic when inhaled or aspirated; it is better to ask for help before inducing vomiting)\nSome people who engage in binge drinking induce vomiting to make room in their stomachs for more alcohol consumption.\nParticipants in milk chugging typically end up vomiting most of the milk they consume, as proteins in the ingested milk (such as casein) rapidly denature and unravel on contact with gastric acid and protease enzymes, rapidly filling the stomach. Once the stomach becomes full, stretch receptors in the stomach wall trigger signals to vomit to expel any further liquid the participant ingests.\nPeople suffering from nausea may induce vomiting in hopes of feeling better.\n\nMiscellaneous\nAfter surgery (postoperative nausea and vomiting)\nDisagreeable sights or disgust, smells, tastes, sounds or thoughts (such as decayed matter, others' vomit, thinking of vomiting), etc.\nExtreme pain, such as an intense headache or myocardial infarction (heart attack)\nExtreme emotions\nCyclic vomiting syndrome (a poorly understood condition with attacks of vomiting)\nCannabinoid hyperemesis syndrome (similar to cyclic vomiting syndrome, but has cannabis use as its underlying cause).\nHigh doses of ionizing radiation sometimes trigger a vomit reflex.\nViolent fits of coughing, hiccups, or asthma\nAnxiety\nDepression\nOverexertion (doing too much strenuous exercise can lead to vomiting shortly afterwards).\n\nOther types\nProjectile vomiting is vomiting that ejects the gastric contents with great force. It is a classic symptom of infantile hypertrophic pyloric stenosis, in which it typically follows feeding and can be so forceful that some material exits through the nose.\n\nTreatment\nAn antiemetic is a drug that is effective against vomiting and nausea. Antiemetics are typically used to treat motion sickness and the side effects of medications such as opioids and chemotherapy.\nAntiemetics act by inhibiting the receptor sites associated with emesis. Hence, anticholinergics, antihistamines, dopamine antagonists, serotonin antagonists, and cannabinoids are used as antiemetics.\nEvidence to support the use of antiemetics for nausea and vomiting among adults in the emergency department is poor. It is unclear if any medication is better than another or better than no active treatment.\n\nEpidemiology\nNausea and\/or vomiting are the main complaints in 1.6% of visits to family physicians in Australia.\n\nSociety and culture\nHerodotus, writing on the culture of the ancient Persians and highlighting the differences with those of the Greeks, notes that to vomit in the presence of others is prohibited among Persians.\n\nSocial cues\nIt is quite common that, when one person vomits, others nearby become nauseated, particularly when smelling the vomit of others, and often to the point of vomiting themselves. It is believed that this is an evolved trait among primates. Many primates in the wild tend to browse for food in small groups. Should one member of the party react adversely to some ingested food, it may be advantageous (in a survival sense) for other members of the party to also vomit. This tendency in human populations has been observed at drinking parties, where excessive consumption of alcoholic beverages may cause a number of party members to vomit nearly simultaneously, this being triggered by the initial vomiting of a single member of the party. This phenomenon has been touched on in popular culture: notorious instances appear in the films Monty Python's The Meaning of Life (1983) and Stand by Me (1986).\nIntense vomiting in ayahuasca ceremonies is a common phenomenon. However, people who experience \"la purga\" after drinking ayahuasca, in general, regard the practise as both a physical and spiritual cleanse and often come to welcome it. It has been suggested that the consistent emetic effects of ayahuasca\u2014in addition to its many other therapeutic properties\u2014was of medicinal benefit to indigenous peoples of the Amazon, in helping to clear parasites from the gastrointestinal system.\nThere have also been documented cases of a single ill and vomiting individual inadvertently causing others to vomit, when they are especially fearful of also becoming ill, through a form of mass hysteria.\n\nMost people try to contain their vomit by vomiting into a sink, toilet, or trash can, as vomit is difficult and unpleasant to clean. On airplanes and boats, special bags are supplied for sick passengers to vomit into. A special disposable bag (leakproof, puncture-resistant, odorless) containing absorbent material that solidifies the vomit quickly is also available, making it convenient and safe to store until there is an opportunity to dispose of it conveniently.\nPeople who vomit chronically (e.g., as part of an eating disorder such as bulimia nervosa) may devise various ways to hide this disorder.\nAn online study of people's responses to \"horrible sounds\" found vomiting \"the most disgusting\". Professor Trevor Cox of the University of Salford's Acoustic Research Centre said, \"We are pre-programmed to be repulsed by horrible things such as vomiting, as it is fundamental to staying alive to avoid nasty stuff.\" It is thought that disgust is triggered by the sound of vomiting to protect those nearby from possibly diseased food.\n\nPsychology\nEmetophilia is sexual arousal from vomiting, or watching others vomit. Emetophobia is a phobia that causes overwhelming, intense anxiety pertaining to vomiting.\n\nSee also\nBulimia nervosa\nEmetophilia\nCancer and nausea\nEmetophobia\nVasodilation\nDiarrhea\nNose-blowing\nBelching\nChyme\n\nNotes\nReferences\nExternal links\n\nCyclical Vomiting Syndrome\n\"Emetics\" . The American Cyclop\u00e6dia. 1879.","207":"The Weber test is a screening test for hearing performed with a tuning fork. It can detect unilateral (one-sided) conductive hearing loss (middle ear hearing loss) and unilateral sensorineural hearing loss (inner ear hearing loss). The test is named after Ernst Heinrich Weber (1795\u20131878).  Conductive hearing ability is mediated by the middle ear composed of the ossicles: the malleus, the incus, and the stapes.  Sensorineural hearing ability is mediated by the inner ear composed of the cochlea with its internal basilar membrane and attached cochlear nerve (cranial nerve VIII).  The outer ear consisting of the pinna, ear canal, and ear drum or tympanic membrane transmits sounds to the middle ear but does not contribute to the conduction or sensorineural hearing ability save for hearing transmissions limited by cerumen impaction (wax collection in the ear canal).\nThe Weber test has had its value as a screening test questioned in the literature.\n\nWeber test performance\nThe Weber and the Rinne test ( RIN-\u0259) are typically performed together when the results of each combined to determine the location and nature of any hearing losses detected.  In the Weber test a vibrating tuning fork (Typically 256 Hz  or 512 Hz  used for Weber vibration test; 512 Hz used for Rinne hearing test) is placed in the middle of the forehead, above the upper lip under the nose over the teeth, or on top of the head equidistant from the patient's ears on top of thin skin in contact with the bone. The patient is asked to report in which ear the sound is heard louder.  A normal Weber test has a patient reporting the sound heard equally in both sides.  In an affected patient, if the defective ear hears the Weber tuning fork louder, the finding indicates a conductive hearing loss in the defective ear.  Also in the affected patient, if the normal ear hears the tuning fork sound better, there is sensorineural hearing loss on the other (defective) ear.  However, this assumes that it is known which ear is defective and which is normal (e.g. by the patient telling the clinician that they cannot hear as well in one ear as in the other), when the testing is being done to characterize the type, conductive or sensorineural, of hearing loss that is occurring.  In the case where the patient is unaware or has acclimated to their hearing loss, the clinician has to use the Rinne test in conjunction with the Weber to characterize and localize any deficits.  That is, an abnormal Weber test is only able to tell the clinician that there is a conductive loss in the ear which hears better or that there is a sensorineural loss in the ear which does not hear as well.\nFor the Rinne test, a vibrating tuning fork (typically 512 Hz) is placed initially on the mastoid process behind each ear until sound is no longer heard. Then, without re-striking the fork, the fork is then quickly placed just outside the ear with the patient asked to report when the sound caused by the vibration is no longer heard.  A normal or positive Rinne test is when sound is still heard when the tuning fork is moved to air near the ear (air conduction or AC), indicating that AC is equal or greater than (bone conduction or BC).  Therefore, AC > BC; which is how it is reported clinically for a normal or positive Rinne result.  In conductive hearing loss, bone conduction is better than air or BC > AC, a negative Rinne, if the patient reports that they do not hear the fork once it is moved.  The Rinne test is not ideal for distinguishing sensorineural hearing loss, as both sensorineural hearing loss and normal hearing report a positive Rinne test (though the sensorineural patient will have a decreased duration of hearing sound once the fork is moved to air).\nIn a normal patient, the Weber tuning fork sound is heard equally loudly in both ears, with no one ear hearing the sound louder than the other (lateralization).  Similarly, a patient with symmetrical hearing loss will hear the Weber tuning fork sound equally well, with diagnostic utility only in asymmetric (one-sided) hearing losses. In a patient with hearing loss, the Weber tuning fork sound is heard louder in one ear (lateralization) than the other.  This clinical finding should be confirmed by repeating the procedure and having the patient occlude one ear with a finger; the sound should be heard best in the occluded ear.\nThe results of both tests are noted and compared accordingly below to localize and characterize the nature of any detected hearing losses.  Note: the Weber and Rinne are screening tests that are not replacements for formal audiometry hearing tests.  Reported test accuracy measurements are very variable for clinical screening, surgical candidacy assessments, and estimation of hearing loss severity.\n\nDetection of air conductive hearing loss\nA patient with a unilateral conductive hearing loss would hear the tuning fork loudest in the affected ear.  This is because the ear with the conductive hearing loss is only receiving input from the bone conduction and no air conduction, and the sound is perceived as louder in that ear. This finding is due to the conduction problem of the middle ear (incus, malleus, stapes, and external auditory meatus) which masks the ambient noise of the room, while the well-functioning inner ear (cochlea with its basilar membrane) picks the sound up via the bones of the skull, causing it to be perceived as a louder sound in the affected ear.  Another theory, however, is based on the occlusion effect described by Tonndorf et al, in 1966.  Lower frequency sounds (as made by the 256 Hz fork) that are transferred through the bone to the ear canal escape from the canal.  If an occlusion is present, the sound cannot escape and appears louder on the ear with the conductive hearing loss.\nConductive hearing loss can be mimicked by plugging one ear with a finger and performing the Rinne and Weber tests, which will help clarify the above. Humming a constant note and then plugging one ear is a good way to mimic the findings of the Weber test in conductive hearing loss.  The simulation of the Weber test is the basis for the Bing test.\n\nDetection of sensorineural hearing loss\nIf air conduction is intact on both sides (therefore no CHL), the patient will report a quieter sound in the ear with the sensorineuronal hearing loss. This is because the ear with the sensorineuronal hearing loss is not converting input from either the air or bone conduction, and the sound is perceived as louder in the normal ear.\n\nConsiderations and limitations\nThis Weber test is most useful in individuals with hearing that is different between the two ears. It cannot confirm normal hearing because it does not measure sound sensitivity in a quantitative manner. Hearing defects affecting both ears equally, as in presbycusis will produce an apparently normal test result.\nWeber test considerations\nThe Weber test reflects conduction loss in the ipsilateral ear because, in the event of impaired conduction, ipsilateral sensorineural hearing is perceived as louder; this is the same reason humming becomes more salient when covering the ears. If the Weber-lateralized ear has a positive Rinne test (AC>BC), that generally means the absence of conduction loss in that ear, and the reason sound had been perceived as louder on that side is because a sensorineural loss is present contralaterally; an ipsilateral negative Rinne test (BC>AC), on the other hand, would confirm ipsilateral conductive hearing loss (although contralateral sensorineural hearing loss may still be present. If the Weber-lateralized ear has a positive Rinne test and the contralateral ear has a negative Rinne test, then both conductive and sensorineural hearing loss are present in the contralateral ear.  This is because sensorineural deficits always take auditory precedence over conductive ones, so even though conductive hearing loss is present in the contralateral ear, it is the sensorineural deficit that is responsible for the ipsilateral perceived elevation of volume. This also means that a Weber-lateralized ear with bilateral negative-Rinne corresponds to only sensorineural hearing on the ipsilateral side not being affected.\nRinne test considerations\nAlthough there is no replacement for formal audiometry, a quick screening test can be made by complementing the Weber test with the Rinne test.\nThe Rinne test is used in cases of unilateral hearing loss and establishes which ear has the greater bone conduction. Combined with the patient's perceived hearing loss, it can be determined if the cause is sensorineural or conductive.   For example, if the Rinne test shows that air conduction (AC) is greater than bone conduction (BC) in both ears and the Weber test lateralizes to a particular ear, then there is sensorineural hearing loss in the opposite (weaker) ear. Conductive hearing loss is confirmed in the weaker ear if bone conduction is greater than air conduction and the Weber test lateralizes to that side. Combined hearing loss is likely if the Weber test lateralizes to the stronger ear and bone conduction is greater than air conduction in the weaker ear.\n\nReferences\nSee also\nRinne test","208":"Wolfram syndrome, also called DIDMOAD (diabetes insipidus, diabetes mellitus, optic atrophy, and deafness), is a rare autosomal-recessive genetic disorder that causes childhood-onset diabetes mellitus, optic atrophy, and deafness as well as various other possible disorders including neurodegeneration. Symptoms can start to appear as early as childhood to adult years (2-65 years old). There is a 25% recurrence risk in children.\nIt was first described in four siblings in 1938 by Dr. Don J. Wolfram, M.D. In 1995, diagnostic criteria were created based on the profiles of 45 patients.  The disease affects the central nervous system (especially the brainstem). There are two subtypes \u2013 Wolfram Syndrome Type 1 (WFS1) and Wolfram Syndrome Type 2 (WFS2), that are distinguished by their causative gene.\nLess than 5,000 people in the US have this disease, with WFS1 being more common than WFS2.\n\nCauses\nWolfram syndrome was initially thought to be caused by mitochondrial dysfunction due to several reports of mitochondrial DNA mutations. However, it has now been established that Wolfram syndrome is caused by a congenital endoplasmic reticulum (ER) dysfunction.\n\nTwo forms have been described: Wolfram syndrome 1 (WFS1), and Wolfram syndrome 2 (WFS2).\n\nWFS1\nThe WFS1 or wolframin gene provides instructions for making the wolframin protein. The WFS1 gene is active in cells throughout the body, with strong activity in the heart, brain, lungs, inner ear, and pancreas. The pancreas provides enzymes that help digest food, and it also produces the hormone insulin. Insulin controls how much glucose (a type of sugar) is passed from the blood into cells for conversion to energy.\nWithin cells, wolframin is located in a structure called the endoplasmic reticulum. Among its many activities, the endoplasmic reticulum folds and modifies newly formed proteins so they have the correct 3-dimensional shape to function properly. The endoplasmic reticulum also helps transport proteins, fats, and other materials to specific sites within the cell or to the cell surface. The function of wolframin is unknown. Based on its location in the endoplasmic reticulum, however, it may play a role in protein folding or cellular transport. In the pancreas, wolframin may help fold a protein precursor of insulin (called proinsulin) into the mature hormone that controls blood glucose levels. Research findings also suggest that wolframin may help maintain the correct cellular level of charged calcium atoms (calcium ions) by controlling how much is stored in the endoplasmic reticulum. In the inner ear, wolframin may help maintain the proper levels of calcium ions or other charged particles that are essential for hearing. Mutation in the WFS1 lead to ER stress due to an increase in the accumulation of misfolded proteins. As there is a high level of misfolded protein, unfolded protein response (UPR) is stimulated and lead to transcriptional and translational process that can restore ER homeostasis, However, if the ER stress is present persistently due to physiological or pathophysiological events, the UPR will induce apoptosis.\nMore than 30 WFS1 mutations have been identified in individuals with a form of nonsyndromic deafness (hearing loss without related signs and symptoms affecting other parts of the body) called DFNA6. Individuals with DFNA6 deafness cannot hear low tones (low-frequency sounds), such as a tuba or the \"m\" in moon. DFNA6 hearing loss is unlike most forms of nonsyndromic deafness that affect high tones (high-frequency sounds), such as birds chirping, or all frequencies of sound. Most WFS1 mutations replace one of the protein building blocks (amino acids) used to make wolframin with an incorrect amino acid. One mutation deletes an amino acid from wolframin. WFS1 mutations probably alter the 3-dimensional shape of wolframin, which could affect its function. Because the function of wolframin is unknown, however, it is unclear how WFS1 mutations cause hearing loss. Some researchers suggest that altered wolframin disturbs the balance of charged particles in the inner ear, which interferes with the hearing process.\n\nOther disorders - caused by mutations in the WFS1 gene\nMutations in the WFS1 gene cause Wolfram syndrome, which is also known by the acronym DIDMOAD. This syndrome is characterised by childhood-onset diabetes mellitus (DM), which results from the improper control of glucose due to the lack of insulin; a gradual loss of vision caused by optic atrophy (OA), in which the nerve that connects the eye to the brain wastes away; and deafness (D). This syndrome can sometimes cause diabetes insipidus (DI), a condition in which the kidneys cannot conserve water. Other complications that affect the bladder and nervous system may also occur.  Researchers have identified more than 100 WFS1 mutations that cause Wolfram syndrome. Some mutations delete or insert DNA from the WFS1 gene. As a result, little or no wolframin is present in cells. Other mutations replace one of the protein building blocks (amino acids) used to make wolframin with an incorrect amino acid. These mutations appear to reduce wolframin activity dramatically. Researchers suggest that the loss of wolframin disrupts the production of insulin, which leads to poor glucose control and diabetes mellitus. It is unclear how WFS1 mutations lead to other features of Wolfram syndrome.\n\nWFS2\nWolfram Syndrome Type 2 (WFS2) is a subtype of Wolfram Syndrome caused by a mutation in the CDGSH iron-sulfur domain-containing protein 2 gene (CISD2 gene). CISD2 is a protein coding gene that is found on the endoplasmic reticulum (ER) and outer mitochondrial membrane. WFS2 is mainly localized in the ER, but studies have also shown that it can be localized in the mitochondrial outer membrane. Mutation of this gene effects the protein folding of the ER and functions of the mitochondria, which leads to the signs and symptoms seen in those with WFS2. In some cases, mutation of the gene can lead to premature aging, mitophagy and mitochondrial dysfunction. In studies using mice, WFS2 caused a decrease in ER Ca2+ and increase in mitochondrial Ca2+. This causes an increase in stress to the ER and activates an unfolded protein response (UPR). Further studies are still needed to better undersatnd WFS2 and the neurodegenerative effects it has. \nClinical features of both WFS1 and WFS2 are diabetes mellitus, optic atrophy\/neuropathy, sensorineural deafness, and  genitourinary problems. Although both types have some overlapping symptoms, there are some differences that help us distinguish between the two. One of the main ones is that WFS2, it is not associated with diabetes insipidus or psychiatric disorders but is instead associated with higher bleed risks and peptic ulcers.\nCISD2 gene consists of 3 exons on chromosome 4q24, which encodes the protein NAF-1 (nutrient deprivation autophagy factor-1). Therefore, if WFS2 were suspected in a patient, it may help to do a gene sequencing of the three exons and their intronic regions for a genetic analysis.\nWFS2 is the rarest and most recently discovered subtype of Wolfram syndrome.\n\nEpidemiology\nWolfram syndrome is considered a rare autosomal recessive neurodegenerative disease. According to the draft International Classification of Disease (ICD-11), Wolfram Syndrome is classified as a rare specified diabetes mellitus. The disease is estimated to affect 1 in 160,000 to 770,00. More specifically, the disease prevalence is 1 in 770,00 in the UK, 1 in 710,000 in Japan, 1 in 100,00 in North America, 0.74 in 1,000,000 in Italy, 1 in 68,000 in Lebanon and the highest prevalence is 1 in 54,478 in a small area of Sicily (Italy). It is believed that the populations with high prevalence have high-rate of consanguinity. The frequency of WSF1 mutation carrier is estimated to be 1 in 354 in the UK population and the disease is estimated to affect 1 out of 150 patient with juvenile-onset insulin-independent diabetes mellitus.\n\nDiagnosis\nThe diagnosis of Wolfram syndrome is multifaceted, involving clinical evaluation, genetic testing, laboratory investigations, and imaging studies. Clinical evaluation typically begins with a detailed medical history and physical examination, where patients often present with juvenile-onset diabetes mellitus followed by progressive optic atrophy, a condition where the optic nerves, which connect the eyes to the brain, deteriorate over time, leading to vision loss. There is an increased suspicion when diabetes is diagnosed in kids under 16. More evidence shows that Wolfram syndrome varies in how it appears. \nThe syndrome can present with various symptoms. In addition to diabetes and optic atrophy, patient may exhibit diabetes insipidus, a condition where the kidneys cannot retain watr, leading to frequent urination and excessive thirst. They might also have sensorineural hearing loss, which is a type of hearing loss caused by damage to the inner ear or the nerves that connect the ear to the brain. Neurological abnormalities such as ataxia (lack of muscle coordination) or myoclonus (sudden, involunatry muscle jerks) may also be observed. The progression of symptoms, starting with type 1 diabetes and subsequent vision loss within the first decade of life, is a critical diagnostic clue. \nImaging studies are essential for understanding the extent of brain and optic nerve damage in Wolfram syndrome. Magnetic resonance imaging (MRI) can show significant shrinkage of the brain stem and cerebellum, region of the brain in motor control and coordination. These change can resemble changes seen in other neurodegenerative disorders, which are diseases that involve the progressive loss of structrue or function of neurons, including death of neurons. Additionally, areas of the pons, part of the brainstem, may show increased signal intensity on T2-weighted images, indicating potential damage or changes in tissue composition. The connections between the cerebellum and the brainstem (middle cerebellar peduncles) can also exhibit atrophy, consistent with Wolfram syndrome. Changes in the optic radiations, which are the pathways transmitting visual information from the eyes to the brain, can be detected, aligning with the optic atrophy characteristic of Wolfram syndrome. Furthermore, the absence of the typical T1 hyperintensity in the posterior pituitary lobe suggests a lack of vasopressin-containing neurons, often linked with diabetes insipidus, another symptom of Wolfram syndrome. Optical coherence tomography (OCT) is used to measure retinal nerve fiber layer thickness, aiding in the assessment of optic atrophy and monitoring disease progression .\nNowadays, genetic testing are used commonly to confirm the diagnosis of Wolfram syndrome. Initially, patient with hereditary optic neuropathy who tested negative for mutation in the common optic neuropathy genes OPA1, OPA3 and LHON were selected for further genetic testing for WS. The primary genetic lotus associated with this syndrome is WFS1, and Sanger sequencing ( a method of reading the gentic code) of this gene typically confirms the diagnosis. Most patient exhibit recessive mutation in WFS1, meaning they inherited two copies of the mutated genes, one from each parents. However, some dominant mutation, such as H313Y, have been identified, where one copy of the mutated gene can cause the disorder. These dominant mutation are often linked to low-frequency sensorineural hearing loss, which affects the ability to hear low-pitched sounds. Additionally, there have been recent discoveries of autosomal dominant diabetes, where diabtes is inherited in a dominant manner, in patients with WFS1 mutations. Intepreting genetic testing results reqire specialized knowledge due to the complexity of the mutation. \nDetailed family history is important as WS2 inherited in an autosomal recessive manner, and genetic counseling is recommended for affected individuals and their families to understand the inheritance pattern, risks to other family members, and reproductive options. A minority of patients have recessive mutation in the CISD2 (WFS2) gene, and for those without WFS1 mutations, Sanger sequencing of WFS2 is conducted. Efforts are underway to develop diagnosis methods based on exome (sequencing all the protein-coding regions of the genes) and genome sequencing (sequencing the entire gentic code) for Wolfram syndrome and related disorder. \nOther diagnostic tools include audiological tests to identify seensorineural hearing loss, a common feature of Wolfram syndrome, and psychiatric evaluations to address cognitive or behavioral issues arising from neurodegenerative nature of the disease. Audiologcial tests help access the extend of hearing loss and guide interventions like hearing aids or other assitive devices. Psychiatric evaluations are important because the neurological aspects of Wolfram syndrome can lead to cognitive decline or behavioral changes, which require appropriate management and support.Combining these diagnostic approaches ensures a comprehensive understanding and management of Wolfram syndrome.\n\nTreatment\nThere is no known direct treatment. Current treatment efforts focus on managing the complications of Wolfram syndrome. Intranasal or oral desmopressin has been shown to improve symptoms for the treatment of diabetes insipidus caused by Wolfram syndrome. Patients with Wolfram syndrome experiencing hearing loss have benefited from the use of cochlear implants and hearing aids. While there are no therapies currently available to slow the progression of neurological manifestations, swallowing therapy and esophagomyotomy have been shown to be useful in alleviating some of the neurological symptoms. Anticholinergic medications, clean intermittent catheterizations, electrical stimulation, and physiotherapy have been shown to be effective at managing urological abnormalities due to Wolfram syndrome such as neurogenic bladder and upper urinary tract dilation.\nWhile there are no direct treatments, many therapies are currently being investigated for their efficacy at treating Wolfram syndrome. Gene and regenerative therapies are currently being studied for their efficacy in replacing damaged tissues due to Wolfram syndrome, such as pancreatic \u03b2-cells, neuronal, and retinal cells.\nWFS1 mutations cause proteins in the ER to fold improperly, leading to ER stress. ER stress stimulates the unfolded protein response (UPR), which causes cell apoptosis for pancreatic \u03b2-cells. Chemical chaperones are being investigated for their effect on reducing the UPR response and thus delaying disease progression by preventing cell death.  The FDA has approved 4-phenylbutyric acid (PBA) and tauroursodeoxycholic acid (TUDCA) as chemical chaperones to reduce ER stress to delay neurodegeneration in patients with Wolfram syndrome. As of 2023, sodium valproate\u2014an anti-epileptic drug\u2014is being investigated as a therapy for Wolfram syndrome due to studies showing its ability to inhibit ER stress-induced apoptosis, reducing neurodegeneration. Liraglutide\u2014a glucagon-like peptide-1 receptor (GLP 1-R) antagonist\u2014has been hypothesized to be an effective therapy, as it has been shown to improve diabetes mellitus, reduce cell death due to ER stress, reduce neuroinflammation, protect retinal ganglion cell death, and prevent optic nerve degeneration.  Dipeptidyl peptidase-4 (DPP-4) inhibitors have also been hypothesized to be efficacious in the treatment of Wolfram syndrome due to their ability to deactivate GLP 1-R, similar to liraglutide.  However, the efficacy and safety of using liraglutide and DPP-4 inhibitors for the treatment of Wolfram syndrome has not been well studied yet. \nER calcium levels have also been identified as a target for Wolfram syndrome therapy. WFS1 mutations increase cytosolic calcium, leading to the activation of cysteine proteases known as calpains. Increased calpains activation is associated which cell death. As of 2021, dantrolene sodium\u2014a medication indicated for the treatment of malignant hyperthermia and muscle spasms\u2014was being investigated in patients with Wolfram syndrome in a phase 2 clinical trial.\nOverall, there is currently no treatment guideline for treating or slowing down the progression of Wolfram Syndrome. Treatment is more so focused on treating and managing the symptoms. Research is still being conducted in finding more effective treatment strategies, including studies on drugs that can reduce dell damage, gene therapies, and regenerative therapies.\n\nPrognosis\nThe first symptom is typically diabetes mellitus, which is usually diagnosed around the age of 6. Insulin-dependent diabetes mellitus associate with Wolfram syndrome is differed from type 1 diabetes mellitus by having earlier diagnosis, rarely having positive auto-antibodies and ketoacidosis, having longer remission, needing less daily insulin, having lower average HbA1c level and more frequent hypoglycemia.\nThe second most common clinical manifestation of the disease is diabetes insipidus, which the kidney is unable to retain water due to renal outflow tract dilation and leads to high level of urine production. This condition affect around 70% of the patients with WSF1 mutation (WFS2 mutation does not typically associate with diabetes insipidus). Diabetes insipidus occurs around the age of 14 but the condition is often diagnosed late. Therefore, there is a high variability in the onset age.\nThe next symptom to appear is often optic atrophy, optical shrinkage that due to retinal ganglion cell axons' degeneration, around the age of 11. Blindness tends to develop a few years after the decrease in visual ability with the loss of color vision. Ophthalmic abnormalities often found in the patient with Wolfram Syndrome are cataract, nystagmus, glaucoma and maculopathy. There is also pigmentary retinopathy due to mitochondrial alteration that associated with Wolfram Syndrome. However, it is very rare and have been found in just a few cases.\nApproximately 65% of the patient with Wolfram Syndrome experienced sensorineural deafness which can manifest as deafness at birth or mild hearing loss in adolescence years and progressively worsen. However, the progression of sensorineural deafness is relatively slow and initially influenced the high-frequency sounds. Patients with WFS1 mutation have degenerative impairment in the central nervous system, as they increased in age they are more likely to suffer a more severe deafness than other patients that have hearing loss.\nThe majority of patient (>60%) with WSF1 mutation develop neurological symptoms around the age of 40; however, some may experience these symptoms earlier in life. Some most common neurological abnormalities are cerebellar ataxia, peripheral neuropathy, epilepsy, cognitive impairement, dysphagia, dysarthria and diminish sense of taste and smell. In addition, patient can also experienced orthostatic hypotension, gastroparesis, hypothermia\/hyperthermia, hypohidrosis or hyperhidrosis, constipation and headache. Furthermore, there were also cases which patients also have severe depression, sleep abnormalities, psychosis and physical aggression. The occurrence of the above conditions can add complexity to the clinical presentation of Wolfram Syndrome.\nUrinary tract disorders are also found in more than 90% patient with Wolfram Syndrome, in which neurogenic bladder is the main manifestation of neurological disorder that can lead to urinary incontinence, hydroureter and recurrent infections. More specifically, recurrent UTIs are one of the most prevalence clinical challenge associated with Wolfram Syndrome. These urological abnormalities are usually onset at the age of 20 and can be peaked at 13, 21 and 33 years of age. Furthermore, bladder dysfunction can progress to megacystis over time.\nEndocrine dysfunction is another clinical manifestation of Wolfram syndrome, which include hypogonadism. More specifically, hypogonadism present more frequent in male than female. Male patients are more likely to experience fertility impairment and erectile dysfunction while female patient will encounter some menstrual abnormalities. Additionally, due to the decrease in function of the anterior pituitary gland, patients with Wolfram syndrome can also have short statue, growth hormone deficiency and corticotrophin secretion deficiency. Since patient with Wolfram Syndrome can experienced diabetes mellitus, diabetes insipidus and urinary tract disorder, they are treated with desmopressin, which can lead to the development of hyponatremia.\nThere are other abnormalities that associated with Wolfram Syndrome such as gastrointestinal disorders (gastroparesis and bowel incontinence) and heart disease. These disorders have been reported in rare cases of WFS1 mutation.\nWolfram Syndrome prognosis is very poor with a median mortality rate of 65% before the age of 35 (age range 25-39). The two main reason for death in patient with Wolfram syndrome are central respiratory failure, due to severe neurological disability, and renal failure secondary to infections. Unfortunately, currently, there is no effective treatment that can delay or reverse the progression of the disease.\n\nSee also\nWFS1\nChromosome 4 disorders\nNonsyndromic deafness\nList of rare diseases\n\nReferences\nThis article incorporates text from the United States National Library of Medicine ([1]), which is in the public domain.\n\nExternal links\n\nOMIM DIDMOAD search","221":"This category combines all articles with unsourced statements from April 2022 (2022-04) to enable us to work through the backlog more systematically. It is a member of Category:Articles with unsourced statements.\nTo add an article to this category add     {{Citation needed|date=April 2022}} to the article. If you omit the date a bot will add it for you at some point.\n\nPlease help improve an article in this category by adding references to reliable sources that verify content within the article. Once the reliable source references have been added, the unsourced (citation needed) tag can be removed.","222":"This is a tracking category for book and encyclopedia citations implemented with {{citation}} templates and for book and encyclopedia citations implemented with {{cite book}} and {{cite encyclopedia}}.  To be included in this category, those templates must include:\n\n|location=, |place=, or |publication-place=\n|date= or |year= with a year-value of 1850 onwards\nbut must not include:\n\n|publisher=.\nThese constraints attempt to acknowledge that many 'old' books and encyclopediae weren't published in the same way as they would be today.  For those cases where a post 1850 book or encyclopedia does not have a publisher, set |publisher=none to suppress this categorization.\nPages in this category should only be added by Module:Citation\/CS1.\nPages with this condition are automatically placed in Category:CS1 maint: location missing publisher.\n\nBy default, Citation Style 1 and Citation Style 2 error messages are visible to all readers and maintenance messages are hidden from all readers.\nTo display maintenance messages in the rendered article, include the following text in your common CSS page (common.css) or your specific skin's CSS page and (skin.css).\n(Note to new editors: those CSS pages are specific to you, and control your view of pages, by adding to your user account's CSS code. If you have not yet created such a page, then clicking one of the .css links above will yield a page that starts \"Wikipedia does not have a user page with this exact name.\" Click the \"Start the User:username\/filename page\" link, paste the text below, save the page, follow the instructions at the bottom of the new page on bypassing your browser's cache, and finally, in order to see the previously hidden maintenance messages, refresh the page you were editing earlier.)\n\nTo display hidden-by-default error messages:\n\nEven with this CSS installed, older pages in Wikipedia's cache may not have been updated to show these error messages even though the page is listed in one of the tracking categories. A null edit will resolve that issue.\nAfter (error and\/maintenance) messages are displayed, it might still not be easy to find them in a large article with a lot of citations. Messages can then be found by searching (with Ctrl-F) for \"(help)\" or \"cs1\".\nTo hide normally-displayed error messages:\n\nYou can personalize the display of these messages (such as changing the color), but you will need to ask someone who knows CSS or at the technical village pump if you do not understand how.\nNota bene: these CSS rules are not obeyed by Navigation popups. They also do not hide script warning messages in the Preview box that begin with \"This is only a preview; your changes have not yet been saved\".\n\n\n== Notes ==","224":"This category combines all articles with unsourced statements from January 2021 (2021-01) to enable us to work through the backlog more systematically. It is a member of Category:Articles with unsourced statements.\nTo add an article to this category add     {{Citation needed|date=January 2021}} to the article. If you omit the date a bot will add it for you at some point.\n\nPlease help improve an article in this category by adding references to reliable sources that verify content within the article. Once the reliable source references have been added, the unsourced (citation needed) tag can be removed.","225":"This is a tracking category for CS1 citations that use:\n\n|last#=, |author-last#=, |author#-last=, |author-surname#=, |author#-surname=, |subject-last#=, |subject#-last=, |subject-surname#=, |subject#-surname=, |author#=, |host#=, |subject#=, |surname#=\n|first#=, |author-first#=, |author#-first=, |author-given#=, |author#-given=, |subject-first#=, |subject#-first=, |subject-given#=, |subject#-given=, |given#=\nwhere the values assigned to these parameters are a mix of letters and digits.\nIn general, digits are not expected to be part of an author's name and so should be removed.  When the author is a corporate or organizational name that legitimately includes digits, the accept-this-as-written markup may be applied to suppress this category.\nPages in this category should only be added by Module:Citation\/CS1.\nPages with this condition are automatically placed in Category:CS1 maint: numeric names: authors list.\n\nBy default, Citation Style 1 and Citation Style 2 error messages are visible to all readers and maintenance messages are hidden from all readers.\nTo display maintenance messages in the rendered article, include the following text in your common CSS page (common.css) or your specific skin's CSS page and (skin.css).\n(Note to new editors: those CSS pages are specific to you, and control your view of pages, by adding to your user account's CSS code. If you have not yet created such a page, then clicking one of the .css links above will yield a page that starts \"Wikipedia does not have a user page with this exact name.\" Click the \"Start the User:username\/filename page\" link, paste the text below, save the page, follow the instructions at the bottom of the new page on bypassing your browser's cache, and finally, in order to see the previously hidden maintenance messages, refresh the page you were editing earlier.)\n\nTo display hidden-by-default error messages:\n\nEven with this CSS installed, older pages in Wikipedia's cache may not have been updated to show these error messages even though the page is listed in one of the tracking categories. A null edit will resolve that issue.\nAfter (error and\/maintenance) messages are displayed, it might still not be easy to find them in a large article with a lot of citations. Messages can then be found by searching (with Ctrl-F) for \"(help)\" or \"cs1\".\nTo hide normally-displayed error messages:\n\nYou can personalize the display of these messages (such as changing the color), but you will need to ask someone who knows CSS or at the technical village pump if you do not understand how.\nNota bene: these CSS rules are not obeyed by Navigation popups. They also do not hide script warning messages in the Preview box that begin with \"This is only a preview; your changes have not yet been saved\".\n\n\n== Notes ==","226":"This hidden tracking category lists pages with CS1 citations that use |url-status=usurped or |url-status=unfit.\nThe keywords unfit and usurped are intended to identify original URLs that point to live sites that are inappropriate: spam, advertising, porn, etc.\nA URL that returns a HTTP 404 error is not considered to be unfit and, in such cases, editors should set |url-status=dead.\nCS1 and CS2 templates in pages listed in this category should be checked to ensure that the unfit and usurped keywords are correctly applied.\nOnly Module:Citation\/CS1 should directly add pages to this category.\nPages with this condition are automatically placed in Category:CS1 maint: unfit URL.\n\nOther values\n|url-status=bot: unknown is tracked at Category:CS1 maint: bot: original URL status unknown.\n|url-status=dead is not tracked.\n|url-status=live is not tracked.\n|url-status=deviated is not tracked.\n|url-status=<anything else> is tracked at Category:CS1 errors: invalid parameter value.\n\nBy default, Citation Style 1 and Citation Style 2 error messages are visible to all readers and maintenance messages are hidden from all readers.\nTo display maintenance messages in the rendered article, include the following text in your common CSS page (common.css) or your specific skin's CSS page and (skin.css).\n(Note to new editors: those CSS pages are specific to you, and control your view of pages, by adding to your user account's CSS code. If you have not yet created such a page, then clicking one of the .css links above will yield a page that starts \"Wikipedia does not have a user page with this exact name.\" Click the \"Start the User:username\/filename page\" link, paste the text below, save the page, follow the instructions at the bottom of the new page on bypassing your browser's cache, and finally, in order to see the previously hidden maintenance messages, refresh the page you were editing earlier.)\n\nTo display hidden-by-default error messages:\n\nEven with this CSS installed, older pages in Wikipedia's cache may not have been updated to show these error messages even though the page is listed in one of the tracking categories. A null edit will resolve that issue.\nAfter (error and\/maintenance) messages are displayed, it might still not be easy to find them in a large article with a lot of citations. Messages can then be found by searching (with Ctrl-F) for \"(help)\" or \"cs1\".\nTo hide normally-displayed error messages:\n\nYou can personalize the display of these messages (such as changing the color), but you will need to ask someone who knows CSS or at the technical village pump if you do not understand how.\nNota bene: these CSS rules are not obeyed by Navigation popups. They also do not hide script warning messages in the Preview box that begin with \"This is only a preview; your changes have not yet been saved\".\n\n\n== Notes ==","227":"This is a hidden tracking category for CS1 citations that use |author=, or its aliases where Module:Citation\/CS1 identifies cs1|2 citation templates that appear to use singular forms of author name-list parameters to list multiple authors' names.  Doing so corrupts the citation's metadata.\nThe citation module code looks for multiple comma or semicolon separator characters in the value assigned to |author=, |last=, their aliases, and enumerated equivalents (e.g. |author2=, |last2=, etc.).  This test displays an error message for multiple authors' names in a single parameter, as well as single author names that include a comma-separated list of post-nominals: |author=FC White, RN, MD, Ph.D.\nTo fix these errors in citations:\n\nRemove post-nominals.\nProvide enumerated author parameters (e.g. either |author2= or |last2= and |first2=) for each author of a cited work.\nWhen multiple separator characters are legitimately present in a name (commonly a corporate, institutional, or governmental author), the name may be wrapped in two sets of parentheses (or <nowiki>...<\/nowiki> tags) to suppress assignment to this category, like this: |author=((Federal Ministry of Transport, Building, and Urban Development)) or |author=<nowiki>Federal Ministry of Transport, Building, and Urban Development<\/nowiki>.\nEditors should not simply replace |author= with |authors=. Using the plural |authors= parameter to replace a singular |author= or |last= parameter that holds multiple authors' names is discouraged because automatically decoding lists of human names is an extraordinarily difficult task.  Because of this difficulty, names listed in |authors= are omitted from the template's COinS metadata. Enumerating the author list with |authorn=, or |lastn= \/ |firstn=, or, where appropriate, |vauthors=, preserves the associated metadata.\nPages in this category should only be added by Module:Citation\/CS1.\nPages with this condition are automatically placed in Category:CS1 maint: multiple names: authors list.\n\nBy default, Citation Style 1 and Citation Style 2 error messages are visible to all readers and maintenance messages are hidden from all readers.\nTo display maintenance messages in the rendered article, include the following text in your common CSS page (common.css) or your specific skin's CSS page and (skin.css).\n(Note to new editors: those CSS pages are specific to you, and control your view of pages, by adding to your user account's CSS code. If you have not yet created such a page, then clicking one of the .css links above will yield a page that starts \"Wikipedia does not have a user page with this exact name.\" Click the \"Start the User:username\/filename page\" link, paste the text below, save the page, follow the instructions at the bottom of the new page on bypassing your browser's cache, and finally, in order to see the previously hidden maintenance messages, refresh the page you were editing earlier.)\n\nTo display hidden-by-default error messages:\n\nEven with this CSS installed, older pages in Wikipedia's cache may not have been updated to show these error messages even though the page is listed in one of the tracking categories. A null edit will resolve that issue.\nAfter (error and\/maintenance) messages are displayed, it might still not be easy to find them in a large article with a lot of citations. Messages can then be found by searching (with Ctrl-F) for \"(help)\" or \"cs1\".\nTo hide normally-displayed error messages:\n\nYou can personalize the display of these messages (such as changing the color), but you will need to ask someone who knows CSS or at the technical village pump if you do not understand how.\nNota bene: these CSS rules are not obeyed by Navigation popups. They also do not hide script warning messages in the Preview box that begin with \"This is only a preview; your changes have not yet been saved\".\n\n\n== Notes ==","228":"This category combines all Wikipedia articles needing clarification from March 2019 (2019-03) to enable us to work through the backlog more systematically. It is a member of Category:Wikipedia articles needing clarification.\nTo add an article to this category add {{Vague|date=March 2019}},{{Incomprehensible|date=March 2019}}, {{Confusing|date=March 2019}}, {{Clarify|date=March 2019}} or  {{Ambiguous|date=March 2019}} to the article. If you omit the date a bot will add it for you at some point."},"link":{"0":"https:\/\/en.wikipedia.org\/wiki\/Vertigo","1":"https:\/\/en.wikipedia.org\/wiki\/Acoustic_reflex","2":"https:\/\/en.wikipedia.org\/wiki\/Aminoglycoside","3":"https:\/\/en.wikipedia.org\/wiki\/Alternobaric_vertigo","4":"https:\/\/en.wikipedia.org\/wiki\/Acrophobia","5":"https:\/\/en.wikipedia.org\/wiki\/Alfred_Hitchcock","6":"https:\/\/en.wikipedia.org\/wiki\/Anticholinergic","7":"https:\/\/en.wikipedia.org\/wiki\/Antiemetic","8":"https:\/\/en.wikipedia.org\/wiki\/Antihistamine","9":"https:\/\/en.wikipedia.org\/wiki\/Anticonvulsant","10":"https:\/\/en.wikipedia.org\/wiki\/Audiometry","11":"https:\/\/en.wikipedia.org\/wiki\/Aspirin","12":"https:\/\/en.wikipedia.org\/wiki\/Auditory_brainstem_response","13":"https:\/\/en.wikipedia.org\/wiki\/Auditory_processing_disorder","14":"https:\/\/en.wikipedia.org\/wiki\/Auditory_system","15":"https:\/\/en.wikipedia.org\/wiki\/Benign_paroxysmal_positional_vertigo","16":"https:\/\/en.wikipedia.org\/wiki\/Balance_disorder","17":"https:\/\/en.wikipedia.org\/wiki\/Benign_paroxysmal_positional_vertigo","18":"https:\/\/en.wikipedia.org\/wiki\/Balance_(ability)","19":"https:\/\/en.wikipedia.org\/wiki\/Beta_blocker","20":"https:\/\/en.wikipedia.org\/wiki\/Betahistine","21":"https:\/\/en.wikipedia.org\/wiki\/Bezold%27s_abscess","22":"https:\/\/en.wikipedia.org\/wiki\/Blurred_vision","23":"https:\/\/en.wikipedia.org\/wiki\/Brain_tumor","24":"https:\/\/en.wikipedia.org\/wiki\/Broken_escalator_phenomenon","25":"https:\/\/en.wikipedia.org\/wiki\/Brain_tumor","26":"https:\/\/en.wikipedia.org\/wiki\/Brainstem","27":"https:\/\/en.wikipedia.org\/wiki\/CT_scan","28":"https:\/\/en.wikipedia.org\/wiki\/Calcium_carbonate","29":"https:\/\/en.wikipedia.org\/wiki\/Caloric_reflex_test","30":"https:\/\/en.wikipedia.org\/wiki\/Carbon_monoxide_poisoning","31":"https:\/\/en.wikipedia.org\/wiki\/Central_nervous_system","32":"https:\/\/en.wikipedia.org\/wiki\/Cerebellopontine_angle","33":"https:\/\/en.wikipedia.org\/wiki\/Cervical_vertebrae","34":"https:\/\/en.wikipedia.org\/wiki\/Spondylosis","35":"https:\/\/en.wikipedia.org\/wiki\/Cerebellum","36":"https:\/\/en.wikipedia.org\/wiki\/Chemical_synapse","37":"https:\/\/en.wikipedia.org\/wiki\/Chiari_malformation","38":"https:\/\/en.wikipedia.org\/wiki\/Cholesteatoma","39":"https:\/\/en.wikipedia.org\/wiki\/Chronic_subjective_dizziness","40":"https:\/\/en.wikipedia.org\/wiki\/Computed_tomography_angiography","41":"https:\/\/en.wikipedia.org\/wiki\/Common_cold","42":"https:\/\/en.wikipedia.org\/wiki\/Conductive_hearing_loss","43":"https:\/\/en.wikipedia.org\/wiki\/CT_scan","44":"https:\/\/en.wikipedia.org\/wiki\/Cortical_deafness","45":"https:\/\/en.wikipedia.org\/wiki\/Corticosteroid","46":"https:\/\/en.wikipedia.org\/wiki\/Consciousness","47":"https:\/\/en.wikipedia.org\/wiki\/Deafblindness","48":"https:\/\/en.wikipedia.org\/wiki\/Deafness","49":"https:\/\/en.wikipedia.org\/wiki\/Developed_country","50":"https:\/\/en.wikipedia.org\/wiki\/Decompression_sickness","51":"https:\/\/en.wikipedia.org\/wiki\/Dimenhydrinate","52":"https:\/\/en.wikipedia.org\/wiki\/Dexamethasone","53":"https:\/\/en.wikipedia.org\/wiki\/Differential_diagnosis","54":"https:\/\/en.wikipedia.org\/wiki\/Diplopia","55":"https:\/\/en.wikipedia.org\/wiki\/Diseases_Database","56":"https:\/\/en.wikipedia.org\/wiki\/Dizziness","57":"https:\/\/en.wikipedia.org\/wiki\/Dix%E2%80%93Hallpike_test","58":"https:\/\/en.wikipedia.org\/wiki\/Dive_computer","59":"https:\/\/en.wikipedia.org\/wiki\/Dix%E2%80%93Hallpike_test","60":"https:\/\/en.wikipedia.org\/wiki\/Dizziness","61":"https:\/\/en.wikipedia.org\/wiki\/Digital_object_identifier","62":"https:\/\/en.wikipedia.org\/wiki\/EMedicine","63":"https:\/\/en.wikipedia.org\/wiki\/Otology","64":"https:\/\/en.wikipedia.org\/wiki\/Ear_pain","65":"https:\/\/en.wikipedia.org\/wiki\/Dopamine","66":"https:\/\/en.wikipedia.org\/wiki\/Edward_D._Thalmann","67":"https:\/\/en.wikipedia.org\/wiki\/Electronystagmography","68":"https:\/\/en.wikipedia.org\/wiki\/Electrocochleography","69":"https:\/\/en.wikipedia.org\/wiki\/Endolymph","70":"https:\/\/en.wikipedia.org\/wiki\/Electrophysiology_study","71":"https:\/\/en.wikipedia.org\/wiki\/Epilepsy","72":"https:\/\/en.wikipedia.org\/wiki\/Epley_maneuver","73":"https:\/\/en.wikipedia.org\/wiki\/Sense_of_balance","74":"https:\/\/en.wikipedia.org\/wiki\/Eustachian_tube_dysfunction","75":"https:\/\/en.wikipedia.org\/wiki\/Ethanol","76":"https:\/\/en.wikipedia.org\/wiki\/GABA","77":"https:\/\/en.wikipedia.org\/wiki\/Fear_of_falling","78":"https:\/\/en.wikipedia.org\/wiki\/Gentamicin","79":"https:\/\/en.wikipedia.org\/wiki\/Gradenigo%27s_syndrome","80":"https:\/\/en.wikipedia.org\/wiki\/Head_for_heights","81":"https:\/\/en.wikipedia.org\/wiki\/Hearing","82":"https:\/\/en.wikipedia.org\/wiki\/Hearing_loss","83":"https:\/\/en.wikipedia.org\/wiki\/Hearing_test","84":"https:\/\/en.wikipedia.org\/wiki\/Histamine","85":"https:\/\/en.wikipedia.org\/wiki\/Stroke","86":"https:\/\/en.wikipedia.org\/wiki\/Histopathology","87":"https:\/\/en.wikipedia.org\/wiki\/Scopolamine","88":"https:\/\/en.wikipedia.org\/wiki\/Hyperacusis","89":"https:\/\/en.wikipedia.org\/wiki\/Hyperventilation_syndrome","90":"https:\/\/en.wikipedia.org\/wiki\/Hyperbaric_medicine","91":"https:\/\/en.wikipedia.org\/wiki\/ICD-10","92":"https:\/\/en.wikipedia.org\/wiki\/ISSN","93":"https:\/\/en.wikipedia.org\/wiki\/Ideomotor_phenomenon","94":"https:\/\/en.wikipedia.org\/wiki\/ISBN","95":"https:\/\/en.wikipedia.org\/wiki\/Illusions_of_self-motion","96":"https:\/\/en.wikipedia.org\/wiki\/Infarction","97":"https:\/\/en.wikipedia.org\/wiki\/Inner_ear","98":"https:\/\/en.wikipedia.org\/wiki\/Ischemia","99":"https:\/\/en.wikipedia.org\/wiki\/International_Classification_of_Diseases","100":"https:\/\/en.wikipedia.org\/wiki\/Isobaric_counterdiffusion","101":"https:\/\/en.wikipedia.org\/wiki\/Influenza","102":"https:\/\/en.wikipedia.org\/wiki\/Joanna_Wardlaw","103":"https:\/\/en.wikipedia.org\/wiki\/Labyrinthine_fistula","104":"https:\/\/en.wikipedia.org\/wiki\/Bony_labyrinth","105":"https:\/\/en.wikipedia.org\/wiki\/Labyrinthitis","106":"https:\/\/en.wikipedia.org\/wiki\/Lateral_medullary_syndrome","107":"https:\/\/en.wikipedia.org\/wiki\/Lateral_vestibular_nucleus","108":"https:\/\/en.wikipedia.org\/wiki\/List_of_ICD-9_codes","109":"https:\/\/en.wikipedia.org\/wiki\/Lesion","110":"https:\/\/en.wikipedia.org\/wiki\/Magnetic_resonance_imaging","111":"https:\/\/en.wikipedia.org\/wiki\/Latin","112":"https:\/\/en.wikipedia.org\/wiki\/Mastoid_part_of_the_temporal_bone","113":"https:\/\/en.wikipedia.org\/wiki\/Mastoiditis","114":"https:\/\/en.wikipedia.org\/wiki\/Meclizine","115":"https:\/\/en.wikipedia.org\/wiki\/Medial_vestibular_nucleus","116":"https:\/\/en.wikipedia.org\/wiki\/Medical_Subject_Headings","117":"https:\/\/en.wikipedia.org\/wiki\/Medical_specialty","118":"https:\/\/en.wikipedia.org\/wiki\/Metoprolol","119":"https:\/\/en.wikipedia.org\/wiki\/Methylprednisolone","120":"https:\/\/en.wikipedia.org\/wiki\/Middle_ear_barotrauma","121":"https:\/\/en.wikipedia.org\/wiki\/Middle_ear","122":"https:\/\/en.wikipedia.org\/wiki\/Migraine","123":"https:\/\/en.wikipedia.org\/wiki\/Motion_sickness","124":"https:\/\/en.wikipedia.org\/wiki\/Multiple_sclerosis","125":"https:\/\/en.wikipedia.org\/wiki\/Nausea","126":"https:\/\/en.wikipedia.org\/wiki\/Functional_neurologic_disorder","127":"https:\/\/en.wikipedia.org\/wiki\/M%C3%A9ni%C3%A8re%27s_disease","128":"https:\/\/en.wikipedia.org\/wiki\/Neurotransmitter","129":"https:\/\/en.wikipedia.org\/wiki\/Nonsyndromic_deafness","130":"https:\/\/en.wikipedia.org\/wiki\/Norepinephrine","131":"https:\/\/en.wikipedia.org\/wiki\/Nystagmus","132":"https:\/\/en.wikipedia.org\/wiki\/OCLC","133":"https:\/\/en.wikipedia.org\/wiki\/Otitis_externa","134":"https:\/\/en.wikipedia.org\/wiki\/Semicircular_canals","135":"https:\/\/en.wikipedia.org\/wiki\/Otitis_media","136":"https:\/\/en.wikipedia.org\/wiki\/Otoacoustic_emission","137":"https:\/\/en.wikipedia.org\/wiki\/Otolith","138":"https:\/\/en.wikipedia.org\/wiki\/Otomycosis","139":"https:\/\/en.wikipedia.org\/wiki\/Otorhinolaryngology","140":"https:\/\/en.wikipedia.org\/wiki\/Otoscope","141":"https:\/\/en.wikipedia.org\/wiki\/Outer_ear","142":"https:\/\/en.wikipedia.org\/wiki\/Otosclerosis","143":"https:\/\/en.wikipedia.org\/wiki\/PubMed_Central","144":"https:\/\/en.wikipedia.org\/wiki\/Parkinsonism","145":"https:\/\/en.wikipedia.org\/wiki\/PubMed","146":"https:\/\/en.wikipedia.org\/wiki\/Pathophysiology","147":"https:\/\/en.wikipedia.org\/wiki\/Nystagmus","148":"https:\/\/en.wikipedia.org\/wiki\/Perforated_eardrum","149":"https:\/\/en.wikipedia.org\/wiki\/Patulous_Eustachian_tube","150":"https:\/\/en.wikipedia.org\/wiki\/Patient_UK","151":"https:\/\/en.wikipedia.org\/wiki\/Phonophobia","152":"https:\/\/en.wikipedia.org\/wiki\/Perspiration","153":"https:\/\/en.wikipedia.org\/wiki\/Otoscope","154":"https:\/\/en.wikipedia.org\/wiki\/Positional_alcohol_nystagmus","155":"https:\/\/en.wikipedia.org\/wiki\/Posterior_cranial_fossa","156":"https:\/\/en.wikipedia.org\/wiki\/Posturography","157":"https:\/\/en.wikipedia.org\/wiki\/Presbycusis","158":"https:\/\/en.wikipedia.org\/wiki\/Lightheadedness","159":"https:\/\/en.wikipedia.org\/wiki\/Pure-tone_audiometry","160":"https:\/\/en.wikipedia.org\/wiki\/Proprioception","161":"https:\/\/en.wikipedia.org\/wiki\/Purkinje_cell","162":"https:\/\/en.wikipedia.org\/wiki\/Quality_of_life","163":"https:\/\/en.wikipedia.org\/wiki\/Rinne_test","164":"https:\/\/en.wikipedia.org\/wiki\/Romberg%27s_test","165":"https:\/\/en.wikipedia.org\/wiki\/Semantic_Scholar","166":"https:\/\/en.wikipedia.org\/wiki\/SNOMED_CT","167":"https:\/\/en.wikipedia.org\/wiki\/Saccule","168":"https:\/\/en.wikipedia.org\/wiki\/Semicircular_canals","169":"https:\/\/en.wikipedia.org\/wiki\/Sense_of_balance","170":"https:\/\/en.wikipedia.org\/wiki\/Sensorineural_hearing_loss","171":"https:\/\/en.wikipedia.org\/wiki\/Signs_and_symptoms","172":"https:\/\/en.wikipedia.org\/wiki\/Dysarthria","173":"https:\/\/en.wikipedia.org\/wiki\/Serotonin","174":"https:\/\/en.wikipedia.org\/wiki\/Spatial_disorientation","175":"https:\/\/en.wikipedia.org\/wiki\/Spatial_hearing_loss","176":"https:\/\/en.wikipedia.org\/wiki\/Superior_canal_dehiscence_syndrome","177":"https:\/\/en.wikipedia.org\/wiki\/Hyperbaric_medicine","178":"https:\/\/en.wikipedia.org\/wiki\/Stroke","179":"https:\/\/en.wikipedia.org\/wiki\/Tone_decay_test","180":"https:\/\/en.wikipedia.org\/wiki\/Tinnitus","181":"https:\/\/en.wikipedia.org\/wiki\/Topiramate","182":"https:\/\/en.wikipedia.org\/wiki\/Transient_ischemic_attack","183":"https:\/\/en.wikipedia.org\/wiki\/Trigeminal_nerve","184":"https:\/\/en.wikipedia.org\/wiki\/Tympanometry","185":"https:\/\/en.wikipedia.org\/wiki\/Tympanosclerosis","186":"https:\/\/en.wikipedia.org\/wiki\/Trimix_(breathing_gas)","187":"https:\/\/en.wikipedia.org\/wiki\/Unterberger_test","188":"https:\/\/en.wikipedia.org\/wiki\/Utricle_(ear)","189":"https:\/\/en.wikipedia.org\/wiki\/Underwater_diving","190":"https:\/\/en.wikipedia.org\/wiki\/Usher_syndrome","191":"https:\/\/en.wikipedia.org\/wiki\/Valproate","192":"https:\/\/en.wikipedia.org\/wiki\/Vasospasm","193":"https:\/\/en.wikipedia.org\/wiki\/Vertebrobasilar_insufficiency","194":"https:\/\/en.wikipedia.org\/wiki\/Vertigo_(disambiguation)","195":"https:\/\/en.wikipedia.org\/wiki\/Migraine-associated_vertigo","196":"https:\/\/en.wikipedia.org\/wiki\/Vertigo_(film)","197":"https:\/\/en.wikipedia.org\/wiki\/Vestibular_nerve","198":"https:\/\/en.wikipedia.org\/wiki\/Labyrinthitis","199":"https:\/\/en.wikipedia.org\/wiki\/Vestibular_schwannoma","200":"https:\/\/en.wikipedia.org\/wiki\/Vestibular_system","201":"https:\/\/en.wikipedia.org\/wiki\/Vestibule_of_the_ear","202":"https:\/\/en.wikipedia.org\/wiki\/Vestibulo%E2%80%93ocular_reflex","203":"https:\/\/en.wikipedia.org\/wiki\/Visual_reinforcement_audiometry","204":"https:\/\/en.wikipedia.org\/wiki\/Vestibulo%E2%80%93ocular_reflex","205":"https:\/\/en.wikipedia.org\/wiki\/Vitiligo","206":"https:\/\/en.wikipedia.org\/wiki\/Vomiting","207":"https:\/\/en.wikipedia.org\/wiki\/Weber_test","208":"https:\/\/en.wikipedia.org\/wiki\/Wolfram_syndrome","221":"https:\/\/en.wikipedia.org\/wiki\/Category:Articles_with_unsourced_statements_from_April_2022","222":"https:\/\/en.wikipedia.org\/wiki\/Category:CS1_maint:_location_missing_publisher","224":"https:\/\/en.wikipedia.org\/wiki\/Category:Articles_with_unsourced_statements_from_January_2021","225":"https:\/\/en.wikipedia.org\/wiki\/Category:CS1_maint:_numeric_names:_authors_list","226":"https:\/\/en.wikipedia.org\/wiki\/Category:CS1_maint:_unfit_URL","227":"https:\/\/en.wikipedia.org\/wiki\/Category:CS1_maint:_multiple_names:_authors_list","228":"https:\/\/en.wikipedia.org\/wiki\/Category:Wikipedia_articles_needing_clarification_from_March_2019"},"categories":{"0":["All articles with unsourced statements","Articles with hAudio microformats","Articles with short description","Articles with unsourced statements from April 2022","Articles with unsourced statements from January 2021","CS1 maint: location missing publisher","CS1 maint: multiple names: authors list","CS1 maint: numeric names: authors list","CS1 maint: unfit URL","Diseases of inner ear","Illusions","Neurological disorders","Pages displaying wikidata descriptions as a fallback via Module:Annotated link","Phenomena","Short description is different from Wikidata","Spoken articles","Symptoms and signs of mental disorders","Wikipedia articles needing clarification from March 2019","Wikipedia emergency medicine articles ready to translate","Wikipedia medicine articles ready to translate"],"1":["Articles with short description","Audiology","Auditory system","CS1 French-language sources (fr)","Reflexes","Short description is different from Wikidata"],"2":["All articles with unsourced statements","Aminoglycoside antibiotics","Articles with short description","Articles with unsourced statements from December 2010","Articles with unsourced statements from February 2014","Nephrotoxins","Short description matches Wikidata"],"3":["All articles to be expanded","All articles with empty sections","Articles to be expanded from December 2018","Articles with empty sections from December 2018","Articles with short description","Aviation medicine","CS1: long volume value","CS1 maint: unfit URL","Short description is different from Wikidata","Underwater diving disorders"],"4":["All articles to be expanded","Articles containing Greek-language text","Articles to be expanded from April 2023","Articles with short description","Environmental phobias","Height","Short description is different from Wikidata","Situational phobias","Use dmy dates from March 2024"],"5":["1899 births","1980 deaths","20th-century American screenwriters","20th-century English screenwriters","AFI Life Achievement Award recipients","Alfred Hitchcock","All articles containing potentially dated statements","All articles with failed verification","All articles with unsourced statements","All pages needing factual verification","American Roman Catholics","American horror film directors","American male screenwriters","American television directors","American television producers","American writers of Irish descent","Articles containing potentially dated statements from 2021","Articles containing video clips","Articles with failed verification from December 2017","Articles with hCards","Articles with short description","Articles with unsourced statements from December 2017","Articles with unsourced statements from January 2018","Artists awarded knighthoods","BAFTA fellows","Biography with signature","British Army personnel of World War I","British horror film directors","CS1 Spanish-language sources (es)","CS1 maint: DOI inactive as of July 2024","Cecil B. DeMille Award Golden Globe winners","Deaths from kidney failure in California","Directors Guild of America Award winners","Directors of Best Picture Academy Award winners","Edgar Award winners","English Roman Catholics","English emigrants to the United States","English film producers","English male screenwriters","English people of Irish descent","English television directors","English television producers","Film directors from London","Film directors from Los Angeles","Film producers from California","Film producers from London","German-language film directors","Good articles","Horror film producers","Knights Commander of the Order of the British Empire","Military personnel from the London Borough of Waltham Forest","Naturalized citizens of the United States","Pages containing London Gazette template with parameter supp set to y","Pages using Sister project links with wikidata namespace mismatch","People educated at St Ignatius' College, Enfield","People from Bel Air, Los Angeles","People from Leytonstone","People with multiple citizenship","Royal Engineers soldiers","Short description is different from Wikidata","Silent film directors","Silent film screenwriters","TCMDb name template using non-numeric ID from Wikidata","Use British English from March 2020","Use dmy dates from June 2024","Webarchive template wayback links","Wikipedia articles incorporating a citation from the ODNB","Wikipedia articles needing factual verification from December 2017","Wikipedia articles needing page number citations from December 2017","Wikipedia articles needing page number citations from January 2018"],"6":["All articles with unsourced statements","Anticholinergics","Articles with short description","Articles with unsourced statements from December 2023","Articles with unsourced statements from January 2019","Medical mnemonics","Short description is different from Wikidata"],"7":["All articles containing potentially dated statements","All articles needing additional references","All articles with unsourced statements","Antiemetics","Articles containing potentially dated statements from June 2018","Articles needing additional references from November 2019","Articles with short description","Articles with unsourced statements from June 2019","Articles with unsourced statements from October 2010","Short description matches Wikidata"],"8":["Antihistamines","Articles with BNE identifiers","Articles with J9U identifiers","Articles with LCCN identifiers","Articles with NKC identifiers","Articles with short description","CS1: long volume value","Pages using div col with small parameter","Short description is different from Wikidata","Use dmy dates from October 2015","Vasoconstrictors","Webarchive template wayback links"],"9":["Anticonvulsants","Articles with short description","CS1 maint: archived copy as title","Epilepsy","Pages using multiple image with auto scaled images","Short description matches Wikidata","Use dmy dates from July 2020","Webarchive template wayback links"],"10":["All articles needing additional references","All articles to be expanded","All articles with unsourced statements","Articles needing additional medical references from May 2019","Articles needing additional references from September 2009","Articles requiring reliable medical sources","Articles to be expanded from November 2015","Articles with short description","Articles with unsourced statements from June 2022","Audiology","Commons link is locally defined","Ear procedures","Hearing","Short description is different from Wikidata"],"11":["1897 in Germany","1897 in science","Acetate esters","Acetylsalicylic acids","All Wikipedia articles in need of updating","All articles containing potentially dated statements","All articles lacking reliable references","All articles to be expanded","All articles with unsourced statements","All pages needing factual verification","Antiplatelet drugs","Articles containing potentially dated statements from 2023","Articles containing potentially dated statements from April 2022","Articles containing potentially dated statements from August 2021","Articles containing potentially dated statements from September 2019","Articles lacking reliable references from August 2016","Articles to be expanded from January 2023","Articles with short description","Articles with unsourced statements from April 2023","Articles with unsourced statements from April 2024","Aspirin","Brands that became generic","Chemical substances for emergency medicine","Commercialization of traditional medicines","Commons link is the pagename","Covalent inhibitors","Drugboxes which contain changes to watched fields","Drugs developed by Bayer","Drugs with non-standard legal status","ECHA InfoCard ID from Wikidata","Equine medications","German inventions","Good articles","Hepatotoxins","Inconsistent wikidata for Commons category","Infobox drug articles with non-default infobox title","Multiple chemicals in Infobox drug","Multiple chemicals in an infobox that need indexing","Nonsteroidal anti-inflammatory drugs","Pages using multiple image with auto scaled images","PubChem ID (CID) same as Wikidata","Salicylic acids","Salicylyl esters","Short description is different from Wikidata","Source attribution","Use dmy dates from December 2023","Wikipedia articles in need of updating from October 2019","Wikipedia articles needing factual verification from August 2016","Wikipedia medicine articles ready to translate","World Health Organization essential medicines"],"12":["Articles with short description","Audiology","Diagnostic neurology","Ear procedures","Otology","Short description is different from Wikidata"],"13":["All Wikipedia articles in need of updating","All articles with minor POV problems","All articles with specifically marked weasel-worded phrases","All articles with unsourced statements","Articles with GND identifiers","Articles with J9U identifiers","Articles with LCCN identifiers","Articles with minor POV problems from November 2019","Articles with short description","Articles with specifically marked weasel-worded phrases from November 2019","Articles with unsourced statements from April 2017","Articles with unsourced statements from December 2020","Articles with unsourced statements from December 2021","Articles with unsourced statements from July 2021","Audiology","CS1 errors: missing periodical","CS1 maint: location missing publisher","CS1 maint: multiple names: authors list","Communication disorders","Hearing","Learning disabilities","Neurological disorders","Oral communication","Phonology","Psychoacoustics","Short description is different from Wikidata","Special education","Speech processing","Syndromes","Wikipedia articles in need of updating from February 2024","Wikipedia articles needing clarification from November 2019"],"14":["All articles with unsourced statements","Articles tagged with the inline citation overkill template from June 2022","Articles using infobox templates with no data rows","Articles with short description","Articles with unsourced statements from April 2023","Articles with unsourced statements from June 2022","Audiology","Auditory system","CS1: long volume value","Citation overkill","Commons category link from Wikidata","Hearing","Neurology","Short description matches Wikidata","Wikipedia articles needing clarification from April 2016"],"15":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from October 2020","Articles with short description","Articles with unsourced statements from March 2024","Articles with unsourced statements from September 2022","Diseases of inner ear","Short description is different from Wikidata","Use dmy dates from April 2024","Wikipedia medicine articles ready to translate","Wikipedia neurology articles ready to translate"],"16":["All articles with unsourced statements","Articles with hAudio microformats","Articles with short description","Articles with unsourced statements from August 2019","Articles with unsourced statements from December 2021","Articles with unsourced statements from September 2018","CS1 maint: location missing publisher","Diseases of the ear and mastoid process","Neurological disorders","Short description is different from Wikidata","Spoken articles"],"17":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from October 2020","Articles with short description","Articles with unsourced statements from March 2024","Articles with unsourced statements from September 2022","Diseases of inner ear","Short description is different from Wikidata","Use dmy dates from April 2024","Wikipedia medicine articles ready to translate","Wikipedia neurology articles ready to translate"],"18":["All articles with unsourced statements","Articles with short description","Articles with specifically marked weasel-worded phrases from August 2014","Articles with unsourced statements from August 2014","Biomechanics","Physical fitness","Short description is different from Wikidata","Wikipedia articles needing clarification from August 2014"],"19":["All articles needing additional references","All articles with dead external links","All articles with unsourced statements","Articles needing additional references from November 2022","Articles with dead external links from October 2023","Articles with permanently dead external links","Articles with short description","Articles with unsourced statements from July 2015","Articles with unsourced statements from June 2023","Articles with unsourced statements from October 2023","Articles with unsourced statements from September 2010","Articles with unsourced statements from September 2023","Beta blockers","British inventions","Scottish inventions","Short description is different from Wikidata","Use mdy dates from November 2016","Wikipedia articles needing page number citations from June 2023"],"20":["2-Pyridyl compounds","All articles needing additional references","All articles with unsourced statements","Amines","Articles needing additional references from September 2016","Articles with short description","Articles with unsourced statements from June 2022","Articles with unsourced statements from October 2012","Articles with unsourced statements from October 2018","Articles with unsourced statements from September 2018","Drugboxes which contain changes to watched fields","Drugs with non-standard legal status","ECHA InfoCard ID from Wikidata","H3 receptor antagonists","Histamine agonists","Short description matches Wikidata","Vasodilators"],"21":["Articles with short description","Diseases of middle ear and mastoid","Short description matches Wikidata"],"22":["All articles needing additional references","Articles needing additional medical references from June 2022","Articles requiring reliable medical sources","Articles with short description","Disorders of ocular muscles, binocular movement, accommodation and refraction","Pages using multiple image with auto scaled images","Short description is different from Wikidata"],"23":["All articles lacking reliable references","All articles with unsourced statements","Articles lacking reliable references from July 2014","Articles with Curlie links","Articles with short description","Articles with unsourced statements from December 2020","Articles with unsourced statements from June 2022","Brain tumor","Commons category link is on Wikidata","Disorders causing seizures","Short description is different from Wikidata","Short description matches Wikidata","Use dmy dates from December 2022","Wikipedia medicine articles ready to translate"],"24":["Articles with short description","Illusions","Motor control","Short description is different from Wikidata"],"25":["All articles lacking reliable references","All articles with unsourced statements","Articles lacking reliable references from July 2014","Articles with Curlie links","Articles with short description","Articles with unsourced statements from December 2020","Articles with unsourced statements from June 2022","Brain tumor","Commons category link is on Wikidata","Disorders causing seizures","Short description is different from Wikidata","Short description matches Wikidata","Use dmy dates from December 2022","Wikipedia medicine articles ready to translate"],"26":["All articles needing additional references","All articles to be expanded","Articles needing additional references from January 2013","Articles to be expanded from April 2014","Articles to be expanded from October 2016","Articles with short description","Brainstem","Commons category link is on Wikidata","Neurophysiology","Pages with missing ISBNs","Short description matches Wikidata"],"27":["1972 introductions","All articles with dead external links","All articles with vague or ambiguous time","Articles containing Ancient Greek (to 1453)-language text","Articles containing video clips","Articles with dead external links from November 2023","Articles with permanently dead external links","Articles with short description","Commons category link is on Wikidata","Diagnostic medical imaging","Good articles","Medical tests","Multidimensional signal processing","Radiology","Short description matches Wikidata","Vague or ambiguous time from December 2023","Webarchive template wayback links","Wikipedia articles needing clarification from December 2023","X-ray computed tomography"],"28":["All articles with dead external links","All articles with unsourced statements","Antacids","Articles containing unverified chemical infoboxes","Articles with changed EBI identifier","Articles with dead external links from June 2019","Articles with permanently dead external links","Articles with short description","Articles with unsourced statements from June 2015","Articles with unsourced statements from September 2023","CS1 errors: periodical ignored","CS1 maint: archived copy as title","CS1 maint: multiple names: authors list","Calcium compounds","Carbonates","Chemical articles with multiple compound IDs","E-number additives","ECHA InfoCard ID from Wikidata","E number from Wikidata","Excipients","Food stabilizers","Limestone","Multiple chemicals in an infobox that need indexing","Phosphate binders","PubChem ID (CID) same as Wikidata","Short description matches Wikidata","Use dmy dates from February 2015","Webarchive template wayback links","Wikipedia articles needing clarification from May 2024"],"29":["Articles with short description","Ear procedures","Medical mnemonics","Physical examination","Short description matches Wikidata"],"30":["Accidents","Articles with short description","CS1: long volume value","CS1 maint: unfit URL","Carbon monoxide","Good articles","Industrial hygiene","Medical emergencies","Natural gas safety","Short description is different from Wikidata","Suicide by poison","Toxic effects of substances chiefly nonmedicinal as to source","Wikipedia articles needing page number citations from March 2024","Wikipedia emergency medicine articles ready to translate","Wikipedia medicine articles ready to translate"],"31":["Articles with short description","CS1 errors: missing periodical","CS1 maint: multiple names: authors list","Central nervous system","Commons category link from Wikidata","Neuroscience","Pages using multiple image with manual scaled images","Short description is different from Wikidata","Use dmy dates from April 2017","Webarchive template wayback links"],"32":["All articles with unsourced statements","All stub articles","Articles containing Latin-language text","Articles with TA98 identifiers","Articles with short description","Articles with unsourced statements from August 2023","CS1 maint: location missing publisher","Neuroanatomy","Neuroanatomy stubs","Short description is different from Wikidata"],"33":["All articles with dead external links","All articles with unsourced statements","Articles with dead external links from November 2023","Articles with permanently dead external links","Articles with short description","Articles with unsourced statements from September 2017","Bones of the head and neck","Bones of the thorax","Bones of the vertebral column","Commons category link is on Wikidata","Short description is different from Wikidata","Webarchive template wayback links","Wikipedia articles incorporating text from the 20th edition of Gray's Anatomy (1918)"],"34":["All articles needing additional references","All articles with incomplete citations","Articles needing additional references from August 2014","Articles with incomplete citations from April 2023","Articles with incomplete citations from July 2015","Articles with short description","CS1 French-language sources (fr)","Commons category link is on Wikidata","Short description is different from Wikidata","Skeletal disorders","Vertebral column disorders"],"35":["Articles with short description","CS1 Spanish-language sources (es)","CS1 maint: location","CS1 maint: others","Cerebellum","Commons category link is on Wikidata","Externally peer reviewed articles","Featured articles","Motor system","Pages using multiple image with manual scaled images","Short description is different from Wikidata","Wikipedia articles incorporating text from open access publications","Wikipedia articles published in WikiJournal of Medicine","Wikipedia articles published in peer-reviewed literature","Wikipedia articles published in peer-reviewed literature (W2J)"],"36":["Articles containing video clips","Articles with hAudio microformats","Articles with short description","Cell signaling","Neural synapse","Short description matches Wikidata","Signal transduction","Spoken articles"],"37":["All articles with incomplete citations","All articles with specifically marked weasel-worded phrases","All articles with unsourced statements","All pages needing factual verification","Articles with incomplete citations from July 2015","Articles with short description","Articles with specifically marked weasel-worded phrases from July 2015","Articles with unsourced statements from August 2024","Articles with unsourced statements from December 2020","Articles with unsourced statements from February 2020","Articles with unsourced statements from July 2015","Articles with unsourced statements from June 2015","CS1 French-language sources (fr)","CS1 Spanish-language sources (es)","Congenital disorders of nervous system","Diseases named after discoverers","Short description is different from Wikidata","Use mdy dates from December 2018","Wikipedia articles needing factual verification from December 2018","Wikipedia articles needing page number citations from July 2015"],"38":["All articles needing additional references","Articles needing additional references from November 2015","Articles with short description","Audiology","Commons category link from Wikidata","Diseases of middle ear and mastoid","Ear","Otology","Otorhinolaryngology","Short description is different from Wikidata","Webarchive template wayback links"],"39":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from December 2021","CS1 maint: numeric names: authors list","Neurological disorders","Short description is different from Wikidata","Symptoms"],"40":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from July 2014","Articles with NKC identifiers","Articles with short description","Articles with unsourced statements from June 2022","Diagnostic radiology","Short description is different from Wikidata","X-ray computed tomography"],"41":["Acute upper respiratory infections","Airborne diseases","All articles containing potentially dated statements","Animal viral diseases","Articles containing potentially dated statements from 2010","Articles intentionally citing retracted publications","Articles with Curlie links","Articles with short description","Common cold","Commons category link is on Wikidata","Coronavirus-associated diseases","Enterovirus-associated diseases","Good articles","Inflammations","Short description is different from Wikidata","Use dmy dates from May 2023","Wikipedia emergency medicine articles ready to translate","Wikipedia indefinitely semi-protected pages","Wikipedia medicine articles ready to translate (full)"],"42":["All articles covered by WikiProject Wikify","All articles needing additional references","All articles to be expanded","All articles with empty sections","All pages needing cleanup","Articles covered by WikiProject Wikify from November 2015","Articles needing additional references from June 2013","Articles to be expanded from November 2015","Articles with NDL identifiers","Articles with empty sections from November 2015","Articles with multiple maintenance issues","Articles with short description","Audiology","Hearing loss","Otology","Short description matches Wikidata","Wikipedia introduction cleanup from November 2015"],"43":["1972 introductions","All articles with dead external links","All articles with vague or ambiguous time","Articles containing Ancient Greek (to 1453)-language text","Articles containing video clips","Articles with dead external links from November 2023","Articles with permanently dead external links","Articles with short description","Commons category link is on Wikidata","Diagnostic medical imaging","Good articles","Medical tests","Multidimensional signal processing","Radiology","Short description matches Wikidata","Vague or ambiguous time from December 2023","Webarchive template wayback links","Wikipedia articles needing clarification from December 2023","X-ray computed tomography"],"44":["Articles with short description","Audiology","Complications of stroke","Deafness","Short description is different from Wikidata"],"45":["All articles containing potentially dated statements","All articles with unsourced statements","All articles with vague or ambiguous time","Articles containing potentially dated statements from 2010","Articles with short description","Articles with unsourced statements from July 2024","Articles with unsourced statements from June 2024","Corticosteroids","Endocrinology","Hormones","Short description is different from Wikidata","Steroid hormones","Steroids","Vague or ambiguous time from August 2023","World Anti-Doping Agency prohibited substances"],"46":["Accuracy disputes from January 2023","All Wikipedia articles written in American English","All accuracy disputes","All articles lacking reliable references","All articles with dead external links","All articles with failed verification","All articles with unsourced statements","Articles containing Latin-language text","Articles lacking reliable references from December 2023","Articles with dead external links from January 2024","Articles with dead external links from June 2023","Articles with failed verification from August 2021","Articles with hAudio microformats","Articles with permanently dead external links","Articles with short description","Articles with unsourced statements from December 2023","CS1: long volume value","Cognitive neuroscience","Cognitive psychology","Commons category link from Wikidata","Concepts in epistemology","Concepts in the philosophy of mind","Concepts in the philosophy of science","Consciousness","Emergence","Good articles","Mental processes","Metaphysical properties","Metaphysics of mind","Neuropsychological assessment","Ontology","Pages displaying wikidata descriptions as a fallback via Module:Annotated link","Phenomenology","Short description is different from Wikidata","Spoken articles","Theory of mind","Use American English from July 2023","Webarchive template wayback links"],"47":["All articles with unsourced statements","All pages needing cleanup","Articles needing cleanup from March 2018","Articles with GND identifiers","Articles with sections that need to be turned into prose from March 2018","Articles with short description","Articles with unsourced statements from February 2023","Articles with unsourced statements from May 2022","Commons category link from Wikidata","Deaf culture","Deafblindness","Short description matches Wikidata"],"48":["Articles with short description","Audiology","Broad-concept articles","Deafness","Hearing loss","Short description is different from Wikidata"],"49":["All articles containing potentially dated statements","Articles containing French-language text","Articles containing potentially dated statements from 2023","Articles with short description","CS1 Chinese-language sources (zh)","CS1 maint: bot: original URL status unknown","Economic country classifications","Economic geography","Human geography","Imperialism studies","International development","Lists of countries","Short description is different from Wikidata","Use dmy dates from May 2024","Webarchive template wayback links","Wikipedia pending changes protected pages"],"50":["All articles to be expanded","All articles with dead external links","All articles with unsourced statements","Articles to be expanded from September 2022","Articles with dead external links from December 2018","Articles with permanently dead external links","Articles with short description","Articles with unsourced statements from March 2024","Aviation medicine","CS1: long volume value","CS1 maint: unfit URL","Decompression sickness","Effects of external causes","Good articles","Medical emergencies","Short description is different from Wikidata","Use dmy dates from August 2020","Webarchive template wayback links"],"51":["All articles with unsourced statements","Antiemetics","Articles with changed EBI identifier","Articles with short description","Articles with unsourced statements from May 2022","Articles without InChI source","CS1 Polish-language sources (pl)","Combination drugs","Commons category link from Wikidata","Deliriants","Drugboxes which contain changes to verified fields","Drugboxes which contain changes to watched fields","Drugs that are a combination of chemicals","Drugs with non-standard legal status","ECHA InfoCard ID from Wikidata","H1 receptor antagonists","Motion sickness","Muscarinic antagonists","Short description matches Wikidata","Use dmy dates from February 2022"],"52":["All Wikipedia articles written in American English","All articles with dead external links","All articles with unsourced statements","Articles with changed DrugBank identifier","Articles with dead external links from January 2023","Articles with permanently dead external links","Articles with short description","Articles with unsourced statements from June 2020","COVID-19 drug development","CS1: long volume value","CYP3A4 inducers","Chemical substances for emergency medicine","Commons category link is on Wikidata","Drugboxes which contain changes to verified fields","Drugboxes which contain changes to watched fields","Drugs developed by AbbVie","Drugs developed by Novartis","Drugs with non-standard legal status","ECHA InfoCard ID from Wikidata","Fluorinated corticosteroids","Glucocorticoids","Halohydrins","Organofluorides","Otologicals","Peripherally selective drugs","Pregnane X receptor agonists","Pregnanes","Short description matches Wikidata","Use American English from July 2020","Use dmy dates from December 2023","Wikipedia medicine articles ready to translate","World Health Organization essential medicines"],"53":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from October 2011","Articles with short description","Articles with unsourced statements from August 2007","Articles with unsourced statements from July 2022","Articles with unsourced statements from June 2022","Medical diagnosis","Medical mnemonics","Medical terminology","Short description is different from Wikidata","Use dmy dates from November 2020","Webarchive template wayback links","Wikipedia articles needing clarification from July 2022"],"54":["All articles that may contain original research","Articles that may contain original research from April 2021","Articles with short description","Short description is different from Wikidata","Vision","Visual disturbances and blindness"],"55":["All articles with self-published sources","All articles with unsourced statements","Articles with self-published sources from January 2014","Articles with short description","Articles with unsourced statements from November 2016","Medical databases in the United Kingdom","Medical literature","Official website different in Wikidata and Wikipedia","Short description matches Wikidata","Use dmy dates from April 2022"],"56":["Articles with short description","Neurological disorders","Short description is different from Wikidata","Webarchive template wayback links"],"57":["Articles with short description","Ear procedures","Medical tests","Short description matches Wikidata","Webarchive template wayback links"],"58":["All articles containing potentially dated statements","All articles needing additional references","All articles to be expanded","All articles with dead external links","All articles with unsourced statements","Articles containing potentially dated statements from 2009","Articles containing potentially dated statements from 2012","Articles containing potentially dated statements from 2019","Articles containing potentially dated statements from 2021","Articles containing potentially dated statements from 2023","Articles needing additional references from March 2019","Articles to be expanded from April 2024","Articles to be expanded from June 2021","Articles to be expanded from May 2021","Articles with dead external links from April 2024","Articles with dead external links from June 2021","Articles with permanently dead external links","Articles with short description","Articles with unsourced statements from August 2016","Articles with unsourced statements from January 2023","Articles with unsourced statements from March 2016","Articles with unsourced statements from May 2021","Articles with unsourced statements from September 2012","CS1: long volume value","CS1 German-language sources (de)","CS1 maint: numeric names: authors list","Commons category link is on Wikidata","Decompression equipment","Short description is different from Wikidata","Underwater diving safety equipment","Wikipedia articles needing clarification from June 2021","Wikipedia articles needing clarification from May 2013"],"59":["Articles with short description","Ear procedures","Medical tests","Short description matches Wikidata","Webarchive template wayback links"],"60":["Articles with short description","Neurological disorders","Short description is different from Wikidata","Webarchive template wayback links"],"61":["Academic publishing","All articles with unsourced statements","Articles with short description","Articles with unsourced statements from August 2024","Articles with unsourced statements from September 2024","Electronic documents","Identifiers","Index (publishing)","Official website different in Wikidata and Wikipedia","Short description matches Wikidata","Use Oxford spelling from August 2022","Use dmy dates from December 2019"],"62":["All articles with unsourced statements","All pages needing factual verification","American medical websites","Articles with short description","Articles with unsourced statements from October 2012","Health care companies established in 1996","Medical databases","Online databases","Short description matches Wikidata","Wikipedia articles needing factual verification from October 2012"],"63":["All articles needing additional references","Articles needing additional medical references from April 2019","Articles requiring reliable medical sources","Articles with short description","Otology","Otorhinolaryngology","Pages displaying wikidata descriptions as a fallback via Module:Annotated link","Short description is different from Wikidata"],"64":["Articles with short description","Diseases of the ear and mastoid process","Otology","Otorhinolaryngology","Short description is different from Wikidata","Short description matches Wikidata"],"65":["Articles containing unverified chemical infoboxes","Articles with short description","Articles without EBI source","Biology of attention deficit hyperactivity disorder","Cardiac stimulants","Catecholamines","Commons category link from Wikidata","Dopamine","Dopamine agonists","Drugs missing an ATC code","Drugs that are a physiological drug","Drugs with no legal status","ECHA InfoCard ID from Wikidata","Good articles","Happy hormones","Hormones of the hypothalamic-pituitary-prolactin axis","Hormones of the hypothalamus","Human female endocrine system","Inotropic agents","Monoaminergic neurotoxins","Motivation","Neurotransmitters","Norepinephrine-dopamine releasing agents","Peripherally selective drugs","Phenethylamines","Short description matches Wikidata","TAAR1 agonists","Use dmy dates from November 2018"],"66":["1945 births","2004 deaths","American medical researchers","Articles with short description","CS1: long volume value","CS1 maint: unfit URL","Decompression researchers","Duke University faculty","Georgetown University School of Medicine alumni","Military personnel from New Jersey","Pages containing links to subscription-only content","Pages using gadget WikiMiniAtlas","People from Jersey City, New Jersey","People from Sayreville, New Jersey","Recipients of the Legion of Merit","Rensselaer Polytechnic Institute alumni","Sayreville War Memorial High School alumni","Short description matches Wikidata","United States Navy officers"],"67":["Articles with NKC identifiers","Ear procedures","Electrodiagnosis"],"68":["Articles with short description","Audiology","CS1 maint: multiple names: authors list","Ear procedures","Medical tests","Otology","Short description is different from Wikidata"],"69":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from September 2008","Body fluids","Ear","Human head and neck","Short description matches Wikidata"],"70":["All articles needing additional references","Articles needing additional medical references from January 2022","Articles requiring reliable medical sources","Articles with short description","CS1 maint: location missing publisher","CS1 maint: others","Cardiac electrophysiology","Cardiac procedures","Catheters","Electrodiagnosis","Electrophysiology","Short description is different from Wikidata"],"71":["All articles containing potentially dated statements","Articles containing Ancient Greek (to 1453)-language text","Articles containing French-language text","Articles containing Latin-language text","Articles containing potentially dated statements from 2011","Articles containing potentially dated statements from 2012","Articles containing potentially dated statements from 2016","Articles containing potentially dated statements from 2021","Articles containing video clips","Articles with Curlie links","Articles with imported Creative Commons Attribution-ShareAlike 4.0 text","Articles with short description","Commons category link is on Wikidata","Disorders causing seizures","Epilepsy","Good articles","Neurological disorders in children","Pages using div col with small parameter","Short description is different from Wikidata","Use dmy dates from February 2023","Wikipedia medicine articles ready to translate (full)","Wikipedia neurology articles ready to translate"],"72":["All Wikipedia articles in need of updating","All Wikipedia articles written in American English","Articles lacking ISBNs","Articles with hAudio microformats","Articles with short description","Ear procedures","Short description is different from Wikidata","Spoken articles","Use American English from March 2021","Use mdy dates from May 2012","Wikipedia articles in need of updating from December 2018"],"73":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from August 2024","Articles with multiple maintenance issues","Articles with short description","Articles with unsourced statements from May 2019","Motor control","Pages with missing ISBNs","Sensory systems","Short description is different from Wikidata","Use mdy dates from August 2024","Vestibular system","Wikipedia articles needing clarification from July 2022","Wikipedia articles needing page number citations from August 2022"],"74":["Articles with short description","Diseases of middle ear and mastoid","Short description is different from Wikidata","Short description matches Wikidata"],"75":["Alcohol chemistry","Alcohol solvents","Alkanols","Anatomical preservation","Articles containing unverified chemical infoboxes","Articles with short description","CS1 Brazilian Portuguese-language sources (pt-br)","CS1 German-language sources (de)","CS1 Marathi-language sources (mr)","CS1 errors: periodical ignored","CS1 maint: location missing publisher","CS1 uses Marathi-language script (mr)","Chembox having GHS data","Chemical articles having a data page","Commodity chemicals","Commons link from Wikidata","Disinfectants","ECHA InfoCard ID from Wikidata","Ethanol","Hepatotoxins","Household chemicals","Human metabolites","IARC Group 1 carcinogens","Oxygenates","Pages containing links to subscription-only content","Pages using multiple image with manual scaled images","Primary alcohols","Rocket fuels","Short description is different from Wikidata","Short description matches Wikidata","Teratogens","Use dmy dates from December 2017","Webarchive template wayback links","Wikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica with Wikisource reference","Wikipedia articles needing page number citations from February 2014","Wikipedia indefinitely move-protected pages"],"76":["All articles needing additional references","All articles with unsourced statements","Articles containing unverified chemical infoboxes","Articles needing additional medical references from June 2015","Articles requiring reliable medical sources","Articles with short description","Articles with unsourced statements from September 2016","Biology of obsessive\u2013compulsive disorder","Chembox image size set","Commons category link is on Wikidata","ECHA InfoCard ID from Wikidata","GABA","GABAA receptor positive allosteric modulators","GABA analogues","GABA receptor agonists","Gamma-Amino acids","Glycine receptor agonists","Inhibitory amino acids","Non-proteinogenic amino acids","Peripherally selective drugs","Short description is different from Wikidata","Short description matches Wikidata","Wikipedia articles needing page number citations from May 2013"],"77":["Articles with short description","CS1 maint: multiple names: authors list","Short description is different from Wikidata","Situational phobias"],"78":["All articles with unsourced statements","Aminoglycoside antibiotics","Articles with changed DrugBank identifier","Articles with short description","Articles with unsourced statements from March 2023","Articles with unsourced statements from September 2015","Drugboxes which contain changes to verified fields","Drugs with non-standard legal status","ECHA InfoCard ID from Wikidata","Micromonosporaceae","Nephrotoxins","Otologicals","PubChem ID (CID) different from Wikidata","Short description is different from Wikidata","Toxicology","Use dmy dates from February 2020","Wikipedia medicine articles ready to translate","World Health Organization essential medicines"],"79":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from August 2021","Diseases of middle ear and mastoid","Medical triads","Short description is different from Wikidata","Syndromes","Syndromes affecting the nervous system"],"80":["Articles with short description","Climbing and health","Height","Short description is different from Wikidata"],"81":["All articles needing additional references","Articles needing additional medical references from May 2019","Articles requiring reliable medical sources","Articles with excerpts","Articles with short description","Audiology","Auditory system","Commons category link from Wikidata","Hearing","Short description matches Wikidata","Sound","Webarchive template wayback links","Wikipedia articles needing clarification from August 2024","Wikipedia pages move-protected due to vandalism"],"82":["All articles with dead external links","Articles with Curlie links","Articles with dead external links from May 2024","Articles with short description","Audiology","CS1: long volume value","CS1 maint: unfit URL","Commons category link is on Wikidata","Commons link is locally defined","Communication disorders","Deafness","Hearing loss","Occupational safety and health","Otorhinolaryngology","Pages with missing ISBNs","Short description is different from Wikidata","Wikipedia medicine articles ready to translate","Wikipedia neurology articles ready to translate"],"83":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from July 2022","CS1 errors: missing periodical","Commons link is locally defined","Ear procedures","Hearing","Physical examination","Short description matches Wikidata"],"84":["All articles containing potentially dated statements","All articles with dead external links","All articles with unsourced statements","Amines","Articles containing potentially dated statements from 2015","Articles containing unverified chemical infoboxes","Articles to be expanded from October 2023","Articles with dead external links from January 2020","Articles with dead external links from March 2024","Articles with imported Creative Commons Attribution 4.0 text","Articles with permanently dead external links","Articles with short description","Articles with unsourced statements from July 2019","Articles with unsourced statements from March 2024","Articles with unsourced statements from September 2023","Biogenic amines","Carbonic anhydrase activators","ECHA InfoCard ID from Wikidata","Histamine","Imidazoles","Immune system","Immunostimulants","Neurotransmitters","Short description is different from Wikidata","Short description matches Wikidata","TAAR1 agonists","Vasodilators"],"85":["All articles needing additional references","All articles with unsourced statements","All pages needing factual verification","Articles needing additional medical references from June 2022","Articles needing additional references from April 2024","Articles requiring reliable medical sources","Articles with Curlie links","Articles with short description","Articles with unsourced statements from August 2020","Articles with unsourced statements from January 2017","Articles with unsourced statements from June 2016","Articles with unsourced statements from March 2022","Articles with unsourced statements from October 2012","Articles with unsourced statements from September 2021","CS1 German-language sources (de)","CS1 maint: DOI inactive as of March 2024","CS1 maint: bot: original URL status unknown","Causes of death","Commons category link is on Wikidata","Pages containing links to subscription-only content","Pages using div col with small parameter","Short description is different from Wikidata","Short description matches Wikidata","Stroke","Webarchive template wayback links","Wikipedia articles needing clarification from September 2021","Wikipedia articles needing factual verification from September 2022","Wikipedia emergency medicine articles ready to translate","Wikipedia indefinitely semi-protected pages","Wikipedia medicine articles ready to translate"],"86":["All articles needing additional references","All articles with unsourced statements","Anatomical pathology","Articles containing Ancient Greek (to 1453)-language text","Articles needing additional references from June 2021","Articles with short description","Articles with unsourced statements from February 2017","Commons category link from Wikidata","Histopathology","Pathology","Short description is different from Wikidata"],"87":["All articles needing rewrite","Articles with changed EBI identifier","Articles with short description","CS1 Czech-language sources (cs)","CS1 German-language sources (de)","CS1 Spanish-language sources (es)","CS1 maint: unfit URL","Carboxylate esters","Chemicals using indexlabels","Commons category link is on Wikidata","Deliriants","Drugboxes which contain changes to verified fields","Drugboxes which contain changes to watched fields","ECHA InfoCard ID from Wikidata","Entheogens","Epoxides","Experimental antidepressants","Motion sickness","Multiple chemicals in Infobox drug","Muscarinic antagonists","Ophthalmology drugs","Plant toxins","Short description is different from Wikidata","Tropane alkaloids","Tropane alkaloids found in Solanaceae","Use dmy dates from June 2024","Wikipedia articles needing clarification from June 2024","Wikipedia articles needing rewrite from August 2024","Wikipedia medicine articles ready to translate","World Health Organization essential medicines"],"88":["Articles with short description","Audiology","CS1: long volume value","CS1 Dutch-language sources (nl)","Diseases of the ear and mastoid process","Otology","Psychoacoustics","Rare diseases","Short description is different from Wikidata"],"89":["Anxiety disorders","Articles with short description","CS1 French-language sources (fr)","Respiration","Short description is different from Wikidata","Short description matches Wikidata","Syndromes"],"90":["All articles needing additional references","All articles to be expanded","All articles with unsourced statements","Articles needing additional references from December 2009","Articles to be expanded from July 2023","Articles with short description","Articles with unsourced statements from January 2024","Articles with unsourced statements from September 2022","Articles with unsourced statements from September 2024","CS1: long volume value","CS1 maint: bot: original URL status unknown","CS1 maint: unfit URL","Critical emergency medicine","Hyperbaric medicine","Medical treatments","Respiratory therapy","Short description is different from Wikidata","Underwater diving medicine","Wikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica with Wikisource reference","Wikipedia articles needing clarification from February 2015","Wikipedia articles needing clarification from March 2016","Wikipedia articles needing page number citations from May 2013"],"91":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from November 2023","CS1 Czech-language sources (cs)","CS1 Russian-language sources (ru)","CS1 errors: missing periodical","International Classification of Diseases","Short description is different from Wikidata","Webarchive template wayback links"],"92":["All articles containing potentially dated statements","Articles containing potentially dated statements from December 2016","Articles with short description","CS1 French-language sources (fr)","CS1 German-language sources (de)","Checksum algorithms","Computer-related introductions in 1976","ISO standards","Library science","Serial numbers","Short description is different from Wikidata","Unique identifiers","Use dmy dates from August 2017","Webarchive template wayback links"],"93":["Articles with short description","Commons category link from Wikidata","Human physiology","Hypnosis","Pages displaying short descriptions of redirect targets via Module:Annotated link","Pages displaying wikidata descriptions as a fallback via Module:Annotated link","Parapsychology","Phenomena","Short description is different from Wikidata"],"94":["All Wikipedia neutral point of view disputes","All articles containing potentially dated statements","All articles lacking reliable references","All articles that may contain original research","Articles containing French-language text","Articles containing German-language text","Articles containing Maltese-language text","Articles containing potentially dated statements from 2011","Articles lacking reliable references from May 2024","Articles that may contain original research from May 2019","Articles with example C code","Articles with multiple maintenance issues","Articles with short description","British inventions","CS1 Italian-language sources (it)","CS1 Maltese-language sources (mt)","CS1 Portuguese-language sources (pt)","Computer-related introductions in 1970","International Standard Book Number","Irish inventions","Pages using Sister project links with hidden wikidata","Short description is different from Wikidata","Use Oxford spelling from August 2022","Use dmy dates from July 2021","Webarchive template wayback links","Wikipedia neutral point of view disputes from May 2024","Wikipedia pages semi-protected against vandalism"],"95":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from December 2021","Illusions","Motor control","Short description is different from Wikidata"],"96":["All articles needing additional references","Articles needing additional references from August 2011","Articles needing additional references from May 2013","Articles with short description","CS1 maint: location missing publisher","CS1 maint: others","Causes of death","Commons category link from Wikidata","Gross pathology","Short description is different from Wikidata"],"97":["Articles with short description","Audiology","Auditory system","CS1: long volume value","CS1 maint: unfit URL","Commons category link from Wikidata","Inner ear","Otology","Otorhinolaryngology","Short description matches Wikidata"],"98":["All articles needing additional references","Angiology","Articles needing additional references from September 2020","Articles with short description","Commons category link from Wikidata","Ischemia","Pages using div col with small parameter","Short description is different from Wikidata"],"99":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from July 2017","Articles with short description","Articles with unsourced statements from February 2016","Articles with unsourced statements from March 2023","Classification of mental disorders","Data coding framework","Diagnosis codes","International Classification of Diseases","Official website different in Wikidata and Wikipedia","Short description is different from Wikidata","Statistical data coding","Webarchive template wayback links","World Health Organization"],"100":["All articles with unsourced statements","Anesthesia","Articles with short description","Articles with unsourced statements from February 2022","CS1: long volume value","CS1 maint: unfit URL","Short description is different from Wikidata","Underwater diving disorders","Webarchive template wayback links"],"101":["Airborne diseases","All articles containing potentially dated statements","Animal viral diseases","Articles containing Italian-language text","Articles containing Latin-language text","Articles containing potentially dated statements from 2018","Articles with imported Creative Commons Attribution 4.0 text","Articles with short description","Featured articles","Healthcare-associated infections","Influenza","Short description is different from Wikidata","Use dmy dates from July 2024","Vaccine-preventable diseases","Wikipedia emergency medicine articles ready to translate","Wikipedia indefinitely move-protected pages","Wikipedia indefinitely semi-protected pages","Wikipedia medicine articles ready to translate","Zoonoses"],"102":["1958 births","20th-century Scottish medical doctors","20th-century Scottish women medical doctors","21st-century Scottish medical doctors","21st-century Scottish women medical doctors","Academics of the University of Edinburgh","Alumni of the University of Edinburgh","Articles with BIBSYS identifiers","Articles with BNF identifiers","Articles with BNFdata identifiers","Articles with CINII identifiers","Articles with DBLP identifiers","Articles with GND identifiers","Articles with ISNI identifiers","Articles with LCCN identifiers","Articles with NKC identifiers","Articles with NTA identifiers","Articles with ORCID identifiers","Articles with PLWABN identifiers","Articles with VIAF identifiers","Articles with WorldCat Entities identifiers","Articles with hCards","Articles with short description","British radiologists","CS1 errors: missing periodical","CS1 errors: periodical ignored","CS1 maint: multiple names: authors list","Commanders of the Order of the British Empire","EngvarB from November 2017","Fellows of the Academy of Medical Sciences (United Kingdom)","Fellows of the Royal College of Physicians","Fellows of the Royal College of Radiologists","Fellows of the Royal Society of Edinburgh","Living people","Medical doctors from Glasgow","Pages containing London Gazette template with parameter supp set to y","Short description is different from Wikidata","Use dmy dates from November 2017","Women radiologists"],"103":["Articles with short description","CS1 maint: unfit URL","Diseases of inner ear","Short description matches Wikidata"],"104":["Articles with J9U identifiers","Articles with LCCN identifiers","Articles with LNB identifiers","Articles with short description","Ear","Short description matches Wikidata","Wikipedia articles incorporating text from the 20th edition of Gray's Anatomy (1918)"],"105":["All accuracy disputes","All articles needing additional references","Articles needing additional references from May 2008","Articles with Curlie links","Articles with disputed statements from November 2023","Articles with short description","CS1: long volume value","CS1 maint: unfit URL","Diseases of inner ear","Inflammations","Short description is different from Wikidata"],"106":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from December 2020","Articles with unsourced statements from March 2024","Short description is different from Wikidata","Syndromes affecting the nervous system","Types of stroke"],"107":["Cranial nerve nuclei","Wikipedia articles incorporating text from the 20th edition of Gray's Anatomy (1918)"],"108":["Articles with short description","International Classification of Diseases","Medical lists","Short description is different from Wikidata","Webarchive template wayback links"],"109":["Anatomical pathology","Articles containing Latin-language text","Articles with short description","CS1 maint: url-status","Lesions","Medical signs","Short description is different from Wikidata"],"110":["1973 introductions","20th-century inventions","All accuracy disputes","All articles lacking reliable references","American inventions","Articles containing video clips","Articles lacking reliable references from November 2021","Articles lacking reliable references from September 2018","Articles with disputed statements from November 2021","Articles with excerpts","Articles with hAudio microformats","Articles with short description","Biomagnetics","CS1 Dutch-language sources (nl)","CS1 maint: multiple names: authors list","Commons category link is on Wikidata","Cryogenics","Discovery and invention controversies","Good articles","Magnetic resonance imaging","Radiology","Short description is different from Wikidata","Wikipedia articles needing clarification from March 2022","Wikipedia articles needing page number citations from July 2013"],"111":["All accuracy disputes","All articles needing additional references","Articles containing Ancient Greek (to 1453)-language text","Articles containing Dutch-language text","Articles containing French-language text","Articles containing German-language text","Articles containing Greek-language text","Articles containing Italian-language text","Articles containing Latin-language text","Articles containing Medieval Latin-language text","Articles needing additional references from May 2020","Articles with Curlie links","Articles with disputed statements from January 2017","Articles with short description","CS1 Latin-language sources (la)","CS1 Spanish-language sources (es)","Commons link is on Wikidata","Forms of Latin","Fusional languages","Language articles with unreferenced extinction date","Languages attested from the 7th century BC","Languages of Andorra","Languages of France","Languages of Italy","Languages of Portugal","Languages of Romania","Languages of Spain","Languages of Vatican City","Languages with ISO 639-1 code","Languages with ISO 639-2 code","Latin language","Pages with Latin IPA","Pages with plain IPA","Short description matches Wikidata","Subject\u2013object\u2013verb languages","Use dmy dates from January 2024","Webarchive template wayback links","Wikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica with Wikisource reference","Wikipedia indefinitely move-protected pages"],"112":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from February 2017","Bones of the head and neck","Commons category link is locally defined","Human head and neck","Otology","Otorhinolaryngology","Short description is different from Wikidata","Skull","Wikipedia articles incorporating text from the 20th edition of Gray's Anatomy (1918)"],"113":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from August 2021","Commons category link is on Wikidata","Diseases of middle ear and mastoid","Inflammations","Otology","Otorhinolaryngology","Short description is different from Wikidata","Skeletal disorders"],"114":["3-Tolyl compounds","4-Chlorophenyl compounds","All articles with unsourced statements","Antiemetics","Articles with changed DrugBank identifier","Articles with changed EBI identifier","Articles with short description","Articles with unsourced statements from August 2020","Articles with unsourced statements from September 2015","CS1 German-language sources (de)","Chlorcyclizines","Drugboxes which contain changes to verified fields","Drugboxes which contain changes to watched fields","ECHA InfoCard ID from Wikidata","Motion sickness","Multiple chemicals in Infobox drug","Multiple chemicals in an infobox that need indexing","Pregnane X receptor agonists","Short description matches Wikidata","Use dmy dates from October 2022","Wikipedia medicine articles ready to translate"],"115":["All stub articles","Articles with TA98 identifiers","Cranial nerve nuclei","Neuroanatomy stubs","Wikipedia articles incorporating text from the 20th edition of Gray's Anatomy (1918)"],"116":["All Wikipedia articles written in American English","All articles containing potentially dated statements","Articles containing potentially dated statements from 2022","Articles with short description","Biological databases","CS1 Spanish-language sources (es)","Library cataloging and classification","Medical Subject Headings","Medical classification","Short description is different from Wikidata","Thesauri","United States National Library of Medicine","Use American English from December 2023","Use mdy dates from December 2023"],"117":["All articles lacking reliable references","All articles with unsourced statements","Articles lacking reliable references from May 2023","Articles with short description","Articles with unsourced statements from August 2020","Articles with unsourced statements from October 2014","Lists of salaries","Medical specialties","Short description matches Wikidata","Webarchive template wayback links"],"118":["All articles lacking reliable references","All articles to be expanded","All pages needing factual verification","Articles lacking reliable references from November 2022","Articles to be expanded from November 2022","Articles with short description","Beta blockers","CS1 German-language sources (de)","CS1 maint: overridden setting","CYP2D6 inhibitors","Chemical substances for emergency medicine","Drugboxes which contain changes to watched fields","Drugs developed by AstraZeneca","Drugs developed by Novartis","Drugs with non-standard legal status","ECHA InfoCard ID from Wikidata","Isopropylamino compounds","N-isopropyl-phenoxypropanolamines","Short description matches Wikidata","Use dmy dates from October 2021","Wikipedia articles needing factual verification from November 2022","Wikipedia medicine articles ready to translate","World Health Organization essential medicines"],"119":["Articles with short description","CYP3A4 inducers","Chemical substances for emergency medicine","Diketones","Drugs developed by Pfizer","Drugs with non-standard legal status","ECHA InfoCard ID from Wikidata","Glucocorticoids","Pregnanes","Primary alcohols","Secondary alcohols","Short description is different from Wikidata","Tertiary alcohols","Triols","Use dmy dates from March 2022","Wikipedia medicine articles ready to translate","World Health Organization essential medicines"],"120":["Articles with short description","Aviation medicine","CS1 maint: numeric names: authors list","CS1 maint: unfit URL","Diseases of middle ear and mastoid","Short description is different from Wikidata","Short description matches Wikidata","Underwater diving disorders","Use British English from August 2022","Use dmy dates from August 2022"],"121":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from January 2023","Articles with short description","Articles with unsourced statements from November 2020","Audiology","Auditory system","Commons category link from Wikidata","Ear","Human anatomy","Middle ear","Otology","Otorhinolaryngology","Pages that use a deprecated format of the math tags","Short description is different from Wikidata"],"122":["Ailments of unknown cause","All articles containing potentially dated statements","All articles lacking reliable references","All articles with unsourced statements","Articles containing potentially dated statements from 2016","Articles lacking reliable references from August 2023","Articles with Curlie links","Articles with medical app sidebar","Articles with short description","Articles with unsourced statements from November 2023","CS1 maint: DOI inactive as of August 2024","Good articles","Migraine","Short description is different from Wikidata","Short description matches Wikidata","Use dmy dates from August 2023","Webarchive template wayback links","Wikipedia medicine articles ready to translate (full)","Wikipedia neurology articles ready to translate"],"123":["All accuracy disputes","All articles needing additional references","All articles with unsourced statements","Articles needing additional references from February 2018","Articles with disputed statements from March 2018","Articles with short description","Articles with unsourced statements from August 2017","Articles with unsourced statements from August 2021","Articles with unsourced statements from February 2017","Articles with unsourced statements from July 2021","CS1: long volume value","CS1 maint: unfit URL","Commons category link from Wikidata","Effects of external causes","Motion sickness","Neurological disorders","Short description is different from Wikidata","Vomiting","Wikipedia articles needing clarification from April 2024","Wikipedia medicine articles ready to translate"],"124":["Ailments of unknown cause","All Wikipedia articles in need of updating","All articles containing potentially dated statements","All articles lacking reliable references","All articles needing additional references","All articles with bare URLs for citations","All articles with unsourced statements","Articles containing potentially dated statements from 2017","Articles lacking reliable references from December 2022","Articles lacking reliable references from November 2023","Articles needing additional medical references from July 2022","Articles requiring reliable medical sources","Articles with Curlie links","Articles with bare URLs for citations from August 2024","Articles with short description","Articles with unsourced statements from December 2022","Articles with unsourced statements from July 2022","Articles with unsourced statements from May 2023","CS1 Japanese-language sources (ja)","CS1 maint: overridden setting","Commons category link is on Wikidata","Epstein\u2013Barr virus\u2013associated diseases","Multiple sclerosis","Myelin disorders","Pages using div col with small parameter","Short description is different from Wikidata","Use dmy dates from March 2023","Webarchive template wayback links","Wikipedia articles in need of updating from July 2022","Wikipedia medicine articles ready to translate (full)","Wikipedia neurology articles ready to translate"],"125":["All articles with unsourced statements","Articles containing Ancient Greek (to 1453)-language text","Articles with BNF identifiers","Articles with BNFdata identifiers","Articles with GND identifiers","Articles with J9U identifiers","Articles with LCCN identifiers","Articles with LNB identifiers","Articles with short description","Articles with unsourced statements from June 2022","Short description is different from Wikidata","Short description matches Wikidata","Symptoms and signs: Digestive system and abdomen","Vomiting"],"126":["All articles with unsourced statements","All pages needing cleanup","Articles needing cleanup from December 2022","Articles with short description","Articles with unsourced statements from December 2020","Cleanup tagged articles with a reason field from December 2022","Neurological disorders","Short description is different from Wikidata","Short description matches Wikidata","Wikipedia pages needing cleanup from December 2022"],"127":["Articles with short description","Audiology","CS1 French-language sources (fr)","CS1 maint: DOI inactive as of April 2024","CS1 maint: multiple names: authors list","Commons category link is on Wikidata","Diseases of inner ear","Otology","Otorhinolaryngology","Short description is different from Wikidata","Use dmy dates from December 2014","Webarchive template wayback links","Wikipedia medicine articles ready to translate","Wikipedia neurology articles ready to translate"],"128":["All articles lacking reliable references","All articles to be expanded","All articles with unsourced statements","Articles lacking reliable references from December 2017","Articles to be expanded from August 2015","Articles with short description","Articles with unsourced statements from December 2017","Articles with unsourced statements from December 2022","CS1 errors: periodical ignored","CS1 maint: location missing publisher","Commons category link from Wikidata","Incomplete lists from January 2017","Molecular neuroscience","Neuroscience","Neurotransmitters","Short description is different from Wikidata","Use dmy dates from June 2018"],"129":["All articles needing additional references","Articles needing additional medical references from November 2016","Articles requiring reliable medical sources","Articles with short description","Channelopathies","Deafness","Mitochondrial diseases","Short description is different from Wikidata"],"130":["All articles with unsourced statements","Alpha-adrenergic agonists","Amphetamine","Articles containing unverified chemical infoboxes","Articles with short description","Articles with unsourced statements from June 2017","Beta-adrenergic agonists","Biology of attention deficit hyperactivity disorder","CS1 German-language sources (de)","Catecholamines","Commons category link is on Wikidata","Drugs missing an ATC code","Drugs that are a physiological drug","Drugs with no legal status","ECHA InfoCard ID from Wikidata","Good articles","Hormones","Neurotransmitters","Norepinephrine","Peripherally selective drugs","Phenylethanolamines","Short description is different from Wikidata","Stress hormones","TAAR1 agonists","Use dmy dates from September 2015"],"131":["All articles needing additional references","All articles with unsourced statements","All self-contradictory articles","Articles needing additional references from March 2023","Articles with BNF identifiers","Articles with BNFdata identifiers","Articles with J9U identifiers","Articles with LCCN identifiers","Articles with NDL identifiers","Articles with NKC identifiers","Articles with short description","Articles with unsourced statements from November 2015","CS1 German-language sources (de)","CS1 Swedish-language sources (sv)","Eye diseases","Medical signs","Neurological disorders","Self-contradictory articles from April 2014","Short description is different from Wikidata","Wikipedia articles needing clarification from January 2022","Wikipedia articles needing page number citations from August 2011"],"132":["1967 establishments in Ohio","All Wikipedia articles written in American English","All articles containing potentially dated statements","Articles containing potentially dated statements from 2021","Articles with short description","Bibliographic database providers","Commons category link from Wikidata","Companies based in Dublin, Ohio","Cooperatives based in Ohio","Coordinates not on Wikidata","Library-related organizations","Library automation","Library cataloging and classification","Library centers","OCLC","Organizations established in 1967","Pages using gadget WikiMiniAtlas","Short description is different from Wikidata","Use American English from May 2017","Use mdy dates from September 2017"],"133":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from February 2014","Articles with unsourced statements from February 2019","Articles with unsourced statements from June 2021","Articles with unsourced statements from May 2016","Articles with unsourced statements from November 2023","Bacterium-related cutaneous conditions","CS1: long volume value","CS1 maint: multiple names: authors list","CS1 maint: unfit URL","Diseases of external ear","Otitis","Pediatrics","Short description is different from Wikidata","Sports injuries","Swimming culture","Webarchive template wayback links","Wikipedia medicine articles ready to translate","Wikipedia neurology articles ready to translate"],"134":["Articles with short description","Bones of the head and neck","Commons category link from Wikidata","Ear","Human head and neck","Inner ear","Otology","Otorhinolaryngology","Sensory organs in animals","Short description is different from Wikidata","Vestibular system","Wikipedia articles incorporating text from the 20th edition of Gray's Anatomy (1918)"],"135":["All articles with unsourced statements","Articles citing retracted publications","Articles with short description","Articles with unsourced statements from June 2020","Audiology","CS1 errors: generic name","CS1 maint: DOI inactive as of March 2024","CS1 maint: multiple names: authors list","CS1 maint: numeric names: authors list","Commons category link is on Wikidata","Diseases of middle ear and mastoid","Otitis","Otology","Otorhinolaryngology","Pages using div col with small parameter","Pediatrics","Short description is different from Wikidata","Wikipedia emergency medicine articles ready to translate","Wikipedia medicine articles ready to translate"],"136":["Acoustics","Articles with short description","Ear procedures","Hearing","Short description matches Wikidata","Webarchive template wayback links"],"137":["All articles that are too technical","All articles with unsourced statements","Articles containing Ancient Greek (to 1453)-language text","Articles containing video clips","Articles with short description","Articles with unsourced statements from February 2016","Auditory system","Commons category link is on Wikidata","Fish anatomy","Paleozoology","Short description matches Wikidata","Wikipedia articles that are too technical from March 2021"],"138":["Articles with short description","Diseases of the ear and mastoid process","Mycosis-related cutaneous conditions","Short description is different from Wikidata"],"139":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from April 2010","Articles with short description","Articles with unsourced statements from June 2022","Auditory system","CS1 maint: url-status","Commons category link from Wikidata","Medical specialties","Otorhinolaryngology","Short description is different from Wikidata","Surgical specialties","Use dmy dates from July 2021"],"140":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from June 2024","Articles with short description","Articles with unsourced statements from June 2024","Commons category link is on Wikidata","Ear procedures","Endoscopes","Medical equipment","Pages displaying wikidata descriptions as a fallback via Module:Annotated link","Short description is different from Wikidata"],"141":["All articles lacking in-text citations","All articles to be expanded","All articles with unsourced statements","Articles lacking in-text citations from May 2014","Articles to be expanded from December 2013","Articles with multiple maintenance issues","Articles with short description","Articles with unsourced statements from April 2023","Auditory system","CS1 maint: multiple names: authors list","Commons category link from Wikidata","Ear","Harv and Sfn no-target errors","Otology","Science and technology articles needing translation from Russian Wikipedia","Short description is different from Wikidata","Wikipedia articles incorporating text from the 20th edition of Gray's Anatomy (1918)"],"142":["All articles needing additional references","All articles needing rewrite","All articles that are too technical","All articles with unsourced statements","Articles needing additional references from February 2018","Articles needing additional references from October 2014","Articles with short description","Articles with unsourced statements from December 2020","Audiology","CS1 maint: unfit URL","Diseases of inner ear","Otology","Short description is different from Wikidata","Webarchive template wayback links","Wikipedia articles needing rewrite from February 2018","Wikipedia articles that are too technical from November 2015"],"143":["All Wikipedia articles written in American English","All articles containing potentially dated statements","All articles with unsourced statements","Articles containing potentially dated statements from December 2018","Articles with short description","Articles with unsourced statements from October 2023","Bibliographic databases and indexes","Biological databases","Full-text scholarly online databases","Medical databases","National Institutes of Health","Open-access archives","Short description matches Wikidata","United States National Library of Medicine","Use American English from December 2023","Use mdy dates from December 2023"],"144":["Articles with short description","CS1 Spanish-language sources (es)","CS1 maint: multiple names: authors list","Extrapyramidal and movement disorders","Geriatrics","Parkinson's disease","Short description is different from Wikidata"],"145":["All articles containing potentially dated statements","All articles with unsourced statements","American medical websites","Articles containing potentially dated statements from March 2010","Articles containing potentially dated statements from May 2023","Articles containing potentially dated statements from October 2008","Articles with short description","Articles with unsourced statements from August 2022","Bibliographic databases and indexes","Biological databases","CS1 Spanish-language sources (es)","Databases in the United States","Medical search engines","National Institutes of Health","Online databases","Short description is different from Wikidata","United States National Library of Medicine","Use dmy dates from September 2020"],"146":["All articles with unsourced statements","Articles with BNE identifiers","Articles with BNF identifiers","Articles with BNFdata identifiers","Articles with J9U identifiers","Articles with LCCN identifiers","Articles with LNB identifiers","Articles with NDL identifiers","Articles with NKC identifiers","Articles with short description","Articles with unsourced statements from June 2022","Pathology","Pathophysiology","Physiology","Short description matches Wikidata"],"147":["All articles needing additional references","All articles with unsourced statements","All self-contradictory articles","Articles needing additional references from March 2023","Articles with BNF identifiers","Articles with BNFdata identifiers","Articles with J9U identifiers","Articles with LCCN identifiers","Articles with NDL identifiers","Articles with NKC identifiers","Articles with short description","Articles with unsourced statements from November 2015","CS1 German-language sources (de)","CS1 Swedish-language sources (sv)","Eye diseases","Medical signs","Neurological disorders","Self-contradictory articles from April 2014","Short description is different from Wikidata","Wikipedia articles needing clarification from January 2022","Wikipedia articles needing page number citations from August 2011"],"148":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from March 2015","Diseases of middle ear and mastoid","Injuries of head","Otology","Otorhinolaryngology","Short description is different from Wikidata"],"149":["All pages needing cleanup","Articles needing cleanup from August 2011","Articles with short description","Audiology","Auditory system","Cleanup tagged articles without a reason field from August 2011","Human head and neck","Otology","Short description is different from Wikidata","Wikipedia pages needing cleanup from August 2011"],"150":["1997 establishments in the United Kingdom","All articles needing additional references","Articles needing additional references from October 2018","Articles with short description","British medical websites","Internet properties established in 1997","Medical and health organisations based in the United Kingdom","Patient","Short description is different from Wikidata"],"151":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from March 2009","Noise","Phobias","Psychoacoustics","Short description is different from Wikidata","Sound"],"152":["All articles needing additional references","All articles with unsourced statements","Animal physiology","Articles containing Italian-language text","Articles needing additional references from March 2016","Articles with short description","Articles with unsourced statements from July 2022","Articles with unsourced statements from March 2024","Articles with unsourced statements from October 2016","Articles with unsourced statements from October 2019","Body fluids","CS1 German-language sources (de)","CS1 Italian-language sources (it)","Commons category link from Wikidata","Excretion","Reflexes","Short description is different from Wikidata","Wikipedia articles needing clarification from August 2023"],"153":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from June 2024","Articles with short description","Articles with unsourced statements from June 2024","Commons category link is on Wikidata","Ear procedures","Endoscopes","Medical equipment","Pages displaying wikidata descriptions as a fallback via Module:Annotated link","Short description is different from Wikidata"],"154":["Articles with short description","Auditory system","Intoxication","Short description matches Wikidata","Use dmy dates from April 2016","Vision"],"155":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from December 2022","Commons category link is on Wikidata","Otorhinolaryngology","Short description is different from Wikidata","Skull"],"156":["Diagnostic neurology"],"157":["Articles with hAudio microformats","Articles with short description","Audiology","CS1 maint: multiple names: authors list","Deafness","Geriatrics","Hearing loss","Short description is different from Wikidata","Short description matches Wikidata"],"158":["All articles needing additional references","Articles needing additional references from November 2016","Articles with short description","Short description is different from Wikidata","Symptoms and signs of mental disorders"],"159":["All articles lacking in-text citations","All articles with specifically marked weasel-worded phrases","Articles lacking in-text citations from November 2014","Articles with short description","Articles with specifically marked weasel-worded phrases from September 2016","Audiology","CS1 maint: others","Ear procedures","Hearing","Short description is different from Wikidata","Webarchive template wayback links"],"160":["All articles with unsourced statements","Articles containing video clips","Articles with short description","Articles with unsourced statements from April 2012","Articles with unsourced statements from December 2021","Articles with unsourced statements from May 2022","Articles with unsourced statements from October 2022","CS1 German-language sources (de)","Pages displaying short descriptions of redirect targets via Module:Annotated link","Proprioception","Sensory systems","Short description is different from Wikidata"],"161":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from January 2024","Central nervous system neurons","Cerebellum","Neurons","Pages including recorded pronunciations","Pages using the Phonos extension","Pages with Czech IPA","Short description matches Wikidata","Webarchive template wayback links"],"162":["All articles with dead external links","All articles with vague or ambiguous time","Articles with dead external links from June 2023","Articles with permanently dead external links","Articles with short description","Happiness","Philosophy of life","Positive mental attitude","Quality of life","Short description is different from Wikidata","Simple living","Sociological terminology","Use dmy dates from August 2023","Vague or ambiguous time from October 2022","Webarchive template wayback links"],"163":["All articles needing additional references","Articles needing additional references from May 2017","Articles with short description","Ear procedures","Medical tests","Short description is different from Wikidata"],"164":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from December 2021","CS1 maint: unfit URL","Diagnostic neurology","Ear procedures","Physical examination","Short description matches Wikidata","Webarchive template wayback links"],"165":["All articles containing potentially dated statements","All articles with unsourced statements","Applications of artificial intelligence","Articles containing potentially dated statements from August 2019","Articles containing potentially dated statements from September 2022","Articles with short description","Articles with unsourced statements from March 2023","Bibliographic databases in computer science","Scholarly search services","Short description matches Wikidata"],"166":["All articles that are too technical","All articles with unsourced statements","Anatomical terminology","Articles with short description","Articles with unsourced statements from March 2024","CS1 maint: archived copy as title","Diagnosis codes","Medical classification","Nursing classification","Short description is different from Wikidata","Standards for electronic health records","Webarchive template wayback links","Wikipedia articles that are too technical from September 2022"],"167":["Articles with short description","Ear","Short description is different from Wikidata","Vestibular system"],"168":["Articles with short description","Bones of the head and neck","Commons category link from Wikidata","Ear","Human head and neck","Inner ear","Otology","Otorhinolaryngology","Sensory organs in animals","Short description is different from Wikidata","Vestibular system","Wikipedia articles incorporating text from the 20th edition of Gray's Anatomy (1918)"],"169":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from August 2024","Articles with multiple maintenance issues","Articles with short description","Articles with unsourced statements from May 2019","Motor control","Pages with missing ISBNs","Sensory systems","Short description is different from Wikidata","Use mdy dates from August 2024","Vestibular system","Wikipedia articles needing clarification from July 2022","Wikipedia articles needing page number citations from August 2022"],"170":["All accuracy disputes","All articles with specifically marked weasel-worded phrases","All articles with unsourced statements","All pages needing cleanup","Articles containing predictions or speculation","Articles needing cleanup from January 2018","Articles with disputed statements from November 2015","Articles with sections that need to be turned into prose from January 2018","Articles with short description","Articles with specifically marked weasel-worded phrases from November 2015","Articles with unsourced statements from December 2021","Articles with unsourced statements from February 2022","Articles with unsourced statements from February 2023","Articles with unsourced statements from November 2015","Articles with unsourced statements from October 2022","Audiology","CS1 errors: missing periodical","CS1 errors: periodical ignored","Hearing loss","Otology","Short description is different from Wikidata","Wikipedia articles needing clarification from January 2018"],"171":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from February 2024","Articles with unsourced statements from June 2021","Articles with unsourced statements from November 2023","CS1: long volume value","CS1 Danish-language sources (da)","CS1 Polish-language sources (pl)","Medical signs","Short description is different from Wikidata","Symptoms","Use dmy dates from April 2017","Webarchive template wayback links","Wikipedia articles needing clarification from July 2023"],"172":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from December 2020","CS1 errors: generic name","Communication disorders","Short description is different from Wikidata","Symptoms and signs: Nervous system","Symptoms and signs: Speech and voice","Wikipedia articles needing page number citations from May 2012","Wikipedia indefinitely semi-protected pages"],"173":["All Wikipedia articles needing clarification","All articles lacking reliable references","All articles with failed verification","All articles with incomplete citations","Articles containing unverified chemical infoboxes","Articles lacking reliable references from August 2024","Articles lacking reliable references from October 2017","Articles with failed verification from May 2023","Articles with incomplete citations from October 2017","Articles with short description","Articles without EBI source","Articles without InChI source","Articles without UNII source","Biogenic amines","CS1 Italian-language sources (it)","Chemical pages without DrugBank identifier","Commons category link from Wikidata","Drugs missing an ATC code","Drugs that are a physiological drug","Drugs with no legal status","ECHA InfoCard ID from Wikidata","Happy hormones","Hydroxyarenes","Multiple Chemboxes","Neurotransmitters","Peripherally selective drugs","Serotonin","Serotonin receptor agonists","Serotonin releasing agents","Short description is different from Wikidata","Short description matches Wikidata","TAAR1 agonists","Tryptamine alkaloids","Use dmy dates from March 2016","Wikipedia articles needing clarification from December 2018","Wikipedia articles needing clarification from January 2012","Wikipedia articles needing clarification from March 2014","Wikipedia articles needing clarification from September 2017"],"174":["All articles with specifically marked weasel-worded phrases","All articles with unsourced statements","Articles with short description","Articles with specifically marked weasel-worded phrases from June 2020","Articles with unsourced statements from July 2016","Articles with unsourced statements from June 2022","Articles with unsourced statements from March 2011","Aviation risks","Proprioception","Short description matches Wikidata","Vision"],"175":["Articles with short description","Hearing loss","Short description matches Wikidata","Spatial cognition"],"176":["All articles lacking reliable references","All articles with unsourced statements","Articles lacking reliable references from December 2019","Articles with short description","Articles with unsourced statements from December 2019","Articles with unsourced statements from September 2020","Diseases of the ear and mastoid process","Rare diseases","Short description is different from Wikidata","Traumatology"],"177":["All articles needing additional references","All articles to be expanded","All articles with unsourced statements","Articles needing additional references from December 2009","Articles to be expanded from July 2023","Articles with short description","Articles with unsourced statements from January 2024","Articles with unsourced statements from September 2022","Articles with unsourced statements from September 2024","CS1: long volume value","CS1 maint: bot: original URL status unknown","CS1 maint: unfit URL","Critical emergency medicine","Hyperbaric medicine","Medical treatments","Respiratory therapy","Short description is different from Wikidata","Underwater diving medicine","Wikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica with Wikisource reference","Wikipedia articles needing clarification from February 2015","Wikipedia articles needing clarification from March 2016","Wikipedia articles needing page number citations from May 2013"],"178":["All articles needing additional references","All articles with unsourced statements","All pages needing factual verification","Articles needing additional medical references from June 2022","Articles needing additional references from April 2024","Articles requiring reliable medical sources","Articles with Curlie links","Articles with short description","Articles with unsourced statements from August 2020","Articles with unsourced statements from January 2017","Articles with unsourced statements from June 2016","Articles with unsourced statements from March 2022","Articles with unsourced statements from October 2012","Articles with unsourced statements from September 2021","CS1 German-language sources (de)","CS1 maint: DOI inactive as of March 2024","CS1 maint: bot: original URL status unknown","Causes of death","Commons category link is on Wikidata","Pages containing links to subscription-only content","Pages using div col with small parameter","Short description is different from Wikidata","Short description matches Wikidata","Stroke","Webarchive template wayback links","Wikipedia articles needing clarification from September 2021","Wikipedia articles needing factual verification from September 2022","Wikipedia emergency medicine articles ready to translate","Wikipedia indefinitely semi-protected pages","Wikipedia medicine articles ready to translate"],"179":["All articles with unsourced statements","Articles with unsourced statements from October 2015","Ear procedures","Hearing","Pages containing links to subscription-only content"],"180":["All articles containing potentially dated statements","All articles needing expert attention","All articles that are too technical","All articles with dead external links","All articles with unsourced statements","Articles containing potentially dated statements from 2018","Articles needing expert attention from April 2023","Articles with dead external links from August 2023","Articles with permanently dead external links","Articles with short description","Articles with unsourced statements from December 2020","Audiology","CS1 Swedish-language sources (sv)","Commons category link from Wikidata","Diseases of the ear and mastoid process","Occupational hazards","Psychoacoustics","Short description is different from Wikidata","Symptoms","Use dmy dates from June 2021","Wikipedia articles needing clarification from April 2023","Wikipedia articles needing factual verification from October 2018","Wikipedia articles that are too technical from April 2023","Wikipedia indefinitely semi-protected pages","Wikipedia medicine articles ready to translate","Wikipedia neurology articles ready to translate"],"181":["AMPA receptor antagonists","All articles with unsourced statements","American inventions","Anticonvulsants","Articles with changed DrugBank identifier","Articles with short description","Articles with unsourced statements from April 2016","CS1 Brazilian Portuguese-language sources (pt-br)","CYP3A4 inducers","Carbonic anhydrase inhibitors","Commons category link from Wikidata","Drugboxes which contain changes to verified fields","Drugboxes which contain changes to watched fields","Drugs developed by Johnson & Johnson","Drugs with non-standard legal status","Drugs with unknown mechanisms of action","ECHA InfoCard ID from Wikidata","GABAA receptor positive allosteric modulators","Kainate receptor antagonists","Monosaccharide derivatives","Short description is different from Wikidata","Sodium channel blockers","Sulfamates","Use dmy dates from January 2024","Wikipedia medicine articles ready to translate"],"182":["Articles with short description","Short description is different from Wikidata","Types of stroke"],"183":["All articles lacking in-text citations","All articles with unsourced statements","Articles lacking in-text citations from October 2014","Articles with short description","Articles with unsourced statements from January 2012","Commons category link from Wikidata","Cranial nerves","Human head and neck","Innervation of the face","Medical mnemonics","Nerves of the head and neck","Nervous system","Neurology","Otorhinolaryngology","Short description is different from Wikidata","Trigeminal nerve"],"184":["Articles with NKC identifiers","Articles with short description","Audiology","Ear procedures","Medical tests","Otology","Short description is different from Wikidata"],"185":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from January 2010","Commons category link is on Wikidata","Diseases of middle ear and mastoid","Short description is different from Wikidata"],"186":["Articles with short description","Breathing gases","CS1 maint: unfit URL","Helium","Short description is different from Wikidata","Underwater diving safety equipment"],"187":["All articles with unsourced statements","All stub articles","Articles with short description","Articles with unsourced statements from February 2015","Medical sign stubs","Medical signs","Otology","Physical examination","Short description matches Wikidata"],"188":["Articles with short description","Commons link is the pagename","Ear","Short description matches Wikidata","Vestibular system","Wikipedia articles incorporating text from the 20th edition of Gray's Anatomy (1918)"],"189":["Articles with short description","CS1: long volume value","CS1 French-language sources (fr)","CS1 Hebrew-language sources (he)","CS1 maint: unfit URL","Commons category link is on Wikidata","Featured articles","Pages using multiple image with manual scaled images","Short description is different from Wikidata","Underwater diving","Use British English from August 2017","Use dmy dates from October 2019"],"190":["All articles with unsourced statements","All articles with vague or ambiguous time","Articles with J9U identifiers","Articles with LCCN identifiers","Articles with NKC identifiers","Articles with short description","Articles with unsourced statements from February 2018","Articles with unsourced statements from October 2021","Articles with unsourced statements from September 2021","Ashkenazi Jews topics","Autosomal recessive disorders","Cytoskeletal defects","Diseases named after discoverers","Diseases of the ear and mastoid process","Rare syndromes","Short description is different from Wikidata","Syndromes affecting hearing","Syndromes affecting the eye","Vague or ambiguous time from October 2018"],"191":["All Wikipedia articles in need of updating","All articles to be merged","All articles with dead external links","All articles with failed verification","Anticonvulsants","Antiprogestogens","Aromatase inhibitors","Articles containing Chinese-language text","Articles containing unverified chemical infoboxes","Articles to be merged from August 2024","Articles with dead external links from October 2019","Articles with failed verification from November 2023","Articles with permanently dead external links","Articles with short description","CS1 Brazilian Portuguese-language sources (pt-br)","CS1 French-language sources (fr)","CS1 German-language sources (de)","CS1 Italian-language sources (it)","CS1 maint: DOI inactive as of August 2024","CYP3A4 inhibitors","Carboxylic acids","Drugboxes which contain changes to watched fields","Drugs developed by AbbVie","Drugs missing an ATC code","Drugs with non-standard legal status","ECHA InfoCard ID from Wikidata","Endocrine disruptors","GABA analogues","GABA transaminase inhibitors","Hepatotoxins","Histone deacetylase inhibitors","Infobox drug articles with non-default infobox title","Infobox drug with local INN variant","Mood stabilizers","Nonsteroidal antiandrogens","Short description is different from Wikidata","Teratogens","Use dmy dates from July 2024","Webarchive template wayback links","Wikipedia articles in need of updating from February 2024","Wikipedia medicine articles ready to translate","World Health Organization essential medicines"],"192":["All accuracy disputes","All articles needing additional references","All articles with failed verification","Angiology","Articles needing additional medical references from September 2021","Articles requiring reliable medical sources","Articles with disputed statements from June 2019","Articles with failed verification from June 2019","Articles with short description","Neurosurgery","Short description is different from Wikidata"],"193":["All articles needing additional references","All articles with dead external links","Articles needing additional references from June 2020","Articles with dead external links from April 2022","Articles with short description","Short description is different from Wikidata","Types of stroke","Vascular diseases"],"194":["All article disambiguation pages","All disambiguation pages","Disambiguation pages","Short description is different from Wikidata"],"195":["All pages needing cleanup","Articles needing cleanup from July 2023","Articles with short description","Cleanup tagged articles with a reason field from July 2023","Neurological disorders","Short description is different from Wikidata","Wikipedia pages needing cleanup from July 2023"],"196":["1950s American films","1950s English-language films","1950s mystery thriller films","1950s psychological thriller films","1950s romantic thriller films","1958 films","All Wikipedia articles written in American English","American detective films","American mystery thriller films","American psychological thriller films","American romantic thriller films","Articles with short description","Articles with specifically marked weasel-worded phrases from June 2024","CS1: long volume value","CS1 German-language sources (de)","CS1 maint: numeric names: authors list","Color film noir","Commons category link from Wikidata","Films about suicide","Films about uxoricide","Films based on French novels","Films based on mystery novels","Films based on works by Boileau-Narcejac","Films directed by Alfred Hitchcock","Films produced by Alfred Hitchcock","Films scored by Bernard Herrmann","Films set in San Francisco","Films set in art museums and galleries","Films shot in San Francisco","Films with screenplays by Alec Coppel","Films with screenplays by Samuel A. Taylor","Paramount Pictures films","Rotten Tomatoes ID same as Wikidata","Rotten Tomatoes template using name parameter","Short description matches Wikidata","Social thriller films","Template film date with 1 release date","United States National Film Registry films","Use American English from September 2019","Use mdy dates from December 2023","Wikipedia articles needing page number citations from January 2021"],"197":["Articles with short description","Short description matches Wikidata","Vestibular system","Vestibulocochlear nerve","Wikipedia articles incorporating text from the 20th edition of Gray's Anatomy (1918)"],"198":["All accuracy disputes","All articles needing additional references","Articles needing additional references from May 2008","Articles with Curlie links","Articles with disputed statements from November 2023","Articles with short description","CS1: long volume value","CS1 maint: unfit URL","Diseases of inner ear","Inflammations","Short description is different from Wikidata"],"199":["All articles with unsourced statements","Articles with Curlie links","Articles with NKC identifiers","Articles with short description","Articles with unsourced statements from April 2022","CS1: long volume value","Commons category link from Wikidata","PNS neoplasia","Rare diseases","Short description is different from Wikidata"],"200":["All articles that are too technical","All articles with unsourced statements","Articles with short description","Articles with unsourced statements from March 2017","Articles with unsourced statements from May 2022","Articles with unsourced statements from November 2015","CS1 maint: multiple names: authors list","Nervous system","Sensory systems","Short description is different from Wikidata","Vestibular system","Webarchive template wayback links","Wikipedia articles that are too technical from June 2023"],"201":["Articles containing Latin-language text","Articles with NDL identifiers","Articles with TA98 identifiers","Articles with short description","Ear","Short description matches Wikidata","Wikipedia articles incorporating text from the 20th edition of Gray's Anatomy (1918)"],"202":["Articles with short description","Pages displaying short descriptions of redirect targets via Module:Annotated link","Pages displaying wikidata descriptions as a fallback via Module:Annotated link","Pages using div col with small parameter","Reflexes","Short description matches Wikidata","Vision"],"203":["Acoustics","All Wikipedia articles needing context","All pages needing cleanup","CS1 maint: archived copy as title","Ear procedures","Hearing","Wikipedia articles needing context from November 2014"],"204":["Articles with short description","Pages displaying short descriptions of redirect targets via Module:Annotated link","Pages displaying wikidata descriptions as a fallback via Module:Annotated link","Pages using div col with small parameter","Reflexes","Short description matches Wikidata","Vision"],"205":["All articles containing potentially dated statements","All articles with unsourced statements","Articles containing potentially dated statements from July 2013","Articles with Curlie links","Articles with short description","Articles with unsourced statements from September 2019","Autoimmune diseases","CS1 French-language sources (fr)","CS1 maint: DOI inactive as of August 2024","Commons category link from Wikidata","Disturbances of human pigmentation","Genodermatoses","Short description is different from Wikidata","Use dmy dates from September 2021","Wikipedia medicine articles ready to translate","Wikipedia neurology articles ready to translate"],"206":["All articles with unsourced statements","Articles with short description","Articles with unsourced statements from June 2015","Body fluids","CS1: long volume value","CS1 maint: bot: original URL status unknown","Commons category link from Wikidata","Emetics","Reflexes","Short description is different from Wikidata","Symptoms and signs: Digestive system and abdomen","Vomiting","Webarchive template wayback links","Wikipedia articles incorporating a citation from The American Cyclopaedia","Wikipedia articles incorporating a citation from The American Cyclopaedia with a Wikisource reference","Wikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica with Wikisource reference","Wikipedia articles incorporating text from the 1911 Encyclop\u00e6dia Britannica","Wikipedia indefinitely semi-protected pages"],"207":["All articles needing additional references","Articles needing additional references from February 2019","Articles with short description","Audiology","Ear procedures","Hearing loss","Short description is different from Wikidata"],"208":["Articles with short description","Commons category link is on Wikidata","Deafness","Rare diseases","Short description is different from Wikidata","Syndromes","Wikipedia articles incorporating text from the United States National Library of Medicine"],"221":["Articles with unsourced statements","Automatic category TOC generates Large category TOC","Clean-up categories from April 2022","Hidden categories","Monthly clean-up category (Articles with unsourced statements) counter","Monthly clean-up category counter","Template Large category TOC via Automatic category TOC on category with 5,001\u201310,000 pages","Tracking categories"],"222":["Automatic category TOC generates Large category TOC","CS1 maintenance","Hidden categories","Template Large category TOC via Automatic category TOC on category with over 20,000 pages","Tracking categories"],"224":["Articles with unsourced statements","Automatic category TOC generates Large category TOC","Clean-up categories from January 2021","Hidden categories","Monthly clean-up category (Articles with unsourced statements) counter","Monthly clean-up category counter","Template Large category TOC via Automatic category TOC on category with 5,001\u201310,000 pages","Tracking categories"],"225":["Automatic category TOC generates Large category TOC","CS1 maintenance","Hidden categories","Template Large category TOC via Automatic category TOC on category with over 20,000 pages","Tracking categories"],"226":["Automatic category TOC generates Large category TOC","CS1 maintenance","Hidden categories","Template Large category TOC via Automatic category TOC on category with over 20,000 pages","Tracking categories"],"227":["Automatic category TOC generates Large category TOC","CS1 maintenance","Hidden categories","Template Large category TOC via Automatic category TOC on category with over 20,000 pages","Tracking categories"],"228":["Automatic category TOC generates standard Category TOC","Clean-up categories from March 2019","Hidden categories","Monthly clean-up category (Wikipedia articles needing clarification) counter","Monthly clean-up category counter","Template Category TOC via Automatic category TOC on category with 301\u2013600 pages","Tracking categories","Wikipedia articles needing clarification"]},"topic":{"0":"Vertigo","1":"Vertigo","2":"Vertigo","3":"Vertigo","4":"Vertigo","5":"Vertigo","6":"Vertigo","7":"Vertigo","8":"Vertigo","9":"Vertigo","10":"Vertigo","11":"Vertigo","12":"Vertigo","13":"Vertigo","14":"Vertigo","15":"Vertigo","16":"Vertigo","17":"Vertigo","18":"Vertigo","19":"Vertigo","20":"Vertigo","21":"Vertigo","22":"Vertigo","23":"Vertigo","24":"Vertigo","25":"Vertigo","26":"Vertigo","27":"Vertigo","28":"Vertigo","29":"Vertigo","30":"Vertigo","31":"Vertigo","32":"Vertigo","33":"Vertigo","34":"Vertigo","35":"Vertigo","36":"Vertigo","37":"Vertigo","38":"Vertigo","39":"Vertigo","40":"Vertigo","41":"Vertigo","42":"Vertigo","43":"Vertigo","44":"Vertigo","45":"Vertigo","46":"Vertigo","47":"Vertigo","48":"Vertigo","49":"Vertigo","50":"Vertigo","51":"Vertigo","52":"Vertigo","53":"Vertigo","54":"Vertigo","55":"Vertigo","56":"Vertigo","57":"Vertigo","58":"Vertigo","59":"Vertigo","60":"Vertigo","61":"Vertigo","62":"Vertigo","63":"Vertigo","64":"Vertigo","65":"Vertigo","66":"Vertigo","67":"Vertigo","68":"Vertigo","69":"Vertigo","70":"Vertigo","71":"Vertigo","72":"Vertigo","73":"Vertigo","74":"Vertigo","75":"Vertigo","76":"Vertigo","77":"Vertigo","78":"Vertigo","79":"Vertigo","80":"Vertigo","81":"Vertigo","82":"Vertigo","83":"Vertigo","84":"Vertigo","85":"Vertigo","86":"Vertigo","87":"Vertigo","88":"Vertigo","89":"Vertigo","90":"Vertigo","91":"Vertigo","92":"Vertigo","93":"Vertigo","94":"Vertigo","95":"Vertigo","96":"Vertigo","97":"Vertigo","98":"Vertigo","99":"Vertigo","100":"Vertigo","101":"Vertigo","102":"Vertigo","103":"Vertigo","104":"Vertigo","105":"Vertigo","106":"Vertigo","107":"Vertigo","108":"Vertigo","109":"Vertigo","110":"Vertigo","111":"Vertigo","112":"Vertigo","113":"Vertigo","114":"Vertigo","115":"Vertigo","116":"Vertigo","117":"Vertigo","118":"Vertigo","119":"Vertigo","120":"Vertigo","121":"Vertigo","122":"Vertigo","123":"Vertigo","124":"Vertigo","125":"Vertigo","126":"Vertigo","127":"Vertigo","128":"Vertigo","129":"Vertigo","130":"Vertigo","131":"Vertigo","132":"Vertigo","133":"Vertigo","134":"Vertigo","135":"Vertigo","136":"Vertigo","137":"Vertigo","138":"Vertigo","139":"Vertigo","140":"Vertigo","141":"Vertigo","142":"Vertigo","143":"Vertigo","144":"Vertigo","145":"Vertigo","146":"Vertigo","147":"Vertigo","148":"Vertigo","149":"Vertigo","150":"Vertigo","151":"Vertigo","152":"Vertigo","153":"Vertigo","154":"Vertigo","155":"Vertigo","156":"Vertigo","157":"Vertigo","158":"Vertigo","159":"Vertigo","160":"Vertigo","161":"Vertigo","162":"Vertigo","163":"Vertigo","164":"Vertigo","165":"Vertigo","166":"Vertigo","167":"Vertigo","168":"Vertigo","169":"Vertigo","170":"Vertigo","171":"Vertigo","172":"Vertigo","173":"Vertigo","174":"Vertigo","175":"Vertigo","176":"Vertigo","177":"Vertigo","178":"Vertigo","179":"Vertigo","180":"Vertigo","181":"Vertigo","182":"Vertigo","183":"Vertigo","184":"Vertigo","185":"Vertigo","186":"Vertigo","187":"Vertigo","188":"Vertigo","189":"Vertigo","190":"Vertigo","191":"Vertigo","192":"Vertigo","193":"Vertigo","194":"Vertigo","195":"Vertigo","196":"Vertigo","197":"Vertigo","198":"Vertigo","199":"Vertigo","200":"Vertigo","201":"Vertigo","202":"Vertigo","203":"Vertigo","204":"Vertigo","205":"Vertigo","206":"Vertigo","207":"Vertigo","208":"Vertigo","221":"Vertigo","222":"Vertigo","224":"Vertigo","225":"Vertigo","226":"Vertigo","227":"Vertigo","228":"Vertigo"}}